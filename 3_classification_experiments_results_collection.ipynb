{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 CNN results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from src.Utils import *\n",
    "\n",
    "from src.datasets import load_dataset_by_name\n",
    "\n",
    "from src.plotting import *\n",
    "BATCH_SIZE = 64\n",
    "##############################################################\n",
    "# Directories, files, parameters\n",
    "#############################################################\n",
    "\n",
    "\n",
    "list_datasets = [\"caltech_birds2011\", \"cars196\", \"cassava\", \"cats_vs_dogs\", \"citrus_leaves\", \"deep_weeds\",  \"malaria\",  \"oxford_flowers102\", \"plant_leaves\", \"plant_village\"]\n",
    "results_cnns = {}\n",
    "for dataset_name in list_datasets:\n",
    "    print(\"DATASET: \" + dataset_name)\n",
    "    data_loaded = load_dataset_by_name(dataset_name=dataset_name, batch_size=BATCH_SIZE)\n",
    "\n",
    "    PATH = \"results/results_\" + dataset_name\n",
    "    plots_dir = os.path.join(PATH, \"plots_\" + dataset_name)\n",
    "\n",
    "    ##############################################################\n",
    "    # Obtaining best CNN execution (in training set)\n",
    "    ##############################################################\n",
    "\n",
    "    file_best_exec_id_vgg19, df_results_cnn_vgg19 = get_best_cnn(path=PATH, cnn_filter=\"vgg19\", plots_dir=plots_dir)\n",
    "    file_best_exec_id_inceptionresnetv2, df_results_cnn_inceptionresnetv2 = get_best_cnn(path=PATH, cnn_filter=\"inceptionresnetv2\", plots_dir=plots_dir)\n",
    "    file_best_exec_id_inceptionv3, df_results_cnn_inceptionv3 = get_best_cnn(path=PATH, cnn_filter=\"inceptionv3\", plots_dir=plots_dir)\n",
    "    file_best_exec_id_densenet201, df_results_cnn_densenet201 = get_best_cnn(path=PATH, cnn_filter=\"densenet201\", plots_dir=plots_dir)\n",
    "    file_best_exec_id_xceptionv1, df_results_cnn_xceptionv1 = get_best_cnn(path=PATH, cnn_filter=\"xceptionv1\", plots_dir=plots_dir)\n",
    "    \n",
    "    ##############################################################\n",
    "    # General statistics CNN\n",
    "    ##############################################################\n",
    "\n",
    "    vgg_stats = generate_cnn_statistics(df_results_cnn=df_results_cnn_vgg19, file_best_exec_id=file_best_exec_id_vgg19, path=PATH, plots_dir=plots_dir, cnn_filter=\"vgg19\", show_tables=False)\n",
    "    inception_resnetv2_stats = generate_cnn_statistics(df_results_cnn=df_results_cnn_inceptionresnetv2, file_best_exec_id=file_best_exec_id_inceptionresnetv2, path=PATH, plots_dir=plots_dir, cnn_filter=\"inceptionresnetv2\", show_tables=False)\n",
    "    inceptionv3_stats = generate_cnn_statistics(df_results_cnn=df_results_cnn_inceptionv3, file_best_exec_id=file_best_exec_id_inceptionv3, path=PATH, plots_dir=plots_dir, cnn_filter=\"inceptionv3\", show_tables=False)\n",
    "    densenet201_stats = generate_cnn_statistics(df_results_cnn=df_results_cnn_densenet201, file_best_exec_id=file_best_exec_id_densenet201, path=PATH, plots_dir=plots_dir, cnn_filter=\"densenet201\", show_tables=False)\n",
    "    xceptionv1_stats = generate_cnn_statistics(df_results_cnn=df_results_cnn_xceptionv1, file_best_exec_id=file_best_exec_id_xceptionv1, path=PATH, plots_dir=plots_dir, cnn_filter=\"xceptionv1\", show_tables=False)\n",
    "\n",
    "\n",
    "    results_cnns[dataset_name] = pd.concat([vgg_stats, inception_resnetv2_stats, inceptionv3_stats, densenet201_stats, xceptionv1_stats])\n",
    "    del results_cnns[dataset_name][\"accuracy\"]\n",
    "    results_cnns[dataset_name].columns =  results_cnns[dataset_name].columns.droplevel(0)\n",
    "#plot_histories_cnn(PATH, plots_dir, file_best_exec_id)\n",
    "\n",
    "\n",
    "for dname in list_datasets:\n",
    "    results_cnns[dname].columns = pd.MultiIndex.from_product([[dname], results_cnns[dname].columns])\n",
    "\n",
    "concat_df = pd.concat(results_cnns.values(), axis=1)\n",
    "concat_df_test = concat_df.query('set == \"Test\"')\n",
    "concat_df_test\n",
    "\n",
    "concat_df_test.reset_index(level=0, inplace=True)\n",
    "concat_df_test.reset_index(level=0, inplace=True)\n",
    "melted = [pd.melt(concat_df_test[[dataset_name, \"model\", \"set\"]], id_vars=[\"model\", \"set\"]) for dataset_name in list_datasets]\n",
    "pd_melted = pd.concat(melted)\n",
    "del pd_melted[\"set\"]\n",
    "pd_melted = pd_melted.reset_index()\n",
    "pd_melted.set_index(\"model\")\n",
    "table_cnns = pd_melted.pivot(index=['model', 'variable_1'], columns='variable_0', values='value')\n",
    "table_cnns = np.round(table_cnns*100, decimals=2)\n",
    "\n",
    "table_cnns_str = table_cnns.astype(str) + '%'\n",
    "table_cnns_str.to_latex(\"paper_tables_plots/table_dataset_info.tex\")\n",
    "table_cnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"caltech_birds2011\"\n",
    "\n",
    "PATH = \"results/results_\" + dataset_name\n",
    "plots_dir = os.path.join(PATH, \"plots_\" + dataset_name)\n",
    "file_best_exec_id_inceptionresnetv2, df_results_cnn_inceptionresnetv2 = get_best_cnn(path=PATH, cnn_filter=\"inceptionresnetv2\", plots_dir=plots_dir)\n",
    "\n",
    "generate_cnn_statistics(df_results_cnn=df_results_cnn_inceptionresnetv2, file_best_exec_id=file_best_exec_id_inceptionresnetv2, path=PATH, plots_dir=plots_dir, cnn_filter=\"inceptionresnetv2\", show_tables=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Classification of statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "list_datasets = [\"caltech_birds2011\", \"cars196\", \"cassava\", \"cats_vs_dogs\", \"citrus_leaves\", \"deep_weeds\",  \"malaria\",  \"oxford_flowers102\", \"plant_leaves\", \"plant_village\"]\n",
    "\n",
    "statistics_results = {}\n",
    "for dataset_name in list_datasets:\n",
    "    PATH = f\"results/results_{dataset_name}/2_classification_experiments_ALL_cnn_execs/dict_exp2.pickle\" \n",
    "    \n",
    "    with open(PATH, \"rb\") as handle:\n",
    "        dict_exp2 = pickle.load(handle)\n",
    "    \n",
    "    statistics_results[dataset_name] = dict_exp2[0][[\"Classifier\", \"Test_macro_weighted_avg_precision\"]]\n",
    "    statistics_results[dataset_name].columns = [\"Classifier\", dataset_name]\n",
    "    statistics_results[dataset_name] = statistics_results[dataset_name].set_index(\"Classifier\")\n",
    "    \n",
    "table_stats = pd.concat(statistics_results, axis=1)\n",
    "\n",
    "table_stats.columns = table_stats.columns.droplevel()\n",
    "table_stats = np.round(table_stats*100, decimals=2)\n",
    "table_stats_str = table_stats.astype(str) + '%'\n",
    "table_stats_str.to_latex(\"paper_tables_plots/table_stats.tex\")\n",
    "table_stats.style.highlight_max(color = 'green', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Classification based on CNN probs + best classifier on stat features (Exp 3 avg + exp 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "list_datasets = [\"caltech_birds2011\", \"cars196\", \"cassava\", \"cats_vs_dogs\", \"citrus_leaves\", \"deep_weeds\",  \"malaria\",  \"oxford_flowers102\", \"plant_leaves\", \"plant_village\"]\n",
    "\n",
    "avg_results = {}\n",
    "for dataset_name in list_datasets:\n",
    "    print(dataset_name)\n",
    "    PATH = f\"results/results_{dataset_name}/2_classification_experiments_ALL_cnn_execs/dict_exp3.pickle\" \n",
    "    \n",
    "    with open(PATH, \"rb\") as handle:\n",
    "        dict_exp3 = pickle.load(handle)\n",
    "    \n",
    "    avg_results[dataset_name] = dict_exp3#[0][[\"Classifier\", \"Test_macro_weighted_avg_precision\"]]\n",
    "\n",
    "    for cnn_model in avg_results[dataset_name].keys():\n",
    "\n",
    "        avg_results[dataset_name][cnn_model] = avg_results[dataset_name][cnn_model].loc[\"weighted avg\"][[\"precision\"]]\n",
    "        \n",
    "        \n",
    "    avg_results[dataset_name] = pd.concat(avg_results[dataset_name], axis=1)\n",
    "    \n",
    "    avg_results[dataset_name].columns = [x.split(\"_\")[1] for x in avg_results[dataset_name].columns ]\n",
    "\n",
    "results_avg = pd.concat(avg_results)\n",
    "\n",
    "results_avg.index = results_avg.index.droplevel(1)\n",
    "results_avg[\"dataset\"] = results_avg.index\n",
    "results_avg = pd.melt(results_avg, id_vars=[\"dataset\"], value_vars=['vgg19', 'inceptionresnetv2', 'inceptionv3', 'densenet201', 'xceptionv1'])\n",
    "results_avg = results_avg.set_index(['dataset', 'variable'])\n",
    "results_avg.index = results_avg.index.rename(['Dataset', 'CNN'])\n",
    "results_avg.columns = [\"Avg.\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining classification of probs features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "list_datasets = [\"caltech_birds2011\", \"cars196\", \"cassava\", \"cats_vs_dogs\", \"citrus_leaves\", \"deep_weeds\",  \"malaria\",  \"oxford_flowers102\", \"plant_leaves\", \"plant_village\"]\n",
    "\n",
    "combination_results = {}\n",
    "for dataset_name in list_datasets:\n",
    "    PATH = f\"results/results_{dataset_name}/2_classification_experiments_ALL_cnn_execs/dict_exp4.pickle\" \n",
    "    \n",
    "    with open(PATH, \"rb\") as handle:\n",
    "        dict_exp4 = pickle.load(handle)\n",
    "    \n",
    "    combination_results[dataset_name] = dict_exp4\n",
    "    \n",
    "    for cnn_model in combination_results[dataset_name].keys():\n",
    "        \n",
    "        # Getting average of executions\n",
    "        averaged_results = pd.concat([x[[\"Classifier\", \"Test_macro_weighted_avg_precision\"]] for x in combination_results[dataset_name][cnn_model]]).groupby(\"Classifier\").agg({'Test_macro_weighted_avg_precision': ['mean', 'std']})\n",
    "        \n",
    "        combination_results[dataset_name][cnn_model] = averaged_results\n",
    "\n",
    "        \n",
    "    combination_results[dataset_name] = pd.concat(combination_results[dataset_name])#.reset_index()\n",
    "    \n",
    "    combination_results[dataset_name].columns = combination_results[dataset_name].columns.droplevel()\n",
    "    \n",
    "results_comb = pd.concat(combination_results).reset_index()\n",
    "\n",
    "results_comb.columns = [\"Dataset\", \"CNN\", \"Classifier\", \"Mean\", \"Std.\"]\n",
    "results_comb[\"CNN\"] = results_comb[\"CNN\"].map(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "results_comb = results_comb.set_index(['Classifier', 'Dataset','CNN']).unstack('Classifier')\n",
    "\n",
    "results_comb.columns = results_comb.columns.swaplevel(0, 1)\n",
    "results_comb.sort_index(axis=1, level=0, inplace=True)\n",
    "\n",
    "del results_comb[(\"LogisticRegression\", \"Std.\")]\n",
    "del results_comb[(\"KNeighborsClassifier\", \"Std.\")]\n",
    "del results_comb[(\"LinearDiscriminantAnalysis\", \"Std.\")]\n",
    "del results_comb[(\"SVM-rbf\", \"Std.\")]\n",
    "del results_comb[(\"SVM-sigmoid\", \"Std.\")]\n",
    "\n",
    "\n",
    "results_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining mean value as CNN base result \n",
    "\n",
    "### PRECAUTION! THIS TAKES THE MEAN VALUE, NEXT CELL USES SAME EXECUTION AS THE ONE USED TO TRAIN THE CLASSIFIERS (BEST IN VALID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_cnns_mean = table_cnns[np.in1d(table_cnns.index.get_level_values(1), ['mean', \"std\"])]\n",
    "\n",
    "table_cnns_mean = table_cnns_mean.reset_index()\n",
    "#del table_cnns_mean[\"variable_0\"]\n",
    "table_cnns_mean = pd.melt(table_cnns_mean, id_vars=[\"model\", \"variable_1\"])\n",
    "table_cnns_mean.columns = ['CNN', 'Measure', 'Dataset', 'CNN base']\n",
    "table_cnns_mean = table_cnns_mean.set_index(['Dataset', 'CNN', \"Measure\"]).unstack('Measure')\n",
    "\n",
    "\n",
    "table_cnns_mean[(\"CNN base mean std\", \"mean std\")] = table_cnns_mean[(\"CNN base\", \"mean\")].astype(str) + \"±\" + table_cnns_mean[(\"CNN base\", \"std\")].astype(str)\n",
    "del table_cnns_mean[(\"CNN base\", \"std\")]\n",
    "table_cnns_mean.columns = table_cnns_mean.columns.droplevel(1)\n",
    "table_cnns_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining base CNN results from best execution (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Utils import get_best_cnn, get_cnn_predictions_by_model\n",
    "from src.datasets import load_dataset_by_name\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "\n",
    "cnn_results_best_execution = {}\n",
    "for dataset_name in list_datasets:\n",
    "    \n",
    "    data_loaded = load_dataset_by_name(dataset_name=dataset_name, batch_size=BATCH_SIZE)  \n",
    "    cnn_resuts_path = f\"results/results_{dataset_name}\"\n",
    "    \n",
    "    cnn_results_best_execution[dataset_name] = {}\n",
    "    \n",
    "    for cnn_model in [\"densenet201\", \"inceptionresnetv2\", \"inceptionv3\", \"vgg19\", \"xceptionv1\"]:\n",
    "    \n",
    "        file_best_exec_id_cnn, _ = get_best_cnn(path=cnn_resuts_path, cnn_filter=cnn_model, plots_dir=\"tmp/\")\n",
    "\n",
    "        report_classes_exp1, cnn_hot_preds_train_labels, cnn_hot_preds_valid_labels, cnn_hot_preds_test_labels, cnn_hot_preds_train, cnn_hot_preds_valid, cnn_hot_preds_test = get_cnn_predictions_by_model(PATH=cnn_resuts_path,\n",
    "                                 dataset_name=dataset_name,\n",
    "                                 data_loaded=data_loaded,\n",
    "                                 BATCH_SIZE=64,\n",
    "                                 file_best_exec_id=file_best_exec_id_cnn)\n",
    "    \n",
    "        cnn_results_best_execution[dataset_name][cnn_model] = report_classes_exp1\n",
    "\n",
    "\n",
    "for dataset_name in list_datasets:\n",
    "    for cnn_model in [\"densenet201\", \"inceptionresnetv2\", \"inceptionv3\", \"vgg19\", \"xceptionv1\"]:\n",
    "        cnn_results_best_execution[dataset_name][cnn_model] = pd.DataFrame(cnn_results_best_execution[dataset_name][cnn_model].loc[\"weighted avg\"][[\"precision\"]]).reset_index()\n",
    "        del cnn_results_best_execution[dataset_name][cnn_model][\"index\"]\n",
    "        cnn_results_best_execution[dataset_name][cnn_model][\"dataset\"] = dataset_name\n",
    "        cnn_results_best_execution[dataset_name][cnn_model] = cnn_results_best_execution[dataset_name][cnn_model].set_index(['dataset'])\n",
    "\n",
    "df_best_exec_cnn_base = pd.concat([pd.concat(cnn_results_best_execution[key]) for key in cnn_results_best_execution.keys()])\n",
    "df_best_exec_cnn_base = df_best_exec_cnn_base.reset_index().set_index([\"dataset\", \"level_0\"])\n",
    "df_best_exec_cnn_base = pd.melt(df_best_exec_cnn_base.reset_index(), id_vars=[\"dataset\", \"level_0\"])\n",
    "del df_best_exec_cnn_base[\"variable\"]\n",
    "df_best_exec_cnn_base.columns = [\"Dataset\", \"CNN\", \"value\"]\n",
    "df_best_exec_cnn_base = df_best_exec_cnn_base.set_index(['Dataset', 'CNN'])\n",
    "df_best_exec_cnn_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING AVERAGE FROM CNN EXECUTIONS\n",
    "all_results_comb = pd.concat([table_cnns_mean, results_avg*100, results_comb*100], axis=1)\n",
    "\n",
    "\n",
    "# USING BEST CNN BASE RESULT\n",
    "#all_results_comb = pd.concat([df_best_exec_cnn_base, results_avg, results_comb], axis=1)\n",
    "\n",
    "table_all = np.round(all_results_comb, decimals=2)\n",
    "del table_all[\"CNN base\"]\n",
    "\n",
    "table_all[(\"RandomForestClassifier\", \"Mean\")] = table_all[(\"RandomForestClassifier\", \"Mean\")].astype(str) + \"±\" + table_all[(\"RandomForestClassifier\", \"Std.\")].astype(str)\n",
    "del table_all[(\"RandomForestClassifier\", \"Std.\")]\n",
    "\n",
    "\n",
    "\n",
    "table_all_str = table_all.astype(str) + '%'\n",
    "table_all_str.to_latex(\"paper_tables_plots/table_all.tex\")\n",
    "table_all_str.to_csv(\"paper_tables_plots/table_all.csv\")\n",
    "\n",
    "def set_color_cell(v, x):\n",
    "    if v == max(x):\n",
    "        return \"background: green; font-weight: bold\" \n",
    "    #elif v > x[\"CNN base\"]:\n",
    "    #    return \"background: blue\"\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def set_bold_cell(v, x):\n",
    "    if v == max(x):\n",
    "        return \"font-weight: bold\" \n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "#all_results_comb.style.apply(lambda x: [set_color_cell(v,x) for v in x], axis = 1)#.apply(lambda x: [set_bold_cell(v,x) for v in x], axis = 1)\n",
    "del all_results_comb[\"CNN base mean std\"]\n",
    "\n",
    "table_styler = all_results_comb.style.apply(lambda x: [set_color_cell(v,x) for v in x], axis = 1)\n",
    "table_styler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RUN THIS CELL BEFORE RUNNING ABLATION STUDY IN ORDER TO OBTAIN THE BEST ENSEMBLE FOR EACH DATASET/CNN\n",
    "\n",
    "all_results_comb.columns = [\"CNN base\", \"Avg.\",\"KNeighborsClassifier\", \"LinearDiscriminantAnalysis\", \"LogisticRegression\",\"RandomForestClassifier\", \"RandomForestClassifierSTD\", \"SVM-rbf\",\"SVM-sigmoid\"]\n",
    "\n",
    "best_classifiers = all_results_comb[[\"Avg.\", \"LogisticRegression\",\"LinearDiscriminantAnalysis\",\"KNeighborsClassifier\",\"SVM-rbf\",\"SVM-sigmoid\",\"RandomForestClassifier\"]].idxmax(axis=1).to_dict()\n",
    "with open(\"best_ensemble.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(best_classifiers, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Ablation study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting avg results for difference calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; sns.set_theme()\n",
    "\n",
    "#cnn_model_selected = \"inceptionv3\"\n",
    "#cnn_model_selected = \"inceptionresnetv2\"\n",
    "#cnn_model_selected = \"xceptionv1\"\n",
    "#cnn_model_selected = \"densenet201\"\n",
    "cnn_model_selected = \"vgg19\"\n",
    "\n",
    "\n",
    "list_datasets = [\"caltech_birds2011\", \"cars196\", \"cassava\", \"cats_vs_dogs\", \"citrus_leaves\", \"deep_weeds\",  \"malaria\",  \"oxford_flowers102\", \"plant_leaves\", \"plant_village\"]\n",
    "\n",
    "ablation_study_dict = {}\n",
    "for dataset_name in list_datasets:\n",
    "    print(dataset_name)\n",
    "    \n",
    "    path = f\"ablation_study_stat_out/results_{dataset_name}\"\n",
    "    path_file = [x for x in os.listdir(path) if x.startswith(\"results_ablation_stat_out\") and x.endswith(cnn_model_selected+\".pickle\")][0]\n",
    "        \n",
    "    with open(os.path.join(path, path_file), \"rb\") as handle:\n",
    "        dict_dataset = pickle.load(handle)\n",
    "    \n",
    "    ablation_study_dict[dataset_name] = dict_dataset\n",
    "    \n",
    "    ablation_study_dict[dataset_name] = {key: ablation_study_dict[dataset_name][key] for key in ablation_study_dict[dataset_name].keys()}\n",
    "    \n",
    "    \n",
    "    \n",
    "    if type(ablation_study_dict[dataset_name][list(ablation_study_dict[dataset_name].keys())[0]]) == list:\n",
    "\n",
    "        for stat_key in ablation_study_dict[dataset_name].keys():\n",
    "    \n",
    "                ablation_study_dict[dataset_name][stat_key] = pd.concat([x[[\"Classifier\", \"Test_macro_weighted_avg_precision\"]] for x in ablation_study_dict[dataset_name][stat_key]]).groupby(\"Classifier\").mean().reset_index()\n",
    "    \n",
    "    for key in ablation_study_dict[dataset_name].keys():\n",
    "        ablation_study_dict[dataset_name][key].columns = [\"Classifier\", key]\n",
    "        ablation_study_dict[dataset_name][key].set_index('Classifier',inplace=True)\n",
    "    \n",
    "    ablation_study_dict[dataset_name] = pd.concat(ablation_study_dict[dataset_name].values(),axis=1,sort=False).reset_index()\n",
    "    ablation_study_dict[dataset_name].rename(columns = {'index':'Classifier'})\n",
    "    ablation_study_dict[dataset_name] = pd.melt(ablation_study_dict[dataset_name], id_vars=['Classifier'])\n",
    "    ablation_study_dict[dataset_name][\"Dataset\"] = dataset_name\n",
    "    ablation_study_dict[dataset_name] = ablation_study_dict[dataset_name][ablation_study_dict[dataset_name][\"Classifier\"] == best_classifiers[(dataset_name, cnn_model_selected)]]\n",
    "    del ablation_study_dict[dataset_name][\"Classifier\"]\n",
    "    #ablation_study_dict[dataset_name][\"value_diff_mean\"] = ablation_study_dict[dataset_name][\"value\"] - mean_cnns[dataset_name]/100\n",
    "    ablation_study_dict[dataset_name][\"value_diff_max\"] = ablation_study_dict[dataset_name][\"value\"] - df_best_exec_cnn_base.loc[dataset_name, cnn_model_selected][0]\n",
    "\n",
    "    ablation_study_dict[dataset_name][\"value_diff_all_stats\"] = ablation_study_dict[dataset_name][\"value\"] - all_results_comb.loc[dataset_name, cnn_model_selected][best_classifiers[(dataset_name, cnn_model_selected)]]/100\n",
    "    \n",
    "        \n",
    "ablation_study_df = pd.concat(ablation_study_dict.values())\n",
    "\n",
    "ablation_study_df.columns = [\"Stat out\", \"value\", \"Dataset\", \"value_diff_max\", \"value_diff_all_stats\"]\n",
    "\n",
    "ablation_study_df[\"norm_value\"] = ablation_study_df['value'] / ablation_study_df.groupby('Dataset')['value'].transform('sum')\n",
    "\n",
    "\n",
    "ablation_study_df_heatmap = ablation_study_df.pivot_table(values='value_diff_all_stats', index='Dataset', columns='Stat out', aggfunc='first')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 5))\n",
    "g = sns.heatmap(ablation_study_df_heatmap, annot=True, center=0.00, cmap=sns.diverging_palette(220, 50, as_cmap=True))\n",
    "ax.set(xlabel='Statistic left out')\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "ax.set_title(cnn_model_selected,  weight='bold')\n",
    "\n",
    "plt.savefig(f\"ablation_stat_out_heatmap_{cnn_model_selected}.pdf\", dpi=1000, bbox_inches='tight')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_negative(cell):\n",
    "    if type(cell) != str and cell < 0 :\n",
    "        return 'background: red; color:black' \n",
    "    if type(cell) != str and cell > 0.01 :\n",
    "        return 'background: green; color:black' \n",
    "    \n",
    "ablation_study_df_heatmap.style.applymap(highlight_negative)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

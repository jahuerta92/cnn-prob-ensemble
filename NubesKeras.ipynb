{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upZEo-LBEFm1",
    "colab_type": "text"
   },
   "source": [
    "# Montaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FMvFSUFeWziP",
    "colab_type": "code",
    "outputId": "641419a3-be8f-468e-e78f-4facb57bfdd3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.57987685674E12,
     "user_tz": -60.0,
     "elapsed": 123892.0,
     "user": {
      "displayName": "JAVIER HUERTAS TATO",
      "photoUrl": "",
      "userId": "11051141025262866472"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "import sys\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "# '/content/drive/My Drive/Universidad/Proyectos/NubesKeras'\n",
    "sys.path.append('/content/drive/My Drive/Proyectos/NubesKeras')\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from math import ceil\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)    \n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk0iCY_xELWO",
    "colab_type": "text"
   },
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eZ8I5aoCWfGy",
    "colab_type": "code",
    "outputId": "bf10bfd2-6e86-4874-c5bf-1f20e19942e9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.57987689373E12,
     "user_tz": -60.0,
     "elapsed": 27083.0,
     "user": {
      "displayName": "JAVIER HUERTAS TATO",
      "photoUrl": "",
      "userId": "11051141025262866472"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMOTE oversampling\n",
      "\n",
      "Label train set shape: 7932, 12\n",
      "Label valid set shape: 461, 12\n",
      "Label test set shape: 923, 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image train set shape: 7932, 256, 256, 3\n",
      "Image valid set shape: 461, 256, 256, 3\n",
      "Image test set shape: 923, 256, 256, 3\n"
     ]
    }
   ],
   "source": [
    "from Model import *\n",
    "from Utils import *\n",
    "\n",
    "file_dir = \"/content/drive/My Drive/Proyectos/NubesKeras/data\"\n",
    "model_dir = \"/content/drive/My Drive/Proyectos/NubesKeras/results\"\n",
    "\n",
    "data = load_data(file_dir)\n",
    "\n",
    "data_generator = make_data_generator(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbnDrqkLEVWL",
    "colab_type": "text"
   },
   "source": [
    "# Aprendizaje pre-builts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paNApN57nKiq",
    "colab_type": "text"
   },
   "source": [
    "#### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "xUO5H7JR0uuJ",
    "colab_type": "code",
    "outputId": "2e17b484-de30-44de-90d2-01d31a7a80f7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.579784495862E12,
     "user_tz": -60.0,
     "elapsed": 8535256.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking train and validation tests\n",
      "Building the network\n",
      "Compiling the network\n",
      "Layers: 30\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 256, 256, 64) 1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 256, 256, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 128, 128, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 128, 128, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 128, 128, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 64, 64, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 64, 64, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 64, 64, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 64, 64, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv4 (Conv2D)           (None, 64, 64, 256)  590080      block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 32, 32, 256)  0           block3_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 32, 32, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 32, 32, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 32, 32, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv4 (Conv2D)           (None, 32, 32, 512)  2359808     block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 16, 16, 512)  0           block4_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 16, 16, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 16, 16, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 16, 16, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv4 (Conv2D)           (None, 16, 16, 512)  2359808     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           128         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 8, 8, 512)    0           block5_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 528)          0           dropout_2[0][0]                  \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2048)         1083392     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2048)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           24588       dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,132,492\n",
      "Trainable params: 21,130,700\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "Fitting the network\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 151s - loss: 1.7464 - acc: 0.3962 - val_loss: 1.6039 - val_acc: 0.4816\n",
      "Epoch 2/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 135s - loss: 1.0113 - acc: 0.6411 - val_loss: 1.1419 - val_acc: 0.6226\n",
      "Epoch 3/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.8182 - acc: 0.7071 - val_loss: 1.1372 - val_acc: 0.6030\n",
      "Epoch 4/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.6955 - acc: 0.7529 - val_loss: 0.8727 - val_acc: 0.7267\n",
      "Epoch 5/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.6541 - acc: 0.7678 - val_loss: 0.9602 - val_acc: 0.6876\n",
      "Epoch 6/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.6068 - acc: 0.7864 - val_loss: 0.9811 - val_acc: 0.6855\n",
      "Epoch 7/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.5598 - acc: 0.7999 - val_loss: 0.7580 - val_acc: 0.7310\n",
      "Epoch 8/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.5215 - acc: 0.8118 - val_loss: 0.8419 - val_acc: 0.7419\n",
      "Epoch 9/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.4917 - acc: 0.8240 - val_loss: 0.9247 - val_acc: 0.7245\n",
      "Epoch 10/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.4588 - acc: 0.8356 - val_loss: 0.7401 - val_acc: 0.7809\n",
      "Epoch 11/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.4519 - acc: 0.8401 - val_loss: 0.8355 - val_acc: 0.7354\n",
      "Epoch 12/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.4401 - acc: 0.8496 - val_loss: 0.6429 - val_acc: 0.7766\n",
      "Epoch 13/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.4210 - acc: 0.8493 - val_loss: 0.6770 - val_acc: 0.7549\n",
      "Epoch 14/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.4096 - acc: 0.8501 - val_loss: 0.8306 - val_acc: 0.7072\n",
      "Epoch 15/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.3895 - acc: 0.8579 - val_loss: 0.7840 - val_acc: 0.7527\n",
      "Epoch 16/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.3597 - acc: 0.8744 - val_loss: 0.6040 - val_acc: 0.7939\n",
      "Epoch 17/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 140s - loss: 0.3660 - acc: 0.8748 - val_loss: 0.6283 - val_acc: 0.8200\n",
      "Epoch 18/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.3290 - acc: 0.8809 - val_loss: 0.6179 - val_acc: 0.8004\n",
      "Epoch 19/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.3449 - acc: 0.8799 - val_loss: 0.6242 - val_acc: 0.8004\n",
      "Epoch 20/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.3316 - acc: 0.8864 - val_loss: 0.7615 - val_acc: 0.7462\n",
      "Epoch 21/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 142s - loss: 0.2974 - acc: 0.8966 - val_loss: 0.4864 - val_acc: 0.8460\n",
      "Epoch 22/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 140s - loss: 0.2827 - acc: 0.8984 - val_loss: 0.5109 - val_acc: 0.8221\n",
      "Epoch 23/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.2925 - acc: 0.8952 - val_loss: 0.5813 - val_acc: 0.8048\n",
      "Epoch 24/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.2779 - acc: 0.8996 - val_loss: 0.4755 - val_acc: 0.8416\n",
      "Epoch 25/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.2645 - acc: 0.9078 - val_loss: 0.5260 - val_acc: 0.8330\n",
      "Epoch 26/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 135s - loss: 0.2671 - acc: 0.9047 - val_loss: 0.5220 - val_acc: 0.8460\n",
      "Epoch 27/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 140s - loss: 0.2518 - acc: 0.9097 - val_loss: 0.4200 - val_acc: 0.8568\n",
      "Epoch 28/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.2761 - acc: 0.9043 - val_loss: 0.5709 - val_acc: 0.8286\n",
      "Epoch 29/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 140s - loss: 0.2574 - acc: 0.9089 - val_loss: 0.3875 - val_acc: 0.8655\n",
      "Epoch 30/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.2469 - acc: 0.9099 - val_loss: 0.4280 - val_acc: 0.8872\n",
      "Epoch 31/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.2367 - acc: 0.9150 - val_loss: 0.4933 - val_acc: 0.8503\n",
      "Epoch 32/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 140s - loss: 0.2418 - acc: 0.9187 - val_loss: 0.5032 - val_acc: 0.8568\n",
      "Epoch 33/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.2396 - acc: 0.9159 - val_loss: 0.3733 - val_acc: 0.8655\n",
      "Epoch 34/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.2305 - acc: 0.9178 - val_loss: 0.5177 - val_acc: 0.8373\n",
      "Epoch 35/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.2270 - acc: 0.9188 - val_loss: 0.4693 - val_acc: 0.8872\n",
      "Epoch 36/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.2336 - acc: 0.9197 - val_loss: 0.4914 - val_acc: 0.8590\n",
      "Epoch 37/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.2296 - acc: 0.9197 - val_loss: 0.3303 - val_acc: 0.8937\n",
      "Epoch 38/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.2268 - acc: 0.9223 - val_loss: 0.4517 - val_acc: 0.8482\n",
      "Epoch 39/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.2087 - acc: 0.9265 - val_loss: 0.4109 - val_acc: 0.8720\n",
      "Epoch 40/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1999 - acc: 0.9323 - val_loss: 0.4115 - val_acc: 0.8785\n",
      "Epoch 41/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.2015 - acc: 0.9309 - val_loss: 0.4181 - val_acc: 0.8742\n",
      "Epoch 42/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.2076 - acc: 0.9288 - val_loss: 0.4253 - val_acc: 0.8547\n",
      "Epoch 43/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.2059 - acc: 0.9289 - val_loss: 0.3623 - val_acc: 0.8742\n",
      "Epoch 44/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1990 - acc: 0.9332 - val_loss: 0.3673 - val_acc: 0.8894\n",
      "Epoch 45/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 142s - loss: 0.1846 - acc: 0.9378 - val_loss: 0.4972 - val_acc: 0.8785\n",
      "Epoch 46/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.2040 - acc: 0.9304 - val_loss: 0.3912 - val_acc: 0.9002\n",
      "Epoch 47/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1890 - acc: 0.9357 - val_loss: 0.4033 - val_acc: 0.8872\n",
      "Epoch 48/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.1903 - acc: 0.9353 - val_loss: 0.4742 - val_acc: 0.8438\n",
      "Epoch 49/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.1722 - acc: 0.9402 - val_loss: 0.4693 - val_acc: 0.8655\n",
      "Epoch 50/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1779 - acc: 0.9391 - val_loss: 0.4617 - val_acc: 0.8894\n",
      "Epoch 51/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1686 - acc: 0.9419 - val_loss: 0.2936 - val_acc: 0.8980\n",
      "Epoch 52/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.1600 - acc: 0.9452 - val_loss: 0.3818 - val_acc: 0.8872\n",
      "Epoch 53/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1772 - acc: 0.9371 - val_loss: 0.3965 - val_acc: 0.8655\n",
      "Epoch 54/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1722 - acc: 0.9381 - val_loss: 0.3335 - val_acc: 0.8915\n",
      "Epoch 55/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1711 - acc: 0.9406 - val_loss: 0.3486 - val_acc: 0.8937\n",
      "Epoch 56/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1571 - acc: 0.9448 - val_loss: 0.3429 - val_acc: 0.8937\n",
      "Epoch 57/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1682 - acc: 0.9423 - val_loss: 0.3430 - val_acc: 0.8807\n",
      "Epoch 58/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1667 - acc: 0.9411 - val_loss: 0.5071 - val_acc: 0.8915\n",
      "Epoch 59/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.1815 - acc: 0.9386 - val_loss: 0.4352 - val_acc: 0.8850\n",
      "Epoch 60/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1653 - acc: 0.9423 - val_loss: 0.3613 - val_acc: 0.8850\n",
      "Epoch 61/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1561 - acc: 0.9462 - val_loss: 0.4538 - val_acc: 0.8764\n",
      "Epoch 62/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1599 - acc: 0.9460 - val_loss: 0.3649 - val_acc: 0.8764\n",
      "Epoch 63/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.1503 - acc: 0.9486 - val_loss: 0.4306 - val_acc: 0.8698\n",
      "Epoch 64/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.1581 - acc: 0.9467 - val_loss: 0.3608 - val_acc: 0.8959\n",
      "Epoch 65/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.1575 - acc: 0.9452 - val_loss: 0.4725 - val_acc: 0.8633\n",
      "Epoch 66/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1549 - acc: 0.9494 - val_loss: 0.3258 - val_acc: 0.9002\n",
      "Epoch 67/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.1470 - acc: 0.9497 - val_loss: 0.3227 - val_acc: 0.9111\n",
      "Epoch 68/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 139s - loss: 0.1408 - acc: 0.9494 - val_loss: 0.2517 - val_acc: 0.9154\n",
      "Epoch 69/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.1388 - acc: 0.9525 - val_loss: 0.3215 - val_acc: 0.9111\n",
      "Epoch 70/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.1436 - acc: 0.9517 - val_loss: 0.3334 - val_acc: 0.9046\n",
      "Epoch 71/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.1388 - acc: 0.9497 - val_loss: 0.3131 - val_acc: 0.9154\n",
      "Epoch 72/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1350 - acc: 0.9528 - val_loss: 0.3658 - val_acc: 0.8980\n",
      "Epoch 73/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1392 - acc: 0.9535 - val_loss: 0.4147 - val_acc: 0.9024\n",
      "Epoch 74/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 135s - loss: 0.1399 - acc: 0.9512 - val_loss: 0.4033 - val_acc: 0.9024\n",
      "Epoch 75/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1367 - acc: 0.9526 - val_loss: 0.4788 - val_acc: 0.8547\n",
      "Epoch 76/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1259 - acc: 0.9561 - val_loss: 0.3725 - val_acc: 0.8829\n",
      "Epoch 77/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1379 - acc: 0.9513 - val_loss: 0.4125 - val_acc: 0.9132\n",
      "Epoch 78/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1334 - acc: 0.9539 - val_loss: 0.3503 - val_acc: 0.8937\n",
      "Epoch 79/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1350 - acc: 0.9528 - val_loss: 0.2917 - val_acc: 0.9111\n",
      "Epoch 80/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 135s - loss: 0.1215 - acc: 0.9578 - val_loss: 0.4684 - val_acc: 0.9067\n",
      "Epoch 81/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 135s - loss: 0.1295 - acc: 0.9549 - val_loss: 0.4099 - val_acc: 0.8850\n",
      "Epoch 82/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 135s - loss: 0.1353 - acc: 0.9547 - val_loss: 0.4352 - val_acc: 0.8872\n",
      "Epoch 83/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1375 - acc: 0.9531 - val_loss: 0.3538 - val_acc: 0.9132\n",
      "Epoch 84/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 135s - loss: 0.1198 - acc: 0.9610 - val_loss: 0.3180 - val_acc: 0.9132\n",
      "Epoch 85/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1155 - acc: 0.9612 - val_loss: 0.3519 - val_acc: 0.8937\n",
      "Epoch 86/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1238 - acc: 0.9575 - val_loss: 0.3789 - val_acc: 0.8872\n",
      "Epoch 87/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 135s - loss: 0.1282 - acc: 0.9557 - val_loss: 0.3139 - val_acc: 0.9046\n",
      "Epoch 88/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 136s - loss: 0.1362 - acc: 0.9537 - val_loss: 0.3677 - val_acc: 0.8785\n",
      "Epoch 89/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1205 - acc: 0.9597 - val_loss: 0.3976 - val_acc: 0.8894\n",
      "Epoch 90/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1227 - acc: 0.9570 - val_loss: 0.3517 - val_acc: 0.8785\n",
      "Epoch 91/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 138s - loss: 0.1128 - acc: 0.9604 - val_loss: 0.3917 - val_acc: 0.8980\n",
      "Epoch 92/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1159 - acc: 0.9588 - val_loss: 0.4664 - val_acc: 0.8915\n",
      "Epoch 93/1000\n",
      "Epoch 1/1000\n",
      "62/61 - 137s - loss: 0.1166 - acc: 0.9610 - val_loss: 0.4801 - val_acc: 0.8764\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol       1.00      1.00      1.00        20\n",
      "   altocumulos       0.94      0.95      0.95        88\n",
      "   altostratos       0.89      0.94      0.92        36\n",
      "cieloDespejado       1.00      1.00      1.00        23\n",
      "  cirrocumulos       1.00      1.00      1.00        21\n",
      "        cirros       0.96      0.90      0.93       145\n",
      "  cirrostratos       0.87      0.96      0.91        68\n",
      "       cumulos       0.93      0.99      0.96        71\n",
      "estratocumulos       0.94      0.96      0.95       142\n",
      "      estratos       0.87      0.98      0.92       108\n",
      "     multinube       0.96      0.84      0.90       189\n",
      "  nimbostratos       0.86      1.00      0.92        12\n",
      "\n",
      "      accuracy                           0.93       923\n",
      "     macro avg       0.94      0.96      0.95       923\n",
      "  weighted avg       0.94      0.93      0.93       923\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFMCAYAAABF3IJ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlE6sUCgUCn3L/wdXBeSJhVK4SwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "model_name = 'vgg19'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "          make_prebuilt(VGG19, .1),\n",
    "          batch_size=128,\n",
    "          model_name=model_name,\n",
    "          model_dir=model_dir)\n",
    "\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "yWu9muoECFTU",
    "colab_type": "code",
    "outputId": "3e593efa-e8b9-41ae-a33b-9f1902f63105",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.579179295447E12,
     "user_tz": -60.0,
     "elapsed": 2117759.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking train and validation tests\n",
      "Building the network\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 6s 0us/step\n",
      "Compiling the network\n",
      "Layers: 34\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 256, 256, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 256, 256, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 128, 128, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 128, 128, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 128, 128, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 64, 64, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 64, 64, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 64, 64, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 64, 64, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv4 (Conv2D)           (None, 64, 64, 256)  590080      block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 32, 32, 256)  0           block3_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 32, 32, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 32, 32, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 32, 32, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv4 (Conv2D)           (None, 32, 32, 512)  2359808     block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 16, 16, 512)  0           block4_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 16, 16, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 16, 16, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 16, 16, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv4 (Conv2D)           (None, 16, 16, 512)  2359808     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           128         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 8, 8, 512)    0           block5_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 528)          0           dropout[0][0]                    \n",
      "                                                                 global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          135424      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           256         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           780         activation_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 20,178,444\n",
      "Trainable params: 20,176,012\n",
      "Non-trainable params: 2,432\n",
      "__________________________________________________________________________________________________\n",
      "Fitting the network\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/1000\n",
      "51/50 - 65s - loss: 1.9142 - acc: 0.3856 - val_loss: 3.2297 - val_acc: 0.2104\n",
      "Epoch 2/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 1.4595 - acc: 0.5418 - val_loss: 1.8450 - val_acc: 0.4273\n",
      "Epoch 3/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 1.2824 - acc: 0.5883 - val_loss: 1.7198 - val_acc: 0.5228\n",
      "Epoch 4/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 55s - loss: 1.1518 - acc: 0.6308 - val_loss: 1.4441 - val_acc: 0.5510\n",
      "Epoch 5/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 53s - loss: 1.0803 - acc: 0.6522 - val_loss: 1.4997 - val_acc: 0.5683\n",
      "Epoch 6/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 1.0036 - acc: 0.6723 - val_loss: 1.5383 - val_acc: 0.5315\n",
      "Epoch 7/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.9192 - acc: 0.7046 - val_loss: 1.3278 - val_acc: 0.5683\n",
      "Epoch 8/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.8605 - acc: 0.7185 - val_loss: 1.4485 - val_acc: 0.5662\n",
      "Epoch 9/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 53s - loss: 0.8352 - acc: 0.7244 - val_loss: 1.2624 - val_acc: 0.6312\n",
      "Epoch 10/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 0.8077 - acc: 0.7340 - val_loss: 0.9902 - val_acc: 0.7050\n",
      "Epoch 11/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.7397 - acc: 0.7533 - val_loss: 1.1273 - val_acc: 0.6508\n",
      "Epoch 12/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.7318 - acc: 0.7474 - val_loss: 1.0922 - val_acc: 0.6855\n",
      "Epoch 13/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.7218 - acc: 0.7576 - val_loss: 1.1189 - val_acc: 0.6811\n",
      "Epoch 14/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.6663 - acc: 0.7734 - val_loss: 1.0990 - val_acc: 0.6811\n",
      "Epoch 15/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.6507 - acc: 0.7864 - val_loss: 0.9861 - val_acc: 0.7050\n",
      "Epoch 16/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.6291 - acc: 0.7886 - val_loss: 1.1601 - val_acc: 0.7007\n",
      "Epoch 17/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.6023 - acc: 0.7929 - val_loss: 1.1144 - val_acc: 0.6768\n",
      "Epoch 18/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.6064 - acc: 0.7982 - val_loss: 0.7846 - val_acc: 0.7831\n",
      "Epoch 19/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.5799 - acc: 0.8060 - val_loss: 1.0597 - val_acc: 0.6421\n",
      "Epoch 20/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.5636 - acc: 0.8118 - val_loss: 1.0339 - val_acc: 0.6985\n",
      "Epoch 21/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 0.5613 - acc: 0.8044 - val_loss: 0.7773 - val_acc: 0.7852\n",
      "Epoch 22/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.5524 - acc: 0.8196 - val_loss: 1.0622 - val_acc: 0.6594\n",
      "Epoch 23/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.5208 - acc: 0.8277 - val_loss: 0.6823 - val_acc: 0.7614\n",
      "Epoch 24/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.5405 - acc: 0.8177 - val_loss: 0.6214 - val_acc: 0.7918\n",
      "Epoch 25/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.5009 - acc: 0.8252 - val_loss: 0.7174 - val_acc: 0.7787\n",
      "Epoch 26/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.4956 - acc: 0.8338 - val_loss: 0.7232 - val_acc: 0.7918\n",
      "Epoch 27/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.4850 - acc: 0.8301 - val_loss: 0.8858 - val_acc: 0.6855\n",
      "Epoch 28/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.5023 - acc: 0.8304 - val_loss: 0.6779 - val_acc: 0.7939\n",
      "Epoch 29/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.4661 - acc: 0.8425 - val_loss: 0.6719 - val_acc: 0.7983\n",
      "Epoch 30/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.4438 - acc: 0.8540 - val_loss: 0.7243 - val_acc: 0.7657\n",
      "Epoch 31/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.4545 - acc: 0.8506 - val_loss: 0.7175 - val_acc: 0.7787\n",
      "Epoch 32/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.4437 - acc: 0.8462 - val_loss: 0.7141 - val_acc: 0.7592\n",
      "Epoch 33/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.4358 - acc: 0.8512 - val_loss: 0.7414 - val_acc: 0.7679\n",
      "Epoch 34/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.4307 - acc: 0.8543 - val_loss: 0.6468 - val_acc: 0.8113\n",
      "Epoch 35/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.4340 - acc: 0.8521 - val_loss: 0.4844 - val_acc: 0.8395\n",
      "Epoch 36/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.4164 - acc: 0.8599 - val_loss: 0.6725 - val_acc: 0.7961\n",
      "Epoch 37/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.4161 - acc: 0.8552 - val_loss: 0.6717 - val_acc: 0.7744\n",
      "Epoch 38/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.4110 - acc: 0.8608 - val_loss: 0.6372 - val_acc: 0.8200\n",
      "Epoch 39/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 47s - loss: 0.4100 - acc: 0.8630 - val_loss: 0.5490 - val_acc: 0.8351\n",
      "Epoch 40/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 47s - loss: 0.3770 - acc: 0.8720 - val_loss: 0.5964 - val_acc: 0.8134\n",
      "Epoch 41/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 47s - loss: 0.4089 - acc: 0.8636 - val_loss: 0.4819 - val_acc: 0.8395\n",
      "Epoch 42/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.3912 - acc: 0.8667 - val_loss: 0.5937 - val_acc: 0.8134\n",
      "Epoch 43/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3963 - acc: 0.8664 - val_loss: 0.6305 - val_acc: 0.8026\n",
      "Epoch 44/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.3824 - acc: 0.8676 - val_loss: 0.4901 - val_acc: 0.8482\n",
      "Epoch 45/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3724 - acc: 0.8763 - val_loss: 0.5135 - val_acc: 0.8460\n",
      "Epoch 46/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3452 - acc: 0.8785 - val_loss: 0.5001 - val_acc: 0.8395\n",
      "Epoch 47/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.3528 - acc: 0.8819 - val_loss: 0.4754 - val_acc: 0.8525\n",
      "Epoch 48/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3527 - acc: 0.8822 - val_loss: 0.5248 - val_acc: 0.8482\n",
      "Epoch 49/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3612 - acc: 0.8732 - val_loss: 0.7355 - val_acc: 0.7592\n",
      "Epoch 50/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3586 - acc: 0.8794 - val_loss: 0.6702 - val_acc: 0.8091\n",
      "Epoch 51/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3588 - acc: 0.8785 - val_loss: 0.5215 - val_acc: 0.8438\n",
      "Epoch 52/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3280 - acc: 0.8872 - val_loss: 0.5171 - val_acc: 0.8265\n",
      "Epoch 53/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3685 - acc: 0.8766 - val_loss: 0.5460 - val_acc: 0.8091\n",
      "Epoch 54/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 47s - loss: 0.3552 - acc: 0.8788 - val_loss: 0.5246 - val_acc: 0.8503\n",
      "Epoch 55/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3470 - acc: 0.8751 - val_loss: 0.4909 - val_acc: 0.8243\n",
      "Epoch 56/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.3402 - acc: 0.8856 - val_loss: 0.5166 - val_acc: 0.8547\n",
      "Epoch 57/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.3202 - acc: 0.8949 - val_loss: 0.4639 - val_acc: 0.8720\n",
      "Epoch 58/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.3195 - acc: 0.8918 - val_loss: 0.4924 - val_acc: 0.8460\n",
      "Epoch 59/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3100 - acc: 0.8915 - val_loss: 0.4865 - val_acc: 0.8503\n",
      "Epoch 60/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3309 - acc: 0.8872 - val_loss: 0.5562 - val_acc: 0.8200\n",
      "Epoch 61/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.2989 - acc: 0.8962 - val_loss: 0.4311 - val_acc: 0.8568\n",
      "Epoch 62/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.3289 - acc: 0.8856 - val_loss: 0.4844 - val_acc: 0.8373\n",
      "Epoch 63/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.3023 - acc: 0.8890 - val_loss: 0.4601 - val_acc: 0.8395\n",
      "Epoch 64/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2896 - acc: 0.8983 - val_loss: 0.5993 - val_acc: 0.7961\n",
      "Epoch 65/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.3147 - acc: 0.8943 - val_loss: 0.6147 - val_acc: 0.7831\n",
      "Epoch 66/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2958 - acc: 0.9011 - val_loss: 0.3457 - val_acc: 0.8872\n",
      "Epoch 67/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.3175 - acc: 0.8952 - val_loss: 0.3979 - val_acc: 0.8590\n",
      "Epoch 68/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.3075 - acc: 0.9008 - val_loss: 0.4945 - val_acc: 0.8330\n",
      "Epoch 69/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.3038 - acc: 0.9011 - val_loss: 0.5040 - val_acc: 0.8438\n",
      "Epoch 70/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2711 - acc: 0.9055 - val_loss: 0.5324 - val_acc: 0.8525\n",
      "Epoch 71/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2807 - acc: 0.9061 - val_loss: 0.4776 - val_acc: 0.8482\n",
      "Epoch 72/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2862 - acc: 0.8958 - val_loss: 0.5942 - val_acc: 0.8438\n",
      "Epoch 73/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2743 - acc: 0.9076 - val_loss: 0.4555 - val_acc: 0.8677\n",
      "Epoch 74/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2835 - acc: 0.9027 - val_loss: 0.5203 - val_acc: 0.8503\n",
      "Epoch 75/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2834 - acc: 0.9036 - val_loss: 0.4620 - val_acc: 0.8482\n",
      "Epoch 76/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.2680 - acc: 0.9089 - val_loss: 0.4032 - val_acc: 0.8677\n",
      "Epoch 77/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2562 - acc: 0.9179 - val_loss: 0.5301 - val_acc: 0.8308\n",
      "Epoch 78/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2616 - acc: 0.9098 - val_loss: 0.4407 - val_acc: 0.8655\n",
      "Epoch 79/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2727 - acc: 0.9011 - val_loss: 0.3832 - val_acc: 0.8850\n",
      "Epoch 80/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 0.2657 - acc: 0.9064 - val_loss: 0.4353 - val_acc: 0.8590\n",
      "Epoch 81/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.2659 - acc: 0.9148 - val_loss: 0.3881 - val_acc: 0.8807\n",
      "Epoch 82/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 0.2689 - acc: 0.9051 - val_loss: 0.5768 - val_acc: 0.8221\n",
      "Epoch 83/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.2511 - acc: 0.9166 - val_loss: 0.5397 - val_acc: 0.8568\n",
      "Epoch 84/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2374 - acc: 0.9160 - val_loss: 0.4375 - val_acc: 0.8416\n",
      "Epoch 85/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 0.2537 - acc: 0.9110 - val_loss: 0.5496 - val_acc: 0.8178\n",
      "Epoch 86/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 0.2615 - acc: 0.9073 - val_loss: 0.4975 - val_acc: 0.8113\n",
      "Epoch 87/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 0.2638 - acc: 0.9064 - val_loss: 0.4258 - val_acc: 0.8633\n",
      "Epoch 88/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 0.2748 - acc: 0.9039 - val_loss: 0.4449 - val_acc: 0.8785\n",
      "Epoch 89/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2564 - acc: 0.9117 - val_loss: 0.5134 - val_acc: 0.8395\n",
      "Epoch 90/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2627 - acc: 0.9082 - val_loss: 0.4430 - val_acc: 0.8590\n",
      "Epoch 91/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 52s - loss: 0.2545 - acc: 0.9169 - val_loss: 0.3425 - val_acc: 0.8937\n",
      "Epoch 92/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2317 - acc: 0.9244 - val_loss: 0.5117 - val_acc: 0.8590\n",
      "Epoch 93/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2349 - acc: 0.9182 - val_loss: 0.3354 - val_acc: 0.8872\n",
      "Epoch 94/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.2336 - acc: 0.9163 - val_loss: 0.3651 - val_acc: 0.9089\n",
      "Epoch 95/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2316 - acc: 0.9228 - val_loss: 0.5463 - val_acc: 0.8503\n",
      "Epoch 96/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2438 - acc: 0.9154 - val_loss: 0.3973 - val_acc: 0.8829\n",
      "Epoch 97/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2185 - acc: 0.9213 - val_loss: 0.4334 - val_acc: 0.8829\n",
      "Epoch 98/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 48s - loss: 0.2166 - acc: 0.9284 - val_loss: 0.4174 - val_acc: 0.8742\n",
      "Epoch 99/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2165 - acc: 0.9275 - val_loss: 0.3949 - val_acc: 0.8742\n",
      "Epoch 100/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2372 - acc: 0.9182 - val_loss: 0.2980 - val_acc: 0.8959\n",
      "Epoch 101/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2287 - acc: 0.9197 - val_loss: 0.3550 - val_acc: 0.9111\n",
      "Epoch 102/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2377 - acc: 0.9169 - val_loss: 0.5343 - val_acc: 0.8265\n",
      "Epoch 103/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2252 - acc: 0.9185 - val_loss: 0.4630 - val_acc: 0.8633\n",
      "Epoch 104/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2275 - acc: 0.9166 - val_loss: 0.3622 - val_acc: 0.8829\n",
      "Epoch 105/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2401 - acc: 0.9135 - val_loss: 0.4590 - val_acc: 0.8568\n",
      "Epoch 106/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.1956 - acc: 0.9355 - val_loss: 0.4089 - val_acc: 0.8677\n",
      "Epoch 107/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2153 - acc: 0.9253 - val_loss: 0.3500 - val_acc: 0.9024\n",
      "Epoch 108/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2020 - acc: 0.9293 - val_loss: 0.3343 - val_acc: 0.8915\n",
      "Epoch 109/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.1938 - acc: 0.9303 - val_loss: 0.4737 - val_acc: 0.8351\n",
      "Epoch 110/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2220 - acc: 0.9191 - val_loss: 0.4619 - val_acc: 0.8677\n",
      "Epoch 111/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2060 - acc: 0.9281 - val_loss: 0.4030 - val_acc: 0.8720\n",
      "Epoch 112/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2144 - acc: 0.9281 - val_loss: 0.3510 - val_acc: 0.8915\n",
      "Epoch 113/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2195 - acc: 0.9253 - val_loss: 0.3585 - val_acc: 0.8894\n",
      "Epoch 114/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.1943 - acc: 0.9321 - val_loss: 0.4410 - val_acc: 0.8959\n",
      "Epoch 115/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.2246 - acc: 0.9216 - val_loss: 0.4571 - val_acc: 0.8590\n",
      "Epoch 116/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.2159 - acc: 0.9231 - val_loss: 0.3227 - val_acc: 0.8959\n",
      "Epoch 117/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.1956 - acc: 0.9299 - val_loss: 0.2819 - val_acc: 0.9046\n",
      "Epoch 118/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.1855 - acc: 0.9399 - val_loss: 0.3457 - val_acc: 0.8894\n",
      "Epoch 119/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.1905 - acc: 0.9358 - val_loss: 0.3802 - val_acc: 0.8785\n",
      "Epoch 120/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 49s - loss: 0.1952 - acc: 0.9318 - val_loss: 0.3901 - val_acc: 0.8807\n",
      "Epoch 121/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2094 - acc: 0.9281 - val_loss: 0.4396 - val_acc: 0.8785\n",
      "Epoch 122/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2035 - acc: 0.9222 - val_loss: 0.4594 - val_acc: 0.8677\n",
      "Epoch 123/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.2000 - acc: 0.9293 - val_loss: 0.3968 - val_acc: 0.8807\n",
      "Epoch 124/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 51s - loss: 0.2124 - acc: 0.9259 - val_loss: 0.2932 - val_acc: 0.8915\n",
      "Epoch 125/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.1806 - acc: 0.9380 - val_loss: 0.3599 - val_acc: 0.9002\n",
      "Epoch 126/1000\n",
      "Epoch 1/1000\n",
      "51/50 - 50s - loss: 0.1947 - acc: 0.9349 - val_loss: 0.3219 - val_acc: 0.9111\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol       1.00      1.00      1.00        20\n",
      "   altocumulos       0.88      0.91      0.89        88\n",
      "   altostratos       0.85      0.78      0.81        36\n",
      "cieloDespejado       0.96      1.00      0.98        23\n",
      "  cirrocumulos       0.62      0.86      0.72        21\n",
      "        cirros       0.93      0.93      0.93       145\n",
      "  cirrostratos       0.93      0.96      0.94        68\n",
      "       cumulos       0.88      0.97      0.93        71\n",
      "estratocumulos       0.90      0.94      0.92       142\n",
      "      estratos       0.93      0.88      0.90       108\n",
      "     multinube       0.92      0.83      0.87       189\n",
      "  nimbostratos       0.73      0.67      0.70        12\n",
      "\n",
      "      accuracy                           0.90       923\n",
      "     macro avg       0.88      0.89      0.88       923\n",
      "  weighted avg       0.90      0.90      0.90       923\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFMCAYAAABF3IJ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "model_name = 'vgg19_alt'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "          make_prebuilt_extended(VGG19, .1), \n",
    "          model_name=model_name,\n",
    "          model_dir=model_dir)\n",
    "\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnSU_HJxk7hp",
    "colab_type": "text"
   },
   "source": [
    "#### Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "q_FltC9djDK5",
    "colab_type": "code",
    "outputId": "1ce3eaab-5af4-420d-e3a3-13a5bb67bd27",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.579701348592E12,
     "user_tz": -60.0,
     "elapsed": 6730141.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking train and validation tests\n",
      "Building the network\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n",
      "Compiling the network\n",
      "Layers: 320\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 127, 127, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 127, 127, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 127, 127, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 125, 125, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 125, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 125, 125, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 125, 125, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 125, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 125, 125, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 62, 62, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 62, 62, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 62, 62, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 62, 62, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 60, 60, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 60, 60, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 60, 60, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 29, 29, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 29, 29, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 29, 29, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 29, 29, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 29, 29, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 29, 29, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 29, 29, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 29, 29, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 29, 29, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 29, 29, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 29, 29, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 29, 29, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 29, 29, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 29, 29, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 29, 29, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 29, 29, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 29, 29, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 29, 29, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 29, 29, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 29, 29, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 29, 29, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 29, 29, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 29, 29, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 29, 29, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 29, 29, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 29, 29, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 29, 29, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 29, 29, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 29, 29, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 29, 29, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 29, 29, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 29, 29, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 29, 29, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 29, 29, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 29, 29, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 29, 29, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 29, 29, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 29, 29, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 29, 29, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 29, 29, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 29, 29, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 29, 29, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 29, 29, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 29, 29, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 29, 29, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 29, 29, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 29, 29, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 29, 29, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 29, 29, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 29, 29, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 29, 29, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 29, 29, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 29, 29, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 29, 29, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 29, 29, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 29, 29, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 29, 29, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 29, 29, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 29, 29, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 29, 29, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 29, 29, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 29, 29, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 29, 29, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 29, 29, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 29, 29, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 29, 29, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 29, 29, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 29, 29, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 29, 29, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 29, 29, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 29, 29, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 29, 29, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 29, 29, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 29, 29, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 29, 29, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 29, 29, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 384)  1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 384)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 14, 14, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 14, 14, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 14, 14, 128)  384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 14, 14, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 14, 14, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 14, 14, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 128)  384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 14, 14, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 14, 14, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 14, 14, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 14, 14, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 14, 14, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 192)  576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 14, 14, 192)  576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 14, 14, 192)  576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 14, 14, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 192)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 192)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 14, 14, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 14, 14, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 14, 14, 160)  480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 14, 14, 160)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 14, 14, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 14, 14, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 14, 14, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 14, 14, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 14, 14, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 14, 14, 160)  480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 14, 14, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 14, 14, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 14, 14, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 14, 14, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 14, 14, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 14, 14, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 14, 14, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 14, 14, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 14, 14, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 14, 14, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 14, 14, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 14, 14, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 14, 14, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 14, 14, 192)  576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 14, 14, 192)  576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 14, 14, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 14, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 14, 14, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 14, 14, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 14, 14, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 14, 14, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 14, 14, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 14, 14, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 14, 14, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 14, 14, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 14, 14, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 14, 14, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 14, 14, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 14, 14, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 14, 14, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 14, 14, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 14, 14, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 14, 14, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 14, 14, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 14, 14, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 14, 14, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 14, 14, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 14, 14, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 14, 14, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 14, 14, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 14, 14, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 14, 14, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 14, 14, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 14, 14, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 14, 14, 192)  576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 14, 14, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 14, 14, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 14, 14, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 14, 14, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 14, 14, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 14, 14, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 14, 14, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 14, 14, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 14, 14, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 14, 14, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 14, 14, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 14, 14, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 14, 14, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 14, 14, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 14, 14, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 14, 14, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 14, 14, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 14, 14, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 14, 14, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 14, 14, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 14, 14, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 14, 14, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 14, 14, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 14, 14, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 14, 14, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 14, 14, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 14, 14, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 14, 14, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 14, 14, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 14, 14, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 14, 14, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 14, 14, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 14, 14, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 14, 14, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 14, 14, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 14, 14, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 14, 14, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 14, 14, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 14, 14, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 14, 14, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 14, 14, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 14, 14, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 14, 14, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 14, 14, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 14, 14, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 14, 14, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 6, 6, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 6, 6, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 6, 6, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 6, 6, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 6, 6, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 6, 6, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 6, 6, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 6, 6, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 6, 6, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 6, 6, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 6, 6, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 6, 6, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 6, 6, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 6, 6, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 6, 6, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 6, 6, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 6, 6, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 6, 6, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 6, 6, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 6, 6, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 6, 6, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 6, 6, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 6, 6, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 6, 6, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 6, 6, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 6, 6, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 6, 6, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 6, 6, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 6, 6, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 6, 6, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 6, 6, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 6, 6, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 6, 6, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 6, 6, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 6, 6, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6, 6, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 6, 6, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 6, 6, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 6, 6, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 6, 6, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 6, 6, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 6, 6, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 6, 6, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 6, 6, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 6, 6, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 6, 6, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 6, 6, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 6, 6, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 6, 6, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 6, 6, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 6, 6, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 6, 6, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 6, 6, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 6, 6, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 6, 6, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 6, 6, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 6, 6, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 6, 6, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 6, 6, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 6, 6, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 6, 6, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 6, 6, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 6, 6, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 6, 6, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 6, 6, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 6, 6, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6, 6, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 6, 6, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           128         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 6, 6, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2064)         0           dropout[0][0]                    \n",
      "                                                                 global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         4229120     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2048)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           24588       activation_94[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 26,056,620\n",
      "Trainable params: 25,053,132\n",
      "Non-trainable params: 1,003,488\n",
      "__________________________________________________________________________________________________\n",
      "Fitting the network\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 682s - loss: 1.1141 - acc: 0.6224 - val_loss: 1.0730 - val_acc: 0.6681\n",
      "Epoch 2/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 138s - loss: 0.5972 - acc: 0.7956 - val_loss: 0.7312 - val_acc: 0.7462\n",
      "Epoch 3/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.4753 - acc: 0.8330 - val_loss: 0.6271 - val_acc: 0.7831\n",
      "Epoch 4/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.4127 - acc: 0.8543 - val_loss: 0.5978 - val_acc: 0.7809\n",
      "Epoch 5/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.3767 - acc: 0.8684 - val_loss: 0.5370 - val_acc: 0.8265\n",
      "Epoch 6/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.3489 - acc: 0.8773 - val_loss: 0.5226 - val_acc: 0.8330\n",
      "Epoch 7/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.3125 - acc: 0.8889 - val_loss: 0.4776 - val_acc: 0.8395\n",
      "Epoch 8/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.3017 - acc: 0.8960 - val_loss: 0.4791 - val_acc: 0.8460\n",
      "Epoch 9/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.2826 - acc: 0.9014 - val_loss: 0.6176 - val_acc: 0.8590\n",
      "Epoch 10/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.2611 - acc: 0.9054 - val_loss: 0.4972 - val_acc: 0.8655\n",
      "Epoch 11/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.2458 - acc: 0.9135 - val_loss: 0.4877 - val_acc: 0.8633\n",
      "Epoch 12/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.2602 - acc: 0.9051 - val_loss: 0.4540 - val_acc: 0.8416\n",
      "Epoch 13/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.2217 - acc: 0.9197 - val_loss: 0.4936 - val_acc: 0.8503\n",
      "Epoch 14/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.2275 - acc: 0.9175 - val_loss: 0.4240 - val_acc: 0.8655\n",
      "Epoch 15/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.2229 - acc: 0.9220 - val_loss: 0.3301 - val_acc: 0.8980\n",
      "Epoch 16/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1966 - acc: 0.9310 - val_loss: 0.3878 - val_acc: 0.8698\n",
      "Epoch 17/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.2013 - acc: 0.9297 - val_loss: 0.4469 - val_acc: 0.8633\n",
      "Epoch 18/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1858 - acc: 0.9356 - val_loss: 0.3890 - val_acc: 0.8742\n",
      "Epoch 19/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1896 - acc: 0.9343 - val_loss: 0.3499 - val_acc: 0.8959\n",
      "Epoch 20/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1767 - acc: 0.9386 - val_loss: 0.4248 - val_acc: 0.8872\n",
      "Epoch 21/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1787 - acc: 0.9342 - val_loss: 0.4643 - val_acc: 0.8829\n",
      "Epoch 22/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1783 - acc: 0.9376 - val_loss: 0.3729 - val_acc: 0.8894\n",
      "Epoch 23/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1625 - acc: 0.9425 - val_loss: 0.3626 - val_acc: 0.8850\n",
      "Epoch 24/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 138s - loss: 0.1614 - acc: 0.9405 - val_loss: 0.3587 - val_acc: 0.8785\n",
      "Epoch 25/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1609 - acc: 0.9447 - val_loss: 0.3396 - val_acc: 0.9024\n",
      "Epoch 26/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1577 - acc: 0.9431 - val_loss: 0.2707 - val_acc: 0.9089\n",
      "Epoch 27/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1497 - acc: 0.9479 - val_loss: 0.3412 - val_acc: 0.8829\n",
      "Epoch 28/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1484 - acc: 0.9484 - val_loss: 0.3992 - val_acc: 0.8742\n",
      "Epoch 29/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1480 - acc: 0.9489 - val_loss: 0.4006 - val_acc: 0.8850\n",
      "Epoch 30/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1465 - acc: 0.9492 - val_loss: 0.3110 - val_acc: 0.8937\n",
      "Epoch 31/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1475 - acc: 0.9496 - val_loss: 0.5029 - val_acc: 0.8677\n",
      "Epoch 32/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1379 - acc: 0.9517 - val_loss: 0.3672 - val_acc: 0.8850\n",
      "Epoch 33/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1294 - acc: 0.9552 - val_loss: 0.3395 - val_acc: 0.8872\n",
      "Epoch 34/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1339 - acc: 0.9525 - val_loss: 0.3681 - val_acc: 0.8937\n",
      "Epoch 35/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1335 - acc: 0.9525 - val_loss: 0.3801 - val_acc: 0.8850\n",
      "Epoch 36/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1232 - acc: 0.9585 - val_loss: 0.3307 - val_acc: 0.9132\n",
      "Epoch 37/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1403 - acc: 0.9556 - val_loss: 0.3163 - val_acc: 0.9002\n",
      "Epoch 38/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1373 - acc: 0.9544 - val_loss: 0.4162 - val_acc: 0.8959\n",
      "Epoch 39/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.1225 - acc: 0.9563 - val_loss: 0.2463 - val_acc: 0.9219\n",
      "Epoch 40/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1194 - acc: 0.9599 - val_loss: 0.4321 - val_acc: 0.9111\n",
      "Epoch 41/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 135s - loss: 0.1074 - acc: 0.9629 - val_loss: 0.2578 - val_acc: 0.9046\n",
      "Epoch 42/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1202 - acc: 0.9599 - val_loss: 0.3715 - val_acc: 0.9111\n",
      "Epoch 43/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1149 - acc: 0.9597 - val_loss: 0.3403 - val_acc: 0.9111\n",
      "Epoch 44/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1139 - acc: 0.9592 - val_loss: 0.4348 - val_acc: 0.8872\n",
      "Epoch 45/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1087 - acc: 0.9602 - val_loss: 0.4016 - val_acc: 0.9154\n",
      "Epoch 46/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0957 - acc: 0.9677 - val_loss: 0.3418 - val_acc: 0.8894\n",
      "Epoch 47/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.1016 - acc: 0.9647 - val_loss: 0.2828 - val_acc: 0.9262\n",
      "Epoch 48/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1032 - acc: 0.9648 - val_loss: 0.3239 - val_acc: 0.9089\n",
      "Epoch 49/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1021 - acc: 0.9651 - val_loss: 0.2548 - val_acc: 0.9176\n",
      "Epoch 50/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0979 - acc: 0.9636 - val_loss: 0.3606 - val_acc: 0.9024\n",
      "Epoch 51/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.1071 - acc: 0.9632 - val_loss: 0.3571 - val_acc: 0.8915\n",
      "Epoch 52/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0914 - acc: 0.9694 - val_loss: 0.2731 - val_acc: 0.9132\n",
      "Epoch 53/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0975 - acc: 0.9672 - val_loss: 0.4071 - val_acc: 0.9002\n",
      "Epoch 54/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1060 - acc: 0.9648 - val_loss: 0.3059 - val_acc: 0.9111\n",
      "Epoch 55/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.0950 - acc: 0.9690 - val_loss: 0.3671 - val_acc: 0.9002\n",
      "Epoch 56/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.1081 - acc: 0.9631 - val_loss: 0.3214 - val_acc: 0.9046\n",
      "Epoch 57/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.0904 - acc: 0.9696 - val_loss: 0.2189 - val_acc: 0.9284\n",
      "Epoch 58/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.0943 - acc: 0.9689 - val_loss: 0.1915 - val_acc: 0.9393\n",
      "Epoch 59/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 135s - loss: 0.0888 - acc: 0.9713 - val_loss: 0.3603 - val_acc: 0.9111\n",
      "Epoch 60/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.0902 - acc: 0.9692 - val_loss: 0.3742 - val_acc: 0.9024\n",
      "Epoch 61/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0880 - acc: 0.9714 - val_loss: 0.3483 - val_acc: 0.9067\n",
      "Epoch 62/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0806 - acc: 0.9731 - val_loss: 0.3111 - val_acc: 0.9219\n",
      "Epoch 63/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0963 - acc: 0.9657 - val_loss: 0.4169 - val_acc: 0.9067\n",
      "Epoch 64/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0864 - acc: 0.9699 - val_loss: 0.3300 - val_acc: 0.9154\n",
      "Epoch 65/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0799 - acc: 0.9737 - val_loss: 0.2340 - val_acc: 0.9219\n",
      "Epoch 66/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0834 - acc: 0.9729 - val_loss: 0.3192 - val_acc: 0.9089\n",
      "Epoch 67/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 138s - loss: 0.0959 - acc: 0.9665 - val_loss: 0.2684 - val_acc: 0.9197\n",
      "Epoch 68/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0744 - acc: 0.9748 - val_loss: 0.3721 - val_acc: 0.8829\n",
      "Epoch 69/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0811 - acc: 0.9723 - val_loss: 0.2170 - val_acc: 0.9262\n",
      "Epoch 70/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0749 - acc: 0.9755 - val_loss: 0.4693 - val_acc: 0.8959\n",
      "Epoch 71/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.0768 - acc: 0.9739 - val_loss: 0.3740 - val_acc: 0.9154\n",
      "Epoch 72/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 136s - loss: 0.0742 - acc: 0.9753 - val_loss: 0.3080 - val_acc: 0.9241\n",
      "Epoch 73/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 135s - loss: 0.0787 - acc: 0.9748 - val_loss: 0.2734 - val_acc: 0.9176\n",
      "Epoch 74/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 135s - loss: 0.0772 - acc: 0.9716 - val_loss: 0.2870 - val_acc: 0.9284\n",
      "Epoch 75/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 134s - loss: 0.0876 - acc: 0.9704 - val_loss: 0.2123 - val_acc: 0.9371\n",
      "Epoch 76/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 133s - loss: 0.0687 - acc: 0.9748 - val_loss: 0.3120 - val_acc: 0.9241\n",
      "Epoch 77/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 133s - loss: 0.0707 - acc: 0.9774 - val_loss: 0.4614 - val_acc: 0.9067\n",
      "Epoch 78/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 131s - loss: 0.0748 - acc: 0.9739 - val_loss: 0.2710 - val_acc: 0.9306\n",
      "Epoch 79/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 131s - loss: 0.0724 - acc: 0.9759 - val_loss: 0.3019 - val_acc: 0.9262\n",
      "Epoch 80/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 134s - loss: 0.0712 - acc: 0.9764 - val_loss: 0.2654 - val_acc: 0.9241\n",
      "Epoch 81/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 135s - loss: 0.0726 - acc: 0.9745 - val_loss: 0.1893 - val_acc: 0.9328\n",
      "Epoch 82/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0839 - acc: 0.9726 - val_loss: 0.2876 - val_acc: 0.9154\n",
      "Epoch 83/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0717 - acc: 0.9758 - val_loss: 0.3426 - val_acc: 0.9241\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol       1.00      1.00      1.00        20\n",
      "   altocumulos       0.91      0.91      0.91        88\n",
      "   altostratos       0.85      0.97      0.91        36\n",
      "cieloDespejado       0.88      1.00      0.94        23\n",
      "  cirrocumulos       0.95      1.00      0.98        21\n",
      "        cirros       0.96      0.93      0.94       145\n",
      "  cirrostratos       0.94      0.94      0.94        68\n",
      "       cumulos       0.97      0.97      0.97        71\n",
      "estratocumulos       0.95      0.95      0.95       142\n",
      "      estratos       0.94      0.94      0.94       108\n",
      "     multinube       0.93      0.90      0.92       189\n",
      "  nimbostratos       0.77      0.83      0.80        12\n",
      "\n",
      "      accuracy                           0.94       923\n",
      "     macro avg       0.92      0.95      0.93       923\n",
      "  weighted avg       0.94      0.94      0.94       923\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFMCAYAAABF3IJ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "model_name = 'inceptionv3'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "                        make_prebuilt(InceptionV3,.1), model_name=model_name,\n",
    "                        model_dir=model_dir)\n",
    "\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sm2ymAK6lARk",
    "colab_type": "text"
   },
   "source": [
    "#### Inception ResNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "3eI7W--6kc1A",
    "colab_type": "code",
    "outputId": "94e89a9f-409a-4cef-b48d-42e3364c843e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.579823588105E12,
     "user_tz": -60.0,
     "elapsed": 1.106696E7,
     "user": {
      "displayName": "JAVIER HUERTAS TATO",
      "photoUrl": "",
      "userId": "11051141025262866472"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking train and validation tests\n",
      "Building the network\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 8s 0us/step\n",
      "Compiling the network\n",
      "Layers: 788\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 127, 127, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 127, 127, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 127, 127, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 125, 125, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 125, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 125, 125, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 125, 125, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 125, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 125, 125, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 62, 62, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 62, 62, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 62, 62, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 62, 62, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 60, 60, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 60, 60, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 60, 60, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 29, 29, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 29, 29, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 29, 29, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 29, 29, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 29, 29, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 29, 29, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 29, 29, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 29, 29, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 29, 29, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 29, 29, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 29, 29, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 29, 29, 96)   18432       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 29, 29, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 29, 29, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 29, 29, 64)   12288       average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 29, 29, 96)   288         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 29, 29, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 29, 29, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 29, 29, 64)   192         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 29, 29, 96)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 29, 29, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 29, 29, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 29, 29, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed_5b (Concatenate)          (None, 29, 29, 320)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 29, 29, 32)   10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 29, 29, 32)   96          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 29, 29, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 29, 29, 32)   10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 29, 29, 48)   13824       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 29, 29, 32)   96          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 29, 29, 48)   144         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 29, 29, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 29, 29, 48)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 29, 29, 32)   10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 29, 29, 32)   9216        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 29, 29, 64)   27648       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 29, 29, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 29, 29, 32)   96          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 29, 29, 64)   192         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 29, 29, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 29, 29, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 29, 29, 64)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_mixed (Concatenate)   (None, 29, 29, 128)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_conv (Conv2D)         (None, 29, 29, 320)  41280       block35_1_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_1 (Lambda)              (None, 29, 29, 320)  0           mixed_5b[0][0]                   \n",
      "                                                                 block35_1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_ac (Activation)       (None, 29, 29, 320)  0           block35_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 29, 29, 32)   10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 29, 29, 32)   96          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 29, 29, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 29, 29, 32)   10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 29, 29, 48)   13824       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 29, 29, 32)   96          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 29, 29, 48)   144         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 29, 29, 32)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 29, 29, 48)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 29, 29, 32)   10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 29, 29, 32)   9216        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 29, 29, 64)   27648       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 29, 29, 32)   96          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 29, 29, 32)   96          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 29, 29, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 29, 29, 32)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 29, 29, 32)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 29, 29, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_mixed (Concatenate)   (None, 29, 29, 128)  0           activation_18[0][0]              \n",
      "                                                                 activation_20[0][0]              \n",
      "                                                                 activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_conv (Conv2D)         (None, 29, 29, 320)  41280       block35_2_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_2 (Lambda)              (None, 29, 29, 320)  0           block35_1_ac[0][0]               \n",
      "                                                                 block35_2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_ac (Activation)       (None, 29, 29, 320)  0           block35_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 29, 29, 32)   10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 29, 29, 32)   96          conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 29, 29, 32)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 29, 29, 32)   10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 29, 29, 48)   13824       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 29, 29, 32)   96          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 29, 29, 48)   144         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 29, 29, 32)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 29, 29, 48)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 29, 29, 32)   10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 29, 29, 32)   9216        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 29, 29, 64)   27648       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 29, 29, 32)   96          conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 29, 29, 32)   96          conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 29, 29, 64)   192         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 29, 29, 32)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 29, 29, 32)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 29, 29, 64)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_mixed (Concatenate)   (None, 29, 29, 128)  0           activation_24[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_conv (Conv2D)         (None, 29, 29, 320)  41280       block35_3_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_3 (Lambda)              (None, 29, 29, 320)  0           block35_2_ac[0][0]               \n",
      "                                                                 block35_3_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_ac (Activation)       (None, 29, 29, 320)  0           block35_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 29, 29, 32)   10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 29, 29, 32)   96          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 29, 29, 32)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 29, 29, 32)   10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 29, 29, 48)   13824       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 29, 29, 32)   96          conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 29, 29, 48)   144         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 29, 29, 32)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 29, 29, 48)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 29, 29, 32)   10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 29, 29, 32)   9216        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 29, 29, 64)   27648       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 29, 29, 32)   96          conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 29, 29, 32)   96          conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 29, 29, 64)   192         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 29, 29, 32)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 29, 29, 32)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 29, 29, 64)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_mixed (Concatenate)   (None, 29, 29, 128)  0           activation_30[0][0]              \n",
      "                                                                 activation_32[0][0]              \n",
      "                                                                 activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_conv (Conv2D)         (None, 29, 29, 320)  41280       block35_4_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_4 (Lambda)              (None, 29, 29, 320)  0           block35_3_ac[0][0]               \n",
      "                                                                 block35_4_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_ac (Activation)       (None, 29, 29, 320)  0           block35_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 29, 29, 32)   10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 29, 29, 32)   96          conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 29, 29, 32)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 29, 29, 32)   10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 29, 29, 48)   13824       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 29, 29, 32)   96          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 29, 29, 48)   144         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 29, 29, 32)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 29, 29, 48)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 29, 29, 32)   10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 29, 29, 32)   9216        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 29, 29, 64)   27648       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 29, 29, 32)   96          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 29, 29, 32)   96          conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 29, 29, 64)   192         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 29, 29, 32)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 29, 29, 32)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 29, 29, 64)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_mixed (Concatenate)   (None, 29, 29, 128)  0           activation_36[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_conv (Conv2D)         (None, 29, 29, 320)  41280       block35_5_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_5 (Lambda)              (None, 29, 29, 320)  0           block35_4_ac[0][0]               \n",
      "                                                                 block35_5_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_ac (Activation)       (None, 29, 29, 320)  0           block35_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 29, 29, 32)   10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 29, 29, 32)   96          conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 29, 29, 32)   0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 29, 29, 32)   10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 29, 29, 48)   13824       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 29, 29, 32)   96          conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 29, 29, 48)   144         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 29, 29, 32)   0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 29, 29, 48)   0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 29, 29, 32)   10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 29, 29, 32)   9216        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 29, 29, 64)   27648       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 29, 29, 32)   96          conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 29, 29, 32)   96          conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 29, 29, 64)   192         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 29, 29, 32)   0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 29, 29, 32)   0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 29, 29, 64)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_mixed (Concatenate)   (None, 29, 29, 128)  0           activation_42[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_conv (Conv2D)         (None, 29, 29, 320)  41280       block35_6_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_6 (Lambda)              (None, 29, 29, 320)  0           block35_5_ac[0][0]               \n",
      "                                                                 block35_6_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_ac (Activation)       (None, 29, 29, 320)  0           block35_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 29, 29, 32)   10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 29, 29, 32)   96          conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 29, 29, 32)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 29, 29, 32)   10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 29, 29, 48)   13824       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 29, 29, 32)   96          conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 29, 29, 48)   144         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 29, 29, 32)   0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 29, 29, 48)   0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 29, 29, 32)   10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 29, 29, 32)   9216        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 29, 29, 64)   27648       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 29, 29, 32)   96          conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 29, 29, 32)   96          conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 29, 29, 64)   192         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 29, 29, 32)   0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 29, 29, 32)   0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 29, 29, 64)   0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_mixed (Concatenate)   (None, 29, 29, 128)  0           activation_48[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_conv (Conv2D)         (None, 29, 29, 320)  41280       block35_7_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_7 (Lambda)              (None, 29, 29, 320)  0           block35_6_ac[0][0]               \n",
      "                                                                 block35_7_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_ac (Activation)       (None, 29, 29, 320)  0           block35_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 29, 29, 32)   10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 29, 29, 32)   96          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 29, 29, 32)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 29, 29, 32)   10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 29, 29, 48)   13824       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 29, 29, 32)   96          conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 29, 29, 48)   144         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 29, 29, 32)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 29, 29, 48)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 29, 29, 32)   10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 29, 29, 32)   9216        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 29, 29, 64)   27648       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 29, 29, 32)   96          conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 29, 29, 32)   96          conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 29, 29, 64)   192         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 29, 29, 32)   0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 29, 29, 32)   0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 29, 29, 64)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_mixed (Concatenate)   (None, 29, 29, 128)  0           activation_54[0][0]              \n",
      "                                                                 activation_56[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_conv (Conv2D)         (None, 29, 29, 320)  41280       block35_8_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_8 (Lambda)              (None, 29, 29, 320)  0           block35_7_ac[0][0]               \n",
      "                                                                 block35_8_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_ac (Activation)       (None, 29, 29, 320)  0           block35_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 29, 29, 32)   10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 29, 29, 32)   96          conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 29, 29, 32)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 29, 29, 32)   10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 29, 29, 48)   13824       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 29, 29, 32)   96          conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 29, 29, 48)   144         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 29, 29, 32)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 29, 29, 48)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 29, 29, 32)   10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 29, 29, 32)   9216        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 29, 29, 64)   27648       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 29, 29, 32)   96          conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 29, 29, 32)   96          conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 29, 29, 64)   192         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 29, 29, 32)   0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 29, 29, 32)   0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 29, 29, 64)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_mixed (Concatenate)   (None, 29, 29, 128)  0           activation_60[0][0]              \n",
      "                                                                 activation_62[0][0]              \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_conv (Conv2D)         (None, 29, 29, 320)  41280       block35_9_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_9 (Lambda)              (None, 29, 29, 320)  0           block35_8_ac[0][0]               \n",
      "                                                                 block35_9_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_ac (Activation)       (None, 29, 29, 320)  0           block35_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 29, 29, 32)   10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 29, 29, 32)   96          conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 29, 29, 32)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 29, 29, 32)   10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 29, 29, 48)   13824       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 29, 29, 32)   96          conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 29, 29, 48)   144         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 29, 29, 32)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 29, 29, 48)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 29, 29, 32)   10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 29, 29, 32)   9216        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 29, 29, 64)   27648       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 29, 29, 32)   96          conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 29, 29, 32)   96          conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 29, 29, 64)   192         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 29, 29, 32)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 29, 29, 32)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 29, 29, 64)   0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_mixed (Concatenate)  (None, 29, 29, 128)  0           activation_66[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_conv (Conv2D)        (None, 29, 29, 320)  41280       block35_10_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block35_10 (Lambda)             (None, 29, 29, 320)  0           block35_9_ac[0][0]               \n",
      "                                                                 block35_10_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_ac (Activation)      (None, 29, 29, 320)  0           block35_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 29, 29, 256)  81920       block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 29, 29, 256)  768         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 29, 29, 256)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 29, 29, 256)  589824      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 29, 29, 256)  768         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 29, 29, 256)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 14, 14, 384)  1105920     block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 14, 14, 384)  884736      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 14, 14, 384)  1152        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 14, 14, 384)  1152        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 384)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 14, 14, 384)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 320)  0           block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mixed_6a (Concatenate)          (None, 14, 14, 1088) 0           activation_72[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 14, 14, 128)  139264      mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 14, 14, 128)  384         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 14, 14, 128)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 14, 14, 160)  143360      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 14, 14, 160)  480         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 14, 14, 160)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 14, 14, 192)  208896      mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 14, 14, 192)  215040      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 14, 14, 192)  576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 14, 14, 192)  576         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 14, 14, 192)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 14, 14, 192)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_mixed (Concatenate)   (None, 14, 14, 384)  0           activation_76[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_conv (Conv2D)         (None, 14, 14, 1088) 418880      block17_1_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_1 (Lambda)              (None, 14, 14, 1088) 0           mixed_6a[0][0]                   \n",
      "                                                                 block17_1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_ac (Activation)       (None, 14, 14, 1088) 0           block17_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 14, 14, 128)  139264      block17_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 14, 14, 128)  384         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 14, 14, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 14, 14, 160)  143360      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 14, 14, 160)  480         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 14, 14, 160)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 14, 14, 192)  208896      block17_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 14, 14, 192)  215040      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 14, 14, 192)  576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 14, 14, 192)  576         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 14, 14, 192)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 14, 14, 192)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_mixed (Concatenate)   (None, 14, 14, 384)  0           activation_80[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_conv (Conv2D)         (None, 14, 14, 1088) 418880      block17_2_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_2 (Lambda)              (None, 14, 14, 1088) 0           block17_1_ac[0][0]               \n",
      "                                                                 block17_2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_ac (Activation)       (None, 14, 14, 1088) 0           block17_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 14, 14, 128)  139264      block17_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 14, 14, 128)  384         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 14, 14, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 14, 14, 160)  143360      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 14, 14, 160)  480         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 14, 14, 160)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 14, 14, 192)  208896      block17_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 14, 14, 192)  215040      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 14, 14, 192)  576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 14, 14, 192)  576         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 14, 14, 192)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 14, 14, 192)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_mixed (Concatenate)   (None, 14, 14, 384)  0           activation_84[0][0]              \n",
      "                                                                 activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_conv (Conv2D)         (None, 14, 14, 1088) 418880      block17_3_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_3 (Lambda)              (None, 14, 14, 1088) 0           block17_2_ac[0][0]               \n",
      "                                                                 block17_3_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_ac (Activation)       (None, 14, 14, 1088) 0           block17_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 14, 14, 128)  139264      block17_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 14, 14, 128)  384         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 14, 14, 128)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 14, 14, 160)  143360      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 14, 14, 160)  480         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 14, 14, 160)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 14, 14, 192)  208896      block17_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 14, 14, 192)  215040      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 14, 14, 192)  576         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 14, 14, 192)  576         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 14, 14, 192)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 14, 14, 192)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_mixed (Concatenate)   (None, 14, 14, 384)  0           activation_88[0][0]              \n",
      "                                                                 activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_conv (Conv2D)         (None, 14, 14, 1088) 418880      block17_4_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_4 (Lambda)              (None, 14, 14, 1088) 0           block17_3_ac[0][0]               \n",
      "                                                                 block17_4_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_ac (Activation)       (None, 14, 14, 1088) 0           block17_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 14, 14, 128)  139264      block17_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 14, 14, 128)  384         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 14, 14, 128)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 14, 14, 160)  143360      activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 14, 14, 160)  480         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 14, 14, 160)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 14, 14, 192)  208896      block17_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 14, 14, 192)  215040      activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 14, 14, 192)  576         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 14, 14, 192)  576         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 14, 14, 192)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 14, 14, 192)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_mixed (Concatenate)   (None, 14, 14, 384)  0           activation_92[0][0]              \n",
      "                                                                 activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_conv (Conv2D)         (None, 14, 14, 1088) 418880      block17_5_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_5 (Lambda)              (None, 14, 14, 1088) 0           block17_4_ac[0][0]               \n",
      "                                                                 block17_5_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_ac (Activation)       (None, 14, 14, 1088) 0           block17_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 14, 14, 128)  139264      block17_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 14, 14, 128)  384         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 14, 14, 128)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 14, 14, 160)  143360      activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 14, 14, 160)  480         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 14, 14, 160)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 14, 14, 192)  208896      block17_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 14, 14, 192)  215040      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 14, 14, 192)  576         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 14, 14, 192)  576         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 14, 14, 192)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 14, 14, 192)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_mixed (Concatenate)   (None, 14, 14, 384)  0           activation_96[0][0]              \n",
      "                                                                 activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_conv (Conv2D)         (None, 14, 14, 1088) 418880      block17_6_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_6 (Lambda)              (None, 14, 14, 1088) 0           block17_5_ac[0][0]               \n",
      "                                                                 block17_6_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_ac (Activation)       (None, 14, 14, 1088) 0           block17_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 14, 14, 128)  139264      block17_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 14, 14, 128)  384         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 14, 14, 128)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 14, 14, 160)  143360      activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 14, 14, 160)  480         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 14, 14, 160)  0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 14, 14, 192)  208896      block17_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 14, 14, 192)  215040      activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 14, 14, 192)  576         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 14, 14, 192)  576         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 14, 14, 192)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 14, 14, 192)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_mixed (Concatenate)   (None, 14, 14, 384)  0           activation_100[0][0]             \n",
      "                                                                 activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_conv (Conv2D)         (None, 14, 14, 1088) 418880      block17_7_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_7 (Lambda)              (None, 14, 14, 1088) 0           block17_6_ac[0][0]               \n",
      "                                                                 block17_7_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_ac (Activation)       (None, 14, 14, 1088) 0           block17_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 14, 14, 128)  139264      block17_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 14, 14, 128)  384         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 14, 14, 128)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 14, 14, 160)  143360      activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 14, 14, 160)  480         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 14, 14, 160)  0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 14, 14, 192)  208896      block17_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 14, 14, 192)  215040      activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 14, 14, 192)  576         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 14, 14, 192)  576         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 14, 14, 192)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 14, 14, 192)  0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_mixed (Concatenate)   (None, 14, 14, 384)  0           activation_104[0][0]             \n",
      "                                                                 activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_conv (Conv2D)         (None, 14, 14, 1088) 418880      block17_8_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_8 (Lambda)              (None, 14, 14, 1088) 0           block17_7_ac[0][0]               \n",
      "                                                                 block17_8_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_ac (Activation)       (None, 14, 14, 1088) 0           block17_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 14, 14, 128)  139264      block17_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 14, 14, 128)  384         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 14, 14, 128)  0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 14, 14, 160)  143360      activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 14, 14, 160)  480         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 14, 14, 160)  0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 14, 14, 192)  208896      block17_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 14, 14, 192)  215040      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 14, 14, 192)  576         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 14, 14, 192)  576         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 14, 14, 192)  0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 14, 14, 192)  0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_mixed (Concatenate)   (None, 14, 14, 384)  0           activation_108[0][0]             \n",
      "                                                                 activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_conv (Conv2D)         (None, 14, 14, 1088) 418880      block17_9_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_9 (Lambda)              (None, 14, 14, 1088) 0           block17_8_ac[0][0]               \n",
      "                                                                 block17_9_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_ac (Activation)       (None, 14, 14, 1088) 0           block17_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 14, 14, 128)  139264      block17_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 14, 14, 128)  384         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 14, 14, 128)  0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 14, 14, 160)  143360      activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 14, 14, 160)  480         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 14, 14, 160)  0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 14, 14, 192)  208896      block17_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 14, 14, 192)  215040      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 14, 14, 192)  576         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 14, 14, 192)  576         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 14, 14, 192)  0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 14, 14, 192)  0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_112[0][0]             \n",
      "                                                                 activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_10_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_10 (Lambda)             (None, 14, 14, 1088) 0           block17_9_ac[0][0]               \n",
      "                                                                 block17_10_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_ac (Activation)      (None, 14, 14, 1088) 0           block17_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 14, 14, 128)  139264      block17_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 14, 14, 128)  384         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 14, 14, 128)  0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 14, 14, 160)  143360      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 14, 14, 160)  480         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 14, 14, 160)  0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 14, 14, 192)  208896      block17_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 14, 14, 192)  215040      activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 14, 14, 192)  576         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 14, 14, 192)  576         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 14, 14, 192)  0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 14, 14, 192)  0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_116[0][0]             \n",
      "                                                                 activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_11_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_11 (Lambda)             (None, 14, 14, 1088) 0           block17_10_ac[0][0]              \n",
      "                                                                 block17_11_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_ac (Activation)      (None, 14, 14, 1088) 0           block17_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 14, 14, 128)  139264      block17_11_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 14, 14, 128)  384         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 14, 14, 128)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 14, 14, 160)  143360      activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 14, 14, 160)  480         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 14, 14, 160)  0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 14, 14, 192)  208896      block17_11_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 14, 14, 192)  215040      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 14, 14, 192)  576         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 14, 14, 192)  576         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 14, 14, 192)  0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 14, 14, 192)  0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_120[0][0]             \n",
      "                                                                 activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_12_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_12 (Lambda)             (None, 14, 14, 1088) 0           block17_11_ac[0][0]              \n",
      "                                                                 block17_12_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_ac (Activation)      (None, 14, 14, 1088) 0           block17_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 14, 14, 128)  139264      block17_12_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 14, 14, 128)  384         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 14, 14, 128)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 14, 14, 160)  143360      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 14, 14, 160)  480         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 14, 14, 160)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 14, 14, 192)  208896      block17_12_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 14, 14, 192)  215040      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 14, 14, 192)  576         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 14, 14, 192)  576         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 14, 14, 192)  0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 14, 14, 192)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_124[0][0]             \n",
      "                                                                 activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_13_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_13 (Lambda)             (None, 14, 14, 1088) 0           block17_12_ac[0][0]              \n",
      "                                                                 block17_13_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_ac (Activation)      (None, 14, 14, 1088) 0           block17_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 14, 14, 128)  139264      block17_13_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 14, 14, 128)  384         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 14, 14, 128)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 14, 14, 160)  143360      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 14, 14, 160)  480         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 14, 14, 160)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 14, 14, 192)  208896      block17_13_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 14, 14, 192)  215040      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 14, 14, 192)  576         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 14, 14, 192)  576         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 14, 14, 192)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 14, 14, 192)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_128[0][0]             \n",
      "                                                                 activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_14_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_14 (Lambda)             (None, 14, 14, 1088) 0           block17_13_ac[0][0]              \n",
      "                                                                 block17_14_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_ac (Activation)      (None, 14, 14, 1088) 0           block17_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 14, 14, 128)  139264      block17_14_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 14, 14, 128)  384         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 14, 14, 128)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 14, 14, 160)  143360      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 14, 14, 160)  480         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 14, 14, 160)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 14, 14, 192)  208896      block17_14_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 14, 14, 192)  215040      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 14, 14, 192)  576         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 14, 14, 192)  576         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 14, 14, 192)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 14, 14, 192)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_132[0][0]             \n",
      "                                                                 activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_15_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_15 (Lambda)             (None, 14, 14, 1088) 0           block17_14_ac[0][0]              \n",
      "                                                                 block17_15_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_ac (Activation)      (None, 14, 14, 1088) 0           block17_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 14, 14, 128)  139264      block17_15_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 14, 14, 128)  384         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 14, 14, 128)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 14, 14, 160)  143360      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 14, 14, 160)  480         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 14, 14, 160)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 14, 14, 192)  208896      block17_15_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 14, 14, 192)  215040      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 14, 14, 192)  576         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 14, 14, 192)  576         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 14, 14, 192)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 14, 14, 192)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_136[0][0]             \n",
      "                                                                 activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_16_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_16 (Lambda)             (None, 14, 14, 1088) 0           block17_15_ac[0][0]              \n",
      "                                                                 block17_16_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_ac (Activation)      (None, 14, 14, 1088) 0           block17_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 14, 14, 128)  139264      block17_16_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 14, 14, 128)  384         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 14, 14, 128)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 14, 14, 160)  143360      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 14, 14, 160)  480         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 14, 14, 160)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 14, 14, 192)  208896      block17_16_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 14, 14, 192)  215040      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 14, 14, 192)  576         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 14, 14, 192)  576         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 14, 14, 192)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 14, 14, 192)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_140[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_17_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_17 (Lambda)             (None, 14, 14, 1088) 0           block17_16_ac[0][0]              \n",
      "                                                                 block17_17_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_ac (Activation)      (None, 14, 14, 1088) 0           block17_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 14, 14, 128)  139264      block17_17_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 14, 14, 128)  384         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 14, 14, 128)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 14, 14, 160)  143360      activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 14, 14, 160)  480         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 14, 14, 160)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 14, 14, 192)  208896      block17_17_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 14, 14, 192)  215040      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 14, 14, 192)  576         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 14, 14, 192)  576         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 14, 14, 192)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 14, 14, 192)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_144[0][0]             \n",
      "                                                                 activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_18_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_18 (Lambda)             (None, 14, 14, 1088) 0           block17_17_ac[0][0]              \n",
      "                                                                 block17_18_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_ac (Activation)      (None, 14, 14, 1088) 0           block17_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 14, 14, 128)  139264      block17_18_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 14, 14, 128)  384         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 14, 14, 128)  0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 14, 14, 160)  143360      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 14, 14, 160)  480         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 14, 14, 160)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 14, 14, 192)  208896      block17_18_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 14, 14, 192)  215040      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 14, 14, 192)  576         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 14, 14, 192)  576         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 14, 14, 192)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 14, 14, 192)  0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_148[0][0]             \n",
      "                                                                 activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_19_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_19 (Lambda)             (None, 14, 14, 1088) 0           block17_18_ac[0][0]              \n",
      "                                                                 block17_19_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_ac (Activation)      (None, 14, 14, 1088) 0           block17_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 14, 14, 128)  139264      block17_19_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 14, 14, 128)  384         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 14, 14, 128)  0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 14, 14, 160)  143360      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 14, 14, 160)  480         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 14, 14, 160)  0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 14, 14, 192)  208896      block17_19_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 14, 14, 192)  215040      activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 14, 14, 192)  576         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 14, 14, 192)  576         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 14, 14, 192)  0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 14, 14, 192)  0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_mixed (Concatenate)  (None, 14, 14, 384)  0           activation_152[0][0]             \n",
      "                                                                 activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_conv (Conv2D)        (None, 14, 14, 1088) 418880      block17_20_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_20 (Lambda)             (None, 14, 14, 1088) 0           block17_19_ac[0][0]              \n",
      "                                                                 block17_20_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_ac (Activation)      (None, 14, 14, 1088) 0           block17_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 14, 14, 256)  278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 14, 14, 256)  768         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 14, 14, 256)  0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 14, 14, 256)  278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 14, 14, 256)  278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 14, 14, 288)  663552      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 14, 14, 256)  768         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 14, 14, 256)  768         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 14, 14, 288)  864         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 14, 14, 256)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 14, 14, 256)  0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 14, 14, 288)  0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 6, 6, 384)    884736      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 6, 6, 288)    663552      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 6, 6, 320)    829440      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 6, 6, 384)    1152        conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 6, 6, 288)    864         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 6, 6, 320)    960         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 6, 6, 384)    0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 6, 6, 288)    0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 6, 6, 320)    0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 1088)   0           block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mixed_7a (Concatenate)          (None, 6, 6, 2080)   0           activation_157[0][0]             \n",
      "                                                                 activation_159[0][0]             \n",
      "                                                                 activation_162[0][0]             \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 6, 6, 192)    399360      mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 6, 6, 192)    576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 6, 6, 192)    0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 6, 6, 224)    129024      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 6, 6, 224)    672         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 6, 6, 224)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 6, 6, 192)    399360      mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 6, 6, 256)    172032      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 6, 6, 192)    576         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 6, 6, 256)    768         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 6, 6, 192)    0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 6, 6, 256)    0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_mixed (Concatenate)    (None, 6, 6, 448)    0           activation_163[0][0]             \n",
      "                                                                 activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_conv (Conv2D)          (None, 6, 6, 2080)   933920      block8_1_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_1 (Lambda)               (None, 6, 6, 2080)   0           mixed_7a[0][0]                   \n",
      "                                                                 block8_1_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_ac (Activation)        (None, 6, 6, 2080)   0           block8_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 6, 6, 192)    399360      block8_1_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 6, 6, 192)    576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 6, 6, 192)    0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 6, 6, 224)    129024      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 6, 6, 224)    672         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 6, 6, 224)    0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 6, 6, 192)    399360      block8_1_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 6, 6, 256)    172032      activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 6, 6, 192)    576         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 6, 6, 256)    768         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 6, 6, 192)    0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 6, 6, 256)    0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_mixed (Concatenate)    (None, 6, 6, 448)    0           activation_167[0][0]             \n",
      "                                                                 activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_conv (Conv2D)          (None, 6, 6, 2080)   933920      block8_2_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_2 (Lambda)               (None, 6, 6, 2080)   0           block8_1_ac[0][0]                \n",
      "                                                                 block8_2_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_ac (Activation)        (None, 6, 6, 2080)   0           block8_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 6, 6, 192)    399360      block8_2_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 6, 6, 192)    576         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 6, 6, 192)    0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 6, 6, 224)    129024      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 6, 6, 224)    672         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 6, 6, 224)    0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 6, 6, 192)    399360      block8_2_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 6, 6, 256)    172032      activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 6, 6, 192)    576         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 6, 6, 256)    768         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 6, 6, 192)    0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 6, 6, 256)    0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_mixed (Concatenate)    (None, 6, 6, 448)    0           activation_171[0][0]             \n",
      "                                                                 activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_conv (Conv2D)          (None, 6, 6, 2080)   933920      block8_3_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_3 (Lambda)               (None, 6, 6, 2080)   0           block8_2_ac[0][0]                \n",
      "                                                                 block8_3_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_ac (Activation)        (None, 6, 6, 2080)   0           block8_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 6, 6, 192)    399360      block8_3_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 6, 6, 192)    576         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 6, 6, 192)    0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 6, 6, 224)    129024      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 6, 6, 224)    672         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 6, 6, 224)    0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 6, 6, 192)    399360      block8_3_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 6, 6, 256)    172032      activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 6, 6, 192)    576         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 6, 6, 256)    768         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 6, 6, 192)    0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 6, 6, 256)    0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_mixed (Concatenate)    (None, 6, 6, 448)    0           activation_175[0][0]             \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_conv (Conv2D)          (None, 6, 6, 2080)   933920      block8_4_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_4 (Lambda)               (None, 6, 6, 2080)   0           block8_3_ac[0][0]                \n",
      "                                                                 block8_4_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_ac (Activation)        (None, 6, 6, 2080)   0           block8_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 6, 6, 192)    399360      block8_4_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 6, 6, 192)    576         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 6, 6, 192)    0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 6, 6, 224)    129024      activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 6, 6, 224)    672         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 6, 6, 224)    0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 6, 6, 192)    399360      block8_4_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 6, 6, 256)    172032      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 6, 6, 192)    576         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 6, 6, 256)    768         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 6, 6, 192)    0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 6, 6, 256)    0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_mixed (Concatenate)    (None, 6, 6, 448)    0           activation_179[0][0]             \n",
      "                                                                 activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_conv (Conv2D)          (None, 6, 6, 2080)   933920      block8_5_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_5 (Lambda)               (None, 6, 6, 2080)   0           block8_4_ac[0][0]                \n",
      "                                                                 block8_5_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_ac (Activation)        (None, 6, 6, 2080)   0           block8_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 6, 6, 192)    399360      block8_5_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 6, 6, 192)    576         conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 6, 6, 192)    0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 6, 6, 224)    129024      activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 6, 6, 224)    672         conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 6, 6, 224)    0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 6, 6, 192)    399360      block8_5_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 6, 6, 256)    172032      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 6, 6, 192)    576         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 6, 6, 256)    768         conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 6, 6, 192)    0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 6, 6, 256)    0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_mixed (Concatenate)    (None, 6, 6, 448)    0           activation_183[0][0]             \n",
      "                                                                 activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_conv (Conv2D)          (None, 6, 6, 2080)   933920      block8_6_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_6 (Lambda)               (None, 6, 6, 2080)   0           block8_5_ac[0][0]                \n",
      "                                                                 block8_6_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_ac (Activation)        (None, 6, 6, 2080)   0           block8_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 6, 6, 192)    399360      block8_6_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 6, 6, 192)    576         conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 6, 6, 192)    0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 6, 6, 224)    129024      activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 6, 6, 224)    672         conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 6, 6, 224)    0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 6, 6, 192)    399360      block8_6_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 6, 6, 256)    172032      activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 6, 6, 192)    576         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 6, 6, 256)    768         conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 6, 6, 192)    0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 6, 6, 256)    0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_mixed (Concatenate)    (None, 6, 6, 448)    0           activation_187[0][0]             \n",
      "                                                                 activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_conv (Conv2D)          (None, 6, 6, 2080)   933920      block8_7_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_7 (Lambda)               (None, 6, 6, 2080)   0           block8_6_ac[0][0]                \n",
      "                                                                 block8_7_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_ac (Activation)        (None, 6, 6, 2080)   0           block8_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 6, 6, 192)    399360      block8_7_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 6, 6, 192)    576         conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 6, 6, 192)    0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 6, 6, 224)    129024      activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 6, 6, 224)    672         conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 6, 6, 224)    0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 6, 6, 192)    399360      block8_7_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 6, 6, 256)    172032      activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 6, 6, 192)    576         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 6, 6, 256)    768         conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 6, 6, 192)    0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 6, 6, 256)    0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_mixed (Concatenate)    (None, 6, 6, 448)    0           activation_191[0][0]             \n",
      "                                                                 activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_conv (Conv2D)          (None, 6, 6, 2080)   933920      block8_8_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_8 (Lambda)               (None, 6, 6, 2080)   0           block8_7_ac[0][0]                \n",
      "                                                                 block8_8_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_ac (Activation)        (None, 6, 6, 2080)   0           block8_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 6, 6, 192)    399360      block8_8_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 6, 6, 192)    576         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 6, 6, 192)    0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 6, 6, 224)    129024      activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 6, 6, 224)    672         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 6, 6, 224)    0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 6, 6, 192)    399360      block8_8_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 6, 6, 256)    172032      activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 6, 6, 192)    576         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 6, 6, 256)    768         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 6, 6, 192)    0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 6, 6, 256)    0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_mixed (Concatenate)    (None, 6, 6, 448)    0           activation_195[0][0]             \n",
      "                                                                 activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_conv (Conv2D)          (None, 6, 6, 2080)   933920      block8_9_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_9 (Lambda)               (None, 6, 6, 2080)   0           block8_8_ac[0][0]                \n",
      "                                                                 block8_9_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_ac (Activation)        (None, 6, 6, 2080)   0           block8_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 6, 6, 192)    399360      block8_9_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 6, 6, 192)    576         conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 6, 6, 192)    0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 6, 6, 224)    129024      activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 6, 6, 224)    672         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 6, 6, 224)    0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 6, 6, 192)    399360      block8_9_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 6, 6, 256)    172032      activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 6, 6, 192)    576         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 6, 6, 256)    768         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 6, 6, 192)    0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 6, 6, 256)    0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_10_mixed (Concatenate)   (None, 6, 6, 448)    0           activation_199[0][0]             \n",
      "                                                                 activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_10_conv (Conv2D)         (None, 6, 6, 2080)   933920      block8_10_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_10 (Lambda)              (None, 6, 6, 2080)   0           block8_9_ac[0][0]                \n",
      "                                                                 block8_10_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b (Conv2D)                (None, 6, 6, 1536)   3194880     block8_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_bn (BatchNormalization) (None, 6, 6, 1536)   4608        conv_7b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           128         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_ac (Activation)         (None, 6, 6, 1536)   0           conv_7b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1536)         0           conv_7b_ac[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1552)         0           dropout[0][0]                    \n",
      "                                                                 global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         3180544     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           24588       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 57,541,996\n",
      "Trainable params: 56,183,500\n",
      "Non-trainable params: 1,358,496\n",
      "__________________________________________________________________________________________________\n",
      "Fitting the network\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/1000\n",
      "124/123 - 529s - loss: 1.0529 - acc: 0.6398 - val_loss: 0.9952 - val_acc: 0.6920\n",
      "Epoch 2/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 149s - loss: 0.5122 - acc: 0.8229 - val_loss: 0.5492 - val_acc: 0.8113\n",
      "Epoch 3/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.4231 - acc: 0.8506 - val_loss: 0.6276 - val_acc: 0.7809\n",
      "Epoch 4/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.3537 - acc: 0.8733 - val_loss: 0.5863 - val_acc: 0.8069\n",
      "Epoch 5/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 150s - loss: 0.3228 - acc: 0.8901 - val_loss: 0.5782 - val_acc: 0.8265\n",
      "Epoch 6/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 139s - loss: 0.2867 - acc: 0.8976 - val_loss: 0.5938 - val_acc: 0.8221\n",
      "Epoch 7/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 151s - loss: 0.2752 - acc: 0.9051 - val_loss: 0.3999 - val_acc: 0.8677\n",
      "Epoch 8/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.2444 - acc: 0.9134 - val_loss: 0.4116 - val_acc: 0.8612\n",
      "Epoch 9/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 150s - loss: 0.2272 - acc: 0.9186 - val_loss: 0.3563 - val_acc: 0.8872\n",
      "Epoch 10/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.2087 - acc: 0.9280 - val_loss: 0.6211 - val_acc: 0.8395\n",
      "Epoch 11/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.2062 - acc: 0.9286 - val_loss: 0.3553 - val_acc: 0.8872\n",
      "Epoch 12/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.1997 - acc: 0.9304 - val_loss: 0.3929 - val_acc: 0.8633\n",
      "Epoch 13/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.1851 - acc: 0.9390 - val_loss: 0.3770 - val_acc: 0.8807\n",
      "Epoch 14/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.1670 - acc: 0.9413 - val_loss: 0.3461 - val_acc: 0.8872\n",
      "Epoch 15/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 151s - loss: 0.1671 - acc: 0.9409 - val_loss: 0.3325 - val_acc: 0.9089\n",
      "Epoch 16/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1694 - acc: 0.9399 - val_loss: 0.3643 - val_acc: 0.8633\n",
      "Epoch 17/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1559 - acc: 0.9463 - val_loss: 0.3750 - val_acc: 0.8915\n",
      "Epoch 18/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 151s - loss: 0.1494 - acc: 0.9487 - val_loss: 0.2655 - val_acc: 0.9154\n",
      "Epoch 19/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1412 - acc: 0.9512 - val_loss: 0.3732 - val_acc: 0.9024\n",
      "Epoch 20/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.1499 - acc: 0.9489 - val_loss: 0.5938 - val_acc: 0.8612\n",
      "Epoch 21/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.1410 - acc: 0.9501 - val_loss: 0.3454 - val_acc: 0.8894\n",
      "Epoch 22/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1427 - acc: 0.9508 - val_loss: 0.3495 - val_acc: 0.8915\n",
      "Epoch 23/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1310 - acc: 0.9527 - val_loss: 0.3542 - val_acc: 0.9024\n",
      "Epoch 24/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1304 - acc: 0.9540 - val_loss: 0.4557 - val_acc: 0.8764\n",
      "Epoch 25/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1307 - acc: 0.9556 - val_loss: 0.2719 - val_acc: 0.8959\n",
      "Epoch 26/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 151s - loss: 0.1326 - acc: 0.9560 - val_loss: 0.3271 - val_acc: 0.9349\n",
      "Epoch 27/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1263 - acc: 0.9556 - val_loss: 0.3171 - val_acc: 0.8980\n",
      "Epoch 28/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.1198 - acc: 0.9579 - val_loss: 0.3279 - val_acc: 0.9002\n",
      "Epoch 29/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1048 - acc: 0.9658 - val_loss: 0.3038 - val_acc: 0.9132\n",
      "Epoch 30/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.1009 - acc: 0.9651 - val_loss: 0.3065 - val_acc: 0.9176\n",
      "Epoch 31/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.1058 - acc: 0.9639 - val_loss: 0.3226 - val_acc: 0.9111\n",
      "Epoch 32/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1056 - acc: 0.9622 - val_loss: 0.3070 - val_acc: 0.9176\n",
      "Epoch 33/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.1123 - acc: 0.9633 - val_loss: 0.2911 - val_acc: 0.9154\n",
      "Epoch 34/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1077 - acc: 0.9642 - val_loss: 0.3540 - val_acc: 0.9154\n",
      "Epoch 35/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0925 - acc: 0.9708 - val_loss: 0.3704 - val_acc: 0.8980\n",
      "Epoch 36/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0988 - acc: 0.9663 - val_loss: 0.2930 - val_acc: 0.9154\n",
      "Epoch 37/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1034 - acc: 0.9653 - val_loss: 0.2856 - val_acc: 0.9154\n",
      "Epoch 38/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1051 - acc: 0.9657 - val_loss: 0.2894 - val_acc: 0.9111\n",
      "Epoch 39/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.1018 - acc: 0.9658 - val_loss: 0.3537 - val_acc: 0.9002\n",
      "Epoch 40/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0995 - acc: 0.9658 - val_loss: 0.5168 - val_acc: 0.8937\n",
      "Epoch 41/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.0883 - acc: 0.9708 - val_loss: 0.2984 - val_acc: 0.9241\n",
      "Epoch 42/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.0945 - acc: 0.9662 - val_loss: 0.4068 - val_acc: 0.8872\n",
      "Epoch 43/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 138s - loss: 0.0814 - acc: 0.9728 - val_loss: 0.3974 - val_acc: 0.9111\n",
      "Epoch 44/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 139s - loss: 0.0988 - acc: 0.9675 - val_loss: 0.5093 - val_acc: 0.8872\n",
      "Epoch 45/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 139s - loss: 0.0978 - acc: 0.9696 - val_loss: 0.2704 - val_acc: 0.9328\n",
      "Epoch 46/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 139s - loss: 0.0958 - acc: 0.9665 - val_loss: 0.3200 - val_acc: 0.9262\n",
      "Epoch 47/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 138s - loss: 0.0833 - acc: 0.9708 - val_loss: 0.3841 - val_acc: 0.9349\n",
      "Epoch 48/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 137s - loss: 0.0780 - acc: 0.9755 - val_loss: 0.2466 - val_acc: 0.9197\n",
      "Epoch 49/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 147s - loss: 0.0828 - acc: 0.9714 - val_loss: 0.2156 - val_acc: 0.9479\n",
      "Epoch 50/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 138s - loss: 0.0827 - acc: 0.9725 - val_loss: 0.3078 - val_acc: 0.9241\n",
      "Epoch 51/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 138s - loss: 0.0773 - acc: 0.9743 - val_loss: 0.3799 - val_acc: 0.9111\n",
      "Epoch 52/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 138s - loss: 0.0761 - acc: 0.9740 - val_loss: 0.2615 - val_acc: 0.9284\n",
      "Epoch 53/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 139s - loss: 0.0760 - acc: 0.9735 - val_loss: 0.3159 - val_acc: 0.9219\n",
      "Epoch 54/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0706 - acc: 0.9757 - val_loss: 0.3702 - val_acc: 0.9132\n",
      "Epoch 55/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0817 - acc: 0.9710 - val_loss: 0.3508 - val_acc: 0.9132\n",
      "Epoch 56/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0679 - acc: 0.9768 - val_loss: 0.2442 - val_acc: 0.9306\n",
      "Epoch 57/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0739 - acc: 0.9764 - val_loss: 0.3331 - val_acc: 0.9219\n",
      "Epoch 58/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0749 - acc: 0.9752 - val_loss: 0.2611 - val_acc: 0.9436\n",
      "Epoch 59/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.0779 - acc: 0.9737 - val_loss: 0.3040 - val_acc: 0.9393\n",
      "Epoch 60/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0702 - acc: 0.9772 - val_loss: 0.2900 - val_acc: 0.9197\n",
      "Epoch 61/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.0720 - acc: 0.9784 - val_loss: 0.3664 - val_acc: 0.9132\n",
      "Epoch 62/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0696 - acc: 0.9760 - val_loss: 0.3147 - val_acc: 0.9154\n",
      "Epoch 63/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.0736 - acc: 0.9752 - val_loss: 0.2386 - val_acc: 0.9393\n",
      "Epoch 64/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.0685 - acc: 0.9772 - val_loss: 0.2164 - val_acc: 0.9132\n",
      "Epoch 65/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0640 - acc: 0.9779 - val_loss: 0.4016 - val_acc: 0.9393\n",
      "Epoch 66/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0679 - acc: 0.9779 - val_loss: 0.2291 - val_acc: 0.9284\n",
      "Epoch 67/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0776 - acc: 0.9760 - val_loss: 0.6301 - val_acc: 0.8178\n",
      "Epoch 68/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0718 - acc: 0.9784 - val_loss: 0.2924 - val_acc: 0.9219\n",
      "Epoch 69/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 142s - loss: 0.0837 - acc: 0.9713 - val_loss: 0.3308 - val_acc: 0.9197\n",
      "Epoch 70/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0616 - acc: 0.9773 - val_loss: 0.3345 - val_acc: 0.9219\n",
      "Epoch 71/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0590 - acc: 0.9812 - val_loss: 0.2166 - val_acc: 0.9306\n",
      "Epoch 72/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.0594 - acc: 0.9798 - val_loss: 0.3486 - val_acc: 0.9241\n",
      "Epoch 73/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 141s - loss: 0.0554 - acc: 0.9801 - val_loss: 0.3532 - val_acc: 0.9219\n",
      "Epoch 74/1000\n",
      "Epoch 1/1000\n",
      "124/123 - 140s - loss: 0.0540 - acc: 0.9829 - val_loss: 0.3070 - val_acc: 0.9284\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol       1.00      1.00      1.00        20\n",
      "   altocumulos       0.95      0.90      0.92        88\n",
      "   altostratos       0.92      0.94      0.93        36\n",
      "cieloDespejado       1.00      1.00      1.00        23\n",
      "  cirrocumulos       1.00      1.00      1.00        21\n",
      "        cirros       0.95      0.93      0.94       145\n",
      "  cirrostratos       0.97      0.99      0.98        68\n",
      "       cumulos       0.97      0.94      0.96        71\n",
      "estratocumulos       0.91      0.95      0.93       142\n",
      "      estratos       0.93      0.91      0.92       108\n",
      "     multinube       0.88      0.88      0.88       189\n",
      "  nimbostratos       0.73      0.92      0.81        12\n",
      "\n",
      "      accuracy                           0.93       923\n",
      "     macro avg       0.93      0.95      0.94       923\n",
      "  weighted avg       0.93      0.93      0.93       923\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFMCAYAAABF3IJ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "model_name = 'inceptionresnetv2'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "                        make_prebuilt(InceptionResNetV2,.25), model_name=model_name,\n",
    "                        model_dir=model_dir)\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXcHqEWzJg2y",
    "colab_type": "text"
   },
   "source": [
    "#### NASNetLarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ar7ajXH3JlrX",
    "colab_type": "code",
    "outputId": "fedbec6b-e889-4f97-bb9c-97f099a3618a",
    "executionInfo": {
     "status": "error",
     "timestamp": 1.579258745626E12,
     "user_tz": -60.0,
     "elapsed": 791.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking train and validation tests\n",
      "Building the network\n",
      "Compiling the network\n",
      "Layers: 1047\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stem_conv1 (Conv2D)             (None, 127, 127, 96) 2592        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stem_bn1 (BatchNormalization)   (None, 127, 127, 96) 384         stem_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 127, 127, 96) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_stem_1 (Conv2D (None, 127, 127, 42) 4032        activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_stem_1 (BatchNor (None, 127, 127, 42) 168         reduction_conv_1_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 127, 127, 42) 0           reduction_bn_1_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 127, 127, 96) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 131, 131, 42) 0           activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 133, 133, 96) 0           activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 64, 64, 42)   2814        separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 64, 64, 42)   8736        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 64, 64, 42)   168         separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 64, 64, 42)   168         separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 64, 64, 42)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 64, 64, 42)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 64, 64, 42)   2814        activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 64, 64, 42)   3822        activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 127, 127, 96) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 64, 64, 42)   168         separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 64, 64, 42)   168         separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 133, 133, 96) 0           activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 127, 127, 96) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_stem_1 (Add)    (None, 64, 64, 42)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 64, 64, 42)   8736        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 131, 131, 96) 0           activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 64, 64, 42)   0           reduction_add_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 64, 64, 42)   168         separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 64, 64, 42)   6432        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 64, 64, 42)   2142        activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 64, 64, 42)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 64, 64, 42)   168         separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 64, 64, 42)   168         separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_stem_1 (ZeroPad (None, 129, 129, 42) 0           reduction_bn_1_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 64, 64, 42)   3822        activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 64, 64, 42)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 64, 64, 42)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_stem_1 (MaxPool (None, 64, 64, 42)   0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 64, 64, 42)   168         separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 64, 64, 42)   2814        activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 64, 64, 42)   2142        activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_stem_2 (Activatio (None, 127, 127, 96) 0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_stem_1 (Add)    (None, 64, 64, 42)   0           reduction_left2_stem_1[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_stem_1 (Average (None, 64, 64, 42)   0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 64, 64, 42)   168         separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_stem_1 (Average (None, 64, 64, 42)   0           reduction_add_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 64, 64, 42)   168         separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_stem_1 (MaxPoo (None, 64, 64, 42)   0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 128, 128, 96) 0           adjust_relu_1_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_stem_1 (Add)     (None, 64, 64, 42)   0           reduction_left3_stem_1[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 64, 42)   0           reduction_add_2_stem_1[0][0]     \n",
      "                                                                 reduction_left4_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_stem_1 (Add)     (None, 64, 64, 42)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d (Cropping2D)         (None, 127, 127, 96) 0           zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_stem_1 (Concat (None, 64, 64, 168)  0           reduction_add_2_stem_1[0][0]     \n",
      "                                                                 reduction_add3_stem_1[0][0]      \n",
      "                                                                 add[0][0]                        \n",
      "                                                                 reduction_add4_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_stem_2 (Avera (None, 64, 64, 96)   0           adjust_relu_1_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_stem_2 (Avera (None, 64, 64, 96)   0           cropping2d[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 64, 64, 168)  0           reduction_concat_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_stem_2 (Conv2D)   (None, 64, 64, 42)   4032        adjust_avg_pool_1_stem_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_stem_2 (Conv2D)   (None, 64, 64, 42)   4032        adjust_avg_pool_2_stem_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_stem_2 (Conv2D (None, 64, 64, 84)   14112       activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 84)   0           adjust_conv_1_stem_2[0][0]       \n",
      "                                                                 adjust_conv_2_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_stem_2 (BatchNor (None, 64, 64, 84)   336         reduction_conv_1_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_stem_2 (BatchNormaliz (None, 64, 64, 84)   336         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 64, 64, 84)   0           reduction_bn_1_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 64, 64, 84)   0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 67, 67, 84)   0           activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 69, 69, 84)   0           activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 32, 32, 84)   9156        separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 32, 32, 84)   11172       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 32, 32, 84)   336         separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 32, 32, 84)   336         separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 32, 32, 84)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 32, 32, 84)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 32, 32, 84)   9156        activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 32, 32, 84)   11172       activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 64, 64, 84)   0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 32, 32, 84)   336         separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 32, 32, 84)   336         separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 69, 69, 84)   0           activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 64, 64, 84)   0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_stem_2 (Add)    (None, 32, 32, 84)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 32, 32, 84)   11172       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 67, 67, 84)   0           activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 32, 32, 84)   0           reduction_add_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 32, 32, 84)   336         separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 32, 32, 84)   9156        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 32, 32, 84)   7812        activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 32, 32, 84)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 32, 32, 84)   336         separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 32, 32, 84)   336         separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_stem_2 (ZeroPad (None, 65, 65, 84)   0           reduction_bn_1_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 32, 32, 84)   11172       activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 32, 32, 84)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 32, 32, 84)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_stem_2 (MaxPool (None, 32, 32, 84)   0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 32, 32, 84)   336         separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 32, 32, 84)   9156        activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 32, 32, 84)   7812        activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_0 (Activation)    (None, 64, 64, 168)  0           reduction_concat_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_stem_2 (Add)    (None, 32, 32, 84)   0           reduction_left2_stem_2[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_stem_2 (Average (None, 32, 32, 84)   0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 32, 32, 84)   336         separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_stem_2 (Average (None, 32, 32, 84)   0           reduction_add_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 32, 32, 84)   336         separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_stem_2 (MaxPoo (None, 32, 32, 84)   0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 65, 65, 168)  0           adjust_relu_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_stem_2 (Add)     (None, 32, 32, 84)   0           reduction_left3_stem_2[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 84)   0           reduction_add_2_stem_2[0][0]     \n",
      "                                                                 reduction_left4_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_stem_2 (Add)     (None, 32, 32, 84)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 64, 64, 168)  0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_stem_2 (Concat (None, 32, 32, 336)  0           reduction_add_2_stem_2[0][0]     \n",
      "                                                                 reduction_add3_stem_2[0][0]      \n",
      "                                                                 add_1[0][0]                      \n",
      "                                                                 reduction_add4_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_0 (AveragePoo (None, 32, 32, 168)  0           adjust_relu_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_0 (AveragePoo (None, 32, 32, 168)  0           cropping2d_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_0 (Conv2D)        (None, 32, 32, 84)   14112       adjust_avg_pool_1_0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_0 (Conv2D)        (None, 32, 32, 84)   14112       adjust_avg_pool_2_0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 32, 32, 336)  0           reduction_concat_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 168)  0           adjust_conv_1_0[0][0]            \n",
      "                                                                 adjust_conv_2_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_0 (Conv2D)        (None, 32, 32, 168)  56448       activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_0 (BatchNormalization (None, 32, 32, 168)  672         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_0 (BatchNormalizati (None, 32, 32, 168)  672         normal_conv_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 32, 32, 168)  0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 32, 32, 168)  0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 32, 32, 168)  0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_0 (None, 32, 32, 168)  32424       activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 32, 32, 168)  29736       activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_0 (None, 32, 32, 168)  32424       activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 32, 32, 168)  29736       activation_232[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_0 (None, 32, 32, 168)  29736       activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left1_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right1_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left2_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right2_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left5_0[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_0 (None, 32, 32, 168)  32424       activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 32, 32, 168)  29736       activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_0 (None, 32, 32, 168)  32424       activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 32, 32, 168)  29736       activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_0 (None, 32, 32, 168)  29736       activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left1_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right1_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left2_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right2_0[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_0 (AveragePooling2 (None, 32, 32, 168)  0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_0 (AveragePooling2 (None, 32, 32, 168)  0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_0 (AveragePooling (None, 32, 32, 168)  0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left5_0[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_0 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_0 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_0 (Add)            (None, 32, 32, 168)  0           normal_left3_0[0][0]             \n",
      "                                                                 adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_0 (Add)            (None, 32, 32, 168)  0           normal_left4_0[0][0]             \n",
      "                                                                 normal_right4_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_0 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_0 (Concatenate)   (None, 32, 32, 1008) 0           adjust_bn_0[0][0]                \n",
      "                                                                 normal_add_1_0[0][0]             \n",
      "                                                                 normal_add_2_0[0][0]             \n",
      "                                                                 normal_add_3_0[0][0]             \n",
      "                                                                 normal_add_4_0[0][0]             \n",
      "                                                                 normal_add_5_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 32, 32, 336)  0           reduction_concat_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 32, 32, 1008) 0           normal_concat_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_1 (Conv2 (None, 32, 32, 168)  56448       activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_1 (Conv2D)        (None, 32, 32, 168)  169344      activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_1 (BatchNormalization (None, 32, 32, 168)  672         adjust_conv_projection_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_1 (BatchNormalizati (None, 32, 32, 168)  672         normal_conv_1_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 32, 32, 168)  0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, 32, 32, 168)  0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 32, 32, 168)  0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 32, 32, 168)  32424       activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 32, 32, 168)  29736       activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 32, 32, 168)  32424       activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 32, 32, 168)  29736       activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 32, 32, 168)  29736       activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left1_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right1_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left2_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right2_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left5_1[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 32, 32, 168)  32424       activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 32, 32, 168)  29736       activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 32, 32, 168)  32424       activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 32, 32, 168)  29736       activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 32, 32, 168)  29736       activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left1_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right1_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left2_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right2_1[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_1 (AveragePooling2 (None, 32, 32, 168)  0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_1 (AveragePooling2 (None, 32, 32, 168)  0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_1 (AveragePooling (None, 32, 32, 168)  0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left5_1[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_1 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_1 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_1 (Add)            (None, 32, 32, 168)  0           normal_left3_1[0][0]             \n",
      "                                                                 adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_1 (Add)            (None, 32, 32, 168)  0           normal_left4_1[0][0]             \n",
      "                                                                 normal_right4_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_1 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_1 (Concatenate)   (None, 32, 32, 1008) 0           adjust_bn_1[0][0]                \n",
      "                                                                 normal_add_1_1[0][0]             \n",
      "                                                                 normal_add_2_1[0][0]             \n",
      "                                                                 normal_add_3_1[0][0]             \n",
      "                                                                 normal_add_4_1[0][0]             \n",
      "                                                                 normal_add_5_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 32, 32, 1008) 0           normal_concat_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 32, 32, 1008) 0           normal_concat_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_2 (Conv2 (None, 32, 32, 168)  169344      activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_2 (Conv2D)        (None, 32, 32, 168)  169344      activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_2 (BatchNormalization (None, 32, 32, 168)  672         adjust_conv_projection_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_2 (BatchNormalizati (None, 32, 32, 168)  672         normal_conv_1_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 32, 32, 168)  0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, 32, 32, 168)  0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, 32, 32, 168)  0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_2 (None, 32, 32, 168)  32424       activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 32, 32, 168)  29736       activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_2 (None, 32, 32, 168)  32424       activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 32, 32, 168)  29736       activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_2 (None, 32, 32, 168)  29736       activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left1_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right1_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left2_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right2_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left5_2[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_2 (None, 32, 32, 168)  32424       activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 32, 32, 168)  29736       activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_2 (None, 32, 32, 168)  32424       activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 32, 32, 168)  29736       activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_2 (None, 32, 32, 168)  29736       activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left1_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right1_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left2_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right2_2[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_2 (AveragePooling2 (None, 32, 32, 168)  0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_2 (AveragePooling2 (None, 32, 32, 168)  0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_2 (AveragePooling (None, 32, 32, 168)  0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left5_2[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_2 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_2 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_2 (Add)            (None, 32, 32, 168)  0           normal_left3_2[0][0]             \n",
      "                                                                 adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_2 (Add)            (None, 32, 32, 168)  0           normal_left4_2[0][0]             \n",
      "                                                                 normal_right4_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_2 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_2 (Concatenate)   (None, 32, 32, 1008) 0           adjust_bn_2[0][0]                \n",
      "                                                                 normal_add_1_2[0][0]             \n",
      "                                                                 normal_add_2_2[0][0]             \n",
      "                                                                 normal_add_3_2[0][0]             \n",
      "                                                                 normal_add_4_2[0][0]             \n",
      "                                                                 normal_add_5_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, 32, 32, 1008) 0           normal_concat_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, 32, 32, 1008) 0           normal_concat_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_3 (Conv2 (None, 32, 32, 168)  169344      activation_260[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_3 (Conv2D)        (None, 32, 32, 168)  169344      activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_3 (BatchNormalization (None, 32, 32, 168)  672         adjust_conv_projection_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_3 (BatchNormalizati (None, 32, 32, 168)  672         normal_conv_1_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, 32, 32, 168)  0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_266 (Activation)     (None, 32, 32, 168)  0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, 32, 32, 168)  0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_270 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_3 (None, 32, 32, 168)  32424       activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 32, 32, 168)  29736       activation_264[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_3 (None, 32, 32, 168)  32424       activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 32, 32, 168)  29736       activation_268[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_3 (None, 32, 32, 168)  29736       activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left1_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right1_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left2_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right2_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left5_3[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_267 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_271 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_3 (None, 32, 32, 168)  32424       activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 32, 32, 168)  29736       activation_265[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_3 (None, 32, 32, 168)  32424       activation_267[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 32, 32, 168)  29736       activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_3 (None, 32, 32, 168)  29736       activation_271[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left1_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right1_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left2_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right2_3[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_3 (AveragePooling2 (None, 32, 32, 168)  0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_3 (AveragePooling2 (None, 32, 32, 168)  0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_3 (AveragePooling (None, 32, 32, 168)  0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left5_3[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_3 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_3 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_3 (Add)            (None, 32, 32, 168)  0           normal_left3_3[0][0]             \n",
      "                                                                 adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_3 (Add)            (None, 32, 32, 168)  0           normal_left4_3[0][0]             \n",
      "                                                                 normal_right4_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_3 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_3 (Concatenate)   (None, 32, 32, 1008) 0           adjust_bn_3[0][0]                \n",
      "                                                                 normal_add_1_3[0][0]             \n",
      "                                                                 normal_add_2_3[0][0]             \n",
      "                                                                 normal_add_3_3[0][0]             \n",
      "                                                                 normal_add_4_3[0][0]             \n",
      "                                                                 normal_add_5_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, 32, 32, 1008) 0           normal_concat_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, 32, 32, 1008) 0           normal_concat_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_4 (Conv2 (None, 32, 32, 168)  169344      activation_272[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_4 (Conv2D)        (None, 32, 32, 168)  169344      activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_4 (BatchNormalization (None, 32, 32, 168)  672         adjust_conv_projection_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_4 (BatchNormalizati (None, 32, 32, 168)  672         normal_conv_1_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_274 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, 32, 32, 168)  0           adjust_bn_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_278 (Activation)     (None, 32, 32, 168)  0           adjust_bn_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, 32, 32, 168)  0           adjust_bn_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_4 (None, 32, 32, 168)  32424       activation_274[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 32, 32, 168)  29736       activation_276[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_4 (None, 32, 32, 168)  32424       activation_278[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 32, 32, 168)  29736       activation_280[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_4 (None, 32, 32, 168)  29736       activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left1_4[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right1_4[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left2_4[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right2_4[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left5_4[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_275 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_279 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_4 (None, 32, 32, 168)  32424       activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 32, 32, 168)  29736       activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_4 (None, 32, 32, 168)  32424       activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 32, 32, 168)  29736       activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_4 (None, 32, 32, 168)  29736       activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left1_4[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right1_4[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left2_4[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right2_4[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_4 (AveragePooling2 (None, 32, 32, 168)  0           normal_bn_1_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_4 (AveragePooling2 (None, 32, 32, 168)  0           adjust_bn_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_4 (AveragePooling (None, 32, 32, 168)  0           adjust_bn_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left5_4[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_4 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_4 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_4 (Add)            (None, 32, 32, 168)  0           normal_left3_4[0][0]             \n",
      "                                                                 adjust_bn_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_4 (Add)            (None, 32, 32, 168)  0           normal_left4_4[0][0]             \n",
      "                                                                 normal_right4_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_4 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_4 (Concatenate)   (None, 32, 32, 1008) 0           adjust_bn_4[0][0]                \n",
      "                                                                 normal_add_1_4[0][0]             \n",
      "                                                                 normal_add_2_4[0][0]             \n",
      "                                                                 normal_add_3_4[0][0]             \n",
      "                                                                 normal_add_4_4[0][0]             \n",
      "                                                                 normal_add_5_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, 32, 32, 1008) 0           normal_concat_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, 32, 32, 1008) 0           normal_concat_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_5 (Conv2 (None, 32, 32, 168)  169344      activation_284[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_5 (Conv2D)        (None, 32, 32, 168)  169344      activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_5 (BatchNormalization (None, 32, 32, 168)  672         adjust_conv_projection_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_5 (BatchNormalizati (None, 32, 32, 168)  672         normal_conv_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_286 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, 32, 32, 168)  0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_290 (Activation)     (None, 32, 32, 168)  0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, 32, 32, 168)  0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, 32, 32, 168)  0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_5 (None, 32, 32, 168)  32424       activation_286[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 32, 32, 168)  29736       activation_288[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_5 (None, 32, 32, 168)  32424       activation_290[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 32, 32, 168)  29736       activation_292[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_5 (None, 32, 32, 168)  29736       activation_294[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left1_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right1_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left2_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_1_normal_right2_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 32, 32, 168)  672         separable_conv_1_normal_left5_5[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_287 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_291 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_295 (Activation)     (None, 32, 32, 168)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_5 (None, 32, 32, 168)  32424       activation_287[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 32, 32, 168)  29736       activation_289[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_5 (None, 32, 32, 168)  32424       activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 32, 32, 168)  29736       activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_5 (None, 32, 32, 168)  29736       activation_295[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left1_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right1_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left2_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 32, 32, 168)  672         separable_conv_2_normal_right2_5[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_5 (AveragePooling2 (None, 32, 32, 168)  0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_5 (AveragePooling2 (None, 32, 32, 168)  0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_5 (AveragePooling (None, 32, 32, 168)  0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 32, 32, 168)  672         separable_conv_2_normal_left5_5[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_5 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_5 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_5 (Add)            (None, 32, 32, 168)  0           normal_left3_5[0][0]             \n",
      "                                                                 adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_5 (Add)            (None, 32, 32, 168)  0           normal_left4_5[0][0]             \n",
      "                                                                 normal_right4_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_5 (Add)            (None, 32, 32, 168)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_5 (Concatenate)   (None, 32, 32, 1008) 0           adjust_bn_5[0][0]                \n",
      "                                                                 normal_add_1_5[0][0]             \n",
      "                                                                 normal_add_2_5[0][0]             \n",
      "                                                                 normal_add_3_5[0][0]             \n",
      "                                                                 normal_add_4_5[0][0]             \n",
      "                                                                 normal_add_5_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, 32, 32, 1008) 0           normal_concat_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, 32, 32, 1008) 0           normal_concat_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_reduce_6 (Conv (None, 32, 32, 336)  338688      activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_reduce_6 (None, 32, 32, 336)  338688      activation_296[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_reduce_6 (BatchN (None, 32, 32, 336)  1344        reduction_conv_1_reduce_6[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_reduce_6 (BatchNormal (None, 32, 32, 336)  1344        adjust_conv_projection_reduce_6[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_298 (Activation)     (None, 32, 32, 336)  0           reduction_bn_1_reduce_6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 32, 32, 336)  0           adjust_bn_reduce_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 35, 35, 336)  0           activation_298[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 37, 37, 336)  0           activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 16, 16, 336)  121296      separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 16, 16, 336)  129360      separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 16, 16, 336)  1344        separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 16, 16, 336)  1344        separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 16, 16, 336)  121296      activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 16, 16, 336)  129360      activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 32, 32, 336)  0           adjust_bn_reduce_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 16, 16, 336)  1344        separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 16, 16, 336)  1344        separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 37, 37, 336)  0           activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 32, 32, 336)  0           adjust_bn_reduce_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_reduce_6 (Add)  (None, 16, 16, 336)  0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 16, 16, 336)  129360      separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 35, 35, 336)  0           activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 16, 16, 336)  0           reduction_add_1_reduce_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 16, 16, 336)  1344        separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 16, 16, 336)  121296      separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 16, 16, 336)  115920      activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 16, 16, 336)  1344        separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 16, 16, 336)  1344        separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_reduce_6 (ZeroP (None, 33, 33, 336)  0           reduction_bn_1_reduce_6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 16, 16, 336)  129360      activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_reduce_6 (MaxPo (None, 16, 16, 336)  0           reduction_pad_1_reduce_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 16, 16, 336)  1344        separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 16, 16, 336)  121296      activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 16, 16, 336)  115920      activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_7 (Activation)    (None, 32, 32, 1008) 0           normal_concat_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_reduce_6 (Add)  (None, 16, 16, 336)  0           reduction_left2_reduce_6[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_reduce_6 (Avera (None, 16, 16, 336)  0           reduction_pad_1_reduce_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 16, 16, 336)  1344        separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_reduce_6 (Avera (None, 16, 16, 336)  0           reduction_add_1_reduce_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 16, 16, 336)  1344        separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_reduce_6 (MaxP (None, 16, 16, 336)  0           reduction_pad_1_reduce_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 33, 33, 1008) 0           adjust_relu_1_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_reduce_6 (Add)   (None, 16, 16, 336)  0           reduction_left3_reduce_6[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 336)  0           reduction_add_2_reduce_6[0][0]   \n",
      "                                                                 reduction_left4_reduce_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_reduce_6 (Add)   (None, 16, 16, 336)  0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_reduce_6[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)       (None, 32, 32, 1008) 0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_reduce_6 (Conc (None, 16, 16, 1344) 0           reduction_add_2_reduce_6[0][0]   \n",
      "                                                                 reduction_add3_reduce_6[0][0]    \n",
      "                                                                 add_2[0][0]                      \n",
      "                                                                 reduction_add4_reduce_6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_7 (AveragePoo (None, 16, 16, 1008) 0           adjust_relu_1_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_7 (AveragePoo (None, 16, 16, 1008) 0           cropping2d_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_7 (Conv2D)        (None, 16, 16, 168)  169344      adjust_avg_pool_1_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_7 (Conv2D)        (None, 16, 16, 168)  169344      adjust_avg_pool_2_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 16, 16, 1344) 0           reduction_concat_reduce_6[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 336)  0           adjust_conv_1_7[0][0]            \n",
      "                                                                 adjust_conv_2_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_7 (Conv2D)        (None, 16, 16, 336)  451584      activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_7 (BatchNormalization (None, 16, 16, 336)  1344        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_7 (BatchNormalizati (None, 16, 16, 336)  1344        normal_conv_1_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 16, 16, 336)  0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 16, 16, 336)  0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 16, 16, 336)  0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_7 (None, 16, 16, 336)  121296      activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 16, 16, 336)  115920      activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_7 (None, 16, 16, 336)  121296      activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 16, 16, 336)  115920      activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_7 (None, 16, 16, 336)  115920      activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left1_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right1_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left2_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right2_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left5_7[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_7 (None, 16, 16, 336)  121296      activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 16, 16, 336)  115920      activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_7 (None, 16, 16, 336)  121296      activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 16, 16, 336)  115920      activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_7 (None, 16, 16, 336)  115920      activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left1_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right1_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left2_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right2_7[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_7 (AveragePooling2 (None, 16, 16, 336)  0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_7 (AveragePooling2 (None, 16, 16, 336)  0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_7 (AveragePooling (None, 16, 16, 336)  0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left5_7[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_7 (Add)            (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_7 (Add)            (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_7 (Add)            (None, 16, 16, 336)  0           normal_left3_7[0][0]             \n",
      "                                                                 adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_7 (Add)            (None, 16, 16, 336)  0           normal_left4_7[0][0]             \n",
      "                                                                 normal_right4_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_7 (Add)            (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_7 (Concatenate)   (None, 16, 16, 2016) 0           adjust_bn_7[0][0]                \n",
      "                                                                 normal_add_1_7[0][0]             \n",
      "                                                                 normal_add_2_7[0][0]             \n",
      "                                                                 normal_add_3_7[0][0]             \n",
      "                                                                 normal_add_4_7[0][0]             \n",
      "                                                                 normal_add_5_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 16, 16, 1344) 0           reduction_concat_reduce_6[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 16, 16, 2016) 0           normal_concat_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_8 (Conv2 (None, 16, 16, 336)  451584      activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_8 (Conv2D)        (None, 16, 16, 336)  677376      activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_8 (BatchNormalization (None, 16, 16, 336)  1344        adjust_conv_projection_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_8 (BatchNormalizati (None, 16, 16, 336)  1344        normal_conv_1_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 16, 16, 336)  0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 16, 16, 336)  0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 16, 16, 336)  0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_8 (None, 16, 16, 336)  121296      activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 16, 16, 336)  115920      activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_8 (None, 16, 16, 336)  121296      activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 16, 16, 336)  115920      activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_8 (None, 16, 16, 336)  115920      activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left1_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right1_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left2_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right2_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left5_8[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_8 (None, 16, 16, 336)  121296      activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 16, 16, 336)  115920      activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_8 (None, 16, 16, 336)  121296      activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 16, 16, 336)  115920      activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_8 (None, 16, 16, 336)  115920      activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left1_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right1_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left2_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right2_8[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_8 (AveragePooling2 (None, 16, 16, 336)  0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_8 (AveragePooling2 (None, 16, 16, 336)  0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_8 (AveragePooling (None, 16, 16, 336)  0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left5_8[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_8 (Add)            (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_8 (Add)            (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_8 (Add)            (None, 16, 16, 336)  0           normal_left3_8[0][0]             \n",
      "                                                                 adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_8 (Add)            (None, 16, 16, 336)  0           normal_left4_8[0][0]             \n",
      "                                                                 normal_right4_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_8 (Add)            (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_8 (Concatenate)   (None, 16, 16, 2016) 0           adjust_bn_8[0][0]                \n",
      "                                                                 normal_add_1_8[0][0]             \n",
      "                                                                 normal_add_2_8[0][0]             \n",
      "                                                                 normal_add_3_8[0][0]             \n",
      "                                                                 normal_add_4_8[0][0]             \n",
      "                                                                 normal_add_5_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 16, 16, 2016) 0           normal_concat_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 16, 16, 2016) 0           normal_concat_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_9 (Conv2 (None, 16, 16, 336)  677376      activation_331[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_9 (Conv2D)        (None, 16, 16, 336)  677376      activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_9 (BatchNormalization (None, 16, 16, 336)  1344        adjust_conv_projection_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_9 (BatchNormalizati (None, 16, 16, 336)  1344        normal_conv_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 16, 16, 336)  0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 16, 16, 336)  0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 16, 16, 336)  0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_9 (None, 16, 16, 336)  121296      activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 16, 16, 336)  115920      activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_9 (None, 16, 16, 336)  121296      activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 16, 16, 336)  115920      activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_9 (None, 16, 16, 336)  115920      activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left1_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right1_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left2_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right2_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left5_9[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_9 (None, 16, 16, 336)  121296      activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 16, 16, 336)  115920      activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_9 (None, 16, 16, 336)  121296      activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 16, 16, 336)  115920      activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_9 (None, 16, 16, 336)  115920      activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left1_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right1_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left2_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right2_9[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_9 (AveragePooling2 (None, 16, 16, 336)  0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_9 (AveragePooling2 (None, 16, 16, 336)  0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_9 (AveragePooling (None, 16, 16, 336)  0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left5_9[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_9 (Add)            (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_9 (Add)            (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_9 (Add)            (None, 16, 16, 336)  0           normal_left3_9[0][0]             \n",
      "                                                                 adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_9 (Add)            (None, 16, 16, 336)  0           normal_left4_9[0][0]             \n",
      "                                                                 normal_right4_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_9 (Add)            (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_9 (Concatenate)   (None, 16, 16, 2016) 0           adjust_bn_9[0][0]                \n",
      "                                                                 normal_add_1_9[0][0]             \n",
      "                                                                 normal_add_2_9[0][0]             \n",
      "                                                                 normal_add_3_9[0][0]             \n",
      "                                                                 normal_add_4_9[0][0]             \n",
      "                                                                 normal_add_5_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 16, 16, 2016) 0           normal_concat_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 16, 16, 2016) 0           normal_concat_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_10 (Conv (None, 16, 16, 336)  677376      activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_10 (Conv2D)       (None, 16, 16, 336)  677376      activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_10 (BatchNormalizatio (None, 16, 16, 336)  1344        adjust_conv_projection_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_10 (BatchNormalizat (None, 16, 16, 336)  1344        normal_conv_1_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 16, 16, 336)  0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 16, 16, 336)  0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_351 (Activation)     (None, 16, 16, 336)  0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_353 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 16, 16, 336)  121296      activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 16, 16, 336)  115920      activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 16, 16, 336)  121296      activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 16, 16, 336)  115920      activation_351[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 16, 16, 336)  115920      activation_353[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left1_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right1_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left2_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right2_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left5_10[\n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_352 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_354 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 16, 16, 336)  121296      activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 16, 16, 336)  115920      activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 16, 16, 336)  121296      activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 16, 16, 336)  115920      activation_352[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 16, 16, 336)  115920      activation_354[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left1_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right1_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left2_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right2_10\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_10 (AveragePooling (None, 16, 16, 336)  0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_10 (AveragePooling (None, 16, 16, 336)  0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_10 (AveragePoolin (None, 16, 16, 336)  0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left5_10[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_10 (Add)           (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_10 (Add)           (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_10 (Add)           (None, 16, 16, 336)  0           normal_left3_10[0][0]            \n",
      "                                                                 adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_10 (Add)           (None, 16, 16, 336)  0           normal_left4_10[0][0]            \n",
      "                                                                 normal_right4_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_10 (Add)           (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_10 (Concatenate)  (None, 16, 16, 2016) 0           adjust_bn_10[0][0]               \n",
      "                                                                 normal_add_1_10[0][0]            \n",
      "                                                                 normal_add_2_10[0][0]            \n",
      "                                                                 normal_add_3_10[0][0]            \n",
      "                                                                 normal_add_4_10[0][0]            \n",
      "                                                                 normal_add_5_10[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_355 (Activation)     (None, 16, 16, 2016) 0           normal_concat_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_356 (Activation)     (None, 16, 16, 2016) 0           normal_concat_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_11 (Conv (None, 16, 16, 336)  677376      activation_355[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_11 (Conv2D)       (None, 16, 16, 336)  677376      activation_356[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_11 (BatchNormalizatio (None, 16, 16, 336)  1344        adjust_conv_projection_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_11 (BatchNormalizat (None, 16, 16, 336)  1344        normal_conv_1_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_357 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_359 (Activation)     (None, 16, 16, 336)  0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_361 (Activation)     (None, 16, 16, 336)  0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_363 (Activation)     (None, 16, 16, 336)  0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_365 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 16, 16, 336)  121296      activation_357[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 16, 16, 336)  115920      activation_359[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 16, 16, 336)  121296      activation_361[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 16, 16, 336)  115920      activation_363[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 16, 16, 336)  115920      activation_365[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left1_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right1_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left2_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right2_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left5_11[\n",
      "__________________________________________________________________________________________________\n",
      "activation_358 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_360 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_362 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_364 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_366 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 16, 16, 336)  121296      activation_358[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 16, 16, 336)  115920      activation_360[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 16, 16, 336)  121296      activation_362[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 16, 16, 336)  115920      activation_364[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 16, 16, 336)  115920      activation_366[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left1_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right1_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left2_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right2_11\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_11 (AveragePooling (None, 16, 16, 336)  0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_11 (AveragePooling (None, 16, 16, 336)  0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_11 (AveragePoolin (None, 16, 16, 336)  0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left5_11[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_11 (Add)           (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_11 (Add)           (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_11 (Add)           (None, 16, 16, 336)  0           normal_left3_11[0][0]            \n",
      "                                                                 adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_11 (Add)           (None, 16, 16, 336)  0           normal_left4_11[0][0]            \n",
      "                                                                 normal_right4_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_11 (Add)           (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_11 (Concatenate)  (None, 16, 16, 2016) 0           adjust_bn_11[0][0]               \n",
      "                                                                 normal_add_1_11[0][0]            \n",
      "                                                                 normal_add_2_11[0][0]            \n",
      "                                                                 normal_add_3_11[0][0]            \n",
      "                                                                 normal_add_4_11[0][0]            \n",
      "                                                                 normal_add_5_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_367 (Activation)     (None, 16, 16, 2016) 0           normal_concat_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_368 (Activation)     (None, 16, 16, 2016) 0           normal_concat_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_12 (Conv (None, 16, 16, 336)  677376      activation_367[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_12 (Conv2D)       (None, 16, 16, 336)  677376      activation_368[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_12 (BatchNormalizatio (None, 16, 16, 336)  1344        adjust_conv_projection_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_12 (BatchNormalizat (None, 16, 16, 336)  1344        normal_conv_1_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_369 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_371 (Activation)     (None, 16, 16, 336)  0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_373 (Activation)     (None, 16, 16, 336)  0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_375 (Activation)     (None, 16, 16, 336)  0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 16, 16, 336)  0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 16, 16, 336)  121296      activation_369[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 16, 16, 336)  115920      activation_371[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 16, 16, 336)  121296      activation_373[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 16, 16, 336)  115920      activation_375[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 16, 16, 336)  115920      activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left1_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right1_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left2_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_1_normal_right2_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_1_normal_left5_12[\n",
      "__________________________________________________________________________________________________\n",
      "activation_370 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_372 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_374 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_376 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 16, 16, 336)  0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 16, 16, 336)  121296      activation_370[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 16, 16, 336)  115920      activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 16, 16, 336)  121296      activation_374[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 16, 16, 336)  115920      activation_376[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 16, 16, 336)  115920      activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left1_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right1_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left2_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 16, 16, 336)  1344        separable_conv_2_normal_right2_12\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_12 (AveragePooling (None, 16, 16, 336)  0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_12 (AveragePooling (None, 16, 16, 336)  0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_12 (AveragePoolin (None, 16, 16, 336)  0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 16, 16, 336)  1344        separable_conv_2_normal_left5_12[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_12 (Add)           (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_12 (Add)           (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_12 (Add)           (None, 16, 16, 336)  0           normal_left3_12[0][0]            \n",
      "                                                                 adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_12 (Add)           (None, 16, 16, 336)  0           normal_left4_12[0][0]            \n",
      "                                                                 normal_right4_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_12 (Add)           (None, 16, 16, 336)  0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_12 (Concatenate)  (None, 16, 16, 2016) 0           adjust_bn_12[0][0]               \n",
      "                                                                 normal_add_1_12[0][0]            \n",
      "                                                                 normal_add_2_12[0][0]            \n",
      "                                                                 normal_add_3_12[0][0]            \n",
      "                                                                 normal_add_4_12[0][0]            \n",
      "                                                                 normal_add_5_12[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 16, 16, 2016) 0           normal_concat_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 16, 16, 2016) 0           normal_concat_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_reduce_12 (Con (None, 16, 16, 672)  1354752     activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_reduce_1 (None, 16, 16, 672)  1354752     activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_reduce_12 (Batch (None, 16, 16, 672)  2688        reduction_conv_1_reduce_12[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_reduce_12 (BatchNorma (None, 16, 16, 672)  2688        adjust_conv_projection_reduce_12[\n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 16, 16, 672)  0           reduction_bn_1_reduce_12[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 16, 16, 672)  0           adjust_bn_reduce_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 19, 19, 672)  0           activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 21, 21, 672)  0           activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 8, 8, 672)    468384      separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 8, 8, 672)    484512      separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 8, 8, 672)    2688        separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 8, 8, 672)    2688        separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 8, 8, 672)    468384      activation_382[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 8, 8, 672)    484512      activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 16, 16, 672)  0           adjust_bn_reduce_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 8, 8, 672)    2688        separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 8, 8, 672)    2688        separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 21, 21, 672)  0           activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 16, 16, 672)  0           adjust_bn_reduce_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_reduce_12 (Add) (None, 8, 8, 672)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 8, 8, 672)    484512      separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 19, 19, 672)  0           activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 8, 8, 672)    0           reduction_add_1_reduce_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 8, 8, 672)    2688        separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 8, 8, 672)    468384      separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 8, 8, 672)    457632      activation_389[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 8, 8, 672)    2688        separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 8, 8, 672)    2688        separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_reduce_12 (Zero (None, 17, 17, 672)  0           reduction_bn_1_reduce_12[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 8, 8, 672)    484512      activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_reduce_12 (MaxP (None, 8, 8, 672)    0           reduction_pad_1_reduce_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 8, 8, 672)    2688        separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 8, 8, 672)    468384      activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 8, 8, 672)    457632      activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_13 (Activation)   (None, 16, 16, 2016) 0           normal_concat_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_reduce_12 (Add) (None, 8, 8, 672)    0           reduction_left2_reduce_12[0][0]  \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_reduce_12 (Aver (None, 8, 8, 672)    0           reduction_pad_1_reduce_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 8, 8, 672)    2688        separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_reduce_12 (Aver (None, 8, 8, 672)    0           reduction_add_1_reduce_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 8, 8, 672)    2688        separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_reduce_12 (Max (None, 8, 8, 672)    0           reduction_pad_1_reduce_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 17, 17, 2016) 0           adjust_relu_1_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_reduce_12 (Add)  (None, 8, 8, 672)    0           reduction_left3_reduce_12[0][0]  \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 672)    0           reduction_add_2_reduce_12[0][0]  \n",
      "                                                                 reduction_left4_reduce_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_reduce_12 (Add)  (None, 8, 8, 672)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_reduce_12[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)       (None, 16, 16, 2016) 0           zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_reduce_12 (Con (None, 8, 8, 2688)   0           reduction_add_2_reduce_12[0][0]  \n",
      "                                                                 reduction_add3_reduce_12[0][0]   \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 reduction_add4_reduce_12[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_13 (AveragePo (None, 8, 8, 2016)   0           adjust_relu_1_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_13 (AveragePo (None, 8, 8, 2016)   0           cropping2d_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_13 (Conv2D)       (None, 8, 8, 336)    677376      adjust_avg_pool_1_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_13 (Conv2D)       (None, 8, 8, 336)    677376      adjust_avg_pool_2_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 8, 8, 2688)   0           reduction_concat_reduce_12[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 672)    0           adjust_conv_1_13[0][0]           \n",
      "                                                                 adjust_conv_2_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_13 (Conv2D)       (None, 8, 8, 672)    1806336     activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_13 (BatchNormalizatio (None, 8, 8, 672)    2688        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_13 (BatchNormalizat (None, 8, 8, 672)    2688        normal_conv_1_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 8, 8, 672)    0           adjust_bn_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 8, 8, 672)    0           adjust_bn_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 8, 8, 672)    0           adjust_bn_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 8, 8, 672)    468384      activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 8, 8, 672)    457632      activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 8, 8, 672)    468384      activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 8, 8, 672)    457632      activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 8, 8, 672)    457632      activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left1_13[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right1_13\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left2_13[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right2_13\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left5_13[\n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 8, 8, 672)    468384      activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 8, 8, 672)    457632      activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 8, 8, 672)    468384      activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 8, 8, 672)    457632      activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 8, 8, 672)    457632      activation_401[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left1_13[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right1_13\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left2_13[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right2_13\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_13 (AveragePooling (None, 8, 8, 672)    0           normal_bn_1_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_13 (AveragePooling (None, 8, 8, 672)    0           adjust_bn_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_13 (AveragePoolin (None, 8, 8, 672)    0           adjust_bn_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left5_13[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_13 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_13 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_13 (Add)           (None, 8, 8, 672)    0           normal_left3_13[0][0]            \n",
      "                                                                 adjust_bn_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_13 (Add)           (None, 8, 8, 672)    0           normal_left4_13[0][0]            \n",
      "                                                                 normal_right4_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_13 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_13 (Concatenate)  (None, 8, 8, 4032)   0           adjust_bn_13[0][0]               \n",
      "                                                                 normal_add_1_13[0][0]            \n",
      "                                                                 normal_add_2_13[0][0]            \n",
      "                                                                 normal_add_3_13[0][0]            \n",
      "                                                                 normal_add_4_13[0][0]            \n",
      "                                                                 normal_add_5_13[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 8, 8, 2688)   0           reduction_concat_reduce_12[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 8, 8, 4032)   0           normal_concat_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_14 (Conv (None, 8, 8, 672)    1806336     activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_14 (Conv2D)       (None, 8, 8, 672)    2709504     activation_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_14 (BatchNormalizatio (None, 8, 8, 672)    2688        adjust_conv_projection_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_14 (BatchNormalizat (None, 8, 8, 672)    2688        normal_conv_1_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 8, 8, 672)    0           adjust_bn_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 8, 8, 672)    0           adjust_bn_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 8, 8, 672)    0           adjust_bn_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 8, 8, 672)    468384      activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 8, 8, 672)    457632      activation_406[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 8, 8, 672)    468384      activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 8, 8, 672)    457632      activation_410[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 8, 8, 672)    457632      activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left1_14[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right1_14\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left2_14[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right2_14\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left5_14[\n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 8, 8, 672)    468384      activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 8, 8, 672)    457632      activation_407[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 8, 8, 672)    468384      activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 8, 8, 672)    457632      activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 8, 8, 672)    457632      activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left1_14[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right1_14\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left2_14[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right2_14\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_14 (AveragePooling (None, 8, 8, 672)    0           normal_bn_1_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_14 (AveragePooling (None, 8, 8, 672)    0           adjust_bn_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_14 (AveragePoolin (None, 8, 8, 672)    0           adjust_bn_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left5_14[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_14 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_14 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_14 (Add)           (None, 8, 8, 672)    0           normal_left3_14[0][0]            \n",
      "                                                                 adjust_bn_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_14 (Add)           (None, 8, 8, 672)    0           normal_left4_14[0][0]            \n",
      "                                                                 normal_right4_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_14 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_14 (Concatenate)  (None, 8, 8, 4032)   0           adjust_bn_14[0][0]               \n",
      "                                                                 normal_add_1_14[0][0]            \n",
      "                                                                 normal_add_2_14[0][0]            \n",
      "                                                                 normal_add_3_14[0][0]            \n",
      "                                                                 normal_add_4_14[0][0]            \n",
      "                                                                 normal_add_5_14[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 8, 8, 4032)   0           normal_concat_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 8, 8, 4032)   0           normal_concat_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_15 (Conv (None, 8, 8, 672)    2709504     activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_15 (Conv2D)       (None, 8, 8, 672)    2709504     activation_415[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_15 (BatchNormalizatio (None, 8, 8, 672)    2688        adjust_conv_projection_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_15 (BatchNormalizat (None, 8, 8, 672)    2688        normal_conv_1_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_418 (Activation)     (None, 8, 8, 672)    0           adjust_bn_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_420 (Activation)     (None, 8, 8, 672)    0           adjust_bn_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_422 (Activation)     (None, 8, 8, 672)    0           adjust_bn_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_424 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 8, 8, 672)    468384      activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 8, 8, 672)    457632      activation_418[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 8, 8, 672)    468384      activation_420[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 8, 8, 672)    457632      activation_422[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 8, 8, 672)    457632      activation_424[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left1_15[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right1_15\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left2_15[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right2_15\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left5_15[\n",
      "__________________________________________________________________________________________________\n",
      "activation_417 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_419 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_421 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_423 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_425 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 8, 8, 672)    468384      activation_417[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 8, 8, 672)    457632      activation_419[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 8, 8, 672)    468384      activation_421[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 8, 8, 672)    457632      activation_423[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 8, 8, 672)    457632      activation_425[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left1_15[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right1_15\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left2_15[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right2_15\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_15 (AveragePooling (None, 8, 8, 672)    0           normal_bn_1_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_15 (AveragePooling (None, 8, 8, 672)    0           adjust_bn_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_15 (AveragePoolin (None, 8, 8, 672)    0           adjust_bn_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left5_15[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_15 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_15 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_15 (Add)           (None, 8, 8, 672)    0           normal_left3_15[0][0]            \n",
      "                                                                 adjust_bn_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_15 (Add)           (None, 8, 8, 672)    0           normal_left4_15[0][0]            \n",
      "                                                                 normal_right4_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_15 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_15 (Concatenate)  (None, 8, 8, 4032)   0           adjust_bn_15[0][0]               \n",
      "                                                                 normal_add_1_15[0][0]            \n",
      "                                                                 normal_add_2_15[0][0]            \n",
      "                                                                 normal_add_3_15[0][0]            \n",
      "                                                                 normal_add_4_15[0][0]            \n",
      "                                                                 normal_add_5_15[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_426 (Activation)     (None, 8, 8, 4032)   0           normal_concat_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_427 (Activation)     (None, 8, 8, 4032)   0           normal_concat_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_16 (Conv (None, 8, 8, 672)    2709504     activation_426[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_16 (Conv2D)       (None, 8, 8, 672)    2709504     activation_427[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_16 (BatchNormalizatio (None, 8, 8, 672)    2688        adjust_conv_projection_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_16 (BatchNormalizat (None, 8, 8, 672)    2688        normal_conv_1_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_428 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_430 (Activation)     (None, 8, 8, 672)    0           adjust_bn_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_432 (Activation)     (None, 8, 8, 672)    0           adjust_bn_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_434 (Activation)     (None, 8, 8, 672)    0           adjust_bn_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_436 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 8, 8, 672)    468384      activation_428[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 8, 8, 672)    457632      activation_430[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 8, 8, 672)    468384      activation_432[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 8, 8, 672)    457632      activation_434[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 8, 8, 672)    457632      activation_436[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left1_16[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right1_16\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left2_16[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right2_16\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left5_16[\n",
      "__________________________________________________________________________________________________\n",
      "activation_429 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_431 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_433 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_435 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_437 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 8, 8, 672)    468384      activation_429[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 8, 8, 672)    457632      activation_431[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 8, 8, 672)    468384      activation_433[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 8, 8, 672)    457632      activation_435[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 8, 8, 672)    457632      activation_437[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left1_16[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right1_16\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left2_16[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right2_16\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_16 (AveragePooling (None, 8, 8, 672)    0           normal_bn_1_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_16 (AveragePooling (None, 8, 8, 672)    0           adjust_bn_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_16 (AveragePoolin (None, 8, 8, 672)    0           adjust_bn_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left5_16[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_16 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_16 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_16 (Add)           (None, 8, 8, 672)    0           normal_left3_16[0][0]            \n",
      "                                                                 adjust_bn_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_16 (Add)           (None, 8, 8, 672)    0           normal_left4_16[0][0]            \n",
      "                                                                 normal_right4_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_16 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_16 (Concatenate)  (None, 8, 8, 4032)   0           adjust_bn_16[0][0]               \n",
      "                                                                 normal_add_1_16[0][0]            \n",
      "                                                                 normal_add_2_16[0][0]            \n",
      "                                                                 normal_add_3_16[0][0]            \n",
      "                                                                 normal_add_4_16[0][0]            \n",
      "                                                                 normal_add_5_16[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_438 (Activation)     (None, 8, 8, 4032)   0           normal_concat_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_439 (Activation)     (None, 8, 8, 4032)   0           normal_concat_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_17 (Conv (None, 8, 8, 672)    2709504     activation_438[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_17 (Conv2D)       (None, 8, 8, 672)    2709504     activation_439[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_17 (BatchNormalizatio (None, 8, 8, 672)    2688        adjust_conv_projection_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_17 (BatchNormalizat (None, 8, 8, 672)    2688        normal_conv_1_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_440 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_442 (Activation)     (None, 8, 8, 672)    0           adjust_bn_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_444 (Activation)     (None, 8, 8, 672)    0           adjust_bn_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_446 (Activation)     (None, 8, 8, 672)    0           adjust_bn_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 8, 8, 672)    468384      activation_440[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 8, 8, 672)    457632      activation_442[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 8, 8, 672)    468384      activation_444[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 8, 8, 672)    457632      activation_446[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 8, 8, 672)    457632      activation_448[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left1_17[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right1_17\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left2_17[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right2_17\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left5_17[\n",
      "__________________________________________________________________________________________________\n",
      "activation_441 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_443 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_445 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_447 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 8, 8, 672)    468384      activation_441[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 8, 8, 672)    457632      activation_443[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 8, 8, 672)    468384      activation_445[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 8, 8, 672)    457632      activation_447[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 8, 8, 672)    457632      activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left1_17[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right1_17\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left2_17[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right2_17\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_17 (AveragePooling (None, 8, 8, 672)    0           normal_bn_1_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_17 (AveragePooling (None, 8, 8, 672)    0           adjust_bn_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_17 (AveragePoolin (None, 8, 8, 672)    0           adjust_bn_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left5_17[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_17 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_17 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_17 (Add)           (None, 8, 8, 672)    0           normal_left3_17[0][0]            \n",
      "                                                                 adjust_bn_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_17 (Add)           (None, 8, 8, 672)    0           normal_left4_17[0][0]            \n",
      "                                                                 normal_right4_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_17 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_17 (Concatenate)  (None, 8, 8, 4032)   0           adjust_bn_17[0][0]               \n",
      "                                                                 normal_add_1_17[0][0]            \n",
      "                                                                 normal_add_2_17[0][0]            \n",
      "                                                                 normal_add_3_17[0][0]            \n",
      "                                                                 normal_add_4_17[0][0]            \n",
      "                                                                 normal_add_5_17[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_450 (Activation)     (None, 8, 8, 4032)   0           normal_concat_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_451 (Activation)     (None, 8, 8, 4032)   0           normal_concat_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_18 (Conv (None, 8, 8, 672)    2709504     activation_450[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_18 (Conv2D)       (None, 8, 8, 672)    2709504     activation_451[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_18 (BatchNormalizatio (None, 8, 8, 672)    2688        adjust_conv_projection_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_18 (BatchNormalizat (None, 8, 8, 672)    2688        normal_conv_1_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_452 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_454 (Activation)     (None, 8, 8, 672)    0           adjust_bn_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_456 (Activation)     (None, 8, 8, 672)    0           adjust_bn_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_458 (Activation)     (None, 8, 8, 672)    0           adjust_bn_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_460 (Activation)     (None, 8, 8, 672)    0           normal_bn_1_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 8, 8, 672)    468384      activation_452[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 8, 8, 672)    457632      activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 8, 8, 672)    468384      activation_456[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 8, 8, 672)    457632      activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 8, 8, 672)    457632      activation_460[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left1_18[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right1_18\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left2_18[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_1_normal_right2_18\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_1_normal_left5_18[\n",
      "__________________________________________________________________________________________________\n",
      "activation_453 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_455 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_457 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_459 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_461 (Activation)     (None, 8, 8, 672)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 8, 8, 672)    468384      activation_453[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 8, 8, 672)    457632      activation_455[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 8, 8, 672)    468384      activation_457[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 8, 8, 672)    457632      activation_459[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 8, 8, 672)    457632      activation_461[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left1_18[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right1_18\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left2_18[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 8, 8, 672)    2688        separable_conv_2_normal_right2_18\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_18 (AveragePooling (None, 8, 8, 672)    0           normal_bn_1_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_18 (AveragePooling (None, 8, 8, 672)    0           adjust_bn_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_18 (AveragePoolin (None, 8, 8, 672)    0           adjust_bn_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 8, 8, 672)    2688        separable_conv_2_normal_left5_18[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_18 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_18 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_18 (Add)           (None, 8, 8, 672)    0           normal_left3_18[0][0]            \n",
      "                                                                 adjust_bn_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_18 (Add)           (None, 8, 8, 672)    0           normal_left4_18[0][0]            \n",
      "                                                                 normal_right4_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_18 (Add)           (None, 8, 8, 672)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_18 (Concatenate)  (None, 8, 8, 4032)   0           adjust_bn_18[0][0]               \n",
      "                                                                 normal_add_1_18[0][0]            \n",
      "                                                                 normal_add_2_18[0][0]            \n",
      "                                                                 normal_add_3_18[0][0]            \n",
      "                                                                 normal_add_4_18[0][0]            \n",
      "                                                                 normal_add_5_18[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           128         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_462 (Activation)     (None, 8, 8, 4032)   0           normal_concat_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 4032)         0           activation_462[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 4048)         0           dropout_2[0][0]                  \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2048)         8292352     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2048)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           24588       dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 93,233,886\n",
      "Trainable params: 83,111,340\n",
      "Non-trainable params: 10,122,546\n",
      "__________________________________________________________________________________________________\n",
      "Fitting the network\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.nasnet import NASNetLarge\n",
    "model_name = 'nasnetlarge'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "                        make_prebuilt(NASNetLarge, .5, wgh=None), model_name=model_name,\n",
    "                        model_dir=model_dir, batch_size=32)\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bilU0s2I8DL",
    "colab_type": "text"
   },
   "source": [
    "# Random Forest Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3-0m1KbACnm",
    "colab_type": "text"
   },
   "source": [
    "##### NN into RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rvT8Yfj5z5st",
    "colab_type": "code",
    "outputId": "82a4f5be-7c32-4e86-95d8-0980aa735124",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.579864757416E12,
     "user_tz": -60.0,
     "elapsed": 93002.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 127, 127, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 127, 127, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 127, 127, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 125, 125, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 125, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 125, 125, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 125, 125, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 125, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 125, 125, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 62, 62, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 62, 62, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 62, 62, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 62, 62, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 60, 60, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 60, 60, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 60, 60, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 29, 29, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 29, 29, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 29, 29, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 29, 29, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 29, 29, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 29, 29, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 29, 29, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 29, 29, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 29, 29, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 29, 29, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 29, 29, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 29, 29, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 29, 29, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 29, 29, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 29, 29, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 29, 29, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 29, 29, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 29, 29, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 29, 29, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 29, 29, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 29, 29, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 29, 29, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 29, 29, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 29, 29, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 29, 29, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 29, 29, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 29, 29, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 29, 29, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 29, 29, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 29, 29, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 29, 29, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 29, 29, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 29, 29, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 29, 29, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 29, 29, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 29, 29, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 29, 29, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 29, 29, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 29, 29, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 29, 29, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 29, 29, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 29, 29, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 29, 29, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 29, 29, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 29, 29, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 29, 29, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 29, 29, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 29, 29, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 29, 29, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 29, 29, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 29, 29, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 29, 29, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 29, 29, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 29, 29, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 29, 29, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 29, 29, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 29, 29, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 29, 29, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 29, 29, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 29, 29, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 29, 29, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 29, 29, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 29, 29, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 29, 29, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 29, 29, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 29, 29, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 29, 29, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 29, 29, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 29, 29, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 29, 29, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 29, 29, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 29, 29, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 29, 29, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 29, 29, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 29, 29, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 29, 29, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 384)  1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 384)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 14, 14, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 14, 14, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 14, 14, 128)  384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 14, 14, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 14, 14, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 14, 14, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 128)  384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 14, 14, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 14, 14, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 14, 14, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 14, 14, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 14, 14, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 192)  576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 14, 14, 192)  576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 14, 14, 192)  576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 14, 14, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 192)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 192)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 14, 14, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 14, 14, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 14, 14, 160)  480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 14, 14, 160)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 14, 14, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 14, 14, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 14, 14, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 14, 14, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 14, 14, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 14, 14, 160)  480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 14, 14, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 14, 14, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 14, 14, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 14, 14, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 14, 14, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 14, 14, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 14, 14, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 14, 14, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 14, 14, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 14, 14, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 14, 14, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 14, 14, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 14, 14, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 14, 14, 192)  576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 14, 14, 192)  576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 14, 14, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 14, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 14, 14, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 14, 14, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 14, 14, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 14, 14, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 14, 14, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 14, 14, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 14, 14, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 14, 14, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 14, 14, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 14, 14, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 14, 14, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 14, 14, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 14, 14, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 14, 14, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 14, 14, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 14, 14, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 14, 14, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 14, 14, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 14, 14, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 14, 14, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 14, 14, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 14, 14, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 14, 14, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 14, 14, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 14, 14, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 14, 14, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 14, 14, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 14, 14, 192)  576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 14, 14, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 14, 14, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 14, 14, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 14, 14, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 14, 14, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 14, 14, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 14, 14, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 14, 14, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 14, 14, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 14, 14, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 14, 14, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 14, 14, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 14, 14, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 14, 14, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 14, 14, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 14, 14, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 14, 14, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 14, 14, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 14, 14, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 14, 14, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 14, 14, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 14, 14, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 14, 14, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 14, 14, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 14, 14, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 14, 14, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 14, 14, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 14, 14, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 14, 14, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 14, 14, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 14, 14, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 14, 14, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 14, 14, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 14, 14, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 14, 14, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 14, 14, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 14, 14, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 14, 14, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 14, 14, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 14, 14, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 14, 14, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 14, 14, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 14, 14, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 14, 14, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 14, 14, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 14, 14, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 6, 6, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 6, 6, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 6, 6, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 6, 6, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 6, 6, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 6, 6, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 6, 6, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 6, 6, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 6, 6, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 6, 6, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 6, 6, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 6, 6, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 6, 6, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 6, 6, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 6, 6, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 6, 6, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 6, 6, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 6, 6, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 6, 6, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 6, 6, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 6, 6, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 6, 6, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 6, 6, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 6, 6, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 6, 6, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 6, 6, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 6, 6, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 6, 6, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 6, 6, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 6, 6, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 6, 6, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 6, 6, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 6, 6, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 6, 6, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 6, 6, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6, 6, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 6, 6, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 6, 6, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 6, 6, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 6, 6, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 6, 6, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 6, 6, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 6, 6, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 6, 6, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 6, 6, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 6, 6, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 6, 6, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 6, 6, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 6, 6, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 6, 6, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 6, 6, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 6, 6, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 6, 6, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 6, 6, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 6, 6, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 6, 6, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 6, 6, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 6, 6, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 6, 6, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 6, 6, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 6, 6, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 6, 6, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 6, 6, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 6, 6, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 6, 6, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 6, 6, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6, 6, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 6, 6, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           128         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 6, 6, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2064)         0           dropout[0][0]                    \n",
      "                                                                 global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         4229120     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2048)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           24588       activation_94[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 26,056,620\n",
      "Trainable params: 25,053,132\n",
      "Non-trainable params: 1,003,488\n",
      "__________________________________________________________________________________________________\n",
      "Train\n",
      "tr - Train classifier\n",
      "Test\n",
      "te - Get predictions from trained model\n",
      "---ensemble_avg---\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol     1.0000    1.0000    1.0000        20\n",
      "   altocumulos     0.9205    0.9205    0.9205        88\n",
      "   altostratos     0.9211    0.9722    0.9459        36\n",
      "cieloDespejado     0.9200    1.0000    0.9583        23\n",
      "  cirrocumulos     0.9545    1.0000    0.9767        21\n",
      "        cirros     0.9650    0.9517    0.9583       145\n",
      "  cirrostratos     0.9429    0.9706    0.9565        68\n",
      "       cumulos     0.9718    0.9718    0.9718        71\n",
      "estratocumulos     0.9714    0.9577    0.9645       142\n",
      "      estratos     0.9550    0.9815    0.9680       108\n",
      "     multinube     0.9511    0.9259    0.9383       189\n",
      "  nimbostratos     1.0000    0.9167    0.9565        12\n",
      "\n",
      "      accuracy                         0.9545       923\n",
      "     macro avg     0.9561    0.9641    0.9596       923\n",
      "  weighted avg     0.9548    0.9545    0.9544       923\n",
      "\n",
      "---ensemble_max---\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol     1.0000    1.0000    1.0000        20\n",
      "   altocumulos     0.9205    0.9205    0.9205        88\n",
      "   altostratos     0.8974    0.9722    0.9333        36\n",
      "cieloDespejado     0.9200    1.0000    0.9583        23\n",
      "  cirrocumulos     0.9545    1.0000    0.9767        21\n",
      "        cirros     0.9650    0.9517    0.9583       145\n",
      "  cirrostratos     0.9429    0.9706    0.9565        68\n",
      "       cumulos     0.9857    0.9718    0.9787        71\n",
      "estratocumulos     0.9577    0.9577    0.9577       142\n",
      "      estratos     0.9550    0.9815    0.9680       108\n",
      "     multinube     0.9505    0.9153    0.9326       189\n",
      "  nimbostratos     1.0000    0.9167    0.9565        12\n",
      "\n",
      "      accuracy                         0.9523       923\n",
      "     macro avg     0.9541    0.9632    0.9581       923\n",
      "  weighted avg     0.9527    0.9523    0.9523       923\n",
      "\n",
      "---random_forest---\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol     0.9524    1.0000    0.9756        20\n",
      "   altocumulos     0.8842    0.9545    0.9180        88\n",
      "   altostratos     0.9444    0.9444    0.9444        36\n",
      "cieloDespejado     1.0000    1.0000    1.0000        23\n",
      "  cirrocumulos     0.9130    1.0000    0.9545        21\n",
      "        cirros     0.9586    0.9586    0.9586       145\n",
      "  cirrostratos     0.9437    0.9853    0.9640        68\n",
      "       cumulos     0.9200    0.9718    0.9452        71\n",
      "estratocumulos     0.9769    0.8944    0.9338       142\n",
      "      estratos     0.9273    0.9444    0.9358       108\n",
      "     multinube     0.9066    0.8730    0.8895       189\n",
      "  nimbostratos     0.9167    0.9167    0.9167        12\n",
      "\n",
      "      accuracy                         0.9339       923\n",
      "     macro avg     0.9370    0.9536    0.9447       923\n",
      "  weighted avg     0.9347    0.9339    0.9337       923\n",
      "\n",
      "---inceptionv3---\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol     1.0000    1.0000    1.0000        20\n",
      "   altocumulos     0.9091    0.9091    0.9091        88\n",
      "   altostratos     0.8537    0.9722    0.9091        36\n",
      "cieloDespejado     0.8846    1.0000    0.9388        23\n",
      "  cirrocumulos     0.9545    1.0000    0.9767        21\n",
      "        cirros     0.9574    0.9310    0.9441       145\n",
      "  cirrostratos     0.9412    0.9412    0.9412        68\n",
      "       cumulos     0.9718    0.9718    0.9718        71\n",
      "estratocumulos     0.9507    0.9507    0.9507       142\n",
      "      estratos     0.9358    0.9444    0.9401       108\n",
      "     multinube     0.9341    0.8995    0.9164       189\n",
      "  nimbostratos     0.7692    0.8333    0.8000        12\n",
      "\n",
      "      accuracy                         0.9361       923\n",
      "     macro avg     0.9218    0.9461    0.9332       923\n",
      "  weighted avg     0.9369    0.9361    0.9361       923\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'inceptionv3'\n",
    "model = load_model('%s/%s' % (model_dir, '%s_model.h5' % model_name))\n",
    "model.summary()\n",
    "\n",
    "# Training rf\n",
    "print('Train')\n",
    "x_train = data['features'][0]\n",
    "y_train = data['label_encoder'].inverse_transform(data['train'][-1])\n",
    "\n",
    "print('tr - Train classifier')\n",
    "classifier = RandomForestClassifier(250, random_state=1,\n",
    "                                    max_features=.35)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "print('Test')\n",
    "img_test, ceil_test, y_test= data['test']\n",
    "x_test = data['features'][-1]\n",
    "standard_img_test = data_generator.standardize(img_test)\n",
    "\n",
    "print('te - Get predictions from trained model')\n",
    "nn_pred = model.predict([standard_img_test, ceil_test])\n",
    "rf_pred = classifier.predict_proba(x_test)\n",
    "avg_pred = (nn_pred + rf_pred)/2\n",
    "max_pred = np.maximum(nn_pred, rf_pred)\n",
    "\n",
    "rf_pred = data['label_encoder'].inverse_transform(rf_pred)\n",
    "nn_pred = data['label_encoder'].inverse_transform(nn_pred)\n",
    "avg_pred = data['label_encoder'].inverse_transform(avg_pred)\n",
    "max_pred = data['label_encoder'].inverse_transform(max_pred)\n",
    "\n",
    "decoded_observations = data['label_encoder'].inverse_transform(y_test)\n",
    "pred_obs = pd.DataFrame(data={'avg_pred': avg_pred,\n",
    "                              'max_pred': max_pred,\n",
    "                              'obs': decoded_observations,\n",
    "                              'rf_pred': rf_pred,\n",
    "                              'nn_pred': nn_pred})\n",
    "\n",
    "\n",
    "print('---{}---'.format('ensemble_avg'))\n",
    "print(classification_report(pred_obs['obs'], pred_obs['avg_pred'], digits=4))\n",
    "print('---{}---'.format('ensemble_max'))\n",
    "print(classification_report(pred_obs['obs'], pred_obs['max_pred'], digits=4))\n",
    "print('---{}---'.format('random_forest'))\n",
    "print(classification_report(pred_obs['obs'], pred_obs['rf_pred'], digits=4))\n",
    "print('---{}---'.format(model_name))\n",
    "print(classification_report(pred_obs['obs'], pred_obs['nn_pred'], digits=4))\n",
    "\n",
    "# Evaluar el Modelo\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "matrix = confusion_matrix(pred_obs['obs'], pred_obs['avg_pred'])\n",
    "#names = pred_obs['obs'].unique()\n",
    "sns.heatmap(matrix, annot=True, cbar=False)\n",
    "\n",
    "ensemble_name='inceptionresnetv2_plus_RandomForest'\n",
    "pred_obs.to_csv('%s/%s' % (file_dir, '%s_preds.csv' % ensemble_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RizjHsayPE-",
    "colab_type": "text"
   },
   "source": [
    "#### NN and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QbdlRGeqyNpa",
    "colab_type": "code",
    "outputId": "3212c694-8fcb-4a39-c062-dc6a820f4f56",
    "executionInfo": {
     "status": "error",
     "timestamp": 1.579689203574E12,
     "user_tz": -60.0,
     "elapsed": 12623.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424.0
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0044536e4d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'inceptionv3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s_model.h5'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    141\u001b[0m   if (h5py is not None and (\n\u001b[1;32m    142\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    691\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[1;32m    692\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3257\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3258\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3259\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    484\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 903\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    904\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "model_name = 'inceptionresnetv2'\n",
    "model = load_model('%s/%s' % (model_dir, '%s_model.h5' % model_name))\n",
    "  \n",
    "rf_model = Model(inputs=model.input, outputs=model.layers[-5].output)\n",
    "rf_model.summary()\n",
    "\n",
    "# Training rf\n",
    "print('Train')\n",
    "img_train, ceil_train, y_train = data['train']\n",
    "print('tr - Get predictions from trained model')\n",
    "_, att = rf_model.output.shape\n",
    "num = img_train.shape[0]\n",
    "batch_size = 128\n",
    "\n",
    "#train_prediction_set = rf_model.predict_generator(data_generator.flow(x=(img_train,ceil_train),\n",
    "#                                                                        batch_size=batch_size),\n",
    "#                                                                        steps= ceil(num/batch_size),\n",
    "#                                                                        verbose=1)\n",
    "y_train_dec = list(data['label_encoder'].inverse_transform(y_train))\n",
    "\n",
    "reductor = PCA(.95, svd_solver='full')\n",
    "classifier = RandomForestClassifier(100, random_state=1,\n",
    "                                    max_features=.35, n_jobs=8)\n",
    "\n",
    "print('tr - Dimensionality reduction')\n",
    "x = reductor.fit_transform(train_prediction_set)\n",
    "print(x.shape)\n",
    "\n",
    "print('tr - Train classifier')\n",
    "classifier.fit(x, y_train_dec)\n",
    "decoded_predictions = classifier.predict(x)\n",
    "pred_obs = pd.DataFrame(data={'pred': decoded_predictions, 'obs': y_train_dec})\n",
    "\n",
    "print(classification_report(pred_obs['obs'], pred_obs['pred']))\n",
    "\n",
    "# Testing rf and saving results\n",
    "print('Test')\n",
    "img_test, ceil_test, y_test= data['test']\n",
    "print('te - Standarize test data')\n",
    "standard_img_test = data_generator.standardize(img_test)\n",
    "print('te - Get predictions from trained model')\n",
    "test_predictions = rf_model.predict([standard_img_test, ceil_test])\n",
    "test_predictions = reductor.transform(test_predictions)\n",
    "print('te - Final predictions')\n",
    "decoded_predictions = classifier.predict(test_predictions)\n",
    "decoded_observations = data['label_encoder'].inverse_transform(y_test)\n",
    "pred_obs = pd.DataFrame(data={'pred': decoded_predictions, 'obs': decoded_observations})\n",
    "\n",
    "# Evaluar el Modelo\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "matrix = confusion_matrix(pred_obs['obs'], pred_obs['pred'])\n",
    "names = pred_obs['obs'].unique()\n",
    "\n",
    "sns.heatmap(matrix, annot=True, cbar=False, xticklabels=names, yticklabels=names)\n",
    "print(classification_report(pred_obs['obs'], pred_obs['pred']))\n",
    "\n",
    "# Almacenar las predicciones del modelo entrenado\n",
    "# pred_obs.to_csv('%s/%s' % (file_dir, '%s_preds.csv' % model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1-ReeOxictF",
    "colab_type": "text"
   },
   "source": [
    "# Aprendizaje Cropnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amBRlm6_lERx",
    "colab_type": "text"
   },
   "source": [
    "#### CropNet v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "19nTZb6Hig8y",
    "colab_type": "code",
    "outputId": "3e63291f-be53-4a66-c576-650392ff6f1e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.577061768091E12,
     "user_tz": -60.0,
     "elapsed": 898.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Layers: 86\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 160, 160, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)       (None, 160, 160, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)       (None, 160, 160, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 160, 160, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 160, 160, 32) 896         cropping2d_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 160, 160, 32) 896         cropping2d_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 160, 160, 32) 896         cropping2d_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 160, 160, 32) 896         cropping2d_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 80, 80, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 80, 80, 32)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 80, 80, 32)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 80, 80, 32)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 256, 256, 32) 896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 80, 80, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 80, 80, 64)   18496       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 80, 80, 64)   18496       max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 80, 80, 64)   18496       max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 40, 40, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 40, 40, 64)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 40, 40, 64)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, 40, 40, 64)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 40, 40, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 40, 40, 128)  73856       max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 40, 40, 128)  73856       max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 40, 40, 128)  73856       max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 20, 20, 128)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 20, 20, 128)  0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 20, 20, 128)  0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 20, 20, 128)  0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 20, 20, 256)  295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 20, 20, 256)  295168      max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 20, 20, 256)  295168      max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 20, 20, 256)  295168      max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 20, 20, 256)  590080      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 10, 10, 256)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 10, 10, 256)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 10, 10, 256)  0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 10, 10, 256)  0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 10, 10, 256)  590080      max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 10, 10, 256)  590080      max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 10, 10, 256)  590080      max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 10, 10, 256)  590080      max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 10, 10, 256)  590080      conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 10, 10, 256)  590080      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 10, 10, 256)  590080      conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 10, 10, 256)  590080      conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 256)  590080      max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 5, 5, 256)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 5, 5, 256)    0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 5, 5, 256)    0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 5, 5, 256)    0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 256)          0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 256)          0           max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 256)          0           max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 256)          0           max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D) (None, 8, 8, 256)    0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         526336      global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2048)         526336      global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2048)         526336      global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2048)         526336      global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 256)          0           max_pooling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           128         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2048)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2048)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2048)         526336      global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10256)        0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dense_5[0][0]                    \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 32)           328224      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           396         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,753,708\n",
      "Trainable params: 13,753,708\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      " - 57s - loss: 2.5837 - acc: 0.1983 - val_loss: 2.4580 - val_acc: 0.2061\n",
      "Epoch 2/1000\n",
      " - 48s - loss: 2.4454 - acc: 0.2043 - val_loss: 2.4329 - val_acc: 0.2061\n",
      "Epoch 3/1000\n",
      " - 53s - loss: 2.4220 - acc: 0.2043 - val_loss: 2.4104 - val_acc: 0.2061\n",
      "Epoch 4/1000\n",
      " - 52s - loss: 2.4011 - acc: 0.2039 - val_loss: 2.3899 - val_acc: 0.2061\n",
      "Epoch 5/1000\n",
      " - 52s - loss: 2.3815 - acc: 0.2056 - val_loss: 2.3716 - val_acc: 0.2061\n",
      "Epoch 6/1000\n",
      " - 51s - loss: 2.3637 - acc: 0.2043 - val_loss: 2.3550 - val_acc: 0.2061\n",
      "Epoch 7/1000\n",
      " - 51s - loss: 2.3478 - acc: 0.2043 - val_loss: 2.3399 - val_acc: 0.2061\n",
      "Epoch 8/1000\n",
      " - 52s - loss: 2.3338 - acc: 0.2061 - val_loss: 2.3261 - val_acc: 0.2061\n",
      "Epoch 9/1000\n",
      " - 52s - loss: 2.3210 - acc: 0.2052 - val_loss: 2.3138 - val_acc: 0.2061\n",
      "Epoch 10/1000\n",
      " - 51s - loss: 2.3106 - acc: 0.2034 - val_loss: 2.3028 - val_acc: 0.2061\n",
      "Epoch 11/1000\n",
      " - 52s - loss: 2.2988 - acc: 0.2052 - val_loss: 2.2930 - val_acc: 0.2061\n",
      "Epoch 12/1000\n",
      " - 50s - loss: 2.2895 - acc: 0.2048 - val_loss: 2.2839 - val_acc: 0.2061\n",
      "Epoch 13/1000\n",
      " - 51s - loss: 2.2806 - acc: 0.2052 - val_loss: 2.2761 - val_acc: 0.2061\n",
      "Epoch 14/1000\n",
      " - 50s - loss: 2.2732 - acc: 0.2056 - val_loss: 2.2688 - val_acc: 0.2061\n",
      "Epoch 15/1000\n",
      " - 50s - loss: 2.2673 - acc: 0.2048 - val_loss: 2.2624 - val_acc: 0.2061\n",
      "Epoch 16/1000\n",
      " - 50s - loss: 2.2605 - acc: 0.2052 - val_loss: 2.2566 - val_acc: 0.2061\n",
      "Epoch 17/1000\n",
      " - 50s - loss: 2.2564 - acc: 0.2052 - val_loss: 2.2515 - val_acc: 0.2061\n",
      "Epoch 18/1000\n",
      " - 52s - loss: 2.2500 - acc: 0.2056 - val_loss: 2.2469 - val_acc: 0.2061\n",
      "Epoch 19/1000\n",
      " - 51s - loss: 2.2457 - acc: 0.2056 - val_loss: 2.2427 - val_acc: 0.2061\n",
      "Epoch 20/1000\n",
      " - 51s - loss: 2.2416 - acc: 0.2061 - val_loss: 2.2391 - val_acc: 0.2061\n",
      "Epoch 21/1000\n",
      " - 50s - loss: 2.2385 - acc: 0.2056 - val_loss: 2.2358 - val_acc: 0.2061\n",
      "Epoch 22/1000\n",
      " - 50s - loss: 2.2371 - acc: 0.2056 - val_loss: 2.2328 - val_acc: 0.2061\n",
      "Epoch 23/1000\n",
      " - 50s - loss: 2.2329 - acc: 0.2061 - val_loss: 2.2302 - val_acc: 0.2061\n",
      "Epoch 24/1000\n",
      " - 50s - loss: 2.2330 - acc: 0.2056 - val_loss: 2.2278 - val_acc: 0.2061\n",
      "Epoch 25/1000\n",
      " - 50s - loss: 2.2297 - acc: 0.2052 - val_loss: 2.2259 - val_acc: 0.2061\n",
      "Epoch 26/1000\n",
      " - 50s - loss: 2.2254 - acc: 0.2061 - val_loss: 2.2241 - val_acc: 0.2061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol       0.00      0.00      0.00        20\n",
      "   altocumulos       0.00      0.00      0.00        88\n",
      "   altostratos       0.00      0.00      0.00        36\n",
      "cieloDespejado       0.00      0.00      0.00        23\n",
      "  cirrocumulos       0.00      0.00      0.00        21\n",
      "        cirros       0.00      0.00      0.00       145\n",
      "  cirrostratos       0.00      0.00      0.00        68\n",
      "       cumulos       0.00      0.00      0.00        71\n",
      "estratocumulos       0.00      0.00      0.00       142\n",
      "      estratos       0.00      0.00      0.00       108\n",
      "     multinube       0.20      1.00      0.34       189\n",
      "  nimbostratos       0.00      0.00      0.00        12\n",
      "\n",
      "      accuracy                           0.20       923\n",
      "     macro avg       0.02      0.08      0.03       923\n",
      "  weighted avg       0.04      0.20      0.07       923\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFMCAYAAABF3IJ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'cropnetv1'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "                        make_cropnetv1, model_name=model_name,\n",
    "                        model_dir=model_dir, lr=1e-3)\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TS92fAIC3eN",
    "colab_type": "text"
   },
   "source": [
    "#### CropNet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "db2LxT9rC2vi",
    "colab_type": "code",
    "outputId": "78a5a378-4a26-404f-bbd5-462acb37d0f5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.577162543717E12,
     "user_tz": -60.0,
     "elapsed": 1.2715995E7,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 116\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_5 (Cropping2D)       (None, 160, 160, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_6 (Cropping2D)       (None, 160, 160, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_7 (Cropping2D)       (None, 160, 160, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_8 (Cropping2D)       (None, 160, 160, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 160, 160, 32) 896         cropping2d_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 160, 160, 32) 896         cropping2d_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 160, 160, 32) 896         cropping2d_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 160, 160, 32) 896         cropping2d_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 160, 160, 32) 9248        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 160, 160, 32) 9248        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 160, 160, 32) 9248        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 160, 160, 32) 9248        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 256, 256, 32) 896         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling2D) (None, 80, 80, 32)   0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling2D) (None, 80, 80, 32)   0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling2D) (None, 80, 80, 32)   0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling2D) (None, 80, 80, 32)   0           conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 80, 80, 64)   18496       max_pooling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 80, 80, 64)   18496       max_pooling2d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 80, 80, 64)   18496       max_pooling2d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 80, 80, 64)   18496       max_pooling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 80, 80, 64)   36928       conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 80, 80, 64)   36928       conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 80, 80, 64)   36928       conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 80, 80, 64)   36928       conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling2D) (None, 40, 40, 64)   0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling2D) (None, 40, 40, 64)   0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling2D) (None, 40, 40, 64)   0           conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling2D) (None, 40, 40, 64)   0           conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 40, 40, 128)  73856       max_pooling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 40, 40, 128)  73856       max_pooling2d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 40, 40, 128)  73856       max_pooling2d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 40, 40, 128)  73856       max_pooling2d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 40, 40, 128)  147584      conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 40, 40, 128)  147584      conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 40, 40, 128)  147584      conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 40, 40, 128)  147584      conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D) (None, 20, 20, 128)  0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling2D) (None, 20, 20, 128)  0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling2D) (None, 20, 20, 128)  0           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling2D) (None, 20, 20, 128)  0           conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 20, 20, 256)  295168      max_pooling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 20, 20, 256)  295168      max_pooling2d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 20, 20, 256)  295168      max_pooling2d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 20, 20, 256)  295168      max_pooling2d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_53 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 20, 20, 256)  590080      conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 20, 20, 256)  590080      conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 20, 20, 256)  590080      conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D) (None, 10, 10, 256)  0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling2D) (None, 10, 10, 256)  0           conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling2D) (None, 10, 10, 256)  0           conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling2D) (None, 10, 10, 256)  0           conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 10, 10, 256)  590080      max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 10, 10, 256)  590080      max_pooling2d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 10, 10, 256)  590080      max_pooling2d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 10, 10, 256)  590080      max_pooling2d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_54 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 10, 10, 256)  590080      conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 10, 10, 256)  590080      conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 10, 10, 256)  590080      conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 10, 10, 256)  590080      conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 16, 16, 256)  590080      max_pooling2d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 10, 10, 256)  590080      conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 10, 10, 256)  590080      conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 10, 10, 256)  590080      conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 10, 10, 256)  590080      conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 16, 16, 256)  590080      conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling2D) (None, 5, 5, 256)    0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling2D) (None, 5, 5, 256)    0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling2D) (None, 5, 5, 256)    0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling2D) (None, 5, 5, 256)    0           conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 16, 16, 256)  590080      conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 256)          0           max_pooling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 256)          0           max_pooling2d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glo (None, 256)          0           max_pooling2d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_9 (Glo (None, 256)          0           max_pooling2d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_55 (MaxPooling2D) (None, 8, 8, 256)    0           conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2048)         526336      global_average_pooling2d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2048)         526336      global_average_pooling2d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2048)         526336      global_average_pooling2d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 2048)         526336      global_average_pooling2d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_10 (Gl (None, 256)          0           max_pooling2d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 2048)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 2048)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 2048)         0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 2048)         0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2048)         526336      global_average_pooling2d_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 16)           128         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_1 (Dense)                   (None, 12)           24588       dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out_2 (Dense)                   (None, 12)           24588       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out_3 (Dense)                   (None, 12)           24588       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "out_4 (Dense)                   (None, 12)           24588       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "out_0 (Dense)                   (None, 12)           24588       dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16)           0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 76)           0           out_1[0][0]                      \n",
      "                                                                 out_2[0][0]                      \n",
      "                                                                 out_3[0][0]                      \n",
      "                                                                 out_4[0][0]                      \n",
      "                                                                 out_0[0][0]                      \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 128)          9856        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128)          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           1548        dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 20,429,032\n",
      "Trainable params: 20,429,032\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      " - 64s - loss: 13.6101 - out_loss: 2.5392 - out_0_loss: 2.1461 - out_1_loss: 2.2445 - out_2_loss: 2.2153 - out_3_loss: 2.2136 - out_4_loss: 2.2515 - out_acc: 0.0733 - out_0_acc: 0.2385 - out_1_acc: 0.1803 - out_2_acc: 0.2033 - out_3_acc: 0.2139 - out_4_acc: 0.1875 - val_loss: 12.8204 - val_out_loss: 2.4608 - val_out_0_loss: 2.0104 - val_out_1_loss: 2.1655 - val_out_2_loss: 1.9916 - val_out_3_loss: 2.0592 - val_out_4_loss: 2.1330 - val_out_acc: 0.0521 - val_out_0_acc: 0.2581 - val_out_1_acc: 0.2386 - val_out_2_acc: 0.2733 - val_out_3_acc: 0.2603 - val_out_4_acc: 0.2234\n",
      "Epoch 2/1000\n",
      " - 44s - loss: 12.7051 - out_loss: 2.4733 - out_0_loss: 1.8992 - out_1_loss: 2.1200 - out_2_loss: 2.0398 - out_3_loss: 2.0604 - out_4_loss: 2.1125 - out_acc: 0.1099 - out_0_acc: 0.3297 - out_1_acc: 0.2182 - out_2_acc: 0.2590 - out_3_acc: 0.2672 - out_4_acc: 0.2430 - val_loss: 12.1901 - val_out_loss: 2.4255 - val_out_0_loss: 1.8210 - val_out_1_loss: 1.9780 - val_out_2_loss: 1.9835 - val_out_3_loss: 1.9922 - val_out_4_loss: 1.9899 - val_out_acc: 0.1280 - val_out_0_acc: 0.3818 - val_out_1_acc: 0.3124 - val_out_2_acc: 0.3102 - val_out_3_acc: 0.3232 - val_out_4_acc: 0.2885\n",
      "Epoch 3/1000\n",
      " - 54s - loss: 12.3376 - out_loss: 2.4559 - out_0_loss: 1.8321 - out_1_loss: 2.0250 - out_2_loss: 2.0246 - out_3_loss: 1.9771 - out_4_loss: 2.0228 - out_acc: 0.1333 - out_0_acc: 0.3327 - out_1_acc: 0.2723 - out_2_acc: 0.2685 - out_3_acc: 0.2854 - out_4_acc: 0.2625 - val_loss: 11.9082 - val_out_loss: 2.3905 - val_out_0_loss: 1.7291 - val_out_1_loss: 2.0467 - val_out_2_loss: 1.9460 - val_out_3_loss: 1.8920 - val_out_4_loss: 1.9038 - val_out_acc: 0.2256 - val_out_0_acc: 0.3774 - val_out_1_acc: 0.2039 - val_out_2_acc: 0.3384 - val_out_3_acc: 0.2972 - val_out_4_acc: 0.3080\n",
      "Epoch 4/1000\n",
      " - 54s - loss: 11.7480 - out_loss: 2.4216 - out_0_loss: 1.6474 - out_1_loss: 1.9279 - out_2_loss: 1.9428 - out_3_loss: 1.8947 - out_4_loss: 1.9137 - out_acc: 0.1493 - out_0_acc: 0.4134 - out_1_acc: 0.3032 - out_2_acc: 0.3106 - out_3_acc: 0.3300 - out_4_acc: 0.3262 - val_loss: 11.1925 - val_out_loss: 2.3500 - val_out_0_loss: 1.5906 - val_out_1_loss: 1.8242 - val_out_2_loss: 1.8699 - val_out_3_loss: 1.7695 - val_out_4_loss: 1.7884 - val_out_acc: 0.2733 - val_out_0_acc: 0.3991 - val_out_1_acc: 0.3623 - val_out_2_acc: 0.3471 - val_out_3_acc: 0.3666 - val_out_4_acc: 0.3601\n",
      "Epoch 5/1000\n",
      " - 53s - loss: 11.3610 - out_loss: 2.3796 - out_0_loss: 1.5156 - out_1_loss: 1.8782 - out_2_loss: 1.9167 - out_3_loss: 1.8366 - out_4_loss: 1.8343 - out_acc: 0.2029 - out_0_acc: 0.4546 - out_1_acc: 0.3422 - out_2_acc: 0.3121 - out_3_acc: 0.3408 - out_4_acc: 0.3498 - val_loss: 10.8295 - val_out_loss: 2.3158 - val_out_0_loss: 1.5162 - val_out_1_loss: 1.7565 - val_out_2_loss: 1.8029 - val_out_3_loss: 1.7199 - val_out_4_loss: 1.7181 - val_out_acc: 0.3471 - val_out_0_acc: 0.4664 - val_out_1_acc: 0.3883 - val_out_2_acc: 0.3861 - val_out_3_acc: 0.3926 - val_out_4_acc: 0.3905\n",
      "Epoch 6/1000\n",
      " - 54s - loss: 11.0723 - out_loss: 2.3403 - out_0_loss: 1.5066 - out_1_loss: 1.8017 - out_2_loss: 1.8545 - out_3_loss: 1.7733 - out_4_loss: 1.7958 - out_acc: 0.2240 - out_0_acc: 0.4545 - out_1_acc: 0.3544 - out_2_acc: 0.3464 - out_3_acc: 0.3692 - out_4_acc: 0.3671 - val_loss: 10.8029 - val_out_loss: 2.2795 - val_out_0_loss: 1.5831 - val_out_1_loss: 1.6932 - val_out_2_loss: 1.7736 - val_out_3_loss: 1.6634 - val_out_4_loss: 1.8101 - val_out_acc: 0.3905 - val_out_0_acc: 0.4382 - val_out_1_acc: 0.3861 - val_out_2_acc: 0.3601 - val_out_3_acc: 0.4100 - val_out_4_acc: 0.3774\n",
      "Epoch 7/1000\n",
      " - 52s - loss: 10.7645 - out_loss: 2.3064 - out_0_loss: 1.4606 - out_1_loss: 1.7366 - out_2_loss: 1.7950 - out_3_loss: 1.7204 - out_4_loss: 1.7454 - out_acc: 0.2582 - out_0_acc: 0.4701 - out_1_acc: 0.3923 - out_2_acc: 0.3672 - out_3_acc: 0.4105 - out_4_acc: 0.3912 - val_loss: 10.4148 - val_out_loss: 2.2291 - val_out_0_loss: 1.4434 - val_out_1_loss: 1.6816 - val_out_2_loss: 1.7239 - val_out_3_loss: 1.6572 - val_out_4_loss: 1.6797 - val_out_acc: 0.4208 - val_out_0_acc: 0.4837 - val_out_1_acc: 0.4187 - val_out_2_acc: 0.3948 - val_out_3_acc: 0.4121 - val_out_4_acc: 0.4208\n",
      "Epoch 8/1000\n",
      " - 53s - loss: 10.5118 - out_loss: 2.2583 - out_0_loss: 1.3918 - out_1_loss: 1.7021 - out_2_loss: 1.7507 - out_3_loss: 1.7027 - out_4_loss: 1.7062 - out_acc: 0.2927 - out_0_acc: 0.4831 - out_1_acc: 0.4022 - out_2_acc: 0.3822 - out_3_acc: 0.4009 - out_4_acc: 0.4060 - val_loss: 10.3481 - val_out_loss: 2.1855 - val_out_0_loss: 1.4964 - val_out_1_loss: 1.7523 - val_out_2_loss: 1.6551 - val_out_3_loss: 1.6061 - val_out_4_loss: 1.6528 - val_out_acc: 0.4338 - val_out_0_acc: 0.5011 - val_out_1_acc: 0.3926 - val_out_2_acc: 0.4599 - val_out_3_acc: 0.4273 - val_out_4_acc: 0.4143\n",
      "Epoch 9/1000\n",
      " - 53s - loss: 10.4095 - out_loss: 2.2177 - out_0_loss: 1.4053 - out_1_loss: 1.7005 - out_2_loss: 1.7151 - out_3_loss: 1.6906 - out_4_loss: 1.6803 - out_acc: 0.3178 - out_0_acc: 0.4816 - out_1_acc: 0.3897 - out_2_acc: 0.3814 - out_3_acc: 0.4026 - out_4_acc: 0.4101 - val_loss: 10.2246 - val_out_loss: 2.1504 - val_out_0_loss: 1.4347 - val_out_1_loss: 1.7279 - val_out_2_loss: 1.6600 - val_out_3_loss: 1.5975 - val_out_4_loss: 1.6542 - val_out_acc: 0.4512 - val_out_0_acc: 0.5011 - val_out_1_acc: 0.3796 - val_out_2_acc: 0.4360 - val_out_3_acc: 0.4317 - val_out_4_acc: 0.4121\n",
      "Epoch 10/1000\n",
      " - 52s - loss: 10.2410 - out_loss: 2.1852 - out_0_loss: 1.4240 - out_1_loss: 1.6516 - out_2_loss: 1.7154 - out_3_loss: 1.6076 - out_4_loss: 1.6571 - out_acc: 0.3372 - out_0_acc: 0.4782 - out_1_acc: 0.4183 - out_2_acc: 0.3992 - out_3_acc: 0.4230 - out_4_acc: 0.4139 - val_loss: 9.7919 - val_out_loss: 2.0847 - val_out_0_loss: 1.3628 - val_out_1_loss: 1.6057 - val_out_2_loss: 1.6928 - val_out_3_loss: 1.5502 - val_out_4_loss: 1.4957 - val_out_acc: 0.4512 - val_out_0_acc: 0.5076 - val_out_1_acc: 0.4187 - val_out_2_acc: 0.3970 - val_out_3_acc: 0.4707 - val_out_4_acc: 0.4685\n",
      "Epoch 11/1000\n",
      " - 53s - loss: 9.9293 - out_loss: 2.1274 - out_0_loss: 1.3356 - out_1_loss: 1.5975 - out_2_loss: 1.6576 - out_3_loss: 1.6257 - out_4_loss: 1.5855 - out_acc: 0.3753 - out_0_acc: 0.5079 - out_1_acc: 0.4221 - out_2_acc: 0.4023 - out_3_acc: 0.4288 - out_4_acc: 0.4386 - val_loss: 9.7013 - val_out_loss: 2.0351 - val_out_0_loss: 1.4046 - val_out_1_loss: 1.5532 - val_out_2_loss: 1.5763 - val_out_3_loss: 1.5578 - val_out_4_loss: 1.5744 - val_out_acc: 0.4685 - val_out_0_acc: 0.5119 - val_out_1_acc: 0.4620 - val_out_2_acc: 0.4664 - val_out_3_acc: 0.4989 - val_out_4_acc: 0.4685\n",
      "Epoch 12/1000\n",
      " - 51s - loss: 9.6742 - out_loss: 2.0777 - out_0_loss: 1.2993 - out_1_loss: 1.5881 - out_2_loss: 1.6108 - out_3_loss: 1.5427 - out_4_loss: 1.5556 - out_acc: 0.3987 - out_0_acc: 0.5099 - out_1_acc: 0.4322 - out_2_acc: 0.4226 - out_3_acc: 0.4527 - out_4_acc: 0.4471 - val_loss: 9.2694 - val_out_loss: 1.9649 - val_out_0_loss: 1.3046 - val_out_1_loss: 1.4887 - val_out_2_loss: 1.5018 - val_out_3_loss: 1.4742 - val_out_4_loss: 1.5351 - val_out_acc: 0.4837 - val_out_0_acc: 0.5098 - val_out_1_acc: 0.4794 - val_out_2_acc: 0.4685 - val_out_3_acc: 0.4859 - val_out_4_acc: 0.4534\n",
      "Epoch 13/1000\n",
      " - 52s - loss: 9.6241 - out_loss: 2.0263 - out_0_loss: 1.3088 - out_1_loss: 1.5658 - out_2_loss: 1.5831 - out_3_loss: 1.5639 - out_4_loss: 1.5760 - out_acc: 0.4201 - out_0_acc: 0.5069 - out_1_acc: 0.4304 - out_2_acc: 0.4353 - out_3_acc: 0.4454 - out_4_acc: 0.4429 - val_loss: 9.4907 - val_out_loss: 1.9215 - val_out_0_loss: 1.4258 - val_out_1_loss: 1.5295 - val_out_2_loss: 1.4882 - val_out_3_loss: 1.5610 - val_out_4_loss: 1.5648 - val_out_acc: 0.4707 - val_out_0_acc: 0.4729 - val_out_1_acc: 0.4685 - val_out_2_acc: 0.4512 - val_out_3_acc: 0.4490 - val_out_4_acc: 0.4382\n",
      "Epoch 14/1000\n",
      " - 52s - loss: 9.6536 - out_loss: 1.9950 - out_0_loss: 1.3958 - out_1_loss: 1.5419 - out_2_loss: 1.5613 - out_3_loss: 1.5774 - out_4_loss: 1.5822 - out_acc: 0.4173 - out_0_acc: 0.4911 - out_1_acc: 0.4487 - out_2_acc: 0.4451 - out_3_acc: 0.4316 - out_4_acc: 0.4352 - val_loss: 9.6749 - val_out_loss: 1.9053 - val_out_0_loss: 1.4313 - val_out_1_loss: 1.7331 - val_out_2_loss: 1.5119 - val_out_3_loss: 1.5607 - val_out_4_loss: 1.5325 - val_out_acc: 0.4360 - val_out_0_acc: 0.4577 - val_out_1_acc: 0.4035 - val_out_2_acc: 0.4707 - val_out_3_acc: 0.4707 - val_out_4_acc: 0.4685\n",
      "Epoch 15/1000\n",
      " - 51s - loss: 9.3460 - out_loss: 1.9466 - out_0_loss: 1.3054 - out_1_loss: 1.5349 - out_2_loss: 1.5173 - out_3_loss: 1.4873 - out_4_loss: 1.5545 - out_acc: 0.4370 - out_0_acc: 0.5193 - out_1_acc: 0.4500 - out_2_acc: 0.4474 - out_3_acc: 0.4691 - out_4_acc: 0.4417 - val_loss: 9.1169 - val_out_loss: 1.8468 - val_out_0_loss: 1.2913 - val_out_1_loss: 1.4750 - val_out_2_loss: 1.4648 - val_out_3_loss: 1.4426 - val_out_4_loss: 1.5964 - val_out_acc: 0.4620 - val_out_0_acc: 0.5184 - val_out_1_acc: 0.4707 - val_out_2_acc: 0.4403 - val_out_3_acc: 0.5054 - val_out_4_acc: 0.4230\n",
      "Epoch 16/1000\n",
      " - 51s - loss: 9.1683 - out_loss: 1.9042 - out_0_loss: 1.2543 - out_1_loss: 1.5029 - out_2_loss: 1.4951 - out_3_loss: 1.4811 - out_4_loss: 1.5305 - out_acc: 0.4432 - out_0_acc: 0.5233 - out_1_acc: 0.4433 - out_2_acc: 0.4618 - out_3_acc: 0.4642 - out_4_acc: 0.4499 - val_loss: 9.0567 - val_out_loss: 1.7634 - val_out_0_loss: 1.2616 - val_out_1_loss: 1.4728 - val_out_2_loss: 1.5423 - val_out_3_loss: 1.5126 - val_out_4_loss: 1.5040 - val_out_acc: 0.5033 - val_out_0_acc: 0.5380 - val_out_1_acc: 0.4512 - val_out_2_acc: 0.4729 - val_out_3_acc: 0.4816 - val_out_4_acc: 0.4729\n",
      "Epoch 17/1000\n",
      " - 51s - loss: 8.9514 - out_loss: 1.8430 - out_0_loss: 1.1958 - out_1_loss: 1.5128 - out_2_loss: 1.4789 - out_3_loss: 1.4708 - out_4_loss: 1.4501 - out_acc: 0.4672 - out_0_acc: 0.5567 - out_1_acc: 0.4481 - out_2_acc: 0.4736 - out_3_acc: 0.4713 - out_4_acc: 0.4717 - val_loss: 8.9249 - val_out_loss: 1.7445 - val_out_0_loss: 1.2350 - val_out_1_loss: 1.5782 - val_out_2_loss: 1.4637 - val_out_3_loss: 1.4638 - val_out_4_loss: 1.4397 - val_out_acc: 0.5141 - val_out_0_acc: 0.5922 - val_out_1_acc: 0.4230 - val_out_2_acc: 0.4881 - val_out_3_acc: 0.4816 - val_out_4_acc: 0.5033\n",
      "Epoch 18/1000\n",
      " - 50s - loss: 8.8723 - out_loss: 1.7972 - out_0_loss: 1.2282 - out_1_loss: 1.4612 - out_2_loss: 1.4616 - out_3_loss: 1.4461 - out_4_loss: 1.4779 - out_acc: 0.4806 - out_0_acc: 0.5414 - out_1_acc: 0.4639 - out_2_acc: 0.4716 - out_3_acc: 0.4800 - out_4_acc: 0.4708 - val_loss: 8.5514 - val_out_loss: 1.6433 - val_out_0_loss: 1.1437 - val_out_1_loss: 1.4233 - val_out_2_loss: 1.5085 - val_out_3_loss: 1.4400 - val_out_4_loss: 1.3925 - val_out_acc: 0.5141 - val_out_0_acc: 0.5879 - val_out_1_acc: 0.4837 - val_out_2_acc: 0.4729 - val_out_3_acc: 0.5098 - val_out_4_acc: 0.4859\n",
      "Epoch 19/1000\n",
      " - 50s - loss: 8.6939 - out_loss: 1.7476 - out_0_loss: 1.1746 - out_1_loss: 1.4426 - out_2_loss: 1.4765 - out_3_loss: 1.4242 - out_4_loss: 1.4284 - out_acc: 0.4915 - out_0_acc: 0.5672 - out_1_acc: 0.4727 - out_2_acc: 0.4698 - out_3_acc: 0.4758 - out_4_acc: 0.4906 - val_loss: 8.6986 - val_out_loss: 1.6359 - val_out_0_loss: 1.2144 - val_out_1_loss: 1.4090 - val_out_2_loss: 1.4843 - val_out_3_loss: 1.4597 - val_out_4_loss: 1.4954 - val_out_acc: 0.5293 - val_out_0_acc: 0.5683 - val_out_1_acc: 0.4837 - val_out_2_acc: 0.4859 - val_out_3_acc: 0.4599 - val_out_4_acc: 0.4469\n",
      "Epoch 20/1000\n",
      " - 50s - loss: 8.6700 - out_loss: 1.7097 - out_0_loss: 1.1448 - out_1_loss: 1.4636 - out_2_loss: 1.4900 - out_3_loss: 1.4037 - out_4_loss: 1.4580 - out_acc: 0.4939 - out_0_acc: 0.5874 - out_1_acc: 0.4719 - out_2_acc: 0.4665 - out_3_acc: 0.4889 - out_4_acc: 0.4681 - val_loss: 8.3431 - val_out_loss: 1.5722 - val_out_0_loss: 1.1277 - val_out_1_loss: 1.4353 - val_out_2_loss: 1.3940 - val_out_3_loss: 1.4120 - val_out_4_loss: 1.4021 - val_out_acc: 0.5380 - val_out_0_acc: 0.5618 - val_out_1_acc: 0.4664 - val_out_2_acc: 0.5011 - val_out_3_acc: 0.4946 - val_out_4_acc: 0.4967\n",
      "Epoch 21/1000\n",
      " - 50s - loss: 8.5242 - out_loss: 1.6787 - out_0_loss: 1.1599 - out_1_loss: 1.4365 - out_2_loss: 1.4149 - out_3_loss: 1.3907 - out_4_loss: 1.4434 - out_acc: 0.4941 - out_0_acc: 0.5735 - out_1_acc: 0.4745 - out_2_acc: 0.4849 - out_3_acc: 0.4939 - out_4_acc: 0.4721 - val_loss: 8.3689 - val_out_loss: 1.5341 - val_out_0_loss: 1.2139 - val_out_1_loss: 1.5290 - val_out_2_loss: 1.3671 - val_out_3_loss: 1.3633 - val_out_4_loss: 1.3615 - val_out_acc: 0.5575 - val_out_0_acc: 0.5531 - val_out_1_acc: 0.4599 - val_out_2_acc: 0.5141 - val_out_3_acc: 0.4772 - val_out_4_acc: 0.5163\n",
      "Epoch 22/1000\n",
      " - 50s - loss: 8.2977 - out_loss: 1.6228 - out_0_loss: 1.1405 - out_1_loss: 1.3755 - out_2_loss: 1.3770 - out_3_loss: 1.3694 - out_4_loss: 1.4126 - out_acc: 0.5119 - out_0_acc: 0.5851 - out_1_acc: 0.4935 - out_2_acc: 0.5008 - out_3_acc: 0.5092 - out_4_acc: 0.4809 - val_loss: 7.8558 - val_out_loss: 1.4722 - val_out_0_loss: 1.1009 - val_out_1_loss: 1.3501 - val_out_2_loss: 1.2842 - val_out_3_loss: 1.3521 - val_out_4_loss: 1.2962 - val_out_acc: 0.5748 - val_out_0_acc: 0.6030 - val_out_1_acc: 0.5098 - val_out_2_acc: 0.5640 - val_out_3_acc: 0.4837 - val_out_4_acc: 0.5293\n",
      "Epoch 23/1000\n",
      " - 51s - loss: 8.2169 - out_loss: 1.5919 - out_0_loss: 1.1106 - out_1_loss: 1.3612 - out_2_loss: 1.3802 - out_3_loss: 1.3500 - out_4_loss: 1.4230 - out_acc: 0.5228 - out_0_acc: 0.5966 - out_1_acc: 0.5116 - out_2_acc: 0.5084 - out_3_acc: 0.5030 - out_4_acc: 0.4825 - val_loss: 7.9506 - val_out_loss: 1.4589 - val_out_0_loss: 1.0694 - val_out_1_loss: 1.3805 - val_out_2_loss: 1.3259 - val_out_3_loss: 1.3086 - val_out_4_loss: 1.4074 - val_out_acc: 0.5531 - val_out_0_acc: 0.5857 - val_out_1_acc: 0.4881 - val_out_2_acc: 0.5098 - val_out_3_acc: 0.5249 - val_out_4_acc: 0.4881\n",
      "Epoch 24/1000\n",
      " - 50s - loss: 8.0535 - out_loss: 1.5422 - out_0_loss: 1.0726 - out_1_loss: 1.3561 - out_2_loss: 1.3802 - out_3_loss: 1.3194 - out_4_loss: 1.3829 - out_acc: 0.5307 - out_0_acc: 0.6105 - out_1_acc: 0.5019 - out_2_acc: 0.4976 - out_3_acc: 0.5180 - out_4_acc: 0.4932 - val_loss: 8.0724 - val_out_loss: 1.4232 - val_out_0_loss: 1.1299 - val_out_1_loss: 1.3742 - val_out_2_loss: 1.4342 - val_out_3_loss: 1.3236 - val_out_4_loss: 1.3872 - val_out_acc: 0.5727 - val_out_0_acc: 0.5965 - val_out_1_acc: 0.5271 - val_out_2_acc: 0.4837 - val_out_3_acc: 0.4881 - val_out_4_acc: 0.4924\n",
      "Epoch 25/1000\n",
      " - 50s - loss: 7.9575 - out_loss: 1.5035 - out_0_loss: 1.1016 - out_1_loss: 1.3327 - out_2_loss: 1.3387 - out_3_loss: 1.3336 - out_4_loss: 1.3473 - out_acc: 0.5391 - out_0_acc: 0.5910 - out_1_acc: 0.5117 - out_2_acc: 0.5162 - out_3_acc: 0.5088 - out_4_acc: 0.5035 - val_loss: 7.9889 - val_out_loss: 1.3968 - val_out_0_loss: 1.1417 - val_out_1_loss: 1.2852 - val_out_2_loss: 1.3868 - val_out_3_loss: 1.3229 - val_out_4_loss: 1.4555 - val_out_acc: 0.5618 - val_out_0_acc: 0.5879 - val_out_1_acc: 0.5401 - val_out_2_acc: 0.4794 - val_out_3_acc: 0.5098 - val_out_4_acc: 0.4642\n",
      "Epoch 26/1000\n",
      " - 49s - loss: 7.7894 - out_loss: 1.4583 - out_0_loss: 1.0373 - out_1_loss: 1.3468 - out_2_loss: 1.3120 - out_3_loss: 1.2830 - out_4_loss: 1.3519 - out_acc: 0.5514 - out_0_acc: 0.6200 - out_1_acc: 0.5086 - out_2_acc: 0.5299 - out_3_acc: 0.5325 - out_4_acc: 0.5112 - val_loss: 7.9876 - val_out_loss: 1.3665 - val_out_0_loss: 1.1323 - val_out_1_loss: 1.3467 - val_out_2_loss: 1.3891 - val_out_3_loss: 1.3172 - val_out_4_loss: 1.4358 - val_out_acc: 0.5813 - val_out_0_acc: 0.5944 - val_out_1_acc: 0.5098 - val_out_2_acc: 0.5011 - val_out_3_acc: 0.5076 - val_out_4_acc: 0.5228\n",
      "Epoch 27/1000\n",
      " - 50s - loss: 7.8674 - out_loss: 1.4450 - out_0_loss: 1.0933 - out_1_loss: 1.3411 - out_2_loss: 1.3280 - out_3_loss: 1.3030 - out_4_loss: 1.3570 - out_acc: 0.5467 - out_0_acc: 0.6031 - out_1_acc: 0.5113 - out_2_acc: 0.5121 - out_3_acc: 0.5149 - out_4_acc: 0.5029 - val_loss: 7.7131 - val_out_loss: 1.3206 - val_out_0_loss: 1.0694 - val_out_1_loss: 1.3666 - val_out_2_loss: 1.3412 - val_out_3_loss: 1.2662 - val_out_4_loss: 1.3491 - val_out_acc: 0.6139 - val_out_0_acc: 0.6421 - val_out_1_acc: 0.5119 - val_out_2_acc: 0.5336 - val_out_3_acc: 0.5618 - val_out_4_acc: 0.5076\n",
      "Epoch 28/1000\n",
      " - 50s - loss: 7.8633 - out_loss: 1.4334 - out_0_loss: 1.0976 - out_1_loss: 1.3192 - out_2_loss: 1.3385 - out_3_loss: 1.3070 - out_4_loss: 1.3675 - out_acc: 0.5534 - out_0_acc: 0.5961 - out_1_acc: 0.5149 - out_2_acc: 0.5126 - out_3_acc: 0.5129 - out_4_acc: 0.5019 - val_loss: 7.6461 - val_out_loss: 1.2896 - val_out_0_loss: 1.0714 - val_out_1_loss: 1.3209 - val_out_2_loss: 1.2805 - val_out_3_loss: 1.2930 - val_out_4_loss: 1.3907 - val_out_acc: 0.5835 - val_out_0_acc: 0.6052 - val_out_1_acc: 0.4946 - val_out_2_acc: 0.5336 - val_out_3_acc: 0.5271 - val_out_4_acc: 0.4967\n",
      "Epoch 29/1000\n",
      " - 49s - loss: 7.7068 - out_loss: 1.3978 - out_0_loss: 1.0652 - out_1_loss: 1.3188 - out_2_loss: 1.3126 - out_3_loss: 1.2712 - out_4_loss: 1.3411 - out_acc: 0.5608 - out_0_acc: 0.6101 - out_1_acc: 0.5155 - out_2_acc: 0.5172 - out_3_acc: 0.5396 - out_4_acc: 0.5208 - val_loss: 7.2761 - val_out_loss: 1.2547 - val_out_0_loss: 0.9982 - val_out_1_loss: 1.2429 - val_out_2_loss: 1.2728 - val_out_3_loss: 1.2588 - val_out_4_loss: 1.2486 - val_out_acc: 0.6226 - val_out_0_acc: 0.5944 - val_out_1_acc: 0.5662 - val_out_2_acc: 0.5575 - val_out_3_acc: 0.5575 - val_out_4_acc: 0.5597\n",
      "Epoch 30/1000\n",
      " - 50s - loss: 7.6259 - out_loss: 1.3814 - out_0_loss: 1.0629 - out_1_loss: 1.2804 - out_2_loss: 1.2876 - out_3_loss: 1.3021 - out_4_loss: 1.3116 - out_acc: 0.5616 - out_0_acc: 0.6150 - out_1_acc: 0.5320 - out_2_acc: 0.5264 - out_3_acc: 0.5266 - out_4_acc: 0.5165 - val_loss: 7.3831 - val_out_loss: 1.2483 - val_out_0_loss: 1.0281 - val_out_1_loss: 1.2534 - val_out_2_loss: 1.2673 - val_out_3_loss: 1.2458 - val_out_4_loss: 1.3402 - val_out_acc: 0.6030 - val_out_0_acc: 0.6377 - val_out_1_acc: 0.5727 - val_out_2_acc: 0.5293 - val_out_3_acc: 0.5445 - val_out_4_acc: 0.5163\n",
      "Epoch 31/1000\n",
      " - 48s - loss: 7.4750 - out_loss: 1.3365 - out_0_loss: 1.0224 - out_1_loss: 1.2698 - out_2_loss: 1.3062 - out_3_loss: 1.2473 - out_4_loss: 1.2927 - out_acc: 0.5793 - out_0_acc: 0.6315 - out_1_acc: 0.5320 - out_2_acc: 0.5287 - out_3_acc: 0.5560 - out_4_acc: 0.5271 - val_loss: 7.5015 - val_out_loss: 1.2439 - val_out_0_loss: 1.1718 - val_out_1_loss: 1.2831 - val_out_2_loss: 1.2635 - val_out_3_loss: 1.2110 - val_out_4_loss: 1.3282 - val_out_acc: 0.6117 - val_out_0_acc: 0.5879 - val_out_1_acc: 0.5249 - val_out_2_acc: 0.5466 - val_out_3_acc: 0.5640 - val_out_4_acc: 0.5336\n",
      "Epoch 32/1000\n",
      " - 49s - loss: 7.4829 - out_loss: 1.3341 - out_0_loss: 1.0196 - out_1_loss: 1.2696 - out_2_loss: 1.2647 - out_3_loss: 1.2892 - out_4_loss: 1.3057 - out_acc: 0.5776 - out_0_acc: 0.6258 - out_1_acc: 0.5448 - out_2_acc: 0.5447 - out_3_acc: 0.5462 - out_4_acc: 0.5314 - val_loss: 7.6498 - val_out_loss: 1.2275 - val_out_0_loss: 1.0578 - val_out_1_loss: 1.2956 - val_out_2_loss: 1.3006 - val_out_3_loss: 1.3999 - val_out_4_loss: 1.3684 - val_out_acc: 0.6226 - val_out_0_acc: 0.6247 - val_out_1_acc: 0.5315 - val_out_2_acc: 0.5445 - val_out_3_acc: 0.4599 - val_out_4_acc: 0.5184\n",
      "Epoch 33/1000\n",
      " - 49s - loss: 7.3440 - out_loss: 1.2957 - out_0_loss: 0.9926 - out_1_loss: 1.2645 - out_2_loss: 1.2615 - out_3_loss: 1.2636 - out_4_loss: 1.2661 - out_acc: 0.5961 - out_0_acc: 0.6445 - out_1_acc: 0.5460 - out_2_acc: 0.5552 - out_3_acc: 0.5339 - out_4_acc: 0.5427 - val_loss: 7.5935 - val_out_loss: 1.2089 - val_out_0_loss: 1.0936 - val_out_1_loss: 1.2900 - val_out_2_loss: 1.3088 - val_out_3_loss: 1.3442 - val_out_4_loss: 1.3481 - val_out_acc: 0.6399 - val_out_0_acc: 0.5965 - val_out_1_acc: 0.4989 - val_out_2_acc: 0.4946 - val_out_3_acc: 0.5163 - val_out_4_acc: 0.5293\n",
      "Epoch 34/1000\n",
      " - 51s - loss: 7.3548 - out_loss: 1.2819 - out_0_loss: 1.0498 - out_1_loss: 1.2502 - out_2_loss: 1.2665 - out_3_loss: 1.2353 - out_4_loss: 1.2711 - out_acc: 0.5961 - out_0_acc: 0.6197 - out_1_acc: 0.5474 - out_2_acc: 0.5448 - out_3_acc: 0.5528 - out_4_acc: 0.5310 - val_loss: 7.1646 - val_out_loss: 1.1633 - val_out_0_loss: 1.0601 - val_out_1_loss: 1.1878 - val_out_2_loss: 1.2324 - val_out_3_loss: 1.2531 - val_out_4_loss: 1.2678 - val_out_acc: 0.6508 - val_out_0_acc: 0.6030 - val_out_1_acc: 0.5640 - val_out_2_acc: 0.5401 - val_out_3_acc: 0.5054 - val_out_4_acc: 0.5748\n",
      "Epoch 35/1000\n",
      " - 50s - loss: 7.2056 - out_loss: 1.2574 - out_0_loss: 0.9735 - out_1_loss: 1.2425 - out_2_loss: 1.2400 - out_3_loss: 1.2389 - out_4_loss: 1.2533 - out_acc: 0.6052 - out_0_acc: 0.6418 - out_1_acc: 0.5461 - out_2_acc: 0.5571 - out_3_acc: 0.5493 - out_4_acc: 0.5581 - val_loss: 7.1219 - val_out_loss: 1.1408 - val_out_0_loss: 1.0285 - val_out_1_loss: 1.2806 - val_out_2_loss: 1.2450 - val_out_3_loss: 1.1985 - val_out_4_loss: 1.2285 - val_out_acc: 0.6421 - val_out_0_acc: 0.6182 - val_out_1_acc: 0.5184 - val_out_2_acc: 0.5358 - val_out_3_acc: 0.5770 - val_out_4_acc: 0.5510\n",
      "Epoch 36/1000\n",
      " - 50s - loss: 7.2566 - out_loss: 1.2404 - out_0_loss: 1.0150 - out_1_loss: 1.2649 - out_2_loss: 1.2495 - out_3_loss: 1.2196 - out_4_loss: 1.2672 - out_acc: 0.6100 - out_0_acc: 0.6263 - out_1_acc: 0.5323 - out_2_acc: 0.5461 - out_3_acc: 0.5485 - out_4_acc: 0.5338 - val_loss: 7.2280 - val_out_loss: 1.1546 - val_out_0_loss: 1.0508 - val_out_1_loss: 1.2693 - val_out_2_loss: 1.2978 - val_out_3_loss: 1.2150 - val_out_4_loss: 1.2405 - val_out_acc: 0.6356 - val_out_0_acc: 0.6204 - val_out_1_acc: 0.5510 - val_out_2_acc: 0.5423 - val_out_3_acc: 0.5575 - val_out_4_acc: 0.5575\n",
      "Epoch 37/1000\n",
      " - 49s - loss: 7.1716 - out_loss: 1.2237 - out_0_loss: 1.0019 - out_1_loss: 1.2343 - out_2_loss: 1.2406 - out_3_loss: 1.2292 - out_4_loss: 1.2419 - out_acc: 0.6190 - out_0_acc: 0.6438 - out_1_acc: 0.5531 - out_2_acc: 0.5539 - out_3_acc: 0.5563 - out_4_acc: 0.5450 - val_loss: 7.4107 - val_out_loss: 1.1400 - val_out_0_loss: 1.1196 - val_out_1_loss: 1.2979 - val_out_2_loss: 1.3243 - val_out_3_loss: 1.1892 - val_out_4_loss: 1.3397 - val_out_acc: 0.6291 - val_out_0_acc: 0.6030 - val_out_1_acc: 0.5228 - val_out_2_acc: 0.5228 - val_out_3_acc: 0.5662 - val_out_4_acc: 0.5163\n",
      "Epoch 38/1000\n",
      " - 49s - loss: 7.0674 - out_loss: 1.2004 - out_0_loss: 0.9715 - out_1_loss: 1.2236 - out_2_loss: 1.2282 - out_3_loss: 1.2185 - out_4_loss: 1.2253 - out_acc: 0.6090 - out_0_acc: 0.6403 - out_1_acc: 0.5406 - out_2_acc: 0.5568 - out_3_acc: 0.5621 - out_4_acc: 0.5646 - val_loss: 7.1386 - val_out_loss: 1.1194 - val_out_0_loss: 1.0238 - val_out_1_loss: 1.2872 - val_out_2_loss: 1.1707 - val_out_3_loss: 1.3131 - val_out_4_loss: 1.2244 - val_out_acc: 0.6464 - val_out_0_acc: 0.6161 - val_out_1_acc: 0.5336 - val_out_2_acc: 0.5705 - val_out_3_acc: 0.5271 - val_out_4_acc: 0.5510\n",
      "Epoch 39/1000\n",
      " - 48s - loss: 6.9556 - out_loss: 1.1704 - out_0_loss: 0.9615 - out_1_loss: 1.2399 - out_2_loss: 1.2141 - out_3_loss: 1.1624 - out_4_loss: 1.2073 - out_acc: 0.6321 - out_0_acc: 0.6601 - out_1_acc: 0.5510 - out_2_acc: 0.5652 - out_3_acc: 0.5745 - out_4_acc: 0.5703 - val_loss: 6.8169 - val_out_loss: 1.0665 - val_out_0_loss: 1.0009 - val_out_1_loss: 1.1735 - val_out_2_loss: 1.1956 - val_out_3_loss: 1.1364 - val_out_4_loss: 1.2439 - val_out_acc: 0.6464 - val_out_0_acc: 0.6095 - val_out_1_acc: 0.5662 - val_out_2_acc: 0.5705 - val_out_3_acc: 0.5922 - val_out_4_acc: 0.5683\n",
      "Epoch 40/1000\n",
      " - 49s - loss: 6.9308 - out_loss: 1.1543 - out_0_loss: 0.9493 - out_1_loss: 1.1848 - out_2_loss: 1.1960 - out_3_loss: 1.1998 - out_4_loss: 1.2466 - out_acc: 0.6346 - out_0_acc: 0.6566 - out_1_acc: 0.5545 - out_2_acc: 0.5717 - out_3_acc: 0.5549 - out_4_acc: 0.5551 - val_loss: 7.1433 - val_out_loss: 1.0848 - val_out_0_loss: 0.9807 - val_out_1_loss: 1.2708 - val_out_2_loss: 1.2582 - val_out_3_loss: 1.3202 - val_out_4_loss: 1.2285 - val_out_acc: 0.6573 - val_out_0_acc: 0.6074 - val_out_1_acc: 0.5228 - val_out_2_acc: 0.5423 - val_out_3_acc: 0.5293 - val_out_4_acc: 0.5401\n",
      "Epoch 41/1000\n",
      " - 49s - loss: 7.0188 - out_loss: 1.1737 - out_0_loss: 0.9906 - out_1_loss: 1.2127 - out_2_loss: 1.2145 - out_3_loss: 1.1950 - out_4_loss: 1.2323 - out_acc: 0.6263 - out_0_acc: 0.6414 - out_1_acc: 0.5621 - out_2_acc: 0.5681 - out_3_acc: 0.5753 - out_4_acc: 0.5472 - val_loss: 6.8915 - val_out_loss: 1.0802 - val_out_0_loss: 0.9614 - val_out_1_loss: 1.1811 - val_out_2_loss: 1.1925 - val_out_3_loss: 1.2405 - val_out_4_loss: 1.2358 - val_out_acc: 0.6508 - val_out_0_acc: 0.6421 - val_out_1_acc: 0.5727 - val_out_2_acc: 0.5575 - val_out_3_acc: 0.5575 - val_out_4_acc: 0.5401\n",
      "Epoch 42/1000\n",
      " - 49s - loss: 6.8498 - out_loss: 1.1268 - out_0_loss: 0.9271 - out_1_loss: 1.1876 - out_2_loss: 1.2063 - out_3_loss: 1.1913 - out_4_loss: 1.2107 - out_acc: 0.6397 - out_0_acc: 0.6571 - out_1_acc: 0.5566 - out_2_acc: 0.5623 - out_3_acc: 0.5573 - out_4_acc: 0.5579 - val_loss: 7.1273 - val_out_loss: 1.0487 - val_out_0_loss: 1.0588 - val_out_1_loss: 1.2689 - val_out_2_loss: 1.1543 - val_out_3_loss: 1.3812 - val_out_4_loss: 1.2154 - val_out_acc: 0.6703 - val_out_0_acc: 0.6269 - val_out_1_acc: 0.5553 - val_out_2_acc: 0.5879 - val_out_3_acc: 0.4946 - val_out_4_acc: 0.5466\n",
      "Epoch 43/1000\n",
      " - 49s - loss: 6.7976 - out_loss: 1.1236 - out_0_loss: 0.9417 - out_1_loss: 1.1819 - out_2_loss: 1.1925 - out_3_loss: 1.1500 - out_4_loss: 1.2078 - out_acc: 0.6431 - out_0_acc: 0.6573 - out_1_acc: 0.5715 - out_2_acc: 0.5679 - out_3_acc: 0.5823 - out_4_acc: 0.5579 - val_loss: 6.8955 - val_out_loss: 1.0445 - val_out_0_loss: 0.9441 - val_out_1_loss: 1.2096 - val_out_2_loss: 1.2087 - val_out_3_loss: 1.1840 - val_out_4_loss: 1.3044 - val_out_acc: 0.6508 - val_out_0_acc: 0.6421 - val_out_1_acc: 0.5618 - val_out_2_acc: 0.5662 - val_out_3_acc: 0.5553 - val_out_4_acc: 0.5141\n",
      "Epoch 44/1000\n",
      " - 49s - loss: 6.7061 - out_loss: 1.0996 - out_0_loss: 0.9307 - out_1_loss: 1.1811 - out_2_loss: 1.1804 - out_3_loss: 1.1327 - out_4_loss: 1.1816 - out_acc: 0.6453 - out_0_acc: 0.6570 - out_1_acc: 0.5774 - out_2_acc: 0.5714 - out_3_acc: 0.5919 - out_4_acc: 0.5725 - val_loss: 7.1247 - val_out_loss: 1.0841 - val_out_0_loss: 1.0525 - val_out_1_loss: 1.2701 - val_out_2_loss: 1.3014 - val_out_3_loss: 1.1961 - val_out_4_loss: 1.2204 - val_out_acc: 0.6594 - val_out_0_acc: 0.6030 - val_out_1_acc: 0.5401 - val_out_2_acc: 0.5445 - val_out_3_acc: 0.5705 - val_out_4_acc: 0.5553\n",
      "Epoch 45/1000\n",
      " - 48s - loss: 6.7460 - out_loss: 1.1059 - out_0_loss: 0.9429 - out_1_loss: 1.1734 - out_2_loss: 1.1885 - out_3_loss: 1.1631 - out_4_loss: 1.1721 - out_acc: 0.6475 - out_0_acc: 0.6530 - out_1_acc: 0.5749 - out_2_acc: 0.5697 - out_3_acc: 0.5782 - out_4_acc: 0.5669 - val_loss: 6.7746 - val_out_loss: 1.0329 - val_out_0_loss: 0.9796 - val_out_1_loss: 1.1828 - val_out_2_loss: 1.2491 - val_out_3_loss: 1.1836 - val_out_4_loss: 1.1467 - val_out_acc: 0.6746 - val_out_0_acc: 0.6312 - val_out_1_acc: 0.5792 - val_out_2_acc: 0.5510 - val_out_3_acc: 0.5662 - val_out_4_acc: 0.5792\n",
      "Epoch 46/1000\n",
      " - 50s - loss: 6.5883 - out_loss: 1.0590 - out_0_loss: 0.9503 - out_1_loss: 1.1408 - out_2_loss: 1.1491 - out_3_loss: 1.1334 - out_4_loss: 1.1556 - out_acc: 0.6705 - out_0_acc: 0.6481 - out_1_acc: 0.5894 - out_2_acc: 0.5843 - out_3_acc: 0.5899 - out_4_acc: 0.5809 - val_loss: 6.6269 - val_out_loss: 0.9871 - val_out_0_loss: 0.9312 - val_out_1_loss: 1.2166 - val_out_2_loss: 1.1225 - val_out_3_loss: 1.2148 - val_out_4_loss: 1.1547 - val_out_acc: 0.6963 - val_out_0_acc: 0.6464 - val_out_1_acc: 0.5727 - val_out_2_acc: 0.5922 - val_out_3_acc: 0.5879 - val_out_4_acc: 0.5770\n",
      "Epoch 47/1000\n",
      " - 49s - loss: 6.5583 - out_loss: 1.0634 - out_0_loss: 0.9204 - out_1_loss: 1.1402 - out_2_loss: 1.1425 - out_3_loss: 1.1242 - out_4_loss: 1.1676 - out_acc: 0.6550 - out_0_acc: 0.6578 - out_1_acc: 0.5691 - out_2_acc: 0.5858 - out_3_acc: 0.5800 - out_4_acc: 0.5669 - val_loss: 6.8148 - val_out_loss: 0.9939 - val_out_0_loss: 0.9674 - val_out_1_loss: 1.2971 - val_out_2_loss: 1.1262 - val_out_3_loss: 1.2273 - val_out_4_loss: 1.2029 - val_out_acc: 0.7072 - val_out_0_acc: 0.6508 - val_out_1_acc: 0.5380 - val_out_2_acc: 0.6095 - val_out_3_acc: 0.5618 - val_out_4_acc: 0.5510\n",
      "Epoch 48/1000\n",
      " - 49s - loss: 6.6357 - out_loss: 1.0658 - out_0_loss: 0.9280 - out_1_loss: 1.1710 - out_2_loss: 1.1828 - out_3_loss: 1.1415 - out_4_loss: 1.1465 - out_acc: 0.6605 - out_0_acc: 0.6588 - out_1_acc: 0.5845 - out_2_acc: 0.5840 - out_3_acc: 0.5879 - out_4_acc: 0.5763 - val_loss: 6.6780 - val_out_loss: 1.0032 - val_out_0_loss: 0.9934 - val_out_1_loss: 1.1885 - val_out_2_loss: 1.1432 - val_out_3_loss: 1.1461 - val_out_4_loss: 1.2035 - val_out_acc: 0.6855 - val_out_0_acc: 0.6703 - val_out_1_acc: 0.5662 - val_out_2_acc: 0.6052 - val_out_3_acc: 0.5792 - val_out_4_acc: 0.5683\n",
      "Epoch 49/1000\n",
      " - 49s - loss: 6.5005 - out_loss: 1.0399 - out_0_loss: 0.9284 - out_1_loss: 1.1587 - out_2_loss: 1.1273 - out_3_loss: 1.1163 - out_4_loss: 1.1298 - out_acc: 0.6616 - out_0_acc: 0.6679 - out_1_acc: 0.5827 - out_2_acc: 0.5933 - out_3_acc: 0.5971 - out_4_acc: 0.5959 - val_loss: 6.3669 - val_out_loss: 0.9536 - val_out_0_loss: 0.8901 - val_out_1_loss: 1.0707 - val_out_2_loss: 1.1935 - val_out_3_loss: 1.0978 - val_out_4_loss: 1.1612 - val_out_acc: 0.7072 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.6139 - val_out_2_acc: 0.5835 - val_out_3_acc: 0.6052 - val_out_4_acc: 0.5770\n",
      "Epoch 50/1000\n",
      " - 50s - loss: 6.6416 - out_loss: 1.0512 - out_0_loss: 0.9061 - out_1_loss: 1.1637 - out_2_loss: 1.1881 - out_3_loss: 1.1550 - out_4_loss: 1.1775 - out_acc: 0.6655 - out_0_acc: 0.6746 - out_1_acc: 0.5738 - out_2_acc: 0.5784 - out_3_acc: 0.5821 - out_4_acc: 0.5717 - val_loss: 6.8413 - val_out_loss: 0.9989 - val_out_0_loss: 1.0960 - val_out_1_loss: 1.1782 - val_out_2_loss: 1.1806 - val_out_3_loss: 1.1946 - val_out_4_loss: 1.1929 - val_out_acc: 0.6681 - val_out_0_acc: 0.5922 - val_out_1_acc: 0.5705 - val_out_2_acc: 0.5727 - val_out_3_acc: 0.5488 - val_out_4_acc: 0.5835\n",
      "Epoch 51/1000\n",
      " - 49s - loss: 6.3825 - out_loss: 1.0043 - out_0_loss: 0.8775 - out_1_loss: 1.1130 - out_2_loss: 1.1391 - out_3_loss: 1.1097 - out_4_loss: 1.1388 - out_acc: 0.6813 - out_0_acc: 0.6687 - out_1_acc: 0.5948 - out_2_acc: 0.5798 - out_3_acc: 0.6072 - out_4_acc: 0.5879 - val_loss: 6.4840 - val_out_loss: 0.9259 - val_out_0_loss: 0.8511 - val_out_1_loss: 1.1879 - val_out_2_loss: 1.1528 - val_out_3_loss: 1.1478 - val_out_4_loss: 1.2184 - val_out_acc: 0.6963 - val_out_0_acc: 0.6876 - val_out_1_acc: 0.5727 - val_out_2_acc: 0.5965 - val_out_3_acc: 0.5662 - val_out_4_acc: 0.5488\n",
      "Epoch 52/1000\n",
      " - 49s - loss: 6.3987 - out_loss: 1.0181 - out_0_loss: 0.8894 - out_1_loss: 1.1358 - out_2_loss: 1.1227 - out_3_loss: 1.1061 - out_4_loss: 1.1265 - out_acc: 0.6801 - out_0_acc: 0.6779 - out_1_acc: 0.5894 - out_2_acc: 0.5976 - out_3_acc: 0.5988 - out_4_acc: 0.5923 - val_loss: 6.4620 - val_out_loss: 0.9365 - val_out_0_loss: 0.9164 - val_out_1_loss: 1.1250 - val_out_2_loss: 1.1086 - val_out_3_loss: 1.1918 - val_out_4_loss: 1.1837 - val_out_acc: 0.6963 - val_out_0_acc: 0.6377 - val_out_1_acc: 0.5640 - val_out_2_acc: 0.5987 - val_out_3_acc: 0.5618 - val_out_4_acc: 0.5748\n",
      "Epoch 53/1000\n",
      " - 50s - loss: 6.3869 - out_loss: 1.0123 - out_0_loss: 0.8880 - out_1_loss: 1.1188 - out_2_loss: 1.1520 - out_3_loss: 1.0997 - out_4_loss: 1.1161 - out_acc: 0.6777 - out_0_acc: 0.6658 - out_1_acc: 0.5838 - out_2_acc: 0.5835 - out_3_acc: 0.6002 - out_4_acc: 0.5908 - val_loss: 6.2190 - val_out_loss: 0.8916 - val_out_0_loss: 1.0376 - val_out_1_loss: 1.0754 - val_out_2_loss: 1.0954 - val_out_3_loss: 1.0407 - val_out_4_loss: 1.0784 - val_out_acc: 0.7028 - val_out_0_acc: 0.6161 - val_out_1_acc: 0.6269 - val_out_2_acc: 0.5835 - val_out_3_acc: 0.6204 - val_out_4_acc: 0.5922\n",
      "Epoch 54/1000\n",
      " - 49s - loss: 6.3294 - out_loss: 0.9961 - out_0_loss: 0.8778 - out_1_loss: 1.1163 - out_2_loss: 1.1107 - out_3_loss: 1.0904 - out_4_loss: 1.1380 - out_acc: 0.6791 - out_0_acc: 0.6686 - out_1_acc: 0.5974 - out_2_acc: 0.5951 - out_3_acc: 0.5960 - out_4_acc: 0.5808 - val_loss: 6.4270 - val_out_loss: 0.9438 - val_out_0_loss: 0.9198 - val_out_1_loss: 1.1110 - val_out_2_loss: 1.1029 - val_out_3_loss: 1.1576 - val_out_4_loss: 1.1920 - val_out_acc: 0.7158 - val_out_0_acc: 0.6573 - val_out_1_acc: 0.5965 - val_out_2_acc: 0.5987 - val_out_3_acc: 0.5922 - val_out_4_acc: 0.5770\n",
      "Epoch 55/1000\n",
      " - 50s - loss: 6.2778 - out_loss: 0.9724 - out_0_loss: 0.8798 - out_1_loss: 1.1006 - out_2_loss: 1.0976 - out_3_loss: 1.1014 - out_4_loss: 1.1261 - out_acc: 0.6929 - out_0_acc: 0.6807 - out_1_acc: 0.6021 - out_2_acc: 0.6040 - out_3_acc: 0.5966 - out_4_acc: 0.5892 - val_loss: 6.3326 - val_out_loss: 0.9404 - val_out_0_loss: 0.8903 - val_out_1_loss: 1.1285 - val_out_2_loss: 1.0714 - val_out_3_loss: 1.1661 - val_out_4_loss: 1.1359 - val_out_acc: 0.6876 - val_out_0_acc: 0.6941 - val_out_1_acc: 0.5857 - val_out_2_acc: 0.6139 - val_out_3_acc: 0.5922 - val_out_4_acc: 0.5792\n",
      "Epoch 56/1000\n",
      " - 49s - loss: 6.2387 - out_loss: 0.9695 - out_0_loss: 0.8622 - out_1_loss: 1.1214 - out_2_loss: 1.0909 - out_3_loss: 1.0974 - out_4_loss: 1.0974 - out_acc: 0.6928 - out_0_acc: 0.6865 - out_1_acc: 0.5970 - out_2_acc: 0.6041 - out_3_acc: 0.6038 - out_4_acc: 0.5986 - val_loss: 6.1645 - val_out_loss: 0.8824 - val_out_0_loss: 0.9102 - val_out_1_loss: 1.0820 - val_out_2_loss: 1.0150 - val_out_3_loss: 1.1302 - val_out_4_loss: 1.1447 - val_out_acc: 0.7115 - val_out_0_acc: 0.6746 - val_out_1_acc: 0.5987 - val_out_2_acc: 0.6269 - val_out_3_acc: 0.5900 - val_out_4_acc: 0.5748\n",
      "Epoch 57/1000\n",
      " - 49s - loss: 6.3067 - out_loss: 0.9852 - out_0_loss: 0.8943 - out_1_loss: 1.1131 - out_2_loss: 1.1225 - out_3_loss: 1.0740 - out_4_loss: 1.1176 - out_acc: 0.6860 - out_0_acc: 0.6675 - out_1_acc: 0.5950 - out_2_acc: 0.6023 - out_3_acc: 0.5998 - out_4_acc: 0.5743 - val_loss: 6.3319 - val_out_loss: 0.9356 - val_out_0_loss: 0.8940 - val_out_1_loss: 1.1684 - val_out_2_loss: 1.1030 - val_out_3_loss: 1.1053 - val_out_4_loss: 1.1257 - val_out_acc: 0.7050 - val_out_0_acc: 0.6573 - val_out_1_acc: 0.5900 - val_out_2_acc: 0.5748 - val_out_3_acc: 0.6117 - val_out_4_acc: 0.5965\n",
      "Epoch 58/1000\n",
      " - 49s - loss: 6.2077 - out_loss: 0.9533 - out_0_loss: 0.8575 - out_1_loss: 1.0952 - out_2_loss: 1.0927 - out_3_loss: 1.0965 - out_4_loss: 1.1126 - out_acc: 0.7012 - out_0_acc: 0.6718 - out_1_acc: 0.5912 - out_2_acc: 0.6122 - out_3_acc: 0.5941 - out_4_acc: 0.5855 - val_loss: 6.2085 - val_out_loss: 0.9024 - val_out_0_loss: 0.9644 - val_out_1_loss: 1.0743 - val_out_2_loss: 1.0736 - val_out_3_loss: 1.0477 - val_out_4_loss: 1.1461 - val_out_acc: 0.7050 - val_out_0_acc: 0.6508 - val_out_1_acc: 0.6161 - val_out_2_acc: 0.6095 - val_out_3_acc: 0.6139 - val_out_4_acc: 0.5792\n",
      "Epoch 59/1000\n",
      " - 50s - loss: 6.1173 - out_loss: 0.9419 - out_0_loss: 0.8268 - out_1_loss: 1.0940 - out_2_loss: 1.0981 - out_3_loss: 1.0652 - out_4_loss: 1.0914 - out_acc: 0.7032 - out_0_acc: 0.7001 - out_1_acc: 0.6027 - out_2_acc: 0.6094 - out_3_acc: 0.6134 - out_4_acc: 0.5982 - val_loss: 6.0695 - val_out_loss: 0.9131 - val_out_0_loss: 0.9285 - val_out_1_loss: 1.0623 - val_out_2_loss: 1.0930 - val_out_3_loss: 1.0257 - val_out_4_loss: 1.0469 - val_out_acc: 0.6898 - val_out_0_acc: 0.6638 - val_out_1_acc: 0.6052 - val_out_2_acc: 0.6095 - val_out_3_acc: 0.6269 - val_out_4_acc: 0.5965\n",
      "Epoch 60/1000\n",
      " - 49s - loss: 6.0809 - out_loss: 0.9301 - out_0_loss: 0.8509 - out_1_loss: 1.0935 - out_2_loss: 1.0768 - out_3_loss: 1.0589 - out_4_loss: 1.0708 - out_acc: 0.7026 - out_0_acc: 0.6873 - out_1_acc: 0.6029 - out_2_acc: 0.6084 - out_3_acc: 0.6069 - out_4_acc: 0.6119 - val_loss: 6.0328 - val_out_loss: 0.8796 - val_out_0_loss: 0.8648 - val_out_1_loss: 1.0835 - val_out_2_loss: 1.0854 - val_out_3_loss: 1.0318 - val_out_4_loss: 1.0876 - val_out_acc: 0.6920 - val_out_0_acc: 0.6486 - val_out_1_acc: 0.6030 - val_out_2_acc: 0.5900 - val_out_3_acc: 0.6139 - val_out_4_acc: 0.6074\n",
      "Epoch 61/1000\n",
      " - 49s - loss: 6.1170 - out_loss: 0.9430 - out_0_loss: 0.8443 - out_1_loss: 1.0931 - out_2_loss: 1.0931 - out_3_loss: 1.0674 - out_4_loss: 1.0761 - out_acc: 0.7038 - out_0_acc: 0.6942 - out_1_acc: 0.6088 - out_2_acc: 0.6082 - out_3_acc: 0.6102 - out_4_acc: 0.6007 - val_loss: 6.2099 - val_out_loss: 0.8837 - val_out_0_loss: 0.8994 - val_out_1_loss: 1.0990 - val_out_2_loss: 1.1669 - val_out_3_loss: 1.0528 - val_out_4_loss: 1.1081 - val_out_acc: 0.7245 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.6009 - val_out_2_acc: 0.5900 - val_out_3_acc: 0.6074 - val_out_4_acc: 0.5965\n",
      "Epoch 62/1000\n",
      " - 50s - loss: 6.0935 - out_loss: 0.9283 - out_0_loss: 0.8606 - out_1_loss: 1.0917 - out_2_loss: 1.0808 - out_3_loss: 1.0520 - out_4_loss: 1.0801 - out_acc: 0.6932 - out_0_acc: 0.6776 - out_1_acc: 0.6047 - out_2_acc: 0.6034 - out_3_acc: 0.6134 - out_4_acc: 0.5968 - val_loss: 5.9674 - val_out_loss: 0.8531 - val_out_0_loss: 0.8617 - val_out_1_loss: 1.0717 - val_out_2_loss: 1.0051 - val_out_3_loss: 1.0845 - val_out_4_loss: 1.0911 - val_out_acc: 0.7072 - val_out_0_acc: 0.6703 - val_out_1_acc: 0.6161 - val_out_2_acc: 0.6291 - val_out_3_acc: 0.6030 - val_out_4_acc: 0.5987\n",
      "Epoch 63/1000\n",
      " - 49s - loss: 6.0073 - out_loss: 0.9258 - out_0_loss: 0.8353 - out_1_loss: 1.0549 - out_2_loss: 1.0531 - out_3_loss: 1.0696 - out_4_loss: 1.0685 - out_acc: 0.7051 - out_0_acc: 0.6926 - out_1_acc: 0.6159 - out_2_acc: 0.6157 - out_3_acc: 0.6094 - out_4_acc: 0.6203 - val_loss: 5.8701 - val_out_loss: 0.8299 - val_out_0_loss: 0.8210 - val_out_1_loss: 1.0687 - val_out_2_loss: 1.0729 - val_out_3_loss: 1.0214 - val_out_4_loss: 1.0561 - val_out_acc: 0.7484 - val_out_0_acc: 0.6768 - val_out_1_acc: 0.6161 - val_out_2_acc: 0.6291 - val_out_3_acc: 0.6226 - val_out_4_acc: 0.6247\n",
      "Epoch 64/1000\n",
      " - 49s - loss: 5.8728 - out_loss: 0.8868 - out_0_loss: 0.8042 - out_1_loss: 1.0544 - out_2_loss: 1.0533 - out_3_loss: 1.0355 - out_4_loss: 1.0385 - out_acc: 0.7150 - out_0_acc: 0.7014 - out_1_acc: 0.6106 - out_2_acc: 0.6167 - out_3_acc: 0.6245 - out_4_acc: 0.6220 - val_loss: 5.9539 - val_out_loss: 0.8356 - val_out_0_loss: 0.8458 - val_out_1_loss: 1.0476 - val_out_2_loss: 1.0663 - val_out_3_loss: 1.0797 - val_out_4_loss: 1.0788 - val_out_acc: 0.7332 - val_out_0_acc: 0.6876 - val_out_1_acc: 0.6139 - val_out_2_acc: 0.6074 - val_out_3_acc: 0.6182 - val_out_4_acc: 0.6139\n",
      "Epoch 65/1000\n",
      " - 48s - loss: 5.8790 - out_loss: 0.8928 - out_0_loss: 0.7839 - out_1_loss: 1.0667 - out_2_loss: 1.0389 - out_3_loss: 1.0437 - out_4_loss: 1.0530 - out_acc: 0.7162 - out_0_acc: 0.7059 - out_1_acc: 0.6095 - out_2_acc: 0.6198 - out_3_acc: 0.6277 - out_4_acc: 0.6102 - val_loss: 6.1946 - val_out_loss: 0.8590 - val_out_0_loss: 0.8140 - val_out_1_loss: 1.1205 - val_out_2_loss: 1.1221 - val_out_3_loss: 1.1305 - val_out_4_loss: 1.1485 - val_out_acc: 0.7267 - val_out_0_acc: 0.7310 - val_out_1_acc: 0.6074 - val_out_2_acc: 0.6030 - val_out_3_acc: 0.5965 - val_out_4_acc: 0.6095\n",
      "Epoch 66/1000\n",
      " - 48s - loss: 5.8634 - out_loss: 0.8836 - out_0_loss: 0.7816 - out_1_loss: 1.0516 - out_2_loss: 1.0740 - out_3_loss: 1.0042 - out_4_loss: 1.0685 - out_acc: 0.7259 - out_0_acc: 0.7111 - out_1_acc: 0.6087 - out_2_acc: 0.6108 - out_3_acc: 0.6408 - out_4_acc: 0.6098 - val_loss: 5.9092 - val_out_loss: 0.8085 - val_out_0_loss: 0.8323 - val_out_1_loss: 1.1007 - val_out_2_loss: 1.0511 - val_out_3_loss: 1.0211 - val_out_4_loss: 1.0955 - val_out_acc: 0.7505 - val_out_0_acc: 0.6941 - val_out_1_acc: 0.5835 - val_out_2_acc: 0.6139 - val_out_3_acc: 0.6508 - val_out_4_acc: 0.5965\n",
      "Epoch 67/1000\n",
      " - 50s - loss: 5.9060 - out_loss: 0.9020 - out_0_loss: 0.8181 - out_1_loss: 1.0212 - out_2_loss: 1.0550 - out_3_loss: 1.0434 - out_4_loss: 1.0663 - out_acc: 0.7096 - out_0_acc: 0.6973 - out_1_acc: 0.6226 - out_2_acc: 0.6103 - out_3_acc: 0.6106 - out_4_acc: 0.6024 - val_loss: 6.0092 - val_out_loss: 0.8569 - val_out_0_loss: 0.8448 - val_out_1_loss: 1.1422 - val_out_2_loss: 1.0759 - val_out_3_loss: 1.0407 - val_out_4_loss: 1.0487 - val_out_acc: 0.7050 - val_out_0_acc: 0.6963 - val_out_1_acc: 0.5857 - val_out_2_acc: 0.6117 - val_out_3_acc: 0.6095 - val_out_4_acc: 0.6117\n",
      "Epoch 68/1000\n",
      " - 52s - loss: 5.8760 - out_loss: 0.8717 - out_0_loss: 0.7866 - out_1_loss: 1.0681 - out_2_loss: 1.0632 - out_3_loss: 1.0541 - out_4_loss: 1.0323 - out_acc: 0.7189 - out_0_acc: 0.7114 - out_1_acc: 0.6155 - out_2_acc: 0.6029 - out_3_acc: 0.6136 - out_4_acc: 0.6224 - val_loss: 6.0568 - val_out_loss: 0.8522 - val_out_0_loss: 0.9611 - val_out_1_loss: 1.0800 - val_out_2_loss: 1.0563 - val_out_3_loss: 1.1151 - val_out_4_loss: 0.9921 - val_out_acc: 0.7180 - val_out_0_acc: 0.6356 - val_out_1_acc: 0.6009 - val_out_2_acc: 0.6226 - val_out_3_acc: 0.6052 - val_out_4_acc: 0.6334\n",
      "Epoch 69/1000\n",
      " - 51s - loss: 5.8311 - out_loss: 0.8707 - out_0_loss: 0.7962 - out_1_loss: 1.0344 - out_2_loss: 1.0320 - out_3_loss: 1.0463 - out_4_loss: 1.0515 - out_acc: 0.7266 - out_0_acc: 0.7044 - out_1_acc: 0.6248 - out_2_acc: 0.6289 - out_3_acc: 0.6162 - out_4_acc: 0.6098 - val_loss: 6.0349 - val_out_loss: 0.8779 - val_out_0_loss: 0.8669 - val_out_1_loss: 1.0126 - val_out_2_loss: 1.0689 - val_out_3_loss: 1.0858 - val_out_4_loss: 1.1227 - val_out_acc: 0.6920 - val_out_0_acc: 0.6746 - val_out_1_acc: 0.6226 - val_out_2_acc: 0.6139 - val_out_3_acc: 0.6117 - val_out_4_acc: 0.5575\n",
      "Epoch 70/1000\n",
      " - 50s - loss: 5.8083 - out_loss: 0.8712 - out_0_loss: 0.8094 - out_1_loss: 1.0237 - out_2_loss: 1.0555 - out_3_loss: 1.0174 - out_4_loss: 1.0311 - out_acc: 0.7224 - out_0_acc: 0.7083 - out_1_acc: 0.6190 - out_2_acc: 0.6154 - out_3_acc: 0.6352 - out_4_acc: 0.6203 - val_loss: 6.1682 - val_out_loss: 0.8741 - val_out_0_loss: 0.8353 - val_out_1_loss: 1.1597 - val_out_2_loss: 1.1284 - val_out_3_loss: 1.0346 - val_out_4_loss: 1.1361 - val_out_acc: 0.7202 - val_out_0_acc: 0.6855 - val_out_1_acc: 0.5879 - val_out_2_acc: 0.5922 - val_out_3_acc: 0.6139 - val_out_4_acc: 0.5900\n",
      "Epoch 71/1000\n",
      " - 51s - loss: 5.7672 - out_loss: 0.8566 - out_0_loss: 0.7637 - out_1_loss: 1.0366 - out_2_loss: 1.0460 - out_3_loss: 1.0128 - out_4_loss: 1.0515 - out_acc: 0.7267 - out_0_acc: 0.7176 - out_1_acc: 0.6236 - out_2_acc: 0.6127 - out_3_acc: 0.6331 - out_4_acc: 0.6165 - val_loss: 5.8379 - val_out_loss: 0.8178 - val_out_0_loss: 0.7779 - val_out_1_loss: 1.0451 - val_out_2_loss: 1.0854 - val_out_3_loss: 1.0365 - val_out_4_loss: 1.0751 - val_out_acc: 0.7202 - val_out_0_acc: 0.7137 - val_out_1_acc: 0.6095 - val_out_2_acc: 0.5835 - val_out_3_acc: 0.6421 - val_out_4_acc: 0.5879\n",
      "Epoch 72/1000\n",
      " - 54s - loss: 5.6723 - out_loss: 0.8361 - out_0_loss: 0.7667 - out_1_loss: 1.0260 - out_2_loss: 1.0058 - out_3_loss: 1.0086 - out_4_loss: 1.0291 - out_acc: 0.7277 - out_0_acc: 0.7159 - out_1_acc: 0.6163 - out_2_acc: 0.6352 - out_3_acc: 0.6321 - out_4_acc: 0.6238 - val_loss: 5.8422 - val_out_loss: 0.7888 - val_out_0_loss: 0.7875 - val_out_1_loss: 1.0829 - val_out_2_loss: 1.0474 - val_out_3_loss: 1.0580 - val_out_4_loss: 1.0777 - val_out_acc: 0.7397 - val_out_0_acc: 0.7115 - val_out_1_acc: 0.6030 - val_out_2_acc: 0.6204 - val_out_3_acc: 0.6009 - val_out_4_acc: 0.6030\n",
      "Epoch 73/1000\n",
      " - 54s - loss: 5.6476 - out_loss: 0.8390 - out_0_loss: 0.7599 - out_1_loss: 1.0127 - out_2_loss: 1.0211 - out_3_loss: 0.9952 - out_4_loss: 1.0196 - out_acc: 0.7283 - out_0_acc: 0.7271 - out_1_acc: 0.6314 - out_2_acc: 0.6343 - out_3_acc: 0.6324 - out_4_acc: 0.6299 - val_loss: 6.1415 - val_out_loss: 0.8757 - val_out_0_loss: 0.9581 - val_out_1_loss: 1.0031 - val_out_2_loss: 1.0197 - val_out_3_loss: 1.1325 - val_out_4_loss: 1.1523 - val_out_acc: 0.6985 - val_out_0_acc: 0.6399 - val_out_1_acc: 0.6291 - val_out_2_acc: 0.6356 - val_out_3_acc: 0.6074 - val_out_4_acc: 0.5748\n",
      "Epoch 74/1000\n",
      " - 52s - loss: 5.6976 - out_loss: 0.8159 - out_0_loss: 0.7791 - out_1_loss: 1.0355 - out_2_loss: 1.0021 - out_3_loss: 1.0382 - out_4_loss: 1.0267 - out_acc: 0.7404 - out_0_acc: 0.7219 - out_1_acc: 0.6238 - out_2_acc: 0.6334 - out_3_acc: 0.6236 - out_4_acc: 0.6288 - val_loss: 5.8054 - val_out_loss: 0.8053 - val_out_0_loss: 0.8331 - val_out_1_loss: 1.0485 - val_out_2_loss: 1.0003 - val_out_3_loss: 1.0508 - val_out_4_loss: 1.0675 - val_out_acc: 0.7267 - val_out_0_acc: 0.6855 - val_out_1_acc: 0.6139 - val_out_2_acc: 0.6486 - val_out_3_acc: 0.6095 - val_out_4_acc: 0.6095\n",
      "Epoch 75/1000\n",
      " - 51s - loss: 5.5678 - out_loss: 0.8076 - out_0_loss: 0.7144 - out_1_loss: 1.0007 - out_2_loss: 1.0203 - out_3_loss: 0.9979 - out_4_loss: 1.0268 - out_acc: 0.7371 - out_0_acc: 0.7380 - out_1_acc: 0.6305 - out_2_acc: 0.6291 - out_3_acc: 0.6449 - out_4_acc: 0.6256 - val_loss: 5.5897 - val_out_loss: 0.7675 - val_out_0_loss: 0.8123 - val_out_1_loss: 0.9801 - val_out_2_loss: 0.9963 - val_out_3_loss: 1.0126 - val_out_4_loss: 1.0208 - val_out_acc: 0.7223 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.6269 - val_out_2_acc: 0.6139 - val_out_3_acc: 0.6269 - val_out_4_acc: 0.6117\n",
      "Epoch 76/1000\n",
      " - 51s - loss: 5.6116 - out_loss: 0.8154 - out_0_loss: 0.7473 - out_1_loss: 1.0240 - out_2_loss: 0.9958 - out_3_loss: 0.9993 - out_4_loss: 1.0297 - out_acc: 0.7427 - out_0_acc: 0.7326 - out_1_acc: 0.6258 - out_2_acc: 0.6361 - out_3_acc: 0.6250 - out_4_acc: 0.6311 - val_loss: 5.6975 - val_out_loss: 0.7991 - val_out_0_loss: 0.7967 - val_out_1_loss: 0.9658 - val_out_2_loss: 1.1178 - val_out_3_loss: 0.9852 - val_out_4_loss: 1.0329 - val_out_acc: 0.7223 - val_out_0_acc: 0.7115 - val_out_1_acc: 0.6421 - val_out_2_acc: 0.6074 - val_out_3_acc: 0.6443 - val_out_4_acc: 0.6204\n",
      "Epoch 77/1000\n",
      " - 51s - loss: 5.5787 - out_loss: 0.8078 - out_0_loss: 0.7527 - out_1_loss: 1.0058 - out_2_loss: 1.0001 - out_3_loss: 0.9871 - out_4_loss: 1.0254 - out_acc: 0.7409 - out_0_acc: 0.7209 - out_1_acc: 0.6264 - out_2_acc: 0.6353 - out_3_acc: 0.6453 - out_4_acc: 0.6297 - val_loss: 5.9748 - val_out_loss: 0.8326 - val_out_0_loss: 0.7693 - val_out_1_loss: 1.1167 - val_out_2_loss: 1.0640 - val_out_3_loss: 1.1497 - val_out_4_loss: 1.0424 - val_out_acc: 0.6941 - val_out_0_acc: 0.7115 - val_out_1_acc: 0.5770 - val_out_2_acc: 0.6095 - val_out_3_acc: 0.5770 - val_out_4_acc: 0.6247\n",
      "Epoch 78/1000\n",
      " - 51s - loss: 5.5286 - out_loss: 0.8037 - out_0_loss: 0.7371 - out_1_loss: 0.9964 - out_2_loss: 0.9929 - out_3_loss: 0.9856 - out_4_loss: 1.0129 - out_acc: 0.7323 - out_0_acc: 0.7325 - out_1_acc: 0.6307 - out_2_acc: 0.6443 - out_3_acc: 0.6339 - out_4_acc: 0.6265 - val_loss: 6.1639 - val_out_loss: 0.8194 - val_out_0_loss: 0.8143 - val_out_1_loss: 1.1829 - val_out_2_loss: 1.1343 - val_out_3_loss: 1.0878 - val_out_4_loss: 1.1253 - val_out_acc: 0.7419 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.5900 - val_out_2_acc: 0.6074 - val_out_3_acc: 0.6117 - val_out_4_acc: 0.5857\n",
      "Epoch 79/1000\n",
      " - 50s - loss: 5.5364 - out_loss: 0.7888 - out_0_loss: 0.7194 - out_1_loss: 1.0235 - out_2_loss: 1.0036 - out_3_loss: 0.9867 - out_4_loss: 1.0143 - out_acc: 0.7499 - out_0_acc: 0.7373 - out_1_acc: 0.6254 - out_2_acc: 0.6360 - out_3_acc: 0.6502 - out_4_acc: 0.6277 - val_loss: 5.7742 - val_out_loss: 0.7685 - val_out_0_loss: 0.7595 - val_out_1_loss: 1.1276 - val_out_2_loss: 1.0104 - val_out_3_loss: 1.0509 - val_out_4_loss: 1.0574 - val_out_acc: 0.7289 - val_out_0_acc: 0.7158 - val_out_1_acc: 0.5900 - val_out_2_acc: 0.6269 - val_out_3_acc: 0.6356 - val_out_4_acc: 0.5857\n",
      "Epoch 80/1000\n",
      " - 51s - loss: 5.4214 - out_loss: 0.7611 - out_0_loss: 0.6964 - out_1_loss: 0.9917 - out_2_loss: 0.9766 - out_3_loss: 0.9806 - out_4_loss: 1.0150 - out_acc: 0.7545 - out_0_acc: 0.7501 - out_1_acc: 0.6438 - out_2_acc: 0.6375 - out_3_acc: 0.6458 - out_4_acc: 0.6343 - val_loss: 5.7672 - val_out_loss: 0.7782 - val_out_0_loss: 0.8033 - val_out_1_loss: 1.0176 - val_out_2_loss: 1.0928 - val_out_3_loss: 0.9517 - val_out_4_loss: 1.1236 - val_out_acc: 0.7137 - val_out_0_acc: 0.6963 - val_out_1_acc: 0.6486 - val_out_2_acc: 0.5944 - val_out_3_acc: 0.6269 - val_out_4_acc: 0.5965\n",
      "Epoch 81/1000\n",
      " - 52s - loss: 5.5371 - out_loss: 0.8019 - out_0_loss: 0.7457 - out_1_loss: 1.0084 - out_2_loss: 0.9920 - out_3_loss: 0.9995 - out_4_loss: 0.9896 - out_acc: 0.7405 - out_0_acc: 0.7256 - out_1_acc: 0.6305 - out_2_acc: 0.6386 - out_3_acc: 0.6336 - out_4_acc: 0.6355 - val_loss: 5.3598 - val_out_loss: 0.7153 - val_out_0_loss: 0.7266 - val_out_1_loss: 1.0040 - val_out_2_loss: 0.9787 - val_out_3_loss: 0.9300 - val_out_4_loss: 1.0053 - val_out_acc: 0.7484 - val_out_0_acc: 0.7332 - val_out_1_acc: 0.6291 - val_out_2_acc: 0.6182 - val_out_3_acc: 0.6421 - val_out_4_acc: 0.6204\n",
      "Epoch 82/1000\n",
      " - 51s - loss: 5.4193 - out_loss: 0.7699 - out_0_loss: 0.7318 - out_1_loss: 0.9723 - out_2_loss: 0.9799 - out_3_loss: 0.9697 - out_4_loss: 0.9958 - out_acc: 0.7493 - out_0_acc: 0.7327 - out_1_acc: 0.6402 - out_2_acc: 0.6418 - out_3_acc: 0.6437 - out_4_acc: 0.6369 - val_loss: 5.6363 - val_out_loss: 0.7456 - val_out_0_loss: 0.7874 - val_out_1_loss: 1.0728 - val_out_2_loss: 1.0154 - val_out_3_loss: 0.9866 - val_out_4_loss: 1.0285 - val_out_acc: 0.7419 - val_out_0_acc: 0.7050 - val_out_1_acc: 0.5965 - val_out_2_acc: 0.6095 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6334\n",
      "Epoch 83/1000\n",
      " - 53s - loss: 5.3061 - out_loss: 0.7429 - out_0_loss: 0.6872 - out_1_loss: 0.9797 - out_2_loss: 0.9462 - out_3_loss: 0.9581 - out_4_loss: 0.9920 - out_acc: 0.7459 - out_0_acc: 0.7459 - out_1_acc: 0.6412 - out_2_acc: 0.6528 - out_3_acc: 0.6498 - out_4_acc: 0.6295 - val_loss: 5.7978 - val_out_loss: 0.8041 - val_out_0_loss: 0.8647 - val_out_1_loss: 1.0855 - val_out_2_loss: 1.0230 - val_out_3_loss: 1.0465 - val_out_4_loss: 0.9741 - val_out_acc: 0.6963 - val_out_0_acc: 0.6790 - val_out_1_acc: 0.5727 - val_out_2_acc: 0.5835 - val_out_3_acc: 0.6074 - val_out_4_acc: 0.6117\n",
      "Epoch 84/1000\n",
      " - 52s - loss: 5.3141 - out_loss: 0.7522 - out_0_loss: 0.6942 - out_1_loss: 0.9814 - out_2_loss: 0.9505 - out_3_loss: 0.9541 - out_4_loss: 0.9816 - out_acc: 0.7589 - out_0_acc: 0.7460 - out_1_acc: 0.6485 - out_2_acc: 0.6534 - out_3_acc: 0.6531 - out_4_acc: 0.6466 - val_loss: 5.7597 - val_out_loss: 0.8085 - val_out_0_loss: 0.8003 - val_out_1_loss: 1.0617 - val_out_2_loss: 1.0727 - val_out_3_loss: 1.0266 - val_out_4_loss: 0.9900 - val_out_acc: 0.7137 - val_out_0_acc: 0.6790 - val_out_1_acc: 0.6226 - val_out_2_acc: 0.6139 - val_out_3_acc: 0.6117 - val_out_4_acc: 0.6095\n",
      "Epoch 85/1000\n",
      " - 53s - loss: 5.4049 - out_loss: 0.7488 - out_0_loss: 0.7166 - out_1_loss: 0.9951 - out_2_loss: 0.9913 - out_3_loss: 0.9808 - out_4_loss: 0.9722 - out_acc: 0.7597 - out_0_acc: 0.7387 - out_1_acc: 0.6331 - out_2_acc: 0.6379 - out_3_acc: 0.6421 - out_4_acc: 0.6484 - val_loss: 5.4902 - val_out_loss: 0.7173 - val_out_0_loss: 0.7879 - val_out_1_loss: 0.9690 - val_out_2_loss: 0.9746 - val_out_3_loss: 1.0344 - val_out_4_loss: 1.0071 - val_out_acc: 0.7636 - val_out_0_acc: 0.7115 - val_out_1_acc: 0.6486 - val_out_2_acc: 0.6377 - val_out_3_acc: 0.6204 - val_out_4_acc: 0.6334\n",
      "Epoch 86/1000\n",
      " - 52s - loss: 5.2265 - out_loss: 0.7332 - out_0_loss: 0.6971 - out_1_loss: 0.9702 - out_2_loss: 0.9462 - out_3_loss: 0.9279 - out_4_loss: 0.9520 - out_acc: 0.7658 - out_0_acc: 0.7511 - out_1_acc: 0.6470 - out_2_acc: 0.6495 - out_3_acc: 0.6601 - out_4_acc: 0.6558 - val_loss: 5.4542 - val_out_loss: 0.7507 - val_out_0_loss: 0.7836 - val_out_1_loss: 1.0232 - val_out_2_loss: 0.9700 - val_out_3_loss: 0.9449 - val_out_4_loss: 0.9819 - val_out_acc: 0.7419 - val_out_0_acc: 0.7072 - val_out_1_acc: 0.6334 - val_out_2_acc: 0.6486 - val_out_3_acc: 0.6508 - val_out_4_acc: 0.6725\n",
      "Epoch 87/1000\n",
      " - 53s - loss: 5.4282 - out_loss: 0.7588 - out_0_loss: 0.7123 - out_1_loss: 1.0037 - out_2_loss: 0.9892 - out_3_loss: 0.9647 - out_4_loss: 0.9994 - out_acc: 0.7487 - out_0_acc: 0.7360 - out_1_acc: 0.6260 - out_2_acc: 0.6393 - out_3_acc: 0.6496 - out_4_acc: 0.6329 - val_loss: 5.3623 - val_out_loss: 0.6985 - val_out_0_loss: 0.7208 - val_out_1_loss: 1.0197 - val_out_2_loss: 1.0459 - val_out_3_loss: 0.8703 - val_out_4_loss: 1.0071 - val_out_acc: 0.7505 - val_out_0_acc: 0.7354 - val_out_1_acc: 0.6247 - val_out_2_acc: 0.6312 - val_out_3_acc: 0.6638 - val_out_4_acc: 0.6226\n",
      "Epoch 88/1000\n",
      " - 52s - loss: 5.3029 - out_loss: 0.7359 - out_0_loss: 0.6805 - out_1_loss: 0.9850 - out_2_loss: 0.9594 - out_3_loss: 0.9612 - out_4_loss: 0.9811 - out_acc: 0.7544 - out_0_acc: 0.7484 - out_1_acc: 0.6344 - out_2_acc: 0.6477 - out_3_acc: 0.6429 - out_4_acc: 0.6417 - val_loss: 5.4603 - val_out_loss: 0.7446 - val_out_0_loss: 0.7911 - val_out_1_loss: 0.9770 - val_out_2_loss: 0.9875 - val_out_3_loss: 0.9754 - val_out_4_loss: 0.9847 - val_out_acc: 0.7310 - val_out_0_acc: 0.7158 - val_out_1_acc: 0.6291 - val_out_2_acc: 0.6551 - val_out_3_acc: 0.6334 - val_out_4_acc: 0.6464\n",
      "Epoch 89/1000\n",
      " - 52s - loss: 5.2257 - out_loss: 0.7236 - out_0_loss: 0.6699 - out_1_loss: 0.9492 - out_2_loss: 0.9644 - out_3_loss: 0.9475 - out_4_loss: 0.9712 - out_acc: 0.7705 - out_0_acc: 0.7575 - out_1_acc: 0.6485 - out_2_acc: 0.6399 - out_3_acc: 0.6555 - out_4_acc: 0.6296 - val_loss: 5.2420 - val_out_loss: 0.6881 - val_out_0_loss: 0.7021 - val_out_1_loss: 0.9625 - val_out_2_loss: 0.9724 - val_out_3_loss: 0.9184 - val_out_4_loss: 0.9987 - val_out_acc: 0.7527 - val_out_0_acc: 0.7462 - val_out_1_acc: 0.6551 - val_out_2_acc: 0.6551 - val_out_3_acc: 0.6573 - val_out_4_acc: 0.6334\n",
      "Epoch 90/1000\n",
      " - 52s - loss: 5.2732 - out_loss: 0.7220 - out_0_loss: 0.6602 - out_1_loss: 0.9810 - out_2_loss: 0.9589 - out_3_loss: 0.9630 - out_4_loss: 0.9881 - out_acc: 0.7649 - out_0_acc: 0.7535 - out_1_acc: 0.6464 - out_2_acc: 0.6484 - out_3_acc: 0.6434 - out_4_acc: 0.6542 - val_loss: 5.5750 - val_out_loss: 0.7535 - val_out_0_loss: 0.8303 - val_out_1_loss: 1.0304 - val_out_2_loss: 0.9858 - val_out_3_loss: 1.0346 - val_out_4_loss: 0.9404 - val_out_acc: 0.7549 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.6139 - val_out_2_acc: 0.6421 - val_out_3_acc: 0.6139 - val_out_4_acc: 0.6616\n",
      "Epoch 91/1000\n",
      " - 52s - loss: 5.2217 - out_loss: 0.7238 - out_0_loss: 0.6776 - out_1_loss: 0.9411 - out_2_loss: 0.9797 - out_3_loss: 0.9478 - out_4_loss: 0.9517 - out_acc: 0.7639 - out_0_acc: 0.7497 - out_1_acc: 0.6499 - out_2_acc: 0.6388 - out_3_acc: 0.6507 - out_4_acc: 0.6441 - val_loss: 5.6268 - val_out_loss: 0.7641 - val_out_0_loss: 0.8886 - val_out_1_loss: 0.9360 - val_out_2_loss: 0.9930 - val_out_3_loss: 1.0774 - val_out_4_loss: 0.9676 - val_out_acc: 0.7310 - val_out_0_acc: 0.6486 - val_out_1_acc: 0.6486 - val_out_2_acc: 0.6508 - val_out_3_acc: 0.5922 - val_out_4_acc: 0.6356\n",
      "Epoch 92/1000\n",
      " - 52s - loss: 5.1025 - out_loss: 0.7138 - out_0_loss: 0.6647 - out_1_loss: 0.9363 - out_2_loss: 0.9154 - out_3_loss: 0.9435 - out_4_loss: 0.9288 - out_acc: 0.7656 - out_0_acc: 0.7507 - out_1_acc: 0.6472 - out_2_acc: 0.6601 - out_3_acc: 0.6591 - out_4_acc: 0.6648 - val_loss: 5.2493 - val_out_loss: 0.6935 - val_out_0_loss: 0.7498 - val_out_1_loss: 0.9595 - val_out_2_loss: 0.9684 - val_out_3_loss: 0.9266 - val_out_4_loss: 0.9515 - val_out_acc: 0.7636 - val_out_0_acc: 0.7050 - val_out_1_acc: 0.6421 - val_out_2_acc: 0.6464 - val_out_3_acc: 0.6703 - val_out_4_acc: 0.6399\n",
      "Epoch 93/1000\n",
      " - 51s - loss: 5.1250 - out_loss: 0.7071 - out_0_loss: 0.6999 - out_1_loss: 0.9442 - out_2_loss: 0.9230 - out_3_loss: 0.9223 - out_4_loss: 0.9285 - out_acc: 0.7685 - out_0_acc: 0.7423 - out_1_acc: 0.6505 - out_2_acc: 0.6629 - out_3_acc: 0.6734 - out_4_acc: 0.6601 - val_loss: 5.5019 - val_out_loss: 0.7101 - val_out_0_loss: 0.6730 - val_out_1_loss: 1.0802 - val_out_2_loss: 0.9862 - val_out_3_loss: 0.9920 - val_out_4_loss: 1.0604 - val_out_acc: 0.7354 - val_out_0_acc: 0.7354 - val_out_1_acc: 0.5770 - val_out_2_acc: 0.6247 - val_out_3_acc: 0.6356 - val_out_4_acc: 0.5900\n",
      "Epoch 94/1000\n",
      " - 51s - loss: 5.2070 - out_loss: 0.7186 - out_0_loss: 0.6723 - out_1_loss: 0.9729 - out_2_loss: 0.9360 - out_3_loss: 0.9490 - out_4_loss: 0.9582 - out_acc: 0.7649 - out_0_acc: 0.7560 - out_1_acc: 0.6394 - out_2_acc: 0.6635 - out_3_acc: 0.6534 - out_4_acc: 0.6460 - val_loss: 5.4008 - val_out_loss: 0.7543 - val_out_0_loss: 0.7689 - val_out_1_loss: 0.9832 - val_out_2_loss: 0.9722 - val_out_3_loss: 0.9546 - val_out_4_loss: 0.9676 - val_out_acc: 0.7570 - val_out_0_acc: 0.7310 - val_out_1_acc: 0.6464 - val_out_2_acc: 0.6508 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6508\n",
      "Epoch 95/1000\n",
      " - 51s - loss: 5.1528 - out_loss: 0.7132 - out_0_loss: 0.6659 - out_1_loss: 0.9442 - out_2_loss: 0.9351 - out_3_loss: 0.9555 - out_4_loss: 0.9389 - out_acc: 0.7686 - out_0_acc: 0.7535 - out_1_acc: 0.6486 - out_2_acc: 0.6606 - out_3_acc: 0.6508 - out_4_acc: 0.6549 - val_loss: 5.3347 - val_out_loss: 0.7220 - val_out_0_loss: 0.7405 - val_out_1_loss: 0.9738 - val_out_2_loss: 0.9488 - val_out_3_loss: 1.0060 - val_out_4_loss: 0.9435 - val_out_acc: 0.7636 - val_out_0_acc: 0.7375 - val_out_1_acc: 0.6291 - val_out_2_acc: 0.6594 - val_out_3_acc: 0.6117 - val_out_4_acc: 0.6464\n",
      "Epoch 96/1000\n",
      " - 51s - loss: 5.0996 - out_loss: 0.7100 - out_0_loss: 0.6707 - out_1_loss: 0.9480 - out_2_loss: 0.9011 - out_3_loss: 0.9404 - out_4_loss: 0.9294 - out_acc: 0.7638 - out_0_acc: 0.7520 - out_1_acc: 0.6468 - out_2_acc: 0.6749 - out_3_acc: 0.6516 - out_4_acc: 0.6460 - val_loss: 4.7907 - val_out_loss: 0.6237 - val_out_0_loss: 0.6839 - val_out_1_loss: 0.8921 - val_out_2_loss: 0.8726 - val_out_3_loss: 0.8288 - val_out_4_loss: 0.8895 - val_out_acc: 0.7852 - val_out_0_acc: 0.7310 - val_out_1_acc: 0.6703 - val_out_2_acc: 0.6833 - val_out_3_acc: 0.6725 - val_out_4_acc: 0.6790\n",
      "Epoch 97/1000\n",
      " - 52s - loss: 4.9500 - out_loss: 0.6696 - out_0_loss: 0.6105 - out_1_loss: 0.9379 - out_2_loss: 0.9042 - out_3_loss: 0.9224 - out_4_loss: 0.9054 - out_acc: 0.7854 - out_0_acc: 0.7853 - out_1_acc: 0.6607 - out_2_acc: 0.6616 - out_3_acc: 0.6474 - out_4_acc: 0.6757 - val_loss: 5.1188 - val_out_loss: 0.6935 - val_out_0_loss: 0.7867 - val_out_1_loss: 0.8954 - val_out_2_loss: 0.9226 - val_out_3_loss: 0.9399 - val_out_4_loss: 0.8807 - val_out_acc: 0.7614 - val_out_0_acc: 0.7397 - val_out_1_acc: 0.6811 - val_out_2_acc: 0.6573 - val_out_3_acc: 0.6594 - val_out_4_acc: 0.6551\n",
      "Epoch 98/1000\n",
      " - 51s - loss: 5.0488 - out_loss: 0.7060 - out_0_loss: 0.6684 - out_1_loss: 0.9047 - out_2_loss: 0.9315 - out_3_loss: 0.9281 - out_4_loss: 0.9100 - out_acc: 0.7772 - out_0_acc: 0.7594 - out_1_acc: 0.6671 - out_2_acc: 0.6579 - out_3_acc: 0.6671 - out_4_acc: 0.6583 - val_loss: 5.1081 - val_out_loss: 0.6751 - val_out_0_loss: 0.7264 - val_out_1_loss: 0.8979 - val_out_2_loss: 0.8848 - val_out_3_loss: 0.9197 - val_out_4_loss: 1.0041 - val_out_acc: 0.7592 - val_out_0_acc: 0.7332 - val_out_1_acc: 0.6551 - val_out_2_acc: 0.6703 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6334\n",
      "Epoch 99/1000\n",
      " - 52s - loss: 5.0584 - out_loss: 0.6886 - out_0_loss: 0.6455 - out_1_loss: 0.9241 - out_2_loss: 0.9300 - out_3_loss: 0.9262 - out_4_loss: 0.9441 - out_acc: 0.7803 - out_0_acc: 0.7636 - out_1_acc: 0.6619 - out_2_acc: 0.6619 - out_3_acc: 0.6597 - out_4_acc: 0.6496 - val_loss: 5.1903 - val_out_loss: 0.6941 - val_out_0_loss: 0.7122 - val_out_1_loss: 0.9758 - val_out_2_loss: 0.8966 - val_out_3_loss: 0.9815 - val_out_4_loss: 0.9301 - val_out_acc: 0.7549 - val_out_0_acc: 0.7527 - val_out_1_acc: 0.6464 - val_out_2_acc: 0.6616 - val_out_3_acc: 0.6594 - val_out_4_acc: 0.6399\n",
      "Epoch 100/1000\n",
      " - 52s - loss: 4.9996 - out_loss: 0.6772 - out_0_loss: 0.6528 - out_1_loss: 0.9369 - out_2_loss: 0.9135 - out_3_loss: 0.9220 - out_4_loss: 0.8973 - out_acc: 0.7783 - out_0_acc: 0.7612 - out_1_acc: 0.6517 - out_2_acc: 0.6631 - out_3_acc: 0.6612 - out_4_acc: 0.6742 - val_loss: 5.1104 - val_out_loss: 0.6843 - val_out_0_loss: 0.8143 - val_out_1_loss: 0.8548 - val_out_2_loss: 0.9585 - val_out_3_loss: 0.8048 - val_out_4_loss: 0.9937 - val_out_acc: 0.7549 - val_out_0_acc: 0.7115 - val_out_1_acc: 0.6746 - val_out_2_acc: 0.6356 - val_out_3_acc: 0.7223 - val_out_4_acc: 0.6291\n",
      "Epoch 101/1000\n",
      " - 52s - loss: 4.9659 - out_loss: 0.6744 - out_0_loss: 0.6348 - out_1_loss: 0.9221 - out_2_loss: 0.8881 - out_3_loss: 0.9110 - out_4_loss: 0.9354 - out_acc: 0.7785 - out_0_acc: 0.7568 - out_1_acc: 0.6548 - out_2_acc: 0.6671 - out_3_acc: 0.6625 - out_4_acc: 0.6502 - val_loss: 5.1587 - val_out_loss: 0.6628 - val_out_0_loss: 0.6734 - val_out_1_loss: 0.9336 - val_out_2_loss: 0.9932 - val_out_3_loss: 0.9348 - val_out_4_loss: 0.9609 - val_out_acc: 0.7809 - val_out_0_acc: 0.7570 - val_out_1_acc: 0.6594 - val_out_2_acc: 0.6464 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6508\n",
      "Epoch 102/1000\n",
      " - 52s - loss: 4.9262 - out_loss: 0.6614 - out_0_loss: 0.6296 - out_1_loss: 0.9243 - out_2_loss: 0.8773 - out_3_loss: 0.9414 - out_4_loss: 0.8923 - out_acc: 0.7896 - out_0_acc: 0.7705 - out_1_acc: 0.6605 - out_2_acc: 0.6767 - out_3_acc: 0.6624 - out_4_acc: 0.6796 - val_loss: 5.2758 - val_out_loss: 0.7300 - val_out_0_loss: 0.7761 - val_out_1_loss: 0.9628 - val_out_2_loss: 0.8892 - val_out_3_loss: 0.9684 - val_out_4_loss: 0.9494 - val_out_acc: 0.7636 - val_out_0_acc: 0.7028 - val_out_1_acc: 0.6551 - val_out_2_acc: 0.6811 - val_out_3_acc: 0.6356 - val_out_4_acc: 0.6377\n",
      "Epoch 103/1000\n",
      " - 52s - loss: 4.9411 - out_loss: 0.6631 - out_0_loss: 0.6508 - out_1_loss: 0.9112 - out_2_loss: 0.8928 - out_3_loss: 0.9068 - out_4_loss: 0.9164 - out_acc: 0.7883 - out_0_acc: 0.7579 - out_1_acc: 0.6646 - out_2_acc: 0.6787 - out_3_acc: 0.6720 - out_4_acc: 0.6647 - val_loss: 4.9628 - val_out_loss: 0.6216 - val_out_0_loss: 0.6432 - val_out_1_loss: 1.0095 - val_out_2_loss: 0.9031 - val_out_3_loss: 0.9134 - val_out_4_loss: 0.8719 - val_out_acc: 0.7766 - val_out_0_acc: 0.7527 - val_out_1_acc: 0.6377 - val_out_2_acc: 0.6746 - val_out_3_acc: 0.6638 - val_out_4_acc: 0.6811\n",
      "Epoch 104/1000\n",
      " - 52s - loss: 5.0086 - out_loss: 0.6697 - out_0_loss: 0.6415 - out_1_loss: 0.9403 - out_2_loss: 0.9339 - out_3_loss: 0.9042 - out_4_loss: 0.9190 - out_acc: 0.7810 - out_0_acc: 0.7680 - out_1_acc: 0.6552 - out_2_acc: 0.6562 - out_3_acc: 0.6753 - out_4_acc: 0.6629 - val_loss: 4.8929 - val_out_loss: 0.6190 - val_out_0_loss: 0.6239 - val_out_1_loss: 0.9104 - val_out_2_loss: 0.8968 - val_out_3_loss: 0.8856 - val_out_4_loss: 0.9572 - val_out_acc: 0.7983 - val_out_0_acc: 0.7722 - val_out_1_acc: 0.6464 - val_out_2_acc: 0.6833 - val_out_3_acc: 0.6681 - val_out_4_acc: 0.6594\n",
      "Epoch 105/1000\n",
      " - 53s - loss: 4.8539 - out_loss: 0.6428 - out_0_loss: 0.6152 - out_1_loss: 0.8796 - out_2_loss: 0.8913 - out_3_loss: 0.8994 - out_4_loss: 0.9255 - out_acc: 0.7920 - out_0_acc: 0.7712 - out_1_acc: 0.6742 - out_2_acc: 0.6720 - out_3_acc: 0.6623 - out_4_acc: 0.6598 - val_loss: 4.8400 - val_out_loss: 0.5972 - val_out_0_loss: 0.5667 - val_out_1_loss: 0.9089 - val_out_2_loss: 0.9164 - val_out_3_loss: 0.9180 - val_out_4_loss: 0.9328 - val_out_acc: 0.8048 - val_out_0_acc: 0.7918 - val_out_1_acc: 0.6920 - val_out_2_acc: 0.6551 - val_out_3_acc: 0.6703 - val_out_4_acc: 0.6638\n",
      "Epoch 106/1000\n",
      " - 51s - loss: 4.8112 - out_loss: 0.6437 - out_0_loss: 0.6007 - out_1_loss: 0.8889 - out_2_loss: 0.8892 - out_3_loss: 0.8949 - out_4_loss: 0.8938 - out_acc: 0.7995 - out_0_acc: 0.7758 - out_1_acc: 0.6707 - out_2_acc: 0.6710 - out_3_acc: 0.6692 - out_4_acc: 0.6744 - val_loss: 4.9381 - val_out_loss: 0.6277 - val_out_0_loss: 0.6000 - val_out_1_loss: 0.9037 - val_out_2_loss: 0.8922 - val_out_3_loss: 0.9898 - val_out_4_loss: 0.9247 - val_out_acc: 0.7787 - val_out_0_acc: 0.7722 - val_out_1_acc: 0.6594 - val_out_2_acc: 0.6594 - val_out_3_acc: 0.6269 - val_out_4_acc: 0.6768\n",
      "Epoch 107/1000\n",
      " - 52s - loss: 4.7983 - out_loss: 0.6064 - out_0_loss: 0.5868 - out_1_loss: 0.8969 - out_2_loss: 0.8837 - out_3_loss: 0.9046 - out_4_loss: 0.9199 - out_acc: 0.7990 - out_0_acc: 0.7815 - out_1_acc: 0.6669 - out_2_acc: 0.6833 - out_3_acc: 0.6670 - out_4_acc: 0.6632 - val_loss: 4.7405 - val_out_loss: 0.5891 - val_out_0_loss: 0.5815 - val_out_1_loss: 0.9408 - val_out_2_loss: 0.8498 - val_out_3_loss: 0.8714 - val_out_4_loss: 0.9078 - val_out_acc: 0.7961 - val_out_0_acc: 0.7874 - val_out_1_acc: 0.6573 - val_out_2_acc: 0.7137 - val_out_3_acc: 0.6833 - val_out_4_acc: 0.6725\n",
      "Epoch 108/1000\n",
      " - 52s - loss: 4.7333 - out_loss: 0.6296 - out_0_loss: 0.5849 - out_1_loss: 0.8868 - out_2_loss: 0.8711 - out_3_loss: 0.8975 - out_4_loss: 0.8634 - out_acc: 0.7902 - out_0_acc: 0.7875 - out_1_acc: 0.6714 - out_2_acc: 0.6656 - out_3_acc: 0.6689 - out_4_acc: 0.6743 - val_loss: 4.9532 - val_out_loss: 0.6180 - val_out_0_loss: 0.6968 - val_out_1_loss: 0.8976 - val_out_2_loss: 0.9858 - val_out_3_loss: 0.8434 - val_out_4_loss: 0.9115 - val_out_acc: 0.7636 - val_out_0_acc: 0.7419 - val_out_1_acc: 0.6529 - val_out_2_acc: 0.6247 - val_out_3_acc: 0.6811 - val_out_4_acc: 0.6681\n",
      "Epoch 109/1000\n",
      " - 51s - loss: 4.7026 - out_loss: 0.6216 - out_0_loss: 0.5815 - out_1_loss: 0.8838 - out_2_loss: 0.8502 - out_3_loss: 0.8799 - out_4_loss: 0.8857 - out_acc: 0.7946 - out_0_acc: 0.7862 - out_1_acc: 0.6700 - out_2_acc: 0.6867 - out_3_acc: 0.6776 - out_4_acc: 0.6739 - val_loss: 5.1812 - val_out_loss: 0.6426 - val_out_0_loss: 0.6514 - val_out_1_loss: 0.9884 - val_out_2_loss: 1.0244 - val_out_3_loss: 0.9507 - val_out_4_loss: 0.9237 - val_out_acc: 0.7722 - val_out_0_acc: 0.7657 - val_out_1_acc: 0.6486 - val_out_2_acc: 0.6009 - val_out_3_acc: 0.6356 - val_out_4_acc: 0.6508\n",
      "Epoch 110/1000\n",
      " - 52s - loss: 4.8241 - out_loss: 0.6385 - out_0_loss: 0.6042 - out_1_loss: 0.9182 - out_2_loss: 0.8814 - out_3_loss: 0.9083 - out_4_loss: 0.8735 - out_acc: 0.7910 - out_0_acc: 0.7680 - out_1_acc: 0.6552 - out_2_acc: 0.6742 - out_3_acc: 0.6672 - out_4_acc: 0.6713 - val_loss: 5.0501 - val_out_loss: 0.6464 - val_out_0_loss: 0.7259 - val_out_1_loss: 0.8768 - val_out_2_loss: 0.9482 - val_out_3_loss: 0.8875 - val_out_4_loss: 0.9654 - val_out_acc: 0.7636 - val_out_0_acc: 0.7093 - val_out_1_acc: 0.6681 - val_out_2_acc: 0.6573 - val_out_3_acc: 0.6659 - val_out_4_acc: 0.6247\n",
      "Epoch 111/1000\n",
      " - 52s - loss: 4.8545 - out_loss: 0.6406 - out_0_loss: 0.6167 - out_1_loss: 0.9111 - out_2_loss: 0.8944 - out_3_loss: 0.8816 - out_4_loss: 0.9102 - out_acc: 0.7928 - out_0_acc: 0.7671 - out_1_acc: 0.6606 - out_2_acc: 0.6724 - out_3_acc: 0.6790 - out_4_acc: 0.6738 - val_loss: 4.7096 - val_out_loss: 0.5585 - val_out_0_loss: 0.5973 - val_out_1_loss: 0.9326 - val_out_2_loss: 0.8325 - val_out_3_loss: 0.8988 - val_out_4_loss: 0.8899 - val_out_acc: 0.8156 - val_out_0_acc: 0.7657 - val_out_1_acc: 0.6443 - val_out_2_acc: 0.7223 - val_out_3_acc: 0.6508 - val_out_4_acc: 0.6594\n",
      "Epoch 112/1000\n",
      " - 52s - loss: 4.7641 - out_loss: 0.6235 - out_0_loss: 0.5765 - out_1_loss: 0.9187 - out_2_loss: 0.8835 - out_3_loss: 0.9019 - out_4_loss: 0.8600 - out_acc: 0.8039 - out_0_acc: 0.7852 - out_1_acc: 0.6618 - out_2_acc: 0.6773 - out_3_acc: 0.6677 - out_4_acc: 0.6789 - val_loss: 4.8117 - val_out_loss: 0.6080 - val_out_0_loss: 0.5679 - val_out_1_loss: 0.9450 - val_out_2_loss: 0.8638 - val_out_3_loss: 0.9587 - val_out_4_loss: 0.8681 - val_out_acc: 0.7874 - val_out_0_acc: 0.7939 - val_out_1_acc: 0.6573 - val_out_2_acc: 0.6790 - val_out_3_acc: 0.6421 - val_out_4_acc: 0.6855\n",
      "Epoch 113/1000\n",
      " - 51s - loss: 4.7500 - out_loss: 0.6176 - out_0_loss: 0.5859 - out_1_loss: 0.8834 - out_2_loss: 0.8758 - out_3_loss: 0.9015 - out_4_loss: 0.8858 - out_acc: 0.8049 - out_0_acc: 0.7876 - out_1_acc: 0.6716 - out_2_acc: 0.6801 - out_3_acc: 0.6625 - out_4_acc: 0.6721 - val_loss: 4.9805 - val_out_loss: 0.6402 - val_out_0_loss: 0.7019 - val_out_1_loss: 0.8791 - val_out_2_loss: 0.9547 - val_out_3_loss: 0.9134 - val_out_4_loss: 0.8912 - val_out_acc: 0.7831 - val_out_0_acc: 0.7267 - val_out_1_acc: 0.6725 - val_out_2_acc: 0.6594 - val_out_3_acc: 0.6486 - val_out_4_acc: 0.6746\n",
      "Epoch 114/1000\n",
      " - 50s - loss: 4.7251 - out_loss: 0.6145 - out_0_loss: 0.5959 - out_1_loss: 0.8718 - out_2_loss: 0.9000 - out_3_loss: 0.8770 - out_4_loss: 0.8658 - out_acc: 0.8035 - out_0_acc: 0.7834 - out_1_acc: 0.6773 - out_2_acc: 0.6710 - out_3_acc: 0.6860 - out_4_acc: 0.6813 - val_loss: 4.9085 - val_out_loss: 0.5838 - val_out_0_loss: 0.6553 - val_out_1_loss: 0.9077 - val_out_2_loss: 0.8627 - val_out_3_loss: 0.9334 - val_out_4_loss: 0.9657 - val_out_acc: 0.8004 - val_out_0_acc: 0.7592 - val_out_1_acc: 0.6573 - val_out_2_acc: 0.6746 - val_out_3_acc: 0.6594 - val_out_4_acc: 0.6703\n",
      "Epoch 115/1000\n",
      " - 50s - loss: 4.6852 - out_loss: 0.6070 - out_0_loss: 0.5821 - out_1_loss: 0.8829 - out_2_loss: 0.8580 - out_3_loss: 0.8624 - out_4_loss: 0.8929 - out_acc: 0.8057 - out_0_acc: 0.7917 - out_1_acc: 0.6672 - out_2_acc: 0.6882 - out_3_acc: 0.6887 - out_4_acc: 0.6747 - val_loss: 4.5608 - val_out_loss: 0.5414 - val_out_0_loss: 0.5806 - val_out_1_loss: 0.8989 - val_out_2_loss: 0.8545 - val_out_3_loss: 0.8157 - val_out_4_loss: 0.8696 - val_out_acc: 0.8048 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.6486 - val_out_2_acc: 0.6790 - val_out_3_acc: 0.6855 - val_out_4_acc: 0.6659\n",
      "Epoch 116/1000\n",
      " - 51s - loss: 4.6069 - out_loss: 0.5860 - out_0_loss: 0.5516 - out_1_loss: 0.8634 - out_2_loss: 0.8501 - out_3_loss: 0.8626 - out_4_loss: 0.8932 - out_acc: 0.8120 - out_0_acc: 0.8010 - out_1_acc: 0.6816 - out_2_acc: 0.6879 - out_3_acc: 0.6823 - out_4_acc: 0.6760 - val_loss: 4.9472 - val_out_loss: 0.6214 - val_out_0_loss: 0.6998 - val_out_1_loss: 0.9729 - val_out_2_loss: 0.8908 - val_out_3_loss: 0.8762 - val_out_4_loss: 0.8862 - val_out_acc: 0.7787 - val_out_0_acc: 0.7484 - val_out_1_acc: 0.6421 - val_out_2_acc: 0.6681 - val_out_3_acc: 0.6746 - val_out_4_acc: 0.6811\n",
      "Epoch 117/1000\n",
      " - 51s - loss: 4.5930 - out_loss: 0.5914 - out_0_loss: 0.5785 - out_1_loss: 0.8495 - out_2_loss: 0.8519 - out_3_loss: 0.8496 - out_4_loss: 0.8721 - out_acc: 0.8119 - out_0_acc: 0.7899 - out_1_acc: 0.6927 - out_2_acc: 0.6946 - out_3_acc: 0.6985 - out_4_acc: 0.6787 - val_loss: 4.9119 - val_out_loss: 0.6233 - val_out_0_loss: 0.6049 - val_out_1_loss: 0.9286 - val_out_2_loss: 0.9328 - val_out_3_loss: 0.9308 - val_out_4_loss: 0.8916 - val_out_acc: 0.7636 - val_out_0_acc: 0.7787 - val_out_1_acc: 0.6464 - val_out_2_acc: 0.6399 - val_out_3_acc: 0.6594 - val_out_4_acc: 0.6421\n",
      "Epoch 118/1000\n",
      " - 51s - loss: 4.7049 - out_loss: 0.6056 - out_0_loss: 0.5578 - out_1_loss: 0.9031 - out_2_loss: 0.9288 - out_3_loss: 0.8342 - out_4_loss: 0.8755 - out_acc: 0.8029 - out_0_acc: 0.7872 - out_1_acc: 0.6770 - out_2_acc: 0.6568 - out_3_acc: 0.6969 - out_4_acc: 0.6752 - val_loss: 4.7133 - val_out_loss: 0.6000 - val_out_0_loss: 0.6713 - val_out_1_loss: 0.8409 - val_out_2_loss: 0.8761 - val_out_3_loss: 0.8740 - val_out_4_loss: 0.8510 - val_out_acc: 0.7852 - val_out_0_acc: 0.7505 - val_out_1_acc: 0.6833 - val_out_2_acc: 0.6833 - val_out_3_acc: 0.6725 - val_out_4_acc: 0.7050\n",
      "Epoch 119/1000\n",
      " - 51s - loss: 4.6264 - out_loss: 0.5760 - out_0_loss: 0.5665 - out_1_loss: 0.8592 - out_2_loss: 0.8626 - out_3_loss: 0.8499 - out_4_loss: 0.9122 - out_acc: 0.8205 - out_0_acc: 0.7947 - out_1_acc: 0.6836 - out_2_acc: 0.6945 - out_3_acc: 0.6931 - out_4_acc: 0.6695 - val_loss: 5.0630 - val_out_loss: 0.6532 - val_out_0_loss: 0.6576 - val_out_1_loss: 0.8976 - val_out_2_loss: 0.9570 - val_out_3_loss: 0.9609 - val_out_4_loss: 0.9367 - val_out_acc: 0.7787 - val_out_0_acc: 0.7636 - val_out_1_acc: 0.6898 - val_out_2_acc: 0.6486 - val_out_3_acc: 0.6681 - val_out_4_acc: 0.6486\n",
      "Epoch 120/1000\n",
      " - 50s - loss: 4.5465 - out_loss: 0.5760 - out_0_loss: 0.5663 - out_1_loss: 0.8526 - out_2_loss: 0.8499 - out_3_loss: 0.8491 - out_4_loss: 0.8526 - out_acc: 0.8143 - out_0_acc: 0.7907 - out_1_acc: 0.6910 - out_2_acc: 0.6869 - out_3_acc: 0.6877 - out_4_acc: 0.6816 - val_loss: 4.4332 - val_out_loss: 0.5602 - val_out_0_loss: 0.5794 - val_out_1_loss: 0.8274 - val_out_2_loss: 0.8159 - val_out_3_loss: 0.8418 - val_out_4_loss: 0.8085 - val_out_acc: 0.8069 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.6768 - val_out_2_acc: 0.7007 - val_out_3_acc: 0.6681 - val_out_4_acc: 0.7137\n",
      "Epoch 121/1000\n",
      " - 52s - loss: 4.5011 - out_loss: 0.5724 - out_0_loss: 0.5524 - out_1_loss: 0.8550 - out_2_loss: 0.8409 - out_3_loss: 0.8345 - out_4_loss: 0.8458 - out_acc: 0.8043 - out_0_acc: 0.7874 - out_1_acc: 0.6868 - out_2_acc: 0.6869 - out_3_acc: 0.6913 - out_4_acc: 0.6944 - val_loss: 4.9348 - val_out_loss: 0.6303 - val_out_0_loss: 0.7048 - val_out_1_loss: 0.9090 - val_out_2_loss: 0.9186 - val_out_3_loss: 0.8885 - val_out_4_loss: 0.8836 - val_out_acc: 0.7722 - val_out_0_acc: 0.7267 - val_out_1_acc: 0.6811 - val_out_2_acc: 0.6334 - val_out_3_acc: 0.6681 - val_out_4_acc: 0.6659\n",
      "Epoch 122/1000\n",
      " - 53s - loss: 4.5591 - out_loss: 0.5832 - out_0_loss: 0.5515 - out_1_loss: 0.8624 - out_2_loss: 0.8625 - out_3_loss: 0.8558 - out_4_loss: 0.8437 - out_acc: 0.8144 - out_0_acc: 0.8023 - out_1_acc: 0.6792 - out_2_acc: 0.6748 - out_3_acc: 0.6820 - out_4_acc: 0.6933 - val_loss: 4.7283 - val_out_loss: 0.5478 - val_out_0_loss: 0.5340 - val_out_1_loss: 0.9486 - val_out_2_loss: 0.8677 - val_out_3_loss: 0.9610 - val_out_4_loss: 0.8692 - val_out_acc: 0.8048 - val_out_0_acc: 0.7961 - val_out_1_acc: 0.6594 - val_out_2_acc: 0.7028 - val_out_3_acc: 0.6594 - val_out_4_acc: 0.6551\n",
      "Epoch 123/1000\n",
      " - 52s - loss: 4.5398 - out_loss: 0.5700 - out_0_loss: 0.5333 - out_1_loss: 0.8624 - out_2_loss: 0.8569 - out_3_loss: 0.8530 - out_4_loss: 0.8643 - out_acc: 0.8170 - out_0_acc: 0.8015 - out_1_acc: 0.6814 - out_2_acc: 0.6825 - out_3_acc: 0.6910 - out_4_acc: 0.6799 - val_loss: 4.8058 - val_out_loss: 0.6341 - val_out_0_loss: 0.6783 - val_out_1_loss: 0.8873 - val_out_2_loss: 0.8690 - val_out_3_loss: 0.8448 - val_out_4_loss: 0.8923 - val_out_acc: 0.7679 - val_out_0_acc: 0.7462 - val_out_1_acc: 0.6616 - val_out_2_acc: 0.6681 - val_out_3_acc: 0.6811 - val_out_4_acc: 0.6833\n",
      "Epoch 124/1000\n",
      " - 52s - loss: 4.5363 - out_loss: 0.5842 - out_0_loss: 0.5784 - out_1_loss: 0.8486 - out_2_loss: 0.8316 - out_3_loss: 0.8468 - out_4_loss: 0.8467 - out_acc: 0.8115 - out_0_acc: 0.7954 - out_1_acc: 0.6855 - out_2_acc: 0.6914 - out_3_acc: 0.6788 - out_4_acc: 0.6858 - val_loss: 4.6738 - val_out_loss: 0.5712 - val_out_0_loss: 0.5970 - val_out_1_loss: 0.8992 - val_out_2_loss: 0.8405 - val_out_3_loss: 0.8966 - val_out_4_loss: 0.8691 - val_out_acc: 0.8004 - val_out_0_acc: 0.7592 - val_out_1_acc: 0.6551 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.6703 - val_out_4_acc: 0.6768\n",
      "Epoch 125/1000\n",
      " - 52s - loss: 4.5494 - out_loss: 0.5811 - out_0_loss: 0.5500 - out_1_loss: 0.8717 - out_2_loss: 0.8302 - out_3_loss: 0.8660 - out_4_loss: 0.8504 - out_acc: 0.8113 - out_0_acc: 0.8009 - out_1_acc: 0.6762 - out_2_acc: 0.6977 - out_3_acc: 0.6806 - out_4_acc: 0.6886 - val_loss: 4.8976 - val_out_loss: 0.6109 - val_out_0_loss: 0.6319 - val_out_1_loss: 1.0098 - val_out_2_loss: 0.8500 - val_out_3_loss: 0.9282 - val_out_4_loss: 0.8668 - val_out_acc: 0.7852 - val_out_0_acc: 0.7679 - val_out_1_acc: 0.6594 - val_out_2_acc: 0.6855 - val_out_3_acc: 0.6790 - val_out_4_acc: 0.6616\n",
      "Epoch 126/1000\n",
      " - 52s - loss: 4.4729 - out_loss: 0.5548 - out_0_loss: 0.5264 - out_1_loss: 0.8424 - out_2_loss: 0.8510 - out_3_loss: 0.8306 - out_4_loss: 0.8678 - out_acc: 0.8164 - out_0_acc: 0.8036 - out_1_acc: 0.6875 - out_2_acc: 0.6887 - out_3_acc: 0.6866 - out_4_acc: 0.6738 - val_loss: 4.5252 - val_out_loss: 0.5359 - val_out_0_loss: 0.6081 - val_out_1_loss: 0.8482 - val_out_2_loss: 0.8501 - val_out_3_loss: 0.8160 - val_out_4_loss: 0.8669 - val_out_acc: 0.8069 - val_out_0_acc: 0.7592 - val_out_1_acc: 0.6768 - val_out_2_acc: 0.6898 - val_out_3_acc: 0.7115 - val_out_4_acc: 0.6941\n",
      "Epoch 127/1000\n",
      " - 51s - loss: 4.5618 - out_loss: 0.5676 - out_0_loss: 0.5666 - out_1_loss: 0.8751 - out_2_loss: 0.8568 - out_3_loss: 0.8394 - out_4_loss: 0.8563 - out_acc: 0.8130 - out_0_acc: 0.7989 - out_1_acc: 0.6826 - out_2_acc: 0.6871 - out_3_acc: 0.6927 - out_4_acc: 0.6809 - val_loss: 4.3285 - val_out_loss: 0.4898 - val_out_0_loss: 0.5290 - val_out_1_loss: 0.8541 - val_out_2_loss: 0.7717 - val_out_3_loss: 0.8623 - val_out_4_loss: 0.8217 - val_out_acc: 0.8308 - val_out_0_acc: 0.8069 - val_out_1_acc: 0.6681 - val_out_2_acc: 0.7158 - val_out_3_acc: 0.6638 - val_out_4_acc: 0.6876\n",
      "Epoch 128/1000\n",
      " - 51s - loss: 4.5275 - out_loss: 0.5531 - out_0_loss: 0.5216 - out_1_loss: 0.8386 - out_2_loss: 0.8514 - out_3_loss: 0.8865 - out_4_loss: 0.8763 - out_acc: 0.8190 - out_0_acc: 0.8082 - out_1_acc: 0.6816 - out_2_acc: 0.6918 - out_3_acc: 0.6728 - out_4_acc: 0.6812 - val_loss: 4.6636 - val_out_loss: 0.5488 - val_out_0_loss: 0.5721 - val_out_1_loss: 0.9006 - val_out_2_loss: 0.9112 - val_out_3_loss: 0.9279 - val_out_4_loss: 0.8031 - val_out_acc: 0.8200 - val_out_0_acc: 0.7896 - val_out_1_acc: 0.6638 - val_out_2_acc: 0.6659 - val_out_3_acc: 0.6594 - val_out_4_acc: 0.7028\n",
      "Epoch 129/1000\n",
      " - 51s - loss: 4.4814 - out_loss: 0.5582 - out_0_loss: 0.5360 - out_1_loss: 0.8753 - out_2_loss: 0.8315 - out_3_loss: 0.8490 - out_4_loss: 0.8314 - out_acc: 0.8208 - out_0_acc: 0.8108 - out_1_acc: 0.6818 - out_2_acc: 0.6865 - out_3_acc: 0.6870 - out_4_acc: 0.6853 - val_loss: 4.6490 - val_out_loss: 0.5602 - val_out_0_loss: 0.6204 - val_out_1_loss: 0.8685 - val_out_2_loss: 0.9004 - val_out_3_loss: 0.8228 - val_out_4_loss: 0.8767 - val_out_acc: 0.8091 - val_out_0_acc: 0.7592 - val_out_1_acc: 0.6833 - val_out_2_acc: 0.6725 - val_out_3_acc: 0.6941 - val_out_4_acc: 0.6963\n",
      "Epoch 130/1000\n",
      " - 51s - loss: 4.4979 - out_loss: 0.5447 - out_0_loss: 0.5337 - out_1_loss: 0.8797 - out_2_loss: 0.8518 - out_3_loss: 0.8404 - out_4_loss: 0.8476 - out_acc: 0.8246 - out_0_acc: 0.8093 - out_1_acc: 0.6692 - out_2_acc: 0.6875 - out_3_acc: 0.6882 - out_4_acc: 0.6897 - val_loss: 4.8941 - val_out_loss: 0.5725 - val_out_0_loss: 0.5938 - val_out_1_loss: 0.9697 - val_out_2_loss: 1.0116 - val_out_3_loss: 0.8428 - val_out_4_loss: 0.9037 - val_out_acc: 0.8069 - val_out_0_acc: 0.8048 - val_out_1_acc: 0.6725 - val_out_2_acc: 0.6247 - val_out_3_acc: 0.6833 - val_out_4_acc: 0.6703\n",
      "Epoch 131/1000\n",
      " - 51s - loss: 4.4074 - out_loss: 0.5496 - out_0_loss: 0.5234 - out_1_loss: 0.8203 - out_2_loss: 0.8164 - out_3_loss: 0.8534 - out_4_loss: 0.8443 - out_acc: 0.8293 - out_0_acc: 0.8133 - out_1_acc: 0.6977 - out_2_acc: 0.7046 - out_3_acc: 0.6837 - out_4_acc: 0.6952 - val_loss: 5.0710 - val_out_loss: 0.6043 - val_out_0_loss: 0.6925 - val_out_1_loss: 0.9603 - val_out_2_loss: 0.8472 - val_out_3_loss: 0.9569 - val_out_4_loss: 1.0099 - val_out_acc: 0.7874 - val_out_0_acc: 0.7527 - val_out_1_acc: 0.6486 - val_out_2_acc: 0.7115 - val_out_3_acc: 0.6312 - val_out_4_acc: 0.6399\n",
      "Epoch 132/1000\n",
      " - 51s - loss: 4.3032 - out_loss: 0.5326 - out_0_loss: 0.5287 - out_1_loss: 0.8055 - out_2_loss: 0.7994 - out_3_loss: 0.8072 - out_4_loss: 0.8298 - out_acc: 0.8246 - out_0_acc: 0.8048 - out_1_acc: 0.7016 - out_2_acc: 0.7122 - out_3_acc: 0.6972 - out_4_acc: 0.6947 - val_loss: 4.4634 - val_out_loss: 0.5056 - val_out_0_loss: 0.6180 - val_out_1_loss: 0.8085 - val_out_2_loss: 0.8362 - val_out_3_loss: 0.8421 - val_out_4_loss: 0.8530 - val_out_acc: 0.8178 - val_out_0_acc: 0.7874 - val_out_1_acc: 0.6985 - val_out_2_acc: 0.6703 - val_out_3_acc: 0.7115 - val_out_4_acc: 0.6811\n",
      "Epoch 133/1000\n",
      " - 51s - loss: 4.3601 - out_loss: 0.5223 - out_0_loss: 0.5244 - out_1_loss: 0.8100 - out_2_loss: 0.8351 - out_3_loss: 0.8088 - out_4_loss: 0.8596 - out_acc: 0.8365 - out_0_acc: 0.8093 - out_1_acc: 0.7115 - out_2_acc: 0.7006 - out_3_acc: 0.6999 - out_4_acc: 0.6824 - val_loss: 4.4031 - val_out_loss: 0.5445 - val_out_0_loss: 0.5803 - val_out_1_loss: 0.7487 - val_out_2_loss: 0.8269 - val_out_3_loss: 0.8560 - val_out_4_loss: 0.8468 - val_out_acc: 0.8026 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.6855 - val_out_3_acc: 0.6768 - val_out_4_acc: 0.6551\n",
      "Epoch 134/1000\n",
      " - 51s - loss: 4.4126 - out_loss: 0.5460 - out_0_loss: 0.5319 - out_1_loss: 0.8286 - out_2_loss: 0.8252 - out_3_loss: 0.8346 - out_4_loss: 0.8463 - out_acc: 0.8210 - out_0_acc: 0.8067 - out_1_acc: 0.6981 - out_2_acc: 0.7005 - out_3_acc: 0.6942 - out_4_acc: 0.6830 - val_loss: 4.6304 - val_out_loss: 0.5566 - val_out_0_loss: 0.6236 - val_out_1_loss: 0.8632 - val_out_2_loss: 0.8678 - val_out_3_loss: 0.7647 - val_out_4_loss: 0.9545 - val_out_acc: 0.8113 - val_out_0_acc: 0.7852 - val_out_1_acc: 0.6746 - val_out_2_acc: 0.6681 - val_out_3_acc: 0.7397 - val_out_4_acc: 0.6464\n",
      "Epoch 135/1000\n",
      " - 50s - loss: 4.3289 - out_loss: 0.5217 - out_0_loss: 0.4947 - out_1_loss: 0.8295 - out_2_loss: 0.8153 - out_3_loss: 0.8309 - out_4_loss: 0.8368 - out_acc: 0.8359 - out_0_acc: 0.8160 - out_1_acc: 0.6988 - out_2_acc: 0.6987 - out_3_acc: 0.6956 - out_4_acc: 0.6983 - val_loss: 4.5009 - val_out_loss: 0.5203 - val_out_0_loss: 0.5766 - val_out_1_loss: 0.8588 - val_out_2_loss: 0.8599 - val_out_3_loss: 0.8416 - val_out_4_loss: 0.8436 - val_out_acc: 0.8286 - val_out_0_acc: 0.8048 - val_out_1_acc: 0.6920 - val_out_2_acc: 0.6985 - val_out_3_acc: 0.6876 - val_out_4_acc: 0.6963\n",
      "Epoch 136/1000\n",
      " - 52s - loss: 4.3416 - out_loss: 0.5339 - out_0_loss: 0.5317 - out_1_loss: 0.8264 - out_2_loss: 0.7931 - out_3_loss: 0.8055 - out_4_loss: 0.8511 - out_acc: 0.8276 - out_0_acc: 0.8006 - out_1_acc: 0.7024 - out_2_acc: 0.7097 - out_3_acc: 0.7050 - out_4_acc: 0.6837 - val_loss: 4.5121 - val_out_loss: 0.5486 - val_out_0_loss: 0.5630 - val_out_1_loss: 0.8470 - val_out_2_loss: 0.8572 - val_out_3_loss: 0.8198 - val_out_4_loss: 0.8765 - val_out_acc: 0.8221 - val_out_0_acc: 0.7896 - val_out_1_acc: 0.6876 - val_out_2_acc: 0.7007 - val_out_3_acc: 0.6833 - val_out_4_acc: 0.6768\n",
      "Epoch 137/1000\n",
      " - 51s - loss: 4.3386 - out_loss: 0.5429 - out_0_loss: 0.5316 - out_1_loss: 0.8055 - out_2_loss: 0.7989 - out_3_loss: 0.8238 - out_4_loss: 0.8358 - out_acc: 0.8294 - out_0_acc: 0.8092 - out_1_acc: 0.7096 - out_2_acc: 0.6999 - out_3_acc: 0.7005 - out_4_acc: 0.6926 - val_loss: 4.3447 - val_out_loss: 0.4579 - val_out_0_loss: 0.5212 - val_out_1_loss: 0.8905 - val_out_2_loss: 0.8714 - val_out_3_loss: 0.8208 - val_out_4_loss: 0.7829 - val_out_acc: 0.8438 - val_out_0_acc: 0.8178 - val_out_1_acc: 0.6703 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6833 - val_out_4_acc: 0.6941\n",
      "Epoch 138/1000\n",
      " - 52s - loss: 4.3035 - out_loss: 0.5209 - out_0_loss: 0.5053 - out_1_loss: 0.8128 - out_2_loss: 0.8157 - out_3_loss: 0.8297 - out_4_loss: 0.8190 - out_acc: 0.8331 - out_0_acc: 0.8155 - out_1_acc: 0.6972 - out_2_acc: 0.6969 - out_3_acc: 0.6959 - out_4_acc: 0.6999 - val_loss: 4.4071 - val_out_loss: 0.5321 - val_out_0_loss: 0.5853 - val_out_1_loss: 0.8001 - val_out_2_loss: 0.8489 - val_out_3_loss: 0.8140 - val_out_4_loss: 0.8268 - val_out_acc: 0.8178 - val_out_0_acc: 0.7896 - val_out_1_acc: 0.6790 - val_out_2_acc: 0.6790 - val_out_3_acc: 0.6963 - val_out_4_acc: 0.7050\n",
      "Epoch 139/1000\n",
      " - 51s - loss: 4.1690 - out_loss: 0.4932 - out_0_loss: 0.4977 - out_1_loss: 0.8008 - out_2_loss: 0.7956 - out_3_loss: 0.7980 - out_4_loss: 0.7837 - out_acc: 0.8452 - out_0_acc: 0.8184 - out_1_acc: 0.7004 - out_2_acc: 0.6992 - out_3_acc: 0.7096 - out_4_acc: 0.7054 - val_loss: 4.2236 - val_out_loss: 0.4638 - val_out_0_loss: 0.4845 - val_out_1_loss: 0.8405 - val_out_2_loss: 0.8197 - val_out_3_loss: 0.7775 - val_out_4_loss: 0.8374 - val_out_acc: 0.8351 - val_out_0_acc: 0.7939 - val_out_1_acc: 0.7028 - val_out_2_acc: 0.6985 - val_out_3_acc: 0.7072 - val_out_4_acc: 0.7028\n",
      "Epoch 140/1000\n",
      " - 51s - loss: 4.2885 - out_loss: 0.5039 - out_0_loss: 0.4945 - out_1_loss: 0.8241 - out_2_loss: 0.8102 - out_3_loss: 0.8200 - out_4_loss: 0.8357 - out_acc: 0.8320 - out_0_acc: 0.8128 - out_1_acc: 0.6972 - out_2_acc: 0.7114 - out_3_acc: 0.6939 - out_4_acc: 0.6881 - val_loss: 4.6651 - val_out_loss: 0.5626 - val_out_0_loss: 0.6203 - val_out_1_loss: 0.8835 - val_out_2_loss: 0.8935 - val_out_3_loss: 0.8402 - val_out_4_loss: 0.8650 - val_out_acc: 0.7918 - val_out_0_acc: 0.7852 - val_out_1_acc: 0.6638 - val_out_2_acc: 0.6551 - val_out_3_acc: 0.6876 - val_out_4_acc: 0.6725\n",
      "Epoch 141/1000\n",
      " - 51s - loss: 4.3692 - out_loss: 0.5386 - out_0_loss: 0.5103 - out_1_loss: 0.8205 - out_2_loss: 0.8201 - out_3_loss: 0.8131 - out_4_loss: 0.8666 - out_acc: 0.8297 - out_0_acc: 0.8161 - out_1_acc: 0.6885 - out_2_acc: 0.6999 - out_3_acc: 0.6958 - out_4_acc: 0.6799 - val_loss: 4.9769 - val_out_loss: 0.5603 - val_out_0_loss: 0.5808 - val_out_1_loss: 0.9573 - val_out_2_loss: 0.9835 - val_out_3_loss: 0.9746 - val_out_4_loss: 0.9204 - val_out_acc: 0.7983 - val_out_0_acc: 0.8026 - val_out_1_acc: 0.6377 - val_out_2_acc: 0.6464 - val_out_3_acc: 0.6269 - val_out_4_acc: 0.6638\n",
      "Epoch 142/1000\n",
      " - 52s - loss: 4.2865 - out_loss: 0.4989 - out_0_loss: 0.4711 - out_1_loss: 0.8250 - out_2_loss: 0.8230 - out_3_loss: 0.8383 - out_4_loss: 0.8302 - out_acc: 0.8438 - out_0_acc: 0.8352 - out_1_acc: 0.6971 - out_2_acc: 0.7035 - out_3_acc: 0.6872 - out_4_acc: 0.6936 - val_loss: 4.3362 - val_out_loss: 0.4927 - val_out_0_loss: 0.5651 - val_out_1_loss: 0.7813 - val_out_2_loss: 0.7782 - val_out_3_loss: 0.8661 - val_out_4_loss: 0.8529 - val_out_acc: 0.8265 - val_out_0_acc: 0.8026 - val_out_1_acc: 0.7223 - val_out_2_acc: 0.7223 - val_out_3_acc: 0.6790 - val_out_4_acc: 0.6876\n",
      "Epoch 143/1000\n",
      " - 52s - loss: 4.3021 - out_loss: 0.5211 - out_0_loss: 0.5209 - out_1_loss: 0.8131 - out_2_loss: 0.7924 - out_3_loss: 0.8126 - out_4_loss: 0.8419 - out_acc: 0.8260 - out_0_acc: 0.8082 - out_1_acc: 0.7011 - out_2_acc: 0.6992 - out_3_acc: 0.7039 - out_4_acc: 0.6854 - val_loss: 4.6930 - val_out_loss: 0.5520 - val_out_0_loss: 0.5808 - val_out_1_loss: 0.8791 - val_out_2_loss: 0.8896 - val_out_3_loss: 0.9038 - val_out_4_loss: 0.8876 - val_out_acc: 0.8156 - val_out_0_acc: 0.7939 - val_out_1_acc: 0.6746 - val_out_2_acc: 0.6725 - val_out_3_acc: 0.6941 - val_out_4_acc: 0.6725\n",
      "Epoch 144/1000\n",
      " - 53s - loss: 4.3558 - out_loss: 0.5264 - out_0_loss: 0.5064 - out_1_loss: 0.8378 - out_2_loss: 0.8182 - out_3_loss: 0.8444 - out_4_loss: 0.8226 - out_acc: 0.8313 - out_0_acc: 0.8146 - out_1_acc: 0.6883 - out_2_acc: 0.6992 - out_3_acc: 0.6949 - out_4_acc: 0.6931 - val_loss: 4.6534 - val_out_loss: 0.5406 - val_out_0_loss: 0.5713 - val_out_1_loss: 0.9140 - val_out_2_loss: 0.8890 - val_out_3_loss: 0.8850 - val_out_4_loss: 0.8535 - val_out_acc: 0.8069 - val_out_0_acc: 0.7679 - val_out_1_acc: 0.6681 - val_out_2_acc: 0.6725 - val_out_3_acc: 0.6486 - val_out_4_acc: 0.6681\n",
      "Epoch 145/1000\n",
      " - 51s - loss: 4.3118 - out_loss: 0.5160 - out_0_loss: 0.4983 - out_1_loss: 0.8466 - out_2_loss: 0.8109 - out_3_loss: 0.8414 - out_4_loss: 0.7986 - out_acc: 0.8345 - out_0_acc: 0.8256 - out_1_acc: 0.6921 - out_2_acc: 0.6989 - out_3_acc: 0.6951 - out_4_acc: 0.7106 - val_loss: 4.5522 - val_out_loss: 0.5174 - val_out_0_loss: 0.5829 - val_out_1_loss: 0.8693 - val_out_2_loss: 0.8634 - val_out_3_loss: 0.8630 - val_out_4_loss: 0.8561 - val_out_acc: 0.8200 - val_out_0_acc: 0.7874 - val_out_1_acc: 0.6790 - val_out_2_acc: 0.6833 - val_out_3_acc: 0.6963 - val_out_4_acc: 0.6811\n",
      "Epoch 146/1000\n",
      " - 51s - loss: 4.2424 - out_loss: 0.5124 - out_0_loss: 0.4819 - out_1_loss: 0.8015 - out_2_loss: 0.8101 - out_3_loss: 0.8123 - out_4_loss: 0.8241 - out_acc: 0.8417 - out_0_acc: 0.8245 - out_1_acc: 0.6997 - out_2_acc: 0.7013 - out_3_acc: 0.7011 - out_4_acc: 0.6983 - val_loss: 4.4929 - val_out_loss: 0.5373 - val_out_0_loss: 0.5824 - val_out_1_loss: 0.8661 - val_out_2_loss: 0.8580 - val_out_3_loss: 0.8358 - val_out_4_loss: 0.8133 - val_out_acc: 0.8156 - val_out_0_acc: 0.7918 - val_out_1_acc: 0.6920 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.6963 - val_out_4_acc: 0.6920\n",
      "Epoch 147/1000\n",
      " - 52s - loss: 4.2704 - out_loss: 0.5092 - out_0_loss: 0.4981 - out_1_loss: 0.8185 - out_2_loss: 0.8123 - out_3_loss: 0.8022 - out_4_loss: 0.8301 - out_acc: 0.8319 - out_0_acc: 0.8135 - out_1_acc: 0.6936 - out_2_acc: 0.7059 - out_3_acc: 0.7118 - out_4_acc: 0.7026 - val_loss: 4.4901 - val_out_loss: 0.5493 - val_out_0_loss: 0.6195 - val_out_1_loss: 0.8293 - val_out_2_loss: 0.8546 - val_out_3_loss: 0.8144 - val_out_4_loss: 0.8230 - val_out_acc: 0.8069 - val_out_0_acc: 0.7636 - val_out_1_acc: 0.7202 - val_out_2_acc: 0.6855 - val_out_3_acc: 0.7050 - val_out_4_acc: 0.7028\n",
      "Epoch 148/1000\n",
      " - 51s - loss: 4.2074 - out_loss: 0.4920 - out_0_loss: 0.4956 - out_1_loss: 0.8103 - out_2_loss: 0.7937 - out_3_loss: 0.7964 - out_4_loss: 0.8194 - out_acc: 0.8392 - out_0_acc: 0.8258 - out_1_acc: 0.7038 - out_2_acc: 0.7071 - out_3_acc: 0.7071 - out_4_acc: 0.7033 - val_loss: 4.3970 - val_out_loss: 0.5011 - val_out_0_loss: 0.5023 - val_out_1_loss: 0.8114 - val_out_2_loss: 0.7922 - val_out_3_loss: 0.8357 - val_out_4_loss: 0.9543 - val_out_acc: 0.8373 - val_out_0_acc: 0.8026 - val_out_1_acc: 0.6920 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.7028 - val_out_4_acc: 0.6551\n",
      "Epoch 149/1000\n",
      " - 52s - loss: 3.9939 - out_loss: 0.4494 - out_0_loss: 0.4412 - out_1_loss: 0.7671 - out_2_loss: 0.7849 - out_3_loss: 0.7612 - out_4_loss: 0.7901 - out_acc: 0.8565 - out_0_acc: 0.8417 - out_1_acc: 0.7172 - out_2_acc: 0.7101 - out_3_acc: 0.7144 - out_4_acc: 0.7034 - val_loss: 4.3657 - val_out_loss: 0.5113 - val_out_0_loss: 0.5839 - val_out_1_loss: 0.7646 - val_out_2_loss: 0.8328 - val_out_3_loss: 0.7962 - val_out_4_loss: 0.8771 - val_out_acc: 0.8091 - val_out_0_acc: 0.7549 - val_out_1_acc: 0.7007 - val_out_2_acc: 0.6876 - val_out_3_acc: 0.6855 - val_out_4_acc: 0.6768\n",
      "Epoch 150/1000\n",
      " - 50s - loss: 3.9975 - out_loss: 0.4561 - out_0_loss: 0.4331 - out_1_loss: 0.7851 - out_2_loss: 0.7713 - out_3_loss: 0.7624 - out_4_loss: 0.7894 - out_acc: 0.8573 - out_0_acc: 0.8380 - out_1_acc: 0.7062 - out_2_acc: 0.7173 - out_3_acc: 0.7156 - out_4_acc: 0.7122 - val_loss: 4.4804 - val_out_loss: 0.5271 - val_out_0_loss: 0.5851 - val_out_1_loss: 0.8141 - val_out_2_loss: 0.8155 - val_out_3_loss: 0.8687 - val_out_4_loss: 0.8699 - val_out_acc: 0.8048 - val_out_0_acc: 0.7744 - val_out_1_acc: 0.6985 - val_out_2_acc: 0.7028 - val_out_3_acc: 0.6811 - val_out_4_acc: 0.6725\n",
      "Epoch 151/1000\n",
      " - 50s - loss: 4.1745 - out_loss: 0.5063 - out_0_loss: 0.4871 - out_1_loss: 0.7665 - out_2_loss: 0.7991 - out_3_loss: 0.7961 - out_4_loss: 0.8194 - out_acc: 0.8337 - out_0_acc: 0.8204 - out_1_acc: 0.7139 - out_2_acc: 0.7057 - out_3_acc: 0.7101 - out_4_acc: 0.7085 - val_loss: 4.3681 - val_out_loss: 0.5142 - val_out_0_loss: 0.6172 - val_out_1_loss: 0.8806 - val_out_2_loss: 0.7293 - val_out_3_loss: 0.8239 - val_out_4_loss: 0.8029 - val_out_acc: 0.8395 - val_out_0_acc: 0.7787 - val_out_1_acc: 0.6746 - val_out_2_acc: 0.7158 - val_out_3_acc: 0.7007 - val_out_4_acc: 0.7137\n",
      "Epoch 152/1000\n",
      " - 51s - loss: 4.0777 - out_loss: 0.4803 - out_0_loss: 0.4684 - out_1_loss: 0.7819 - out_2_loss: 0.7766 - out_3_loss: 0.7767 - out_4_loss: 0.7937 - out_acc: 0.8565 - out_0_acc: 0.8357 - out_1_acc: 0.7193 - out_2_acc: 0.7230 - out_3_acc: 0.7148 - out_4_acc: 0.7064 - val_loss: 4.4627 - val_out_loss: 0.4944 - val_out_0_loss: 0.5809 - val_out_1_loss: 0.8336 - val_out_2_loss: 0.8252 - val_out_3_loss: 0.8373 - val_out_4_loss: 0.8912 - val_out_acc: 0.8134 - val_out_0_acc: 0.7918 - val_out_1_acc: 0.6898 - val_out_2_acc: 0.6725 - val_out_3_acc: 0.6616 - val_out_4_acc: 0.6616\n",
      "Epoch 153/1000\n",
      " - 51s - loss: 3.9268 - out_loss: 0.4424 - out_0_loss: 0.4453 - out_1_loss: 0.7750 - out_2_loss: 0.7477 - out_3_loss: 0.7386 - out_4_loss: 0.7779 - out_acc: 0.8599 - out_0_acc: 0.8375 - out_1_acc: 0.7118 - out_2_acc: 0.7180 - out_3_acc: 0.7312 - out_4_acc: 0.7144 - val_loss: 4.0782 - val_out_loss: 0.4616 - val_out_0_loss: 0.4490 - val_out_1_loss: 0.7396 - val_out_2_loss: 0.7804 - val_out_3_loss: 0.8638 - val_out_4_loss: 0.7838 - val_out_acc: 0.8416 - val_out_0_acc: 0.8373 - val_out_1_acc: 0.7137 - val_out_2_acc: 0.7115 - val_out_3_acc: 0.6768 - val_out_4_acc: 0.7072\n",
      "Epoch 154/1000\n",
      " - 51s - loss: 4.1228 - out_loss: 0.4785 - out_0_loss: 0.4651 - out_1_loss: 0.7827 - out_2_loss: 0.7731 - out_3_loss: 0.8299 - out_4_loss: 0.7934 - out_acc: 0.8497 - out_0_acc: 0.8224 - out_1_acc: 0.7093 - out_2_acc: 0.7175 - out_3_acc: 0.6962 - out_4_acc: 0.7111 - val_loss: 4.2044 - val_out_loss: 0.4918 - val_out_0_loss: 0.4819 - val_out_1_loss: 0.8184 - val_out_2_loss: 0.8052 - val_out_3_loss: 0.8171 - val_out_4_loss: 0.7899 - val_out_acc: 0.8221 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7028 - val_out_2_acc: 0.7028 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.6941\n",
      "Epoch 155/1000\n",
      " - 52s - loss: 4.0873 - out_loss: 0.4769 - out_0_loss: 0.4474 - out_1_loss: 0.7913 - out_2_loss: 0.7871 - out_3_loss: 0.8067 - out_4_loss: 0.7778 - out_acc: 0.8535 - out_0_acc: 0.8323 - out_1_acc: 0.7108 - out_2_acc: 0.7141 - out_3_acc: 0.7012 - out_4_acc: 0.7187 - val_loss: 4.4844 - val_out_loss: 0.5298 - val_out_0_loss: 0.5257 - val_out_1_loss: 0.8921 - val_out_2_loss: 0.8598 - val_out_3_loss: 0.8509 - val_out_4_loss: 0.8261 - val_out_acc: 0.8113 - val_out_0_acc: 0.7961 - val_out_1_acc: 0.6681 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6898 - val_out_4_acc: 0.7050\n",
      "Epoch 156/1000\n",
      " - 51s - loss: 4.0376 - out_loss: 0.4660 - out_0_loss: 0.4673 - out_1_loss: 0.7814 - out_2_loss: 0.7724 - out_3_loss: 0.7871 - out_4_loss: 0.7633 - out_acc: 0.8536 - out_0_acc: 0.8267 - out_1_acc: 0.7167 - out_2_acc: 0.7207 - out_3_acc: 0.7113 - out_4_acc: 0.7251 - val_loss: 4.4182 - val_out_loss: 0.5426 - val_out_0_loss: 0.5885 - val_out_1_loss: 0.8516 - val_out_2_loss: 0.7873 - val_out_3_loss: 0.8272 - val_out_4_loss: 0.8212 - val_out_acc: 0.8113 - val_out_0_acc: 0.7722 - val_out_1_acc: 0.6725 - val_out_2_acc: 0.7354 - val_out_3_acc: 0.7028 - val_out_4_acc: 0.6833\n",
      "Epoch 157/1000\n",
      " - 51s - loss: 3.9954 - out_loss: 0.4566 - out_0_loss: 0.4535 - out_1_loss: 0.7592 - out_2_loss: 0.7707 - out_3_loss: 0.7523 - out_4_loss: 0.8031 - out_acc: 0.8561 - out_0_acc: 0.8313 - out_1_acc: 0.7206 - out_2_acc: 0.7107 - out_3_acc: 0.7204 - out_4_acc: 0.7049 - val_loss: 4.3633 - val_out_loss: 0.4985 - val_out_0_loss: 0.6011 - val_out_1_loss: 0.7980 - val_out_2_loss: 0.7879 - val_out_3_loss: 0.8570 - val_out_4_loss: 0.8209 - val_out_acc: 0.8416 - val_out_0_acc: 0.8026 - val_out_1_acc: 0.7007 - val_out_2_acc: 0.7050 - val_out_3_acc: 0.6855 - val_out_4_acc: 0.6963\n",
      "Epoch 158/1000\n",
      " - 51s - loss: 3.9849 - out_loss: 0.4504 - out_0_loss: 0.4394 - out_1_loss: 0.7770 - out_2_loss: 0.7630 - out_3_loss: 0.7861 - out_4_loss: 0.7690 - out_acc: 0.8577 - out_0_acc: 0.8375 - out_1_acc: 0.7254 - out_2_acc: 0.7202 - out_3_acc: 0.7133 - out_4_acc: 0.7170 - val_loss: 4.4601 - val_out_loss: 0.4901 - val_out_0_loss: 0.6008 - val_out_1_loss: 0.7855 - val_out_2_loss: 0.9094 - val_out_3_loss: 0.7815 - val_out_4_loss: 0.8928 - val_out_acc: 0.8286 - val_out_0_acc: 0.7983 - val_out_1_acc: 0.7072 - val_out_2_acc: 0.6855 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.6573\n",
      "Epoch 159/1000\n",
      " - 52s - loss: 3.9749 - out_loss: 0.4503 - out_0_loss: 0.4584 - out_1_loss: 0.7543 - out_2_loss: 0.7682 - out_3_loss: 0.7540 - out_4_loss: 0.7896 - out_acc: 0.8510 - out_0_acc: 0.8342 - out_1_acc: 0.7307 - out_2_acc: 0.7216 - out_3_acc: 0.7375 - out_4_acc: 0.7041 - val_loss: 4.3246 - val_out_loss: 0.4623 - val_out_0_loss: 0.5447 - val_out_1_loss: 0.8305 - val_out_2_loss: 0.8509 - val_out_3_loss: 0.7793 - val_out_4_loss: 0.8569 - val_out_acc: 0.8330 - val_out_0_acc: 0.7874 - val_out_1_acc: 0.6768 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.7267 - val_out_4_acc: 0.6746\n",
      "Epoch 160/1000\n",
      " - 52s - loss: 4.0277 - out_loss: 0.4643 - out_0_loss: 0.4521 - out_1_loss: 0.7862 - out_2_loss: 0.7636 - out_3_loss: 0.7744 - out_4_loss: 0.7872 - out_acc: 0.8535 - out_0_acc: 0.8410 - out_1_acc: 0.7106 - out_2_acc: 0.7156 - out_3_acc: 0.7144 - out_4_acc: 0.7083 - val_loss: 4.3646 - val_out_loss: 0.4892 - val_out_0_loss: 0.5521 - val_out_1_loss: 0.8525 - val_out_2_loss: 0.8044 - val_out_3_loss: 0.8299 - val_out_4_loss: 0.8366 - val_out_acc: 0.8221 - val_out_0_acc: 0.8004 - val_out_1_acc: 0.6920 - val_out_2_acc: 0.7028 - val_out_3_acc: 0.7007 - val_out_4_acc: 0.6811\n",
      "Epoch 161/1000\n",
      " - 51s - loss: 3.9537 - out_loss: 0.4539 - out_0_loss: 0.4423 - out_1_loss: 0.7766 - out_2_loss: 0.7905 - out_3_loss: 0.7381 - out_4_loss: 0.7522 - out_acc: 0.8584 - out_0_acc: 0.8416 - out_1_acc: 0.7172 - out_2_acc: 0.7117 - out_3_acc: 0.7267 - out_4_acc: 0.7287 - val_loss: 4.2012 - val_out_loss: 0.4585 - val_out_0_loss: 0.5029 - val_out_1_loss: 0.8124 - val_out_2_loss: 0.8017 - val_out_3_loss: 0.7884 - val_out_4_loss: 0.8373 - val_out_acc: 0.8460 - val_out_0_acc: 0.8221 - val_out_1_acc: 0.7028 - val_out_2_acc: 0.7050 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.6941\n",
      "Epoch 162/1000\n",
      " - 51s - loss: 4.1298 - out_loss: 0.4814 - out_0_loss: 0.4800 - out_1_loss: 0.7935 - out_2_loss: 0.7815 - out_3_loss: 0.7786 - out_4_loss: 0.8149 - out_acc: 0.8447 - out_0_acc: 0.8190 - out_1_acc: 0.7147 - out_2_acc: 0.7134 - out_3_acc: 0.7164 - out_4_acc: 0.7053 - val_loss: 4.6857 - val_out_loss: 0.5770 - val_out_0_loss: 0.6384 - val_out_1_loss: 0.8682 - val_out_2_loss: 0.8467 - val_out_3_loss: 0.9115 - val_out_4_loss: 0.8439 - val_out_acc: 0.8178 - val_out_0_acc: 0.7787 - val_out_1_acc: 0.6963 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.6681 - val_out_4_acc: 0.6941\n",
      "Epoch 163/1000\n",
      " - 50s - loss: 4.0075 - out_loss: 0.4542 - out_0_loss: 0.4528 - out_1_loss: 0.7779 - out_2_loss: 0.7818 - out_3_loss: 0.7607 - out_4_loss: 0.7800 - out_acc: 0.8471 - out_0_acc: 0.8371 - out_1_acc: 0.7230 - out_2_acc: 0.7213 - out_3_acc: 0.7121 - out_4_acc: 0.7139 - val_loss: 4.1966 - val_out_loss: 0.4466 - val_out_0_loss: 0.4611 - val_out_1_loss: 0.8622 - val_out_2_loss: 0.8061 - val_out_3_loss: 0.7986 - val_out_4_loss: 0.8221 - val_out_acc: 0.8395 - val_out_0_acc: 0.8503 - val_out_1_acc: 0.6855 - val_out_2_acc: 0.7007 - val_out_3_acc: 0.6876 - val_out_4_acc: 0.6963\n",
      "Epoch 164/1000\n",
      " - 50s - loss: 3.9940 - out_loss: 0.4476 - out_0_loss: 0.4327 - out_1_loss: 0.7731 - out_2_loss: 0.7672 - out_3_loss: 0.7787 - out_4_loss: 0.7946 - out_acc: 0.8625 - out_0_acc: 0.8415 - out_1_acc: 0.7204 - out_2_acc: 0.7247 - out_3_acc: 0.7168 - out_4_acc: 0.7136 - val_loss: 4.3716 - val_out_loss: 0.5095 - val_out_0_loss: 0.6019 - val_out_1_loss: 0.8082 - val_out_2_loss: 0.7826 - val_out_3_loss: 0.8513 - val_out_4_loss: 0.8182 - val_out_acc: 0.8243 - val_out_0_acc: 0.7809 - val_out_1_acc: 0.7202 - val_out_2_acc: 0.7245 - val_out_3_acc: 0.6920 - val_out_4_acc: 0.7289\n",
      "Epoch 165/1000\n",
      " - 51s - loss: 3.9984 - out_loss: 0.4466 - out_0_loss: 0.4545 - out_1_loss: 0.7807 - out_2_loss: 0.7581 - out_3_loss: 0.7946 - out_4_loss: 0.7640 - out_acc: 0.8657 - out_0_acc: 0.8367 - out_1_acc: 0.7239 - out_2_acc: 0.7324 - out_3_acc: 0.7083 - out_4_acc: 0.7263 - val_loss: 3.9447 - val_out_loss: 0.4180 - val_out_0_loss: 0.4651 - val_out_1_loss: 0.7372 - val_out_2_loss: 0.8067 - val_out_3_loss: 0.7642 - val_out_4_loss: 0.7535 - val_out_acc: 0.8482 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7354 - val_out_2_acc: 0.6876 - val_out_3_acc: 0.7115 - val_out_4_acc: 0.7007\n",
      "Epoch 166/1000\n",
      " - 50s - loss: 4.0824 - out_loss: 0.4767 - out_0_loss: 0.4909 - out_1_loss: 0.7663 - out_2_loss: 0.7811 - out_3_loss: 0.7708 - out_4_loss: 0.7967 - out_acc: 0.8510 - out_0_acc: 0.8222 - out_1_acc: 0.7188 - out_2_acc: 0.7063 - out_3_acc: 0.7181 - out_4_acc: 0.7056 - val_loss: 4.3623 - val_out_loss: 0.5001 - val_out_0_loss: 0.6678 - val_out_1_loss: 0.7889 - val_out_2_loss: 0.8318 - val_out_3_loss: 0.7747 - val_out_4_loss: 0.7990 - val_out_acc: 0.8265 - val_out_0_acc: 0.7484 - val_out_1_acc: 0.7202 - val_out_2_acc: 0.7180 - val_out_3_acc: 0.7072 - val_out_4_acc: 0.6703\n",
      "Epoch 167/1000\n",
      " - 50s - loss: 4.0899 - out_loss: 0.4641 - out_0_loss: 0.4577 - out_1_loss: 0.7780 - out_2_loss: 0.7937 - out_3_loss: 0.8084 - out_4_loss: 0.7879 - out_acc: 0.8526 - out_0_acc: 0.8357 - out_1_acc: 0.7161 - out_2_acc: 0.7164 - out_3_acc: 0.7183 - out_4_acc: 0.7053 - val_loss: 4.2875 - val_out_loss: 0.5033 - val_out_0_loss: 0.5821 - val_out_1_loss: 0.8064 - val_out_2_loss: 0.8713 - val_out_3_loss: 0.7194 - val_out_4_loss: 0.8050 - val_out_acc: 0.8265 - val_out_0_acc: 0.7809 - val_out_1_acc: 0.7289 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.7375 - val_out_4_acc: 0.6941\n",
      "Epoch 168/1000\n",
      " - 51s - loss: 4.0243 - out_loss: 0.4489 - out_0_loss: 0.4525 - out_1_loss: 0.7830 - out_2_loss: 0.7907 - out_3_loss: 0.7761 - out_4_loss: 0.7730 - out_acc: 0.8571 - out_0_acc: 0.8353 - out_1_acc: 0.7125 - out_2_acc: 0.7068 - out_3_acc: 0.7175 - out_4_acc: 0.7199 - val_loss: 4.1193 - val_out_loss: 0.4940 - val_out_0_loss: 0.5259 - val_out_1_loss: 0.7373 - val_out_2_loss: 0.8070 - val_out_3_loss: 0.7545 - val_out_4_loss: 0.8005 - val_out_acc: 0.8178 - val_out_0_acc: 0.7961 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.6985 - val_out_3_acc: 0.7115 - val_out_4_acc: 0.7158\n",
      "Epoch 169/1000\n",
      " - 51s - loss: 3.8195 - out_loss: 0.4082 - out_0_loss: 0.4171 - out_1_loss: 0.7630 - out_2_loss: 0.7434 - out_3_loss: 0.7394 - out_4_loss: 0.7483 - out_acc: 0.8716 - out_0_acc: 0.8487 - out_1_acc: 0.7111 - out_2_acc: 0.7237 - out_3_acc: 0.7361 - out_4_acc: 0.7340 - val_loss: 4.2064 - val_out_loss: 0.4667 - val_out_0_loss: 0.5258 - val_out_1_loss: 0.7402 - val_out_2_loss: 0.7829 - val_out_3_loss: 0.8404 - val_out_4_loss: 0.8503 - val_out_acc: 0.8373 - val_out_0_acc: 0.8178 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7158 - val_out_3_acc: 0.7050 - val_out_4_acc: 0.6920\n",
      "Epoch 170/1000\n",
      " - 50s - loss: 3.9530 - out_loss: 0.4604 - out_0_loss: 0.4304 - out_1_loss: 0.7685 - out_2_loss: 0.7705 - out_3_loss: 0.7496 - out_4_loss: 0.7735 - out_acc: 0.8534 - out_0_acc: 0.8423 - out_1_acc: 0.7038 - out_2_acc: 0.7197 - out_3_acc: 0.7240 - out_4_acc: 0.7154 - val_loss: 4.4233 - val_out_loss: 0.5228 - val_out_0_loss: 0.6106 - val_out_1_loss: 0.8537 - val_out_2_loss: 0.8074 - val_out_3_loss: 0.8076 - val_out_4_loss: 0.8212 - val_out_acc: 0.8091 - val_out_0_acc: 0.7918 - val_out_1_acc: 0.6855 - val_out_2_acc: 0.7115 - val_out_3_acc: 0.7050 - val_out_4_acc: 0.6855\n",
      "Epoch 171/1000\n",
      " - 51s - loss: 3.8580 - out_loss: 0.4486 - out_0_loss: 0.4459 - out_1_loss: 0.7519 - out_2_loss: 0.7143 - out_3_loss: 0.7367 - out_4_loss: 0.7606 - out_acc: 0.8539 - out_0_acc: 0.8332 - out_1_acc: 0.7139 - out_2_acc: 0.7409 - out_3_acc: 0.7224 - out_4_acc: 0.7228 - val_loss: 4.2706 - val_out_loss: 0.4912 - val_out_0_loss: 0.4786 - val_out_1_loss: 0.8771 - val_out_2_loss: 0.8429 - val_out_3_loss: 0.7693 - val_out_4_loss: 0.8115 - val_out_acc: 0.8221 - val_out_0_acc: 0.8351 - val_out_1_acc: 0.6790 - val_out_2_acc: 0.6876 - val_out_3_acc: 0.6855 - val_out_4_acc: 0.7158\n",
      "Epoch 172/1000\n",
      " - 51s - loss: 3.8156 - out_loss: 0.4218 - out_0_loss: 0.4101 - out_1_loss: 0.7486 - out_2_loss: 0.7294 - out_3_loss: 0.7408 - out_4_loss: 0.7650 - out_acc: 0.8645 - out_0_acc: 0.8587 - out_1_acc: 0.7237 - out_2_acc: 0.7309 - out_3_acc: 0.7214 - out_4_acc: 0.7196 - val_loss: 4.1817 - val_out_loss: 0.4599 - val_out_0_loss: 0.5703 - val_out_1_loss: 0.8256 - val_out_2_loss: 0.7715 - val_out_3_loss: 0.7762 - val_out_4_loss: 0.7781 - val_out_acc: 0.8482 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.6768 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.6985\n",
      "Epoch 173/1000\n",
      " - 51s - loss: 3.8951 - out_loss: 0.4302 - out_0_loss: 0.4459 - out_1_loss: 0.7642 - out_2_loss: 0.7610 - out_3_loss: 0.7456 - out_4_loss: 0.7483 - out_acc: 0.8646 - out_0_acc: 0.8445 - out_1_acc: 0.7136 - out_2_acc: 0.7227 - out_3_acc: 0.7251 - out_4_acc: 0.7191 - val_loss: 4.2584 - val_out_loss: 0.4879 - val_out_0_loss: 0.5218 - val_out_1_loss: 0.9257 - val_out_2_loss: 0.7774 - val_out_3_loss: 0.7858 - val_out_4_loss: 0.7598 - val_out_acc: 0.8221 - val_out_0_acc: 0.8026 - val_out_1_acc: 0.6594 - val_out_2_acc: 0.7093 - val_out_3_acc: 0.7245 - val_out_4_acc: 0.7223\n",
      "Epoch 174/1000\n",
      " - 51s - loss: 3.8756 - out_loss: 0.4266 - out_0_loss: 0.4324 - out_1_loss: 0.7552 - out_2_loss: 0.7563 - out_3_loss: 0.7379 - out_4_loss: 0.7673 - out_acc: 0.8621 - out_0_acc: 0.8506 - out_1_acc: 0.7261 - out_2_acc: 0.7199 - out_3_acc: 0.7306 - out_4_acc: 0.7174 - val_loss: 4.4270 - val_out_loss: 0.5181 - val_out_0_loss: 0.5627 - val_out_1_loss: 0.8172 - val_out_2_loss: 0.8091 - val_out_3_loss: 0.8891 - val_out_4_loss: 0.8308 - val_out_acc: 0.8026 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.6855 - val_out_2_acc: 0.7050 - val_out_3_acc: 0.6855 - val_out_4_acc: 0.7093\n",
      "Epoch 175/1000\n",
      " - 51s - loss: 3.7853 - out_loss: 0.4170 - out_0_loss: 0.4288 - out_1_loss: 0.7426 - out_2_loss: 0.7106 - out_3_loss: 0.7429 - out_4_loss: 0.7436 - out_acc: 0.8695 - out_0_acc: 0.8456 - out_1_acc: 0.7322 - out_2_acc: 0.7426 - out_3_acc: 0.7290 - out_4_acc: 0.7313 - val_loss: 4.3903 - val_out_loss: 0.5389 - val_out_0_loss: 0.5805 - val_out_1_loss: 0.7849 - val_out_2_loss: 0.8207 - val_out_3_loss: 0.7981 - val_out_4_loss: 0.8672 - val_out_acc: 0.8134 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.6876 - val_out_2_acc: 0.7093 - val_out_3_acc: 0.6811 - val_out_4_acc: 0.6659\n",
      "Epoch 176/1000\n",
      " - 51s - loss: 3.8558 - out_loss: 0.4255 - out_0_loss: 0.4343 - out_1_loss: 0.7371 - out_2_loss: 0.7393 - out_3_loss: 0.7520 - out_4_loss: 0.7676 - out_acc: 0.8683 - out_0_acc: 0.8448 - out_1_acc: 0.7415 - out_2_acc: 0.7320 - out_3_acc: 0.7332 - out_4_acc: 0.7201 - val_loss: 4.2591 - val_out_loss: 0.5074 - val_out_0_loss: 0.5765 - val_out_1_loss: 0.7970 - val_out_2_loss: 0.7655 - val_out_3_loss: 0.8122 - val_out_4_loss: 0.8005 - val_out_acc: 0.8265 - val_out_0_acc: 0.7852 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7332 - val_out_3_acc: 0.6985 - val_out_4_acc: 0.7050\n",
      "Epoch 177/1000\n",
      " - 52s - loss: 3.7906 - out_loss: 0.4251 - out_0_loss: 0.4408 - out_1_loss: 0.7258 - out_2_loss: 0.7238 - out_3_loss: 0.7227 - out_4_loss: 0.7524 - out_acc: 0.8653 - out_0_acc: 0.8372 - out_1_acc: 0.7358 - out_2_acc: 0.7388 - out_3_acc: 0.7328 - out_4_acc: 0.7174 - val_loss: 4.4911 - val_out_loss: 0.4798 - val_out_0_loss: 0.5104 - val_out_1_loss: 0.8806 - val_out_2_loss: 0.8625 - val_out_3_loss: 0.8134 - val_out_4_loss: 0.9444 - val_out_acc: 0.8482 - val_out_0_acc: 0.8460 - val_out_1_acc: 0.6985 - val_out_2_acc: 0.6833 - val_out_3_acc: 0.6920 - val_out_4_acc: 0.6725\n",
      "Epoch 178/1000\n",
      " - 51s - loss: 3.8151 - out_loss: 0.4147 - out_0_loss: 0.4081 - out_1_loss: 0.7684 - out_2_loss: 0.7459 - out_3_loss: 0.7315 - out_4_loss: 0.7464 - out_acc: 0.8661 - out_0_acc: 0.8511 - out_1_acc: 0.7225 - out_2_acc: 0.7272 - out_3_acc: 0.7369 - out_4_acc: 0.7368 - val_loss: 4.1334 - val_out_loss: 0.4152 - val_out_0_loss: 0.5074 - val_out_1_loss: 0.8590 - val_out_2_loss: 0.6923 - val_out_3_loss: 0.8735 - val_out_4_loss: 0.7861 - val_out_acc: 0.8330 - val_out_0_acc: 0.8156 - val_out_1_acc: 0.6855 - val_out_2_acc: 0.7267 - val_out_3_acc: 0.6898 - val_out_4_acc: 0.7158\n",
      "Epoch 179/1000\n",
      " - 51s - loss: 3.8843 - out_loss: 0.4319 - out_0_loss: 0.4369 - out_1_loss: 0.7644 - out_2_loss: 0.7190 - out_3_loss: 0.7664 - out_4_loss: 0.7658 - out_acc: 0.8617 - out_0_acc: 0.8470 - out_1_acc: 0.7220 - out_2_acc: 0.7339 - out_3_acc: 0.7148 - out_4_acc: 0.7179 - val_loss: 3.8775 - val_out_loss: 0.4090 - val_out_0_loss: 0.4707 - val_out_1_loss: 0.7308 - val_out_2_loss: 0.7132 - val_out_3_loss: 0.7900 - val_out_4_loss: 0.7638 - val_out_acc: 0.8503 - val_out_0_acc: 0.8113 - val_out_1_acc: 0.7419 - val_out_2_acc: 0.7332 - val_out_3_acc: 0.7202 - val_out_4_acc: 0.7028\n",
      "Epoch 180/1000\n",
      " - 51s - loss: 3.7808 - out_loss: 0.4097 - out_0_loss: 0.4189 - out_1_loss: 0.7565 - out_2_loss: 0.7207 - out_3_loss: 0.7264 - out_4_loss: 0.7484 - out_acc: 0.8693 - out_0_acc: 0.8446 - out_1_acc: 0.7185 - out_2_acc: 0.7429 - out_3_acc: 0.7377 - out_4_acc: 0.7241 - val_loss: 4.1221 - val_out_loss: 0.4716 - val_out_0_loss: 0.5098 - val_out_1_loss: 0.8309 - val_out_2_loss: 0.8086 - val_out_3_loss: 0.7698 - val_out_4_loss: 0.7313 - val_out_acc: 0.8373 - val_out_0_acc: 0.8091 - val_out_1_acc: 0.6855 - val_out_2_acc: 0.6898 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.7332\n",
      "Epoch 181/1000\n",
      " - 50s - loss: 3.7622 - out_loss: 0.4097 - out_0_loss: 0.4121 - out_1_loss: 0.7313 - out_2_loss: 0.7354 - out_3_loss: 0.7112 - out_4_loss: 0.7626 - out_acc: 0.8647 - out_0_acc: 0.8439 - out_1_acc: 0.7313 - out_2_acc: 0.7259 - out_3_acc: 0.7389 - out_4_acc: 0.7225 - val_loss: 4.0755 - val_out_loss: 0.4478 - val_out_0_loss: 0.5094 - val_out_1_loss: 0.8248 - val_out_2_loss: 0.7952 - val_out_3_loss: 0.7272 - val_out_4_loss: 0.7712 - val_out_acc: 0.8373 - val_out_0_acc: 0.7983 - val_out_1_acc: 0.7050 - val_out_2_acc: 0.7050 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7072\n",
      "Epoch 182/1000\n",
      " - 51s - loss: 3.6687 - out_loss: 0.4094 - out_0_loss: 0.3901 - out_1_loss: 0.7272 - out_2_loss: 0.7197 - out_3_loss: 0.7032 - out_4_loss: 0.7191 - out_acc: 0.8723 - out_0_acc: 0.8620 - out_1_acc: 0.7287 - out_2_acc: 0.7386 - out_3_acc: 0.7389 - out_4_acc: 0.7374 - val_loss: 4.2681 - val_out_loss: 0.4442 - val_out_0_loss: 0.4742 - val_out_1_loss: 0.8626 - val_out_2_loss: 0.8131 - val_out_3_loss: 0.8302 - val_out_4_loss: 0.8437 - val_out_acc: 0.8416 - val_out_0_acc: 0.8547 - val_out_1_acc: 0.6790 - val_out_2_acc: 0.6898 - val_out_3_acc: 0.6920 - val_out_4_acc: 0.6790\n",
      "Epoch 183/1000\n",
      " - 51s - loss: 3.8130 - out_loss: 0.4104 - out_0_loss: 0.4188 - out_1_loss: 0.7334 - out_2_loss: 0.7195 - out_3_loss: 0.7554 - out_4_loss: 0.7754 - out_acc: 0.8694 - out_0_acc: 0.8553 - out_1_acc: 0.7343 - out_2_acc: 0.7332 - out_3_acc: 0.7207 - out_4_acc: 0.7174 - val_loss: 4.1653 - val_out_loss: 0.5147 - val_out_0_loss: 0.5622 - val_out_1_loss: 0.7853 - val_out_2_loss: 0.6896 - val_out_3_loss: 0.8503 - val_out_4_loss: 0.7633 - val_out_acc: 0.8265 - val_out_0_acc: 0.8004 - val_out_1_acc: 0.7050 - val_out_2_acc: 0.7484 - val_out_3_acc: 0.6659 - val_out_4_acc: 0.6985\n",
      "Epoch 184/1000\n",
      " - 51s - loss: 3.8659 - out_loss: 0.4453 - out_0_loss: 0.4354 - out_1_loss: 0.7354 - out_2_loss: 0.7426 - out_3_loss: 0.7715 - out_4_loss: 0.7359 - out_acc: 0.8549 - out_0_acc: 0.8397 - out_1_acc: 0.7327 - out_2_acc: 0.7264 - out_3_acc: 0.7250 - out_4_acc: 0.7389 - val_loss: 3.9301 - val_out_loss: 0.4183 - val_out_0_loss: 0.4723 - val_out_1_loss: 0.7830 - val_out_2_loss: 0.7173 - val_out_3_loss: 0.7655 - val_out_4_loss: 0.7736 - val_out_acc: 0.8460 - val_out_0_acc: 0.8330 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7310 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.7332\n",
      "Epoch 185/1000\n",
      " - 52s - loss: 3.5919 - out_loss: 0.3743 - out_0_loss: 0.3707 - out_1_loss: 0.7101 - out_2_loss: 0.6919 - out_3_loss: 0.7181 - out_4_loss: 0.7268 - out_acc: 0.8788 - out_0_acc: 0.8639 - out_1_acc: 0.7456 - out_2_acc: 0.7519 - out_3_acc: 0.7452 - out_4_acc: 0.7325 - val_loss: 4.2141 - val_out_loss: 0.4887 - val_out_0_loss: 0.5232 - val_out_1_loss: 0.7520 - val_out_2_loss: 0.7883 - val_out_3_loss: 0.8607 - val_out_4_loss: 0.8013 - val_out_acc: 0.8265 - val_out_0_acc: 0.8113 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7137 - val_out_3_acc: 0.6855 - val_out_4_acc: 0.7202\n",
      "Epoch 186/1000\n",
      " - 52s - loss: 3.6126 - out_loss: 0.3934 - out_0_loss: 0.4077 - out_1_loss: 0.6942 - out_2_loss: 0.7083 - out_3_loss: 0.6980 - out_4_loss: 0.7110 - out_acc: 0.8751 - out_0_acc: 0.8547 - out_1_acc: 0.7476 - out_2_acc: 0.7396 - out_3_acc: 0.7491 - out_4_acc: 0.7339 - val_loss: 4.0140 - val_out_loss: 0.4003 - val_out_0_loss: 0.4540 - val_out_1_loss: 0.7968 - val_out_2_loss: 0.7341 - val_out_3_loss: 0.8180 - val_out_4_loss: 0.8109 - val_out_acc: 0.8785 - val_out_0_acc: 0.8395 - val_out_1_acc: 0.7115 - val_out_2_acc: 0.7375 - val_out_3_acc: 0.7050 - val_out_4_acc: 0.6876\n",
      "Epoch 187/1000\n",
      " - 52s - loss: 3.8096 - out_loss: 0.4205 - out_0_loss: 0.4339 - out_1_loss: 0.7493 - out_2_loss: 0.7209 - out_3_loss: 0.7439 - out_4_loss: 0.7412 - out_acc: 0.8666 - out_0_acc: 0.8460 - out_1_acc: 0.7271 - out_2_acc: 0.7264 - out_3_acc: 0.7264 - out_4_acc: 0.7317 - val_loss: 4.0259 - val_out_loss: 0.4675 - val_out_0_loss: 0.5031 - val_out_1_loss: 0.7952 - val_out_2_loss: 0.7420 - val_out_3_loss: 0.7334 - val_out_4_loss: 0.7847 - val_out_acc: 0.8308 - val_out_0_acc: 0.7918 - val_out_1_acc: 0.6898 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7115 - val_out_4_acc: 0.7202\n",
      "Epoch 188/1000\n",
      " - 52s - loss: 3.6652 - out_loss: 0.3917 - out_0_loss: 0.4034 - out_1_loss: 0.7111 - out_2_loss: 0.7064 - out_3_loss: 0.7138 - out_4_loss: 0.7388 - out_acc: 0.8723 - out_0_acc: 0.8626 - out_1_acc: 0.7468 - out_2_acc: 0.7468 - out_3_acc: 0.7389 - out_4_acc: 0.7320 - val_loss: 3.9477 - val_out_loss: 0.3807 - val_out_0_loss: 0.4413 - val_out_1_loss: 0.7444 - val_out_2_loss: 0.8268 - val_out_3_loss: 0.7706 - val_out_4_loss: 0.7838 - val_out_acc: 0.8698 - val_out_0_acc: 0.8482 - val_out_1_acc: 0.7245 - val_out_2_acc: 0.7137 - val_out_3_acc: 0.7072 - val_out_4_acc: 0.7180\n",
      "Epoch 189/1000\n",
      " - 52s - loss: 3.7400 - out_loss: 0.4222 - out_0_loss: 0.4284 - out_1_loss: 0.7169 - out_2_loss: 0.7154 - out_3_loss: 0.7201 - out_4_loss: 0.7369 - out_acc: 0.8646 - out_0_acc: 0.8473 - out_1_acc: 0.7327 - out_2_acc: 0.7369 - out_3_acc: 0.7408 - out_4_acc: 0.7225 - val_loss: 4.2614 - val_out_loss: 0.4748 - val_out_0_loss: 0.4960 - val_out_1_loss: 0.8321 - val_out_2_loss: 0.8484 - val_out_3_loss: 0.7893 - val_out_4_loss: 0.8207 - val_out_acc: 0.8482 - val_out_0_acc: 0.8134 - val_out_1_acc: 0.6941 - val_out_2_acc: 0.6985 - val_out_3_acc: 0.6963 - val_out_4_acc: 0.6985\n",
      "Epoch 190/1000\n",
      " - 52s - loss: 3.6411 - out_loss: 0.3830 - out_0_loss: 0.3749 - out_1_loss: 0.7122 - out_2_loss: 0.7144 - out_3_loss: 0.7258 - out_4_loss: 0.7307 - out_acc: 0.8822 - out_0_acc: 0.8655 - out_1_acc: 0.7470 - out_2_acc: 0.7422 - out_3_acc: 0.7419 - out_4_acc: 0.7407 - val_loss: 3.8892 - val_out_loss: 0.4287 - val_out_0_loss: 0.5144 - val_out_1_loss: 0.7308 - val_out_2_loss: 0.6583 - val_out_3_loss: 0.7676 - val_out_4_loss: 0.7894 - val_out_acc: 0.8525 - val_out_0_acc: 0.8221 - val_out_1_acc: 0.7375 - val_out_2_acc: 0.7614 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7115\n",
      "Epoch 191/1000\n",
      " - 52s - loss: 3.6612 - out_loss: 0.3937 - out_0_loss: 0.4109 - out_1_loss: 0.7267 - out_2_loss: 0.7169 - out_3_loss: 0.7003 - out_4_loss: 0.7127 - out_acc: 0.8782 - out_0_acc: 0.8556 - out_1_acc: 0.7277 - out_2_acc: 0.7339 - out_3_acc: 0.7409 - out_4_acc: 0.7413 - val_loss: 3.9872 - val_out_loss: 0.4394 - val_out_0_loss: 0.4365 - val_out_1_loss: 0.7893 - val_out_2_loss: 0.7579 - val_out_3_loss: 0.7544 - val_out_4_loss: 0.8098 - val_out_acc: 0.8460 - val_out_0_acc: 0.8200 - val_out_1_acc: 0.7332 - val_out_2_acc: 0.7180 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.6876\n",
      "Epoch 192/1000\n",
      " - 52s - loss: 3.5390 - out_loss: 0.3624 - out_0_loss: 0.3873 - out_1_loss: 0.6819 - out_2_loss: 0.6881 - out_3_loss: 0.6979 - out_4_loss: 0.7214 - out_acc: 0.8850 - out_0_acc: 0.8637 - out_1_acc: 0.7477 - out_2_acc: 0.7551 - out_3_acc: 0.7466 - out_4_acc: 0.7309 - val_loss: 4.1462 - val_out_loss: 0.5007 - val_out_0_loss: 0.5138 - val_out_1_loss: 0.7825 - val_out_2_loss: 0.7443 - val_out_3_loss: 0.7908 - val_out_4_loss: 0.8143 - val_out_acc: 0.8308 - val_out_0_acc: 0.8048 - val_out_1_acc: 0.7028 - val_out_2_acc: 0.7375 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7028\n",
      "Epoch 193/1000\n",
      " - 52s - loss: 3.5221 - out_loss: 0.3584 - out_0_loss: 0.3726 - out_1_loss: 0.6922 - out_2_loss: 0.6842 - out_3_loss: 0.6982 - out_4_loss: 0.7165 - out_acc: 0.8801 - out_0_acc: 0.8662 - out_1_acc: 0.7480 - out_2_acc: 0.7488 - out_3_acc: 0.7393 - out_4_acc: 0.7382 - val_loss: 3.9620 - val_out_loss: 0.4385 - val_out_0_loss: 0.5050 - val_out_1_loss: 0.8231 - val_out_2_loss: 0.7569 - val_out_3_loss: 0.7119 - val_out_4_loss: 0.7266 - val_out_acc: 0.8568 - val_out_0_acc: 0.8286 - val_out_1_acc: 0.6963 - val_out_2_acc: 0.7245 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7397\n",
      "Epoch 194/1000\n",
      " - 53s - loss: 3.5266 - out_loss: 0.3773 - out_0_loss: 0.3835 - out_1_loss: 0.6901 - out_2_loss: 0.6924 - out_3_loss: 0.6734 - out_4_loss: 0.7098 - out_acc: 0.8822 - out_0_acc: 0.8604 - out_1_acc: 0.7561 - out_2_acc: 0.7420 - out_3_acc: 0.7586 - out_4_acc: 0.7337 - val_loss: 3.8328 - val_out_loss: 0.4236 - val_out_0_loss: 0.4846 - val_out_1_loss: 0.7055 - val_out_2_loss: 0.7282 - val_out_3_loss: 0.7424 - val_out_4_loss: 0.7485 - val_out_acc: 0.8547 - val_out_0_acc: 0.8091 - val_out_1_acc: 0.7310 - val_out_2_acc: 0.7180 - val_out_3_acc: 0.7267 - val_out_4_acc: 0.7375\n",
      "Epoch 195/1000\n",
      " - 52s - loss: 3.6547 - out_loss: 0.3873 - out_0_loss: 0.3908 - out_1_loss: 0.7263 - out_2_loss: 0.7232 - out_3_loss: 0.6886 - out_4_loss: 0.7384 - out_acc: 0.8824 - out_0_acc: 0.8632 - out_1_acc: 0.7379 - out_2_acc: 0.7377 - out_3_acc: 0.7461 - out_4_acc: 0.7300 - val_loss: 3.9773 - val_out_loss: 0.4092 - val_out_0_loss: 0.5135 - val_out_1_loss: 0.8096 - val_out_2_loss: 0.8142 - val_out_3_loss: 0.7205 - val_out_4_loss: 0.7103 - val_out_acc: 0.8568 - val_out_0_acc: 0.7961 - val_out_1_acc: 0.6898 - val_out_2_acc: 0.6985 - val_out_3_acc: 0.7050 - val_out_4_acc: 0.7223\n",
      "Epoch 196/1000\n",
      " - 52s - loss: 3.6293 - out_loss: 0.3882 - out_0_loss: 0.3565 - out_1_loss: 0.7226 - out_2_loss: 0.7429 - out_3_loss: 0.6974 - out_4_loss: 0.7217 - out_acc: 0.8799 - out_0_acc: 0.8660 - out_1_acc: 0.7370 - out_2_acc: 0.7309 - out_3_acc: 0.7382 - out_4_acc: 0.7383 - val_loss: 4.2172 - val_out_loss: 0.4973 - val_out_0_loss: 0.5943 - val_out_1_loss: 0.7200 - val_out_2_loss: 0.7459 - val_out_3_loss: 0.8151 - val_out_4_loss: 0.8446 - val_out_acc: 0.8308 - val_out_0_acc: 0.8026 - val_out_1_acc: 0.7419 - val_out_2_acc: 0.7050 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.6941\n",
      "Epoch 197/1000\n",
      " - 52s - loss: 3.6478 - out_loss: 0.3885 - out_0_loss: 0.4118 - out_1_loss: 0.7176 - out_2_loss: 0.6728 - out_3_loss: 0.7296 - out_4_loss: 0.7277 - out_acc: 0.8815 - out_0_acc: 0.8518 - out_1_acc: 0.7404 - out_2_acc: 0.7566 - out_3_acc: 0.7325 - out_4_acc: 0.7348 - val_loss: 4.0692 - val_out_loss: 0.4205 - val_out_0_loss: 0.4564 - val_out_1_loss: 0.7978 - val_out_2_loss: 0.7891 - val_out_3_loss: 0.7614 - val_out_4_loss: 0.8440 - val_out_acc: 0.8460 - val_out_0_acc: 0.8373 - val_out_1_acc: 0.7093 - val_out_2_acc: 0.7115 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.6659\n",
      "Epoch 198/1000\n",
      " - 51s - loss: 3.6303 - out_loss: 0.3885 - out_0_loss: 0.4005 - out_1_loss: 0.7241 - out_2_loss: 0.6796 - out_3_loss: 0.7171 - out_4_loss: 0.7205 - out_acc: 0.8718 - out_0_acc: 0.8594 - out_1_acc: 0.7265 - out_2_acc: 0.7419 - out_3_acc: 0.7337 - out_4_acc: 0.7329 - val_loss: 4.1150 - val_out_loss: 0.4427 - val_out_0_loss: 0.4861 - val_out_1_loss: 0.8151 - val_out_2_loss: 0.8621 - val_out_3_loss: 0.7235 - val_out_4_loss: 0.7855 - val_out_acc: 0.8438 - val_out_0_acc: 0.8308 - val_out_1_acc: 0.7093 - val_out_2_acc: 0.6876 - val_out_3_acc: 0.7354 - val_out_4_acc: 0.6941\n",
      "Epoch 199/1000\n",
      " - 52s - loss: 3.7097 - out_loss: 0.4024 - out_0_loss: 0.3997 - out_1_loss: 0.7048 - out_2_loss: 0.7242 - out_3_loss: 0.7262 - out_4_loss: 0.7523 - out_acc: 0.8715 - out_0_acc: 0.8529 - out_1_acc: 0.7390 - out_2_acc: 0.7333 - out_3_acc: 0.7303 - out_4_acc: 0.7214 - val_loss: 3.7771 - val_out_loss: 0.3854 - val_out_0_loss: 0.4588 - val_out_1_loss: 0.7380 - val_out_2_loss: 0.7381 - val_out_3_loss: 0.7533 - val_out_4_loss: 0.7035 - val_out_acc: 0.8677 - val_out_0_acc: 0.8351 - val_out_1_acc: 0.7636 - val_out_2_acc: 0.7419 - val_out_3_acc: 0.7202 - val_out_4_acc: 0.7375\n",
      "Epoch 200/1000\n",
      " - 52s - loss: 3.5367 - out_loss: 0.3732 - out_0_loss: 0.3572 - out_1_loss: 0.7113 - out_2_loss: 0.7016 - out_3_loss: 0.7038 - out_4_loss: 0.6895 - out_acc: 0.8842 - out_0_acc: 0.8683 - out_1_acc: 0.7279 - out_2_acc: 0.7503 - out_3_acc: 0.7446 - out_4_acc: 0.7459 - val_loss: 3.9586 - val_out_loss: 0.4451 - val_out_0_loss: 0.5175 - val_out_1_loss: 0.7423 - val_out_2_loss: 0.7513 - val_out_3_loss: 0.7110 - val_out_4_loss: 0.7913 - val_out_acc: 0.8395 - val_out_0_acc: 0.8156 - val_out_1_acc: 0.7202 - val_out_2_acc: 0.7223 - val_out_3_acc: 0.7440 - val_out_4_acc: 0.7245\n",
      "Epoch 201/1000\n",
      " - 52s - loss: 3.6712 - out_loss: 0.3954 - out_0_loss: 0.4277 - out_1_loss: 0.7217 - out_2_loss: 0.7013 - out_3_loss: 0.7177 - out_4_loss: 0.7074 - out_acc: 0.8777 - out_0_acc: 0.8433 - out_1_acc: 0.7370 - out_2_acc: 0.7445 - out_3_acc: 0.7476 - out_4_acc: 0.7460 - val_loss: 3.8335 - val_out_loss: 0.4008 - val_out_0_loss: 0.4896 - val_out_1_loss: 0.7343 - val_out_2_loss: 0.7018 - val_out_3_loss: 0.7586 - val_out_4_loss: 0.7483 - val_out_acc: 0.8590 - val_out_0_acc: 0.8373 - val_out_1_acc: 0.7375 - val_out_2_acc: 0.7223 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.7202\n",
      "Epoch 202/1000\n",
      " - 52s - loss: 3.6034 - out_loss: 0.3754 - out_0_loss: 0.3880 - out_1_loss: 0.7112 - out_2_loss: 0.7042 - out_3_loss: 0.6926 - out_4_loss: 0.7321 - out_acc: 0.8821 - out_0_acc: 0.8617 - out_1_acc: 0.7374 - out_2_acc: 0.7405 - out_3_acc: 0.7516 - out_4_acc: 0.7378 - val_loss: 3.6727 - val_out_loss: 0.3846 - val_out_0_loss: 0.4436 - val_out_1_loss: 0.7144 - val_out_2_loss: 0.6711 - val_out_3_loss: 0.7379 - val_out_4_loss: 0.7212 - val_out_acc: 0.8655 - val_out_0_acc: 0.8438 - val_out_1_acc: 0.7245 - val_out_2_acc: 0.7527 - val_out_3_acc: 0.7310 - val_out_4_acc: 0.7180\n",
      "Epoch 203/1000\n",
      " - 52s - loss: 3.5216 - out_loss: 0.3513 - out_0_loss: 0.3670 - out_1_loss: 0.7030 - out_2_loss: 0.6963 - out_3_loss: 0.6828 - out_4_loss: 0.7212 - out_acc: 0.8828 - out_0_acc: 0.8632 - out_1_acc: 0.7444 - out_2_acc: 0.7415 - out_3_acc: 0.7505 - out_4_acc: 0.7356 - val_loss: 4.1279 - val_out_loss: 0.4482 - val_out_0_loss: 0.5126 - val_out_1_loss: 0.8513 - val_out_2_loss: 0.7934 - val_out_3_loss: 0.7764 - val_out_4_loss: 0.7459 - val_out_acc: 0.8698 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7050 - val_out_2_acc: 0.7267 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7267\n",
      "Epoch 204/1000\n",
      " - 52s - loss: 3.5650 - out_loss: 0.3660 - out_0_loss: 0.3817 - out_1_loss: 0.7005 - out_2_loss: 0.6873 - out_3_loss: 0.7175 - out_4_loss: 0.7120 - out_acc: 0.8765 - out_0_acc: 0.8605 - out_1_acc: 0.7484 - out_2_acc: 0.7493 - out_3_acc: 0.7334 - out_4_acc: 0.7369 - val_loss: 4.0428 - val_out_loss: 0.4258 - val_out_0_loss: 0.5127 - val_out_1_loss: 0.8347 - val_out_2_loss: 0.7489 - val_out_3_loss: 0.7426 - val_out_4_loss: 0.7781 - val_out_acc: 0.8503 - val_out_0_acc: 0.8200 - val_out_1_acc: 0.7028 - val_out_2_acc: 0.7267 - val_out_3_acc: 0.7332 - val_out_4_acc: 0.7158\n",
      "Epoch 205/1000\n",
      " - 51s - loss: 3.6187 - out_loss: 0.3840 - out_0_loss: 0.3969 - out_1_loss: 0.7217 - out_2_loss: 0.7115 - out_3_loss: 0.6824 - out_4_loss: 0.7221 - out_acc: 0.8769 - out_0_acc: 0.8593 - out_1_acc: 0.7385 - out_2_acc: 0.7392 - out_3_acc: 0.7498 - out_4_acc: 0.7295 - val_loss: 3.7568 - val_out_loss: 0.4084 - val_out_0_loss: 0.4714 - val_out_1_loss: 0.7544 - val_out_2_loss: 0.6918 - val_out_3_loss: 0.6999 - val_out_4_loss: 0.7308 - val_out_acc: 0.8503 - val_out_0_acc: 0.8200 - val_out_1_acc: 0.7093 - val_out_2_acc: 0.7527 - val_out_3_acc: 0.7484 - val_out_4_acc: 0.7462\n",
      "Epoch 206/1000\n",
      " - 52s - loss: 3.4888 - out_loss: 0.3505 - out_0_loss: 0.3523 - out_1_loss: 0.7106 - out_2_loss: 0.6769 - out_3_loss: 0.7008 - out_4_loss: 0.6978 - out_acc: 0.8914 - out_0_acc: 0.8715 - out_1_acc: 0.7421 - out_2_acc: 0.7595 - out_3_acc: 0.7404 - out_4_acc: 0.7516 - val_loss: 4.1855 - val_out_loss: 0.4445 - val_out_0_loss: 0.5270 - val_out_1_loss: 0.8136 - val_out_2_loss: 0.8514 - val_out_3_loss: 0.7093 - val_out_4_loss: 0.8397 - val_out_acc: 0.8503 - val_out_0_acc: 0.8113 - val_out_1_acc: 0.7158 - val_out_2_acc: 0.6898 - val_out_3_acc: 0.7223 - val_out_4_acc: 0.6941\n",
      "Epoch 207/1000\n",
      " - 52s - loss: 3.5407 - out_loss: 0.3476 - out_0_loss: 0.3837 - out_1_loss: 0.7178 - out_2_loss: 0.6853 - out_3_loss: 0.6946 - out_4_loss: 0.7117 - out_acc: 0.8878 - out_0_acc: 0.8613 - out_1_acc: 0.7303 - out_2_acc: 0.7412 - out_3_acc: 0.7400 - out_4_acc: 0.7357 - val_loss: 3.9684 - val_out_loss: 0.4685 - val_out_0_loss: 0.5656 - val_out_1_loss: 0.7695 - val_out_2_loss: 0.7079 - val_out_3_loss: 0.7046 - val_out_4_loss: 0.7524 - val_out_acc: 0.8416 - val_out_0_acc: 0.8091 - val_out_1_acc: 0.7202 - val_out_2_acc: 0.7440 - val_out_3_acc: 0.7462 - val_out_4_acc: 0.7158\n",
      "Epoch 208/1000\n",
      " - 52s - loss: 3.5361 - out_loss: 0.3709 - out_0_loss: 0.3820 - out_1_loss: 0.6968 - out_2_loss: 0.6969 - out_3_loss: 0.7006 - out_4_loss: 0.6888 - out_acc: 0.8866 - out_0_acc: 0.8558 - out_1_acc: 0.7491 - out_2_acc: 0.7415 - out_3_acc: 0.7473 - out_4_acc: 0.7442 - val_loss: 4.0003 - val_out_loss: 0.4551 - val_out_0_loss: 0.4995 - val_out_1_loss: 0.8262 - val_out_2_loss: 0.7612 - val_out_3_loss: 0.7372 - val_out_4_loss: 0.7213 - val_out_acc: 0.8330 - val_out_0_acc: 0.8134 - val_out_1_acc: 0.7007 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.7267 - val_out_4_acc: 0.7440\n",
      "Epoch 209/1000\n",
      " - 51s - loss: 3.5577 - out_loss: 0.3669 - out_0_loss: 0.3905 - out_1_loss: 0.6912 - out_2_loss: 0.6846 - out_3_loss: 0.7008 - out_4_loss: 0.7236 - out_acc: 0.8831 - out_0_acc: 0.8577 - out_1_acc: 0.7420 - out_2_acc: 0.7515 - out_3_acc: 0.7441 - out_4_acc: 0.7336 - val_loss: 4.0161 - val_out_loss: 0.4172 - val_out_0_loss: 0.4909 - val_out_1_loss: 0.7422 - val_out_2_loss: 0.8497 - val_out_3_loss: 0.7271 - val_out_4_loss: 0.7891 - val_out_acc: 0.8460 - val_out_0_acc: 0.8091 - val_out_1_acc: 0.7137 - val_out_2_acc: 0.6833 - val_out_3_acc: 0.7527 - val_out_4_acc: 0.7137\n",
      "Epoch 210/1000\n",
      " - 50s - loss: 3.5040 - out_loss: 0.3480 - out_0_loss: 0.3537 - out_1_loss: 0.6971 - out_2_loss: 0.6964 - out_3_loss: 0.6987 - out_4_loss: 0.7102 - out_acc: 0.8891 - out_0_acc: 0.8741 - out_1_acc: 0.7358 - out_2_acc: 0.7403 - out_3_acc: 0.7513 - out_4_acc: 0.7408 - val_loss: 3.5369 - val_out_loss: 0.3461 - val_out_0_loss: 0.4218 - val_out_1_loss: 0.7075 - val_out_2_loss: 0.7373 - val_out_3_loss: 0.6388 - val_out_4_loss: 0.6854 - val_out_acc: 0.8829 - val_out_0_acc: 0.8416 - val_out_1_acc: 0.7375 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7679 - val_out_4_acc: 0.7419\n",
      "Epoch 211/1000\n",
      " - 51s - loss: 3.4534 - out_loss: 0.3624 - out_0_loss: 0.3796 - out_1_loss: 0.6710 - out_2_loss: 0.6541 - out_3_loss: 0.6724 - out_4_loss: 0.7139 - out_acc: 0.8859 - out_0_acc: 0.8652 - out_1_acc: 0.7588 - out_2_acc: 0.7557 - out_3_acc: 0.7588 - out_4_acc: 0.7351 - val_loss: 3.8326 - val_out_loss: 0.3873 - val_out_0_loss: 0.4971 - val_out_1_loss: 0.7367 - val_out_2_loss: 0.7193 - val_out_3_loss: 0.7150 - val_out_4_loss: 0.7772 - val_out_acc: 0.8633 - val_out_0_acc: 0.8265 - val_out_1_acc: 0.7050 - val_out_2_acc: 0.7223 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.7202\n",
      "Epoch 212/1000\n",
      " - 51s - loss: 3.4936 - out_loss: 0.3536 - out_0_loss: 0.3711 - out_1_loss: 0.7038 - out_2_loss: 0.6684 - out_3_loss: 0.6995 - out_4_loss: 0.6972 - out_acc: 0.8876 - out_0_acc: 0.8613 - out_1_acc: 0.7381 - out_2_acc: 0.7534 - out_3_acc: 0.7458 - out_4_acc: 0.7436 - val_loss: 3.8401 - val_out_loss: 0.3547 - val_out_0_loss: 0.4845 - val_out_1_loss: 0.7568 - val_out_2_loss: 0.7545 - val_out_3_loss: 0.7039 - val_out_4_loss: 0.7858 - val_out_acc: 0.8785 - val_out_0_acc: 0.8221 - val_out_1_acc: 0.7158 - val_out_2_acc: 0.7028 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7050\n",
      "Epoch 213/1000\n",
      " - 51s - loss: 3.5057 - out_loss: 0.3707 - out_0_loss: 0.3749 - out_1_loss: 0.6928 - out_2_loss: 0.6957 - out_3_loss: 0.6674 - out_4_loss: 0.7040 - out_acc: 0.8875 - out_0_acc: 0.8658 - out_1_acc: 0.7432 - out_2_acc: 0.7501 - out_3_acc: 0.7562 - out_4_acc: 0.7480 - val_loss: 3.9459 - val_out_loss: 0.4467 - val_out_0_loss: 0.5342 - val_out_1_loss: 0.7604 - val_out_2_loss: 0.7677 - val_out_3_loss: 0.7266 - val_out_4_loss: 0.7103 - val_out_acc: 0.8460 - val_out_0_acc: 0.7874 - val_out_1_acc: 0.7137 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.7310 - val_out_4_acc: 0.7419\n",
      "Epoch 214/1000\n",
      " - 51s - loss: 3.4911 - out_loss: 0.3596 - out_0_loss: 0.3702 - out_1_loss: 0.7186 - out_2_loss: 0.6671 - out_3_loss: 0.6770 - out_4_loss: 0.6987 - out_acc: 0.8898 - out_0_acc: 0.8630 - out_1_acc: 0.7258 - out_2_acc: 0.7689 - out_3_acc: 0.7555 - out_4_acc: 0.7438 - val_loss: 3.8695 - val_out_loss: 0.4132 - val_out_0_loss: 0.4804 - val_out_1_loss: 0.7548 - val_out_2_loss: 0.7043 - val_out_3_loss: 0.7804 - val_out_4_loss: 0.7364 - val_out_acc: 0.8547 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7137 - val_out_2_acc: 0.7310 - val_out_3_acc: 0.7223 - val_out_4_acc: 0.7245\n",
      "Epoch 215/1000\n",
      " - 51s - loss: 3.4753 - out_loss: 0.3536 - out_0_loss: 0.3603 - out_1_loss: 0.6779 - out_2_loss: 0.6885 - out_3_loss: 0.7004 - out_4_loss: 0.6947 - out_acc: 0.8927 - out_0_acc: 0.8636 - out_1_acc: 0.7398 - out_2_acc: 0.7501 - out_3_acc: 0.7406 - out_4_acc: 0.7512 - val_loss: 3.6069 - val_out_loss: 0.3484 - val_out_0_loss: 0.4097 - val_out_1_loss: 0.7123 - val_out_2_loss: 0.7162 - val_out_3_loss: 0.7273 - val_out_4_loss: 0.6930 - val_out_acc: 0.8785 - val_out_0_acc: 0.8286 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7332 - val_out_3_acc: 0.7354 - val_out_4_acc: 0.7505\n",
      "Epoch 216/1000\n",
      " - 51s - loss: 3.4286 - out_loss: 0.3409 - out_0_loss: 0.3518 - out_1_loss: 0.6772 - out_2_loss: 0.6812 - out_3_loss: 0.6744 - out_4_loss: 0.7030 - out_acc: 0.8925 - out_0_acc: 0.8714 - out_1_acc: 0.7542 - out_2_acc: 0.7447 - out_3_acc: 0.7471 - out_4_acc: 0.7523 - val_loss: 3.9717 - val_out_loss: 0.4074 - val_out_0_loss: 0.4816 - val_out_1_loss: 0.7211 - val_out_2_loss: 0.8268 - val_out_3_loss: 0.7624 - val_out_4_loss: 0.7724 - val_out_acc: 0.8677 - val_out_0_acc: 0.8351 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.6941 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7115\n",
      "Epoch 217/1000\n",
      " - 50s - loss: 3.4976 - out_loss: 0.3653 - out_0_loss: 0.3872 - out_1_loss: 0.6734 - out_2_loss: 0.6938 - out_3_loss: 0.6821 - out_4_loss: 0.6957 - out_acc: 0.8825 - out_0_acc: 0.8580 - out_1_acc: 0.7616 - out_2_acc: 0.7442 - out_3_acc: 0.7541 - out_4_acc: 0.7411 - val_loss: 3.6866 - val_out_loss: 0.3626 - val_out_0_loss: 0.4371 - val_out_1_loss: 0.7592 - val_out_2_loss: 0.6926 - val_out_3_loss: 0.6871 - val_out_4_loss: 0.7480 - val_out_acc: 0.8633 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7289 - val_out_3_acc: 0.7419 - val_out_4_acc: 0.7419\n",
      "Epoch 218/1000\n",
      " - 51s - loss: 3.3740 - out_loss: 0.3184 - out_0_loss: 0.3427 - out_1_loss: 0.7094 - out_2_loss: 0.6698 - out_3_loss: 0.6514 - out_4_loss: 0.6821 - out_acc: 0.8998 - out_0_acc: 0.8752 - out_1_acc: 0.7379 - out_2_acc: 0.7510 - out_3_acc: 0.7743 - out_4_acc: 0.7576 - val_loss: 4.0754 - val_out_loss: 0.4890 - val_out_0_loss: 0.5189 - val_out_1_loss: 0.7482 - val_out_2_loss: 0.7419 - val_out_3_loss: 0.7492 - val_out_4_loss: 0.8283 - val_out_acc: 0.8330 - val_out_0_acc: 0.8069 - val_out_1_acc: 0.6985 - val_out_2_acc: 0.7007 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7028\n",
      "Epoch 219/1000\n",
      " - 52s - loss: 3.4082 - out_loss: 0.3390 - out_0_loss: 0.3487 - out_1_loss: 0.6798 - out_2_loss: 0.6672 - out_3_loss: 0.6878 - out_4_loss: 0.6856 - out_acc: 0.8920 - out_0_acc: 0.8732 - out_1_acc: 0.7473 - out_2_acc: 0.7614 - out_3_acc: 0.7456 - out_4_acc: 0.7505 - val_loss: 3.9570 - val_out_loss: 0.4236 - val_out_0_loss: 0.5058 - val_out_1_loss: 0.7457 - val_out_2_loss: 0.7696 - val_out_3_loss: 0.7463 - val_out_4_loss: 0.7660 - val_out_acc: 0.8612 - val_out_0_acc: 0.8200 - val_out_1_acc: 0.7332 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.7223 - val_out_4_acc: 0.7202\n",
      "Epoch 220/1000\n",
      " - 52s - loss: 3.4645 - out_loss: 0.3593 - out_0_loss: 0.3656 - out_1_loss: 0.6865 - out_2_loss: 0.6852 - out_3_loss: 0.6765 - out_4_loss: 0.6914 - out_acc: 0.8893 - out_0_acc: 0.8666 - out_1_acc: 0.7472 - out_2_acc: 0.7501 - out_3_acc: 0.7520 - out_4_acc: 0.7463 - val_loss: 3.7486 - val_out_loss: 0.4019 - val_out_0_loss: 0.4191 - val_out_1_loss: 0.7050 - val_out_2_loss: 0.7035 - val_out_3_loss: 0.7370 - val_out_4_loss: 0.7822 - val_out_acc: 0.8590 - val_out_0_acc: 0.8482 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7375 - val_out_3_acc: 0.7223 - val_out_4_acc: 0.7093\n",
      "Epoch 221/1000\n",
      " - 52s - loss: 3.2992 - out_loss: 0.3210 - out_0_loss: 0.3452 - out_1_loss: 0.6389 - out_2_loss: 0.6733 - out_3_loss: 0.6505 - out_4_loss: 0.6704 - out_acc: 0.8994 - out_0_acc: 0.8751 - out_1_acc: 0.7618 - out_2_acc: 0.7564 - out_3_acc: 0.7671 - out_4_acc: 0.7586 - val_loss: 4.1238 - val_out_loss: 0.4517 - val_out_0_loss: 0.6061 - val_out_1_loss: 0.6906 - val_out_2_loss: 0.7688 - val_out_3_loss: 0.7701 - val_out_4_loss: 0.8366 - val_out_acc: 0.8568 - val_out_0_acc: 0.8113 - val_out_1_acc: 0.7484 - val_out_2_acc: 0.6898 - val_out_3_acc: 0.7115 - val_out_4_acc: 0.6985\n",
      "Epoch 222/1000\n",
      " - 51s - loss: 3.3494 - out_loss: 0.3366 - out_0_loss: 0.3574 - out_1_loss: 0.6578 - out_2_loss: 0.6678 - out_3_loss: 0.6486 - out_4_loss: 0.6812 - out_acc: 0.8920 - out_0_acc: 0.8694 - out_1_acc: 0.7608 - out_2_acc: 0.7535 - out_3_acc: 0.7588 - out_4_acc: 0.7545 - val_loss: 3.8912 - val_out_loss: 0.3820 - val_out_0_loss: 0.4210 - val_out_1_loss: 0.7911 - val_out_2_loss: 0.7984 - val_out_3_loss: 0.7591 - val_out_4_loss: 0.7396 - val_out_acc: 0.8764 - val_out_0_acc: 0.8460 - val_out_1_acc: 0.7093 - val_out_2_acc: 0.6920 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7180\n",
      "Epoch 223/1000\n",
      " - 50s - loss: 3.3005 - out_loss: 0.3180 - out_0_loss: 0.3082 - out_1_loss: 0.6708 - out_2_loss: 0.6828 - out_3_loss: 0.6529 - out_4_loss: 0.6678 - out_acc: 0.9047 - out_0_acc: 0.8899 - out_1_acc: 0.7500 - out_2_acc: 0.7499 - out_3_acc: 0.7562 - out_4_acc: 0.7512 - val_loss: 4.0130 - val_out_loss: 0.4379 - val_out_0_loss: 0.5261 - val_out_1_loss: 0.7371 - val_out_2_loss: 0.7498 - val_out_3_loss: 0.7979 - val_out_4_loss: 0.7642 - val_out_acc: 0.8525 - val_out_0_acc: 0.8156 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7289 - val_out_3_acc: 0.6920 - val_out_4_acc: 0.7072\n",
      "Epoch 224/1000\n",
      " - 51s - loss: 3.2859 - out_loss: 0.3154 - out_0_loss: 0.3215 - out_1_loss: 0.6627 - out_2_loss: 0.6470 - out_3_loss: 0.6761 - out_4_loss: 0.6632 - out_acc: 0.9008 - out_0_acc: 0.8812 - out_1_acc: 0.7558 - out_2_acc: 0.7713 - out_3_acc: 0.7491 - out_4_acc: 0.7577 - val_loss: 3.7004 - val_out_loss: 0.3602 - val_out_0_loss: 0.3858 - val_out_1_loss: 0.7380 - val_out_2_loss: 0.7688 - val_out_3_loss: 0.7352 - val_out_4_loss: 0.7124 - val_out_acc: 0.8894 - val_out_0_acc: 0.8655 - val_out_1_acc: 0.7223 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.7007 - val_out_4_acc: 0.7028\n",
      "Epoch 225/1000\n",
      " - 52s - loss: 3.3585 - out_loss: 0.3316 - out_0_loss: 0.3455 - out_1_loss: 0.6806 - out_2_loss: 0.6822 - out_3_loss: 0.6682 - out_4_loss: 0.6504 - out_acc: 0.8941 - out_0_acc: 0.8772 - out_1_acc: 0.7563 - out_2_acc: 0.7480 - out_3_acc: 0.7613 - out_4_acc: 0.7601 - val_loss: 3.7260 - val_out_loss: 0.3623 - val_out_0_loss: 0.3957 - val_out_1_loss: 0.7710 - val_out_2_loss: 0.6864 - val_out_3_loss: 0.7059 - val_out_4_loss: 0.8047 - val_out_acc: 0.8612 - val_out_0_acc: 0.8460 - val_out_1_acc: 0.7223 - val_out_2_acc: 0.7397 - val_out_3_acc: 0.7766 - val_out_4_acc: 0.7180\n",
      "Epoch 226/1000\n",
      " - 51s - loss: 3.2269 - out_loss: 0.3131 - out_0_loss: 0.3314 - out_1_loss: 0.6463 - out_2_loss: 0.6121 - out_3_loss: 0.6515 - out_4_loss: 0.6725 - out_acc: 0.9014 - out_0_acc: 0.8822 - out_1_acc: 0.7687 - out_2_acc: 0.7791 - out_3_acc: 0.7669 - out_4_acc: 0.7524 - val_loss: 3.3603 - val_out_loss: 0.3223 - val_out_0_loss: 0.4011 - val_out_1_loss: 0.6224 - val_out_2_loss: 0.6198 - val_out_3_loss: 0.7204 - val_out_4_loss: 0.6743 - val_out_acc: 0.8894 - val_out_0_acc: 0.8525 - val_out_1_acc: 0.7766 - val_out_2_acc: 0.7679 - val_out_3_acc: 0.7397 - val_out_4_acc: 0.7440\n",
      "Epoch 227/1000\n",
      " - 51s - loss: 3.3400 - out_loss: 0.3255 - out_0_loss: 0.3398 - out_1_loss: 0.6781 - out_2_loss: 0.6823 - out_3_loss: 0.6536 - out_4_loss: 0.6607 - out_acc: 0.8960 - out_0_acc: 0.8753 - out_1_acc: 0.7513 - out_2_acc: 0.7442 - out_3_acc: 0.7641 - out_4_acc: 0.7608 - val_loss: 3.4943 - val_out_loss: 0.3368 - val_out_0_loss: 0.4641 - val_out_1_loss: 0.7057 - val_out_2_loss: 0.6732 - val_out_3_loss: 0.6752 - val_out_4_loss: 0.6392 - val_out_acc: 0.8785 - val_out_0_acc: 0.8633 - val_out_1_acc: 0.7397 - val_out_2_acc: 0.7592 - val_out_3_acc: 0.7310 - val_out_4_acc: 0.7636\n",
      "Epoch 228/1000\n",
      " - 52s - loss: 3.2871 - out_loss: 0.3117 - out_0_loss: 0.3401 - out_1_loss: 0.6501 - out_2_loss: 0.6568 - out_3_loss: 0.6627 - out_4_loss: 0.6657 - out_acc: 0.8974 - out_0_acc: 0.8784 - out_1_acc: 0.7578 - out_2_acc: 0.7582 - out_3_acc: 0.7660 - out_4_acc: 0.7576 - val_loss: 3.8079 - val_out_loss: 0.3774 - val_out_0_loss: 0.4682 - val_out_1_loss: 0.6672 - val_out_2_loss: 0.8164 - val_out_3_loss: 0.7096 - val_out_4_loss: 0.7691 - val_out_acc: 0.8915 - val_out_0_acc: 0.8482 - val_out_1_acc: 0.7636 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.7462 - val_out_4_acc: 0.7028\n",
      "Epoch 229/1000\n",
      " - 52s - loss: 3.3596 - out_loss: 0.3367 - out_0_loss: 0.3754 - out_1_loss: 0.6646 - out_2_loss: 0.6632 - out_3_loss: 0.6530 - out_4_loss: 0.6667 - out_acc: 0.8890 - out_0_acc: 0.8680 - out_1_acc: 0.7535 - out_2_acc: 0.7632 - out_3_acc: 0.7611 - out_4_acc: 0.7658 - val_loss: 3.5014 - val_out_loss: 0.3654 - val_out_0_loss: 0.4269 - val_out_1_loss: 0.6530 - val_out_2_loss: 0.6997 - val_out_3_loss: 0.6625 - val_out_4_loss: 0.6939 - val_out_acc: 0.8698 - val_out_0_acc: 0.8200 - val_out_1_acc: 0.7636 - val_out_2_acc: 0.7310 - val_out_3_acc: 0.7354 - val_out_4_acc: 0.7332\n",
      "Epoch 230/1000\n",
      " - 53s - loss: 3.3315 - out_loss: 0.3220 - out_0_loss: 0.3430 - out_1_loss: 0.6526 - out_2_loss: 0.6764 - out_3_loss: 0.6448 - out_4_loss: 0.6927 - out_acc: 0.8972 - out_0_acc: 0.8779 - out_1_acc: 0.7689 - out_2_acc: 0.7618 - out_3_acc: 0.7680 - out_4_acc: 0.7461 - val_loss: 3.8798 - val_out_loss: 0.3974 - val_out_0_loss: 0.4401 - val_out_1_loss: 0.6400 - val_out_2_loss: 0.7867 - val_out_3_loss: 0.7994 - val_out_4_loss: 0.8161 - val_out_acc: 0.8677 - val_out_0_acc: 0.8308 - val_out_1_acc: 0.7570 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.6941 - val_out_4_acc: 0.7007\n",
      "Epoch 231/1000\n",
      " - 53s - loss: 3.4270 - out_loss: 0.3346 - out_0_loss: 0.3687 - out_1_loss: 0.6796 - out_2_loss: 0.6724 - out_3_loss: 0.6983 - out_4_loss: 0.6734 - out_acc: 0.8928 - out_0_acc: 0.8684 - out_1_acc: 0.7576 - out_2_acc: 0.7546 - out_3_acc: 0.7522 - out_4_acc: 0.7459 - val_loss: 3.5506 - val_out_loss: 0.3481 - val_out_0_loss: 0.4141 - val_out_1_loss: 0.7425 - val_out_2_loss: 0.6909 - val_out_3_loss: 0.6736 - val_out_4_loss: 0.6814 - val_out_acc: 0.8655 - val_out_0_acc: 0.8438 - val_out_1_acc: 0.7072 - val_out_2_acc: 0.7570 - val_out_3_acc: 0.7332 - val_out_4_acc: 0.7809\n",
      "Epoch 232/1000\n",
      " - 53s - loss: 3.2670 - out_loss: 0.2991 - out_0_loss: 0.3126 - out_1_loss: 0.6659 - out_2_loss: 0.6676 - out_3_loss: 0.6547 - out_4_loss: 0.6671 - out_acc: 0.9032 - out_0_acc: 0.8836 - out_1_acc: 0.7556 - out_2_acc: 0.7463 - out_3_acc: 0.7629 - out_4_acc: 0.7474 - val_loss: 3.5891 - val_out_loss: 0.3507 - val_out_0_loss: 0.4228 - val_out_1_loss: 0.7539 - val_out_2_loss: 0.6758 - val_out_3_loss: 0.7041 - val_out_4_loss: 0.6818 - val_out_acc: 0.8764 - val_out_0_acc: 0.8503 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7289 - val_out_3_acc: 0.7354 - val_out_4_acc: 0.7440\n",
      "Epoch 233/1000\n",
      " - 54s - loss: 3.4197 - out_loss: 0.3396 - out_0_loss: 0.3509 - out_1_loss: 0.6899 - out_2_loss: 0.6698 - out_3_loss: 0.6789 - out_4_loss: 0.6906 - out_acc: 0.8937 - out_0_acc: 0.8751 - out_1_acc: 0.7492 - out_2_acc: 0.7600 - out_3_acc: 0.7517 - out_4_acc: 0.7522 - val_loss: 3.5697 - val_out_loss: 0.3390 - val_out_0_loss: 0.3879 - val_out_1_loss: 0.6691 - val_out_2_loss: 0.7464 - val_out_3_loss: 0.6694 - val_out_4_loss: 0.7577 - val_out_acc: 0.8807 - val_out_0_acc: 0.8720 - val_out_1_acc: 0.7310 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7419 - val_out_4_acc: 0.7440\n",
      "Epoch 234/1000\n",
      " - 53s - loss: 3.3563 - out_loss: 0.3446 - out_0_loss: 0.3332 - out_1_loss: 0.6859 - out_2_loss: 0.6712 - out_3_loss: 0.6488 - out_4_loss: 0.6726 - out_acc: 0.8856 - out_0_acc: 0.8701 - out_1_acc: 0.7459 - out_2_acc: 0.7518 - out_3_acc: 0.7565 - out_4_acc: 0.7597 - val_loss: 4.0066 - val_out_loss: 0.4523 - val_out_0_loss: 0.4828 - val_out_1_loss: 0.7780 - val_out_2_loss: 0.7694 - val_out_3_loss: 0.7374 - val_out_4_loss: 0.7867 - val_out_acc: 0.8482 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7050 - val_out_2_acc: 0.7245 - val_out_3_acc: 0.7267 - val_out_4_acc: 0.6941\n",
      "Epoch 235/1000\n",
      " - 52s - loss: 3.2544 - out_loss: 0.3118 - out_0_loss: 0.3209 - out_1_loss: 0.6442 - out_2_loss: 0.6521 - out_3_loss: 0.6475 - out_4_loss: 0.6780 - out_acc: 0.9022 - out_0_acc: 0.8848 - out_1_acc: 0.7712 - out_2_acc: 0.7659 - out_3_acc: 0.7669 - out_4_acc: 0.7533 - val_loss: 3.4823 - val_out_loss: 0.3480 - val_out_0_loss: 0.4122 - val_out_1_loss: 0.6966 - val_out_2_loss: 0.6938 - val_out_3_loss: 0.6534 - val_out_4_loss: 0.6782 - val_out_acc: 0.8829 - val_out_0_acc: 0.8525 - val_out_1_acc: 0.7462 - val_out_2_acc: 0.7223 - val_out_3_acc: 0.7570 - val_out_4_acc: 0.7462\n",
      "Epoch 236/1000\n",
      " - 52s - loss: 3.2891 - out_loss: 0.3138 - out_0_loss: 0.3415 - out_1_loss: 0.6372 - out_2_loss: 0.6508 - out_3_loss: 0.6605 - out_4_loss: 0.6853 - out_acc: 0.9043 - out_0_acc: 0.8832 - out_1_acc: 0.7697 - out_2_acc: 0.7675 - out_3_acc: 0.7621 - out_4_acc: 0.7537 - val_loss: 3.7236 - val_out_loss: 0.3934 - val_out_0_loss: 0.4712 - val_out_1_loss: 0.7335 - val_out_2_loss: 0.7110 - val_out_3_loss: 0.7067 - val_out_4_loss: 0.7078 - val_out_acc: 0.8698 - val_out_0_acc: 0.8200 - val_out_1_acc: 0.7072 - val_out_2_acc: 0.7310 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.7549\n",
      "Epoch 237/1000\n",
      " - 53s - loss: 3.2739 - out_loss: 0.3130 - out_0_loss: 0.3327 - out_1_loss: 0.6621 - out_2_loss: 0.6670 - out_3_loss: 0.6422 - out_4_loss: 0.6568 - out_acc: 0.9019 - out_0_acc: 0.8842 - out_1_acc: 0.7608 - out_2_acc: 0.7533 - out_3_acc: 0.7627 - out_4_acc: 0.7570 - val_loss: 3.7588 - val_out_loss: 0.4525 - val_out_0_loss: 0.5417 - val_out_1_loss: 0.7047 - val_out_2_loss: 0.6660 - val_out_3_loss: 0.6978 - val_out_4_loss: 0.6961 - val_out_acc: 0.8416 - val_out_0_acc: 0.8048 - val_out_1_acc: 0.7375 - val_out_2_acc: 0.7570 - val_out_3_acc: 0.7397 - val_out_4_acc: 0.7397\n",
      "Epoch 238/1000\n",
      " - 52s - loss: 3.2843 - out_loss: 0.3208 - out_0_loss: 0.3471 - out_1_loss: 0.6440 - out_2_loss: 0.6486 - out_3_loss: 0.6618 - out_4_loss: 0.6620 - out_acc: 0.8949 - out_0_acc: 0.8716 - out_1_acc: 0.7671 - out_2_acc: 0.7654 - out_3_acc: 0.7463 - out_4_acc: 0.7648 - val_loss: 3.7435 - val_out_loss: 0.3876 - val_out_0_loss: 0.4418 - val_out_1_loss: 0.6804 - val_out_2_loss: 0.7584 - val_out_3_loss: 0.7542 - val_out_4_loss: 0.7209 - val_out_acc: 0.8764 - val_out_0_acc: 0.8416 - val_out_1_acc: 0.7375 - val_out_2_acc: 0.7028 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7137\n",
      "Epoch 239/1000\n",
      " - 52s - loss: 3.2881 - out_loss: 0.3271 - out_0_loss: 0.3504 - out_1_loss: 0.6457 - out_2_loss: 0.6388 - out_3_loss: 0.6556 - out_4_loss: 0.6705 - out_acc: 0.8939 - out_0_acc: 0.8735 - out_1_acc: 0.7611 - out_2_acc: 0.7585 - out_3_acc: 0.7473 - out_4_acc: 0.7510 - val_loss: 3.6177 - val_out_loss: 0.3664 - val_out_0_loss: 0.4389 - val_out_1_loss: 0.7221 - val_out_2_loss: 0.6786 - val_out_3_loss: 0.7086 - val_out_4_loss: 0.7031 - val_out_acc: 0.8764 - val_out_0_acc: 0.8221 - val_out_1_acc: 0.7375 - val_out_2_acc: 0.7636 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.7332\n",
      "Epoch 240/1000\n",
      " - 52s - loss: 3.2074 - out_loss: 0.2950 - out_0_loss: 0.3096 - out_1_loss: 0.6510 - out_2_loss: 0.6701 - out_3_loss: 0.6241 - out_4_loss: 0.6576 - out_acc: 0.9083 - out_0_acc: 0.8902 - out_1_acc: 0.7678 - out_2_acc: 0.7495 - out_3_acc: 0.7738 - out_4_acc: 0.7523 - val_loss: 3.6334 - val_out_loss: 0.3599 - val_out_0_loss: 0.4414 - val_out_1_loss: 0.7530 - val_out_2_loss: 0.6617 - val_out_3_loss: 0.7559 - val_out_4_loss: 0.6616 - val_out_acc: 0.8915 - val_out_0_acc: 0.8482 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7375 - val_out_3_acc: 0.7050 - val_out_4_acc: 0.7527\n",
      "Epoch 241/1000\n",
      " - 52s - loss: 3.3625 - out_loss: 0.3322 - out_0_loss: 0.3574 - out_1_loss: 0.6374 - out_2_loss: 0.6749 - out_3_loss: 0.6686 - out_4_loss: 0.6920 - out_acc: 0.8979 - out_0_acc: 0.8773 - out_1_acc: 0.7716 - out_2_acc: 0.7452 - out_3_acc: 0.7485 - out_4_acc: 0.7488 - val_loss: 3.5834 - val_out_loss: 0.3926 - val_out_0_loss: 0.4782 - val_out_1_loss: 0.6731 - val_out_2_loss: 0.7143 - val_out_3_loss: 0.6605 - val_out_4_loss: 0.6647 - val_out_acc: 0.8655 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7462 - val_out_2_acc: 0.7223 - val_out_3_acc: 0.7614 - val_out_4_acc: 0.7289\n",
      "Epoch 242/1000\n",
      " - 52s - loss: 3.2403 - out_loss: 0.3138 - out_0_loss: 0.3273 - out_1_loss: 0.6384 - out_2_loss: 0.6723 - out_3_loss: 0.6296 - out_4_loss: 0.6590 - out_acc: 0.8949 - out_0_acc: 0.8686 - out_1_acc: 0.7623 - out_2_acc: 0.7566 - out_3_acc: 0.7646 - out_4_acc: 0.7560 - val_loss: 3.7231 - val_out_loss: 0.4058 - val_out_0_loss: 0.5174 - val_out_1_loss: 0.7344 - val_out_2_loss: 0.7495 - val_out_3_loss: 0.6562 - val_out_4_loss: 0.6597 - val_out_acc: 0.8482 - val_out_0_acc: 0.8113 - val_out_1_acc: 0.7397 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.7505 - val_out_4_acc: 0.7462\n",
      "Epoch 243/1000\n",
      " - 52s - loss: 3.3741 - out_loss: 0.3493 - out_0_loss: 0.3660 - out_1_loss: 0.6763 - out_2_loss: 0.6547 - out_3_loss: 0.6428 - out_4_loss: 0.6849 - out_acc: 0.8888 - out_0_acc: 0.8652 - out_1_acc: 0.7563 - out_2_acc: 0.7560 - out_3_acc: 0.7561 - out_4_acc: 0.7415 - val_loss: 3.7148 - val_out_loss: 0.3932 - val_out_0_loss: 0.4343 - val_out_1_loss: 0.7757 - val_out_2_loss: 0.7231 - val_out_3_loss: 0.6727 - val_out_4_loss: 0.7157 - val_out_acc: 0.8764 - val_out_0_acc: 0.8503 - val_out_1_acc: 0.7354 - val_out_2_acc: 0.7180 - val_out_3_acc: 0.7744 - val_out_4_acc: 0.7397\n",
      "Epoch 244/1000\n",
      " - 51s - loss: 3.1217 - out_loss: 0.2890 - out_0_loss: 0.3208 - out_1_loss: 0.6349 - out_2_loss: 0.6012 - out_3_loss: 0.6241 - out_4_loss: 0.6516 - out_acc: 0.9030 - out_0_acc: 0.8885 - out_1_acc: 0.7677 - out_2_acc: 0.7794 - out_3_acc: 0.7671 - out_4_acc: 0.7674 - val_loss: 3.7264 - val_out_loss: 0.4048 - val_out_0_loss: 0.5287 - val_out_1_loss: 0.6318 - val_out_2_loss: 0.6662 - val_out_3_loss: 0.6813 - val_out_4_loss: 0.8136 - val_out_acc: 0.8438 - val_out_0_acc: 0.8048 - val_out_1_acc: 0.7527 - val_out_2_acc: 0.7484 - val_out_3_acc: 0.7527 - val_out_4_acc: 0.6920\n",
      "Epoch 245/1000\n",
      " - 51s - loss: 3.1795 - out_loss: 0.2996 - out_0_loss: 0.3415 - out_1_loss: 0.6415 - out_2_loss: 0.6285 - out_3_loss: 0.6138 - out_4_loss: 0.6547 - out_acc: 0.9074 - out_0_acc: 0.8833 - out_1_acc: 0.7582 - out_2_acc: 0.7660 - out_3_acc: 0.7819 - out_4_acc: 0.7569 - val_loss: 3.6413 - val_out_loss: 0.3840 - val_out_0_loss: 0.4421 - val_out_1_loss: 0.7473 - val_out_2_loss: 0.7394 - val_out_3_loss: 0.6703 - val_out_4_loss: 0.6582 - val_out_acc: 0.8742 - val_out_0_acc: 0.8503 - val_out_1_acc: 0.7310 - val_out_2_acc: 0.7397 - val_out_3_acc: 0.7375 - val_out_4_acc: 0.7570\n",
      "Epoch 246/1000\n",
      " - 51s - loss: 3.2542 - out_loss: 0.3069 - out_0_loss: 0.3211 - out_1_loss: 0.6605 - out_2_loss: 0.6463 - out_3_loss: 0.6673 - out_4_loss: 0.6521 - out_acc: 0.9008 - out_0_acc: 0.8810 - out_1_acc: 0.7540 - out_2_acc: 0.7614 - out_3_acc: 0.7608 - out_4_acc: 0.7549 - val_loss: 3.3223 - val_out_loss: 0.3396 - val_out_0_loss: 0.4478 - val_out_1_loss: 0.6050 - val_out_2_loss: 0.6268 - val_out_3_loss: 0.6277 - val_out_4_loss: 0.6753 - val_out_acc: 0.8915 - val_out_0_acc: 0.8265 - val_out_1_acc: 0.7636 - val_out_2_acc: 0.7527 - val_out_3_acc: 0.7592 - val_out_4_acc: 0.7462\n",
      "Epoch 247/1000\n",
      " - 50s - loss: 3.2374 - out_loss: 0.3160 - out_0_loss: 0.3452 - out_1_loss: 0.6510 - out_2_loss: 0.6462 - out_3_loss: 0.6252 - out_4_loss: 0.6539 - out_acc: 0.9001 - out_0_acc: 0.8723 - out_1_acc: 0.7587 - out_2_acc: 0.7594 - out_3_acc: 0.7703 - out_4_acc: 0.7543 - val_loss: 3.7638 - val_out_loss: 0.3727 - val_out_0_loss: 0.4386 - val_out_1_loss: 0.7414 - val_out_2_loss: 0.7343 - val_out_3_loss: 0.7467 - val_out_4_loss: 0.7301 - val_out_acc: 0.8720 - val_out_0_acc: 0.8438 - val_out_1_acc: 0.7072 - val_out_2_acc: 0.7354 - val_out_3_acc: 0.7462 - val_out_4_acc: 0.7679\n",
      "Epoch 248/1000\n",
      " - 50s - loss: 3.2268 - out_loss: 0.3062 - out_0_loss: 0.3282 - out_1_loss: 0.6734 - out_2_loss: 0.6463 - out_3_loss: 0.6285 - out_4_loss: 0.6441 - out_acc: 0.9066 - out_0_acc: 0.8789 - out_1_acc: 0.7523 - out_2_acc: 0.7724 - out_3_acc: 0.7622 - out_4_acc: 0.7653 - val_loss: 3.6191 - val_out_loss: 0.3697 - val_out_0_loss: 0.4897 - val_out_1_loss: 0.6984 - val_out_2_loss: 0.7214 - val_out_3_loss: 0.6371 - val_out_4_loss: 0.7028 - val_out_acc: 0.8785 - val_out_0_acc: 0.8438 - val_out_1_acc: 0.7592 - val_out_2_acc: 0.7397 - val_out_3_acc: 0.7787 - val_out_4_acc: 0.7375\n",
      "Epoch 249/1000\n",
      " - 52s - loss: 3.2162 - out_loss: 0.3345 - out_0_loss: 0.3455 - out_1_loss: 0.6616 - out_2_loss: 0.6382 - out_3_loss: 0.6163 - out_4_loss: 0.6202 - out_acc: 0.8946 - out_0_acc: 0.8740 - out_1_acc: 0.7462 - out_2_acc: 0.7678 - out_3_acc: 0.7715 - out_4_acc: 0.7794 - val_loss: 3.7666 - val_out_loss: 0.3822 - val_out_0_loss: 0.4551 - val_out_1_loss: 0.7721 - val_out_2_loss: 0.6362 - val_out_3_loss: 0.7178 - val_out_4_loss: 0.8032 - val_out_acc: 0.8677 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7028 - val_out_2_acc: 0.7679 - val_out_3_acc: 0.7397 - val_out_4_acc: 0.7223\n",
      "Epoch 250/1000\n",
      " - 52s - loss: 3.1736 - out_loss: 0.2998 - out_0_loss: 0.3174 - out_1_loss: 0.6434 - out_2_loss: 0.6187 - out_3_loss: 0.6438 - out_4_loss: 0.6506 - out_acc: 0.9060 - out_0_acc: 0.8884 - out_1_acc: 0.7582 - out_2_acc: 0.7762 - out_3_acc: 0.7623 - out_4_acc: 0.7533 - val_loss: 3.4112 - val_out_loss: 0.3282 - val_out_0_loss: 0.3902 - val_out_1_loss: 0.6225 - val_out_2_loss: 0.6985 - val_out_3_loss: 0.6431 - val_out_4_loss: 0.7286 - val_out_acc: 0.8915 - val_out_0_acc: 0.8568 - val_out_1_acc: 0.7440 - val_out_2_acc: 0.7636 - val_out_3_acc: 0.7419 - val_out_4_acc: 0.7158\n",
      "Epoch 251/1000\n",
      " - 52s - loss: 3.1945 - out_loss: 0.2957 - out_0_loss: 0.3417 - out_1_loss: 0.6255 - out_2_loss: 0.6149 - out_3_loss: 0.6463 - out_4_loss: 0.6704 - out_acc: 0.9076 - out_0_acc: 0.8754 - out_1_acc: 0.7687 - out_2_acc: 0.7745 - out_3_acc: 0.7698 - out_4_acc: 0.7661 - val_loss: 3.5284 - val_out_loss: 0.3477 - val_out_0_loss: 0.4081 - val_out_1_loss: 0.6337 - val_out_2_loss: 0.6513 - val_out_3_loss: 0.7452 - val_out_4_loss: 0.7424 - val_out_acc: 0.8720 - val_out_0_acc: 0.8547 - val_out_1_acc: 0.7657 - val_out_2_acc: 0.7657 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.7419\n",
      "Epoch 252/1000\n",
      " - 53s - loss: 3.0999 - out_loss: 0.2924 - out_0_loss: 0.2909 - out_1_loss: 0.6370 - out_2_loss: 0.6347 - out_3_loss: 0.6133 - out_4_loss: 0.6317 - out_acc: 0.9112 - out_0_acc: 0.8940 - out_1_acc: 0.7620 - out_2_acc: 0.7649 - out_3_acc: 0.7824 - out_4_acc: 0.7753 - val_loss: 3.8505 - val_out_loss: 0.4102 - val_out_0_loss: 0.4756 - val_out_1_loss: 0.7933 - val_out_2_loss: 0.7956 - val_out_3_loss: 0.6848 - val_out_4_loss: 0.6910 - val_out_acc: 0.8525 - val_out_0_acc: 0.8351 - val_out_1_acc: 0.7050 - val_out_2_acc: 0.7050 - val_out_3_acc: 0.7614 - val_out_4_acc: 0.7289\n",
      "Epoch 253/1000\n",
      " - 53s - loss: 3.1837 - out_loss: 0.2870 - out_0_loss: 0.3149 - out_1_loss: 0.6504 - out_2_loss: 0.6259 - out_3_loss: 0.6577 - out_4_loss: 0.6477 - out_acc: 0.9136 - out_0_acc: 0.8822 - out_1_acc: 0.7669 - out_2_acc: 0.7712 - out_3_acc: 0.7665 - out_4_acc: 0.7635 - val_loss: 3.4624 - val_out_loss: 0.3702 - val_out_0_loss: 0.3983 - val_out_1_loss: 0.6672 - val_out_2_loss: 0.6567 - val_out_3_loss: 0.6198 - val_out_4_loss: 0.7502 - val_out_acc: 0.8742 - val_out_0_acc: 0.8612 - val_out_1_acc: 0.7636 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7592 - val_out_4_acc: 0.7050\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol       0.95      1.00      0.98        20\n",
      "   altocumulos       0.73      0.93      0.82        88\n",
      "   altostratos       0.58      0.81      0.67        36\n",
      "cieloDespejado       0.85      1.00      0.92        23\n",
      "  cirrocumulos       0.74      0.67      0.70        21\n",
      "        cirros       0.96      0.84      0.90       145\n",
      "  cirrostratos       0.86      0.93      0.89        68\n",
      "       cumulos       0.78      0.96      0.86        71\n",
      "estratocumulos       0.87      0.80      0.84       142\n",
      "      estratos       0.91      0.40      0.55       108\n",
      "     multinube       0.70      0.83      0.76       189\n",
      "  nimbostratos       0.83      0.42      0.56        12\n",
      "\n",
      "      accuracy                           0.80       923\n",
      "     macro avg       0.81      0.80      0.79       923\n",
      "  weighted avg       0.82      0.80      0.79       923\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFMCAYAAABF3IJ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nko1YJpPJZJqW/w8CicQlVIHayAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'cropnetv2'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "                        make_cropnetv2, model_name=model_name,\n",
    "                        model_dir=model_dir, n_outputs=6)\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'], n_outputs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXmQIxcPG59v",
    "colab_type": "text"
   },
   "source": [
    "#### Cropnet v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "W1VsXp9VG-IY",
    "colab_type": "code",
    "cellView": "both",
    "outputId": "2255d157-d7e8-4b54-a091-fffe6b14eb2b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.577312918915E12,
     "user_tz": -60.0,
     "elapsed": 9314806.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking train and validation tests\n",
      "Building the network\n",
      "Compiling the network\n",
      "Layers: 362\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 160, 160, 3)  0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_5 (Cropping2D)       (None, 160, 160, 3)  0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_6 (Cropping2D)       (None, 160, 160, 3)  0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_7 (Cropping2D)       (None, 160, 160, 3)  0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_586 (Conv2D)             (None, 160, 160, 32) 128         cropping2d_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_604 (Conv2D)             (None, 160, 160, 32) 128         cropping2d_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_622 (Conv2D)             (None, 160, 160, 32) 128         cropping2d_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_640 (Conv2D)             (None, 160, 160, 32) 128         cropping2d_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_658 (Conv2D)             (None, 256, 256, 32) 128         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_538 (BatchN (None, 160, 160, 32) 128         conv2d_586[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_556 (BatchN (None, 160, 160, 32) 128         conv2d_604[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 160, 160, 32) 128         conv2d_622[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 160, 160, 32) 128         conv2d_640[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_610 (BatchN (None, 256, 256, 32) 128         conv2d_658[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_538 (Activation)     (None, 160, 160, 32) 0           batch_normalization_538[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 160, 160, 32) 0           batch_normalization_556[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_574 (Activation)     (None, 160, 160, 32) 0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_592 (Activation)     (None, 160, 160, 32) 0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_610 (Activation)     (None, 256, 256, 32) 0           batch_normalization_610[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_587 (Conv2D)             (None, 160, 160, 32) 9248        activation_538[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_605 (Conv2D)             (None, 160, 160, 32) 9248        activation_556[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_623 (Conv2D)             (None, 160, 160, 32) 9248        activation_574[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_641 (Conv2D)             (None, 160, 160, 32) 9248        activation_592[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_659 (Conv2D)             (None, 256, 256, 32) 9248        activation_610[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_539 (BatchN (None, 160, 160, 32) 128         conv2d_587[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_557 (BatchN (None, 160, 160, 32) 128         conv2d_605[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 160, 160, 32) 128         conv2d_623[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 160, 160, 32) 128         conv2d_641[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_611 (BatchN (None, 256, 256, 32) 128         conv2d_659[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_539 (Activation)     (None, 160, 160, 32) 0           batch_normalization_539[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 160, 160, 32) 0           batch_normalization_557[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_575 (Activation)     (None, 160, 160, 32) 0           batch_normalization_575[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_593 (Activation)     (None, 160, 160, 32) 0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_611 (Activation)     (None, 256, 256, 32) 0           batch_normalization_611[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_588 (Conv2D)             (None, 160, 160, 32) 9248        activation_539[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_606 (Conv2D)             (None, 160, 160, 32) 9248        activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_624 (Conv2D)             (None, 160, 160, 32) 9248        activation_575[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_642 (Conv2D)             (None, 160, 160, 32) 9248        activation_593[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 256, 256, 32) 9248        activation_611[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_540 (BatchN (None, 160, 160, 32) 128         conv2d_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_558 (BatchN (None, 160, 160, 32) 128         conv2d_606[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 160, 160, 32) 128         conv2d_624[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 160, 160, 32) 128         conv2d_642[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_612 (BatchN (None, 256, 256, 32) 128         conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 160, 160, 32) 0           batch_normalization_540[0][0]    \n",
      "                                                                 conv2d_586[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 160, 160, 32) 0           batch_normalization_558[0][0]    \n",
      "                                                                 conv2d_604[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 160, 160, 32) 0           batch_normalization_576[0][0]    \n",
      "                                                                 conv2d_622[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 160, 160, 32) 0           batch_normalization_594[0][0]    \n",
      "                                                                 conv2d_640[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 256, 256, 32) 0           batch_normalization_612[0][0]    \n",
      "                                                                 conv2d_658[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_540 (Activation)     (None, 160, 160, 32) 0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 160, 160, 32) 0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_576 (Activation)     (None, 160, 160, 32) 0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_594 (Activation)     (None, 160, 160, 32) 0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_612 (Activation)     (None, 256, 256, 32) 0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_498 (MaxPooling2D (None, 79, 79, 32)   0           activation_540[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_504 (MaxPooling2D (None, 79, 79, 32)   0           activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_510 (MaxPooling2D (None, 79, 79, 32)   0           activation_576[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_516 (MaxPooling2D (None, 79, 79, 32)   0           activation_594[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_522 (MaxPooling2D (None, 127, 127, 32) 0           activation_612[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_589 (Conv2D)             (None, 79, 79, 32)   1056        max_pooling2d_498[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_607 (Conv2D)             (None, 79, 79, 32)   1056        max_pooling2d_504[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_625 (Conv2D)             (None, 79, 79, 32)   1056        max_pooling2d_510[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_643 (Conv2D)             (None, 79, 79, 32)   1056        max_pooling2d_516[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 127, 127, 32) 1056        max_pooling2d_522[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_541 (BatchN (None, 79, 79, 32)   128         conv2d_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 79, 79, 32)   128         conv2d_607[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_577 (BatchN (None, 79, 79, 32)   128         conv2d_625[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 79, 79, 32)   128         conv2d_643[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_613 (BatchN (None, 127, 127, 32) 128         conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_541 (Activation)     (None, 79, 79, 32)   0           batch_normalization_541[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 79, 79, 32)   0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_577 (Activation)     (None, 79, 79, 32)   0           batch_normalization_577[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_595 (Activation)     (None, 79, 79, 32)   0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_613 (Activation)     (None, 127, 127, 32) 0           batch_normalization_613[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_590 (Conv2D)             (None, 79, 79, 32)   9248        activation_541[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_608 (Conv2D)             (None, 79, 79, 32)   9248        activation_559[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_626 (Conv2D)             (None, 79, 79, 32)   9248        activation_577[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_644 (Conv2D)             (None, 79, 79, 32)   9248        activation_595[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 127, 127, 32) 9248        activation_613[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_542 (BatchN (None, 79, 79, 32)   128         conv2d_590[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 79, 79, 32)   128         conv2d_608[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 79, 79, 32)   128         conv2d_626[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 79, 79, 32)   128         conv2d_644[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_614 (BatchN (None, 127, 127, 32) 128         conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_542 (Activation)     (None, 79, 79, 32)   0           batch_normalization_542[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 79, 79, 32)   0           batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_578 (Activation)     (None, 79, 79, 32)   0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_596 (Activation)     (None, 79, 79, 32)   0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_614 (Activation)     (None, 127, 127, 32) 0           batch_normalization_614[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_591 (Conv2D)             (None, 79, 79, 32)   9248        activation_542[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_609 (Conv2D)             (None, 79, 79, 32)   9248        activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_627 (Conv2D)             (None, 79, 79, 32)   9248        activation_578[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_645 (Conv2D)             (None, 79, 79, 32)   9248        activation_596[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 127, 127, 32) 9248        activation_614[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_543 (BatchN (None, 79, 79, 32)   128         conv2d_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_561 (BatchN (None, 79, 79, 32)   128         conv2d_609[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 79, 79, 32)   128         conv2d_627[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 79, 79, 32)   128         conv2d_645[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_615 (BatchN (None, 127, 127, 32) 128         conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 79, 79, 32)   0           batch_normalization_543[0][0]    \n",
      "                                                                 conv2d_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 79, 79, 32)   0           batch_normalization_561[0][0]    \n",
      "                                                                 conv2d_607[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 79, 79, 32)   0           batch_normalization_579[0][0]    \n",
      "                                                                 conv2d_625[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 79, 79, 32)   0           batch_normalization_597[0][0]    \n",
      "                                                                 conv2d_643[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 127, 127, 32) 0           batch_normalization_615[0][0]    \n",
      "                                                                 conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_543 (Activation)     (None, 79, 79, 32)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 79, 79, 32)   0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_579 (Activation)     (None, 79, 79, 32)   0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_597 (Activation)     (None, 79, 79, 32)   0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 127, 127, 32) 0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_499 (MaxPooling2D (None, 39, 39, 32)   0           activation_543[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_505 (MaxPooling2D (None, 39, 39, 32)   0           activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_511 (MaxPooling2D (None, 39, 39, 32)   0           activation_579[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_517 (MaxPooling2D (None, 39, 39, 32)   0           activation_597[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_523 (MaxPooling2D (None, 63, 63, 32)   0           activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_592 (Conv2D)             (None, 39, 39, 64)   2112        max_pooling2d_499[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_610 (Conv2D)             (None, 39, 39, 64)   2112        max_pooling2d_505[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_628 (Conv2D)             (None, 39, 39, 64)   2112        max_pooling2d_511[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_646 (Conv2D)             (None, 39, 39, 64)   2112        max_pooling2d_517[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 63, 63, 64)   2112        max_pooling2d_523[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_544 (BatchN (None, 39, 39, 64)   256         conv2d_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 39, 39, 64)   256         conv2d_610[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 39, 39, 64)   256         conv2d_628[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 39, 39, 64)   256         conv2d_646[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_616 (BatchN (None, 63, 63, 64)   256         conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_544 (Activation)     (None, 39, 39, 64)   0           batch_normalization_544[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 39, 39, 64)   0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_580 (Activation)     (None, 39, 39, 64)   0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_598 (Activation)     (None, 39, 39, 64)   0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 63, 63, 64)   0           batch_normalization_616[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_593 (Conv2D)             (None, 39, 39, 64)   36928       activation_544[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_611 (Conv2D)             (None, 39, 39, 64)   36928       activation_562[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_629 (Conv2D)             (None, 39, 39, 64)   36928       activation_580[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_647 (Conv2D)             (None, 39, 39, 64)   36928       activation_598[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 63, 63, 64)   36928       activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_545 (BatchN (None, 39, 39, 64)   256         conv2d_593[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 39, 39, 64)   256         conv2d_611[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 39, 39, 64)   256         conv2d_629[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 39, 39, 64)   256         conv2d_647[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_617 (BatchN (None, 63, 63, 64)   256         conv2d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_545 (Activation)     (None, 39, 39, 64)   0           batch_normalization_545[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 39, 39, 64)   0           batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_581 (Activation)     (None, 39, 39, 64)   0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_599 (Activation)     (None, 39, 39, 64)   0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 63, 63, 64)   0           batch_normalization_617[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_594 (Conv2D)             (None, 39, 39, 64)   36928       activation_545[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_612 (Conv2D)             (None, 39, 39, 64)   36928       activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_630 (Conv2D)             (None, 39, 39, 64)   36928       activation_581[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_648 (Conv2D)             (None, 39, 39, 64)   36928       activation_599[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 63, 63, 64)   36928       activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_546 (BatchN (None, 39, 39, 64)   256         conv2d_594[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_564 (BatchN (None, 39, 39, 64)   256         conv2d_612[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 39, 39, 64)   256         conv2d_630[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 39, 39, 64)   256         conv2d_648[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_618 (BatchN (None, 63, 63, 64)   256         conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 39, 39, 64)   0           batch_normalization_546[0][0]    \n",
      "                                                                 conv2d_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 39, 39, 64)   0           batch_normalization_564[0][0]    \n",
      "                                                                 conv2d_610[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 39, 39, 64)   0           batch_normalization_582[0][0]    \n",
      "                                                                 conv2d_628[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 39, 39, 64)   0           batch_normalization_600[0][0]    \n",
      "                                                                 conv2d_646[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 63, 63, 64)   0           batch_normalization_618[0][0]    \n",
      "                                                                 conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_546 (Activation)     (None, 39, 39, 64)   0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 39, 39, 64)   0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_582 (Activation)     (None, 39, 39, 64)   0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_600 (Activation)     (None, 39, 39, 64)   0           add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 63, 63, 64)   0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_500 (MaxPooling2D (None, 19, 19, 64)   0           activation_546[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_506 (MaxPooling2D (None, 19, 19, 64)   0           activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_512 (MaxPooling2D (None, 19, 19, 64)   0           activation_582[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_518 (MaxPooling2D (None, 19, 19, 64)   0           activation_600[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_524 (MaxPooling2D (None, 31, 31, 64)   0           activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_595 (Conv2D)             (None, 19, 19, 64)   4160        max_pooling2d_500[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_613 (Conv2D)             (None, 19, 19, 64)   4160        max_pooling2d_506[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_631 (Conv2D)             (None, 19, 19, 64)   4160        max_pooling2d_512[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_649 (Conv2D)             (None, 19, 19, 64)   4160        max_pooling2d_518[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 31, 31, 64)   4160        max_pooling2d_524[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_547 (BatchN (None, 19, 19, 64)   256         conv2d_595[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 19, 19, 64)   256         conv2d_613[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 19, 19, 64)   256         conv2d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 19, 19, 64)   256         conv2d_649[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_619 (BatchN (None, 31, 31, 64)   256         conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_547 (Activation)     (None, 19, 19, 64)   0           batch_normalization_547[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 19, 19, 64)   0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_583 (Activation)     (None, 19, 19, 64)   0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_601 (Activation)     (None, 19, 19, 64)   0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 31, 31, 64)   0           batch_normalization_619[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_596 (Conv2D)             (None, 19, 19, 64)   36928       activation_547[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_614 (Conv2D)             (None, 19, 19, 64)   36928       activation_565[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_632 (Conv2D)             (None, 19, 19, 64)   36928       activation_583[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_650 (Conv2D)             (None, 19, 19, 64)   36928       activation_601[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 31, 31, 64)   36928       activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_548 (BatchN (None, 19, 19, 64)   256         conv2d_596[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 19, 19, 64)   256         conv2d_614[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 19, 19, 64)   256         conv2d_632[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_602 (BatchN (None, 19, 19, 64)   256         conv2d_650[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_620 (BatchN (None, 31, 31, 64)   256         conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_548 (Activation)     (None, 19, 19, 64)   0           batch_normalization_548[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 19, 19, 64)   0           batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_584 (Activation)     (None, 19, 19, 64)   0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_602 (Activation)     (None, 19, 19, 64)   0           batch_normalization_602[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 31, 31, 64)   0           batch_normalization_620[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_597 (Conv2D)             (None, 19, 19, 64)   36928       activation_548[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_615 (Conv2D)             (None, 19, 19, 64)   36928       activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_633 (Conv2D)             (None, 19, 19, 64)   36928       activation_584[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_651 (Conv2D)             (None, 19, 19, 64)   36928       activation_602[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 31, 31, 64)   36928       activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_549 (BatchN (None, 19, 19, 64)   256         conv2d_597[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_567 (BatchN (None, 19, 19, 64)   256         conv2d_615[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 19, 19, 64)   256         conv2d_633[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_603 (BatchN (None, 19, 19, 64)   256         conv2d_651[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_621 (BatchN (None, 31, 31, 64)   256         conv2d_669[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 19, 19, 64)   0           batch_normalization_549[0][0]    \n",
      "                                                                 conv2d_595[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 19, 19, 64)   0           batch_normalization_567[0][0]    \n",
      "                                                                 conv2d_613[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 19, 19, 64)   0           batch_normalization_585[0][0]    \n",
      "                                                                 conv2d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 19, 19, 64)   0           batch_normalization_603[0][0]    \n",
      "                                                                 conv2d_649[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 31, 31, 64)   0           batch_normalization_621[0][0]    \n",
      "                                                                 conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_549 (Activation)     (None, 19, 19, 64)   0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 19, 19, 64)   0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_585 (Activation)     (None, 19, 19, 64)   0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_603 (Activation)     (None, 19, 19, 64)   0           add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 31, 31, 64)   0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_501 (MaxPooling2D (None, 9, 9, 64)     0           activation_549[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_507 (MaxPooling2D (None, 9, 9, 64)     0           activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_513 (MaxPooling2D (None, 9, 9, 64)     0           activation_585[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_519 (MaxPooling2D (None, 9, 9, 64)     0           activation_603[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_525 (MaxPooling2D (None, 15, 15, 64)   0           activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_598 (Conv2D)             (None, 9, 9, 128)    8320        max_pooling2d_501[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_616 (Conv2D)             (None, 9, 9, 128)    8320        max_pooling2d_507[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_634 (Conv2D)             (None, 9, 9, 128)    8320        max_pooling2d_513[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_652 (Conv2D)             (None, 9, 9, 128)    8320        max_pooling2d_519[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 15, 15, 128)  8320        max_pooling2d_525[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_550 (BatchN (None, 9, 9, 128)    512         conv2d_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 9, 9, 128)    512         conv2d_616[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 9, 9, 128)    512         conv2d_634[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_604 (BatchN (None, 9, 9, 128)    512         conv2d_652[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_622 (BatchN (None, 15, 15, 128)  512         conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 9, 9, 128)    0           batch_normalization_550[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_568 (Activation)     (None, 9, 9, 128)    0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_586 (Activation)     (None, 9, 9, 128)    0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_604 (Activation)     (None, 9, 9, 128)    0           batch_normalization_604[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 15, 15, 128)  0           batch_normalization_622[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_599 (Conv2D)             (None, 9, 9, 128)    147584      activation_550[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_617 (Conv2D)             (None, 9, 9, 128)    147584      activation_568[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_635 (Conv2D)             (None, 9, 9, 128)    147584      activation_586[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_653 (Conv2D)             (None, 9, 9, 128)    147584      activation_604[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 15, 15, 128)  147584      activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_551 (BatchN (None, 9, 9, 128)    512         conv2d_599[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 9, 9, 128)    512         conv2d_617[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 9, 9, 128)    512         conv2d_635[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_605 (BatchN (None, 9, 9, 128)    512         conv2d_653[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_623 (BatchN (None, 15, 15, 128)  512         conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 9, 9, 128)    0           batch_normalization_551[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_569 (Activation)     (None, 9, 9, 128)    0           batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_587 (Activation)     (None, 9, 9, 128)    0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_605 (Activation)     (None, 9, 9, 128)    0           batch_normalization_605[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 15, 15, 128)  0           batch_normalization_623[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_600 (Conv2D)             (None, 9, 9, 128)    147584      activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_618 (Conv2D)             (None, 9, 9, 128)    147584      activation_569[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_636 (Conv2D)             (None, 9, 9, 128)    147584      activation_587[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_654 (Conv2D)             (None, 9, 9, 128)    147584      activation_605[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 15, 15, 128)  147584      activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_552 (BatchN (None, 9, 9, 128)    512         conv2d_600[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_570 (BatchN (None, 9, 9, 128)    512         conv2d_618[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 9, 9, 128)    512         conv2d_636[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_606 (BatchN (None, 9, 9, 128)    512         conv2d_654[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_624 (BatchN (None, 15, 15, 128)  512         conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 9, 9, 128)    0           batch_normalization_552[0][0]    \n",
      "                                                                 conv2d_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 9, 9, 128)    0           batch_normalization_570[0][0]    \n",
      "                                                                 conv2d_616[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 9, 9, 128)    0           batch_normalization_588[0][0]    \n",
      "                                                                 conv2d_634[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 9, 9, 128)    0           batch_normalization_606[0][0]    \n",
      "                                                                 conv2d_652[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 15, 15, 128)  0           batch_normalization_624[0][0]    \n",
      "                                                                 conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 9, 9, 128)    0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_570 (Activation)     (None, 9, 9, 128)    0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_588 (Activation)     (None, 9, 9, 128)    0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_606 (Activation)     (None, 9, 9, 128)    0           add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 15, 15, 128)  0           add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_502 (MaxPooling2D (None, 4, 4, 128)    0           activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_508 (MaxPooling2D (None, 4, 4, 128)    0           activation_570[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_514 (MaxPooling2D (None, 4, 4, 128)    0           activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_520 (MaxPooling2D (None, 4, 4, 128)    0           activation_606[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_526 (MaxPooling2D (None, 7, 7, 128)    0           activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_601 (Conv2D)             (None, 4, 4, 128)    16512       max_pooling2d_502[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_619 (Conv2D)             (None, 4, 4, 128)    16512       max_pooling2d_508[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_637 (Conv2D)             (None, 4, 4, 128)    16512       max_pooling2d_514[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_655 (Conv2D)             (None, 4, 4, 128)    16512       max_pooling2d_520[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 7, 7, 128)    16512       max_pooling2d_526[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_553 (BatchN (None, 4, 4, 128)    512         conv2d_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 4, 4, 128)    512         conv2d_619[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 4, 4, 128)    512         conv2d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_607 (BatchN (None, 4, 4, 128)    512         conv2d_655[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_625 (BatchN (None, 7, 7, 128)    512         conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 4, 4, 128)    0           batch_normalization_553[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_571 (Activation)     (None, 4, 4, 128)    0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_589 (Activation)     (None, 4, 4, 128)    0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_607 (Activation)     (None, 4, 4, 128)    0           batch_normalization_607[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 7, 7, 128)    0           batch_normalization_625[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_602 (Conv2D)             (None, 4, 4, 128)    147584      activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_620 (Conv2D)             (None, 4, 4, 128)    147584      activation_571[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_638 (Conv2D)             (None, 4, 4, 128)    147584      activation_589[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_656 (Conv2D)             (None, 4, 4, 128)    147584      activation_607[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 7, 7, 128)    147584      activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_554 (BatchN (None, 4, 4, 128)    512         conv2d_602[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 4, 4, 128)    512         conv2d_620[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 4, 4, 128)    512         conv2d_638[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 4, 4, 128)    512         conv2d_656[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_626 (BatchN (None, 7, 7, 128)    512         conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 4, 4, 128)    0           batch_normalization_554[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_572 (Activation)     (None, 4, 4, 128)    0           batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_590 (Activation)     (None, 4, 4, 128)    0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_608 (Activation)     (None, 4, 4, 128)    0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 7, 7, 128)    0           batch_normalization_626[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_603 (Conv2D)             (None, 4, 4, 128)    147584      activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_621 (Conv2D)             (None, 4, 4, 128)    147584      activation_572[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_639 (Conv2D)             (None, 4, 4, 128)    147584      activation_590[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_657 (Conv2D)             (None, 4, 4, 128)    147584      activation_608[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 7, 7, 128)    147584      activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_555 (BatchN (None, 4, 4, 128)    512         conv2d_603[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_573 (BatchN (None, 4, 4, 128)    512         conv2d_621[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 4, 4, 128)    512         conv2d_639[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 4, 4, 128)    512         conv2d_657[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_627 (BatchN (None, 7, 7, 128)    512         conv2d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 4, 4, 128)    0           batch_normalization_555[0][0]    \n",
      "                                                                 conv2d_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 4, 4, 128)    0           batch_normalization_573[0][0]    \n",
      "                                                                 conv2d_619[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 4, 4, 128)    0           batch_normalization_591[0][0]    \n",
      "                                                                 conv2d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 4, 4, 128)    0           batch_normalization_609[0][0]    \n",
      "                                                                 conv2d_655[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 7, 7, 128)    0           batch_normalization_627[0][0]    \n",
      "                                                                 conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 4, 4, 128)    0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_573 (Activation)     (None, 4, 4, 128)    0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_591 (Activation)     (None, 4, 4, 128)    0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_609 (Activation)     (None, 4, 4, 128)    0           add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 7, 7, 128)    0           add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_503 (MaxPooling2D (None, 1, 1, 128)    0           activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_509 (MaxPooling2D (None, 1, 1, 128)    0           activation_573[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_515 (MaxPooling2D (None, 1, 1, 128)    0           activation_591[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_521 (MaxPooling2D (None, 1, 1, 128)    0           activation_609[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_527 (MaxPooling2D (None, 3, 3, 128)    0           activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_121 (G (None, 128)          0           max_pooling2d_503[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_122 (G (None, 128)          0           max_pooling2d_509[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_123 (G (None, 128)          0           max_pooling2d_515[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_124 (G (None, 128)          0           max_pooling2d_521[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_125 (G (None, 128)          0           max_pooling2d_527[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_131 (Dense)               (None, 256)          33024       global_average_pooling2d_121[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_132 (Dense)               (None, 256)          33024       global_average_pooling2d_122[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_133 (Dense)               (None, 256)          33024       global_average_pooling2d_123[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_134 (Dense)               (None, 256)          33024       global_average_pooling2d_124[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_135 (Dense)               (None, 256)          33024       global_average_pooling2d_125[0][0\n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_131 (Dropout)           (None, 256)          0           dense_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_132 (Dropout)           (None, 256)          0           dense_132[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_133 (Dropout)           (None, 256)          0           dense_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_134 (Dropout)           (None, 256)          0           dense_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_135 (Dropout)           (None, 256)          0           dense_135[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_136 (Dense)               (None, 16)           128         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out_1 (Dense)                   (None, 12)           3084        dropout_131[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_2 (Dense)                   (None, 12)           3084        dropout_132[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_3 (Dense)                   (None, 12)           3084        dropout_133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_4 (Dense)                   (None, 12)           3084        dropout_134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_0 (Dense)                   (None, 12)           3084        dropout_135[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_136 (Dropout)           (None, 16)           0           dense_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 76)           0           out_1[0][0]                      \n",
      "                                                                 out_2[0][0]                      \n",
      "                                                                 out_3[0][0]                      \n",
      "                                                                 out_4[0][0]                      \n",
      "                                                                 out_0[0][0]                      \n",
      "                                                                 dropout_136[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, 128)          9856        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_137 (Dropout)           (None, 128)          0           dense_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           1548        dropout_137[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 4,255,592\n",
      "Trainable params: 4,242,152\n",
      "Non-trainable params: 13,440\n",
      "__________________________________________________________________________________________________\n",
      "Fitting the network\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 1103s - loss: 18.8678 - out_loss: 2.5577 - out_0_loss: 2.7811 - out_1_loss: 3.5288 - out_2_loss: 2.9207 - out_3_loss: 3.2949 - out_4_loss: 3.7846 - out_acc: 0.0887 - out_0_acc: 0.2288 - out_1_acc: 0.1844 - out_2_acc: 0.2037 - out_3_acc: 0.1835 - out_4_acc: 0.1686 - val_loss: 13.6264 - val_out_loss: 2.4128 - val_out_0_loss: 2.0087 - val_out_1_loss: 2.1350 - val_out_2_loss: 2.2450 - val_out_3_loss: 2.1272 - val_out_4_loss: 2.1583 - val_out_acc: 0.1302 - val_out_0_acc: 0.3319 - val_out_1_acc: 0.2842 - val_out_2_acc: 0.2560 - val_out_3_acc: 0.2777 - val_out_4_acc: 0.2560\n",
      "Epoch 2/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 62s - loss: 13.0738 - out_loss: 2.4216 - out_0_loss: 1.8476 - out_1_loss: 2.2527 - out_2_loss: 2.2175 - out_3_loss: 2.1195 - out_4_loss: 2.2149 - out_acc: 0.1482 - out_0_acc: 0.3748 - out_1_acc: 0.2684 - out_2_acc: 0.2855 - out_3_acc: 0.3035 - out_4_acc: 0.2771 - val_loss: 12.1301 - val_out_loss: 2.2880 - val_out_0_loss: 1.6249 - val_out_1_loss: 1.9643 - val_out_2_loss: 1.9587 - val_out_3_loss: 1.9292 - val_out_4_loss: 1.8847 - val_out_acc: 0.1670 - val_out_0_acc: 0.4599 - val_out_1_acc: 0.3080 - val_out_2_acc: 0.3731 - val_out_3_acc: 0.3471 - val_out_4_acc: 0.3861\n",
      "Epoch 3/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 62s - loss: 11.9807 - out_loss: 2.3055 - out_0_loss: 1.6643 - out_1_loss: 2.0239 - out_2_loss: 2.0190 - out_3_loss: 1.9688 - out_4_loss: 1.9991 - out_acc: 0.1922 - out_0_acc: 0.4225 - out_1_acc: 0.3032 - out_2_acc: 0.3255 - out_3_acc: 0.3230 - out_4_acc: 0.3180 - val_loss: 11.4674 - val_out_loss: 2.1720 - val_out_0_loss: 1.5360 - val_out_1_loss: 1.8519 - val_out_2_loss: 1.8681 - val_out_3_loss: 1.7891 - val_out_4_loss: 1.7963 - val_out_acc: 0.3037 - val_out_0_acc: 0.4902 - val_out_1_acc: 0.3753 - val_out_2_acc: 0.3948 - val_out_3_acc: 0.3926 - val_out_4_acc: 0.3839\n",
      "Epoch 4/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 63s - loss: 11.4463 - out_loss: 2.2138 - out_0_loss: 1.5721 - out_1_loss: 1.9117 - out_2_loss: 1.9399 - out_3_loss: 1.8771 - out_4_loss: 1.9316 - out_acc: 0.2443 - out_0_acc: 0.4523 - out_1_acc: 0.3342 - out_2_acc: 0.3351 - out_3_acc: 0.3543 - out_4_acc: 0.3304 - val_loss: 10.5833 - val_out_loss: 2.0540 - val_out_0_loss: 1.3676 - val_out_1_loss: 1.7191 - val_out_2_loss: 1.6902 - val_out_3_loss: 1.6401 - val_out_4_loss: 1.6934 - val_out_acc: 0.3926 - val_out_0_acc: 0.5206 - val_out_1_acc: 0.4295 - val_out_2_acc: 0.4208 - val_out_3_acc: 0.4729 - val_out_4_acc: 0.4360\n",
      "Epoch 5/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 10.8961 - out_loss: 2.1202 - out_0_loss: 1.4825 - out_1_loss: 1.8645 - out_2_loss: 1.8293 - out_3_loss: 1.7843 - out_4_loss: 1.8153 - out_acc: 0.2920 - out_0_acc: 0.4864 - out_1_acc: 0.3621 - out_2_acc: 0.3673 - out_3_acc: 0.3729 - out_4_acc: 0.3726 - val_loss: 10.6481 - val_out_loss: 2.0138 - val_out_0_loss: 1.3741 - val_out_1_loss: 1.7577 - val_out_2_loss: 1.7118 - val_out_3_loss: 1.6473 - val_out_4_loss: 1.7219 - val_out_acc: 0.3492 - val_out_0_acc: 0.5011 - val_out_1_acc: 0.3731 - val_out_2_acc: 0.3774 - val_out_3_acc: 0.4230 - val_out_4_acc: 0.3861\n",
      "Epoch 6/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 61s - loss: 10.4972 - out_loss: 2.0402 - out_0_loss: 1.4040 - out_1_loss: 1.7891 - out_2_loss: 1.7771 - out_3_loss: 1.7238 - out_4_loss: 1.7630 - out_acc: 0.3277 - out_0_acc: 0.5099 - out_1_acc: 0.3779 - out_2_acc: 0.3862 - out_3_acc: 0.3965 - out_4_acc: 0.3943 - val_loss: 10.1262 - val_out_loss: 1.9001 - val_out_0_loss: 1.3385 - val_out_1_loss: 1.6166 - val_out_2_loss: 1.5931 - val_out_3_loss: 1.6535 - val_out_4_loss: 1.6235 - val_out_acc: 0.4143 - val_out_0_acc: 0.5249 - val_out_1_acc: 0.4273 - val_out_2_acc: 0.4360 - val_out_3_acc: 0.4056 - val_out_4_acc: 0.4121\n",
      "Epoch 7/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 62s - loss: 10.1679 - out_loss: 1.9827 - out_0_loss: 1.3325 - out_1_loss: 1.7176 - out_2_loss: 1.7176 - out_3_loss: 1.6917 - out_4_loss: 1.7258 - out_acc: 0.3614 - out_0_acc: 0.5276 - out_1_acc: 0.4027 - out_2_acc: 0.3999 - out_3_acc: 0.4039 - out_4_acc: 0.4024 - val_loss: 9.5585 - val_out_loss: 1.7632 - val_out_0_loss: 1.2436 - val_out_1_loss: 1.5307 - val_out_2_loss: 1.5532 - val_out_3_loss: 1.5230 - val_out_4_loss: 1.5664 - val_out_acc: 0.4447 - val_out_0_acc: 0.5618 - val_out_1_acc: 0.4751 - val_out_2_acc: 0.4599 - val_out_3_acc: 0.4599 - val_out_4_acc: 0.4230\n",
      "Epoch 8/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 61s - loss: 9.9914 - out_loss: 1.9092 - out_0_loss: 1.3010 - out_1_loss: 1.7041 - out_2_loss: 1.7190 - out_3_loss: 1.6718 - out_4_loss: 1.6863 - out_acc: 0.3797 - out_0_acc: 0.5577 - out_1_acc: 0.4005 - out_2_acc: 0.4039 - out_3_acc: 0.4175 - out_4_acc: 0.4182 - val_loss: 9.3657 - val_out_loss: 1.7181 - val_out_0_loss: 1.2462 - val_out_1_loss: 1.5328 - val_out_2_loss: 1.4774 - val_out_3_loss: 1.4987 - val_out_4_loss: 1.5219 - val_out_acc: 0.4642 - val_out_0_acc: 0.5705 - val_out_1_acc: 0.4707 - val_out_2_acc: 0.4816 - val_out_3_acc: 0.4685 - val_out_4_acc: 0.4469\n",
      "Epoch 9/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 61s - loss: 9.7868 - out_loss: 1.8625 - out_0_loss: 1.2841 - out_1_loss: 1.6719 - out_2_loss: 1.6865 - out_3_loss: 1.6306 - out_4_loss: 1.6511 - out_acc: 0.4179 - out_0_acc: 0.5518 - out_1_acc: 0.4104 - out_2_acc: 0.4039 - out_3_acc: 0.4281 - out_4_acc: 0.4160 - val_loss: 9.3750 - val_out_loss: 1.6773 - val_out_0_loss: 1.1957 - val_out_1_loss: 1.5558 - val_out_2_loss: 1.5838 - val_out_3_loss: 1.4704 - val_out_4_loss: 1.5210 - val_out_acc: 0.4664 - val_out_0_acc: 0.5792 - val_out_1_acc: 0.4512 - val_out_2_acc: 0.4208 - val_out_3_acc: 0.4599 - val_out_4_acc: 0.4490\n",
      "Epoch 10/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 63s - loss: 9.6255 - out_loss: 1.7976 - out_0_loss: 1.2739 - out_1_loss: 1.6687 - out_2_loss: 1.6318 - out_3_loss: 1.6121 - out_4_loss: 1.6415 - out_acc: 0.4296 - out_0_acc: 0.5561 - out_1_acc: 0.4123 - out_2_acc: 0.4253 - out_3_acc: 0.4225 - out_4_acc: 0.4241 - val_loss: 8.6724 - val_out_loss: 1.5514 - val_out_0_loss: 1.1256 - val_out_1_loss: 1.4373 - val_out_2_loss: 1.3967 - val_out_3_loss: 1.4469 - val_out_4_loss: 1.3713 - val_out_acc: 0.5249 - val_out_0_acc: 0.5857 - val_out_1_acc: 0.4751 - val_out_2_acc: 0.4881 - val_out_3_acc: 0.4490 - val_out_4_acc: 0.5119\n",
      "Epoch 11/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 64s - loss: 9.3690 - out_loss: 1.7398 - out_0_loss: 1.2323 - out_1_loss: 1.6098 - out_2_loss: 1.5996 - out_3_loss: 1.5919 - out_4_loss: 1.5956 - out_acc: 0.4482 - out_0_acc: 0.5645 - out_1_acc: 0.4222 - out_2_acc: 0.4355 - out_3_acc: 0.4290 - out_4_acc: 0.4392 - val_loss: 9.0058 - val_out_loss: 1.5852 - val_out_0_loss: 1.0797 - val_out_1_loss: 1.5325 - val_out_2_loss: 1.4919 - val_out_3_loss: 1.4643 - val_out_4_loss: 1.4957 - val_out_acc: 0.5271 - val_out_0_acc: 0.6269 - val_out_1_acc: 0.4534 - val_out_2_acc: 0.4772 - val_out_3_acc: 0.5033 - val_out_4_acc: 0.4599\n",
      "Epoch 12/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 61s - loss: 9.0288 - out_loss: 1.6601 - out_0_loss: 1.1845 - out_1_loss: 1.5582 - out_2_loss: 1.5491 - out_3_loss: 1.5443 - out_4_loss: 1.5327 - out_acc: 0.4882 - out_0_acc: 0.5914 - out_1_acc: 0.4538 - out_2_acc: 0.4523 - out_3_acc: 0.4625 - out_4_acc: 0.4591 - val_loss: 8.8149 - val_out_loss: 1.5205 - val_out_0_loss: 1.1200 - val_out_1_loss: 1.4512 - val_out_2_loss: 1.4492 - val_out_3_loss: 1.4607 - val_out_4_loss: 1.4644 - val_out_acc: 0.5510 - val_out_0_acc: 0.6074 - val_out_1_acc: 0.4685 - val_out_2_acc: 0.4837 - val_out_3_acc: 0.4902 - val_out_4_acc: 0.4837\n",
      "Epoch 13/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 9.0351 - out_loss: 1.6379 - out_0_loss: 1.1825 - out_1_loss: 1.5456 - out_2_loss: 1.5754 - out_3_loss: 1.5665 - out_4_loss: 1.5274 - out_acc: 0.4907 - out_0_acc: 0.5856 - out_1_acc: 0.4625 - out_2_acc: 0.4554 - out_3_acc: 0.4420 - out_4_acc: 0.4554 - val_loss: 8.3966 - val_out_loss: 1.4315 - val_out_0_loss: 1.1233 - val_out_1_loss: 1.3821 - val_out_2_loss: 1.3584 - val_out_3_loss: 1.3774 - val_out_4_loss: 1.3915 - val_out_acc: 0.5705 - val_out_0_acc: 0.5987 - val_out_1_acc: 0.5011 - val_out_2_acc: 0.5141 - val_out_3_acc: 0.5098 - val_out_4_acc: 0.4859\n",
      "Epoch 14/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 8.8041 - out_loss: 1.5715 - out_0_loss: 1.1443 - out_1_loss: 1.5421 - out_2_loss: 1.5211 - out_3_loss: 1.5181 - out_4_loss: 1.5070 - out_acc: 0.4966 - out_0_acc: 0.5945 - out_1_acc: 0.4510 - out_2_acc: 0.4609 - out_3_acc: 0.4609 - out_4_acc: 0.4727 - val_loss: 8.3273 - val_out_loss: 1.3579 - val_out_0_loss: 0.9908 - val_out_1_loss: 1.4673 - val_out_2_loss: 1.4542 - val_out_3_loss: 1.3623 - val_out_4_loss: 1.3653 - val_out_acc: 0.5813 - val_out_0_acc: 0.6486 - val_out_1_acc: 0.4837 - val_out_2_acc: 0.4902 - val_out_3_acc: 0.5033 - val_out_4_acc: 0.5206\n",
      "Epoch 15/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 8.6701 - out_loss: 1.5302 - out_0_loss: 1.1363 - out_1_loss: 1.5037 - out_2_loss: 1.5069 - out_3_loss: 1.4978 - out_4_loss: 1.4953 - out_acc: 0.5146 - out_0_acc: 0.5970 - out_1_acc: 0.4619 - out_2_acc: 0.4749 - out_3_acc: 0.4653 - out_4_acc: 0.4702 - val_loss: 8.4808 - val_out_loss: 1.3715 - val_out_0_loss: 1.0796 - val_out_1_loss: 1.4361 - val_out_2_loss: 1.4198 - val_out_3_loss: 1.4366 - val_out_4_loss: 1.4016 - val_out_acc: 0.5727 - val_out_0_acc: 0.6269 - val_out_1_acc: 0.4859 - val_out_2_acc: 0.4729 - val_out_3_acc: 0.4881 - val_out_4_acc: 0.5119\n",
      "Epoch 16/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 8.4888 - out_loss: 1.4906 - out_0_loss: 1.0961 - out_1_loss: 1.4834 - out_2_loss: 1.4884 - out_3_loss: 1.4667 - out_4_loss: 1.4635 - out_acc: 0.5236 - out_0_acc: 0.6060 - out_1_acc: 0.4768 - out_2_acc: 0.4842 - out_3_acc: 0.4733 - out_4_acc: 0.4712 - val_loss: 8.4221 - val_out_loss: 1.3321 - val_out_0_loss: 1.1125 - val_out_1_loss: 1.3915 - val_out_2_loss: 1.3910 - val_out_3_loss: 1.4171 - val_out_4_loss: 1.4445 - val_out_acc: 0.5879 - val_out_0_acc: 0.6334 - val_out_1_acc: 0.5249 - val_out_2_acc: 0.5098 - val_out_3_acc: 0.5098 - val_out_4_acc: 0.4729\n",
      "Epoch 17/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 8.4260 - out_loss: 1.4401 - out_0_loss: 1.0880 - out_1_loss: 1.4875 - out_2_loss: 1.4905 - out_3_loss: 1.4689 - out_4_loss: 1.4509 - out_acc: 0.5409 - out_0_acc: 0.6234 - out_1_acc: 0.4628 - out_2_acc: 0.4795 - out_3_acc: 0.4783 - out_4_acc: 0.4870 - val_loss: 8.2643 - val_out_loss: 1.3052 - val_out_0_loss: 1.0256 - val_out_1_loss: 1.4358 - val_out_2_loss: 1.4046 - val_out_3_loss: 1.3395 - val_out_4_loss: 1.4263 - val_out_acc: 0.5813 - val_out_0_acc: 0.6334 - val_out_1_acc: 0.4902 - val_out_2_acc: 0.5163 - val_out_3_acc: 0.4989 - val_out_4_acc: 0.4924\n",
      "Epoch 18/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 8.3868 - out_loss: 1.4268 - out_0_loss: 1.0759 - out_1_loss: 1.4903 - out_2_loss: 1.4660 - out_3_loss: 1.4595 - out_4_loss: 1.4684 - out_acc: 0.5428 - out_0_acc: 0.6138 - out_1_acc: 0.4678 - out_2_acc: 0.4845 - out_3_acc: 0.4904 - out_4_acc: 0.4786 - val_loss: 8.3439 - val_out_loss: 1.2964 - val_out_0_loss: 1.0569 - val_out_1_loss: 1.4024 - val_out_2_loss: 1.3588 - val_out_3_loss: 1.4986 - val_out_4_loss: 1.4004 - val_out_acc: 0.5835 - val_out_0_acc: 0.6052 - val_out_1_acc: 0.4902 - val_out_2_acc: 0.4924 - val_out_3_acc: 0.4382 - val_out_4_acc: 0.5076\n",
      "Epoch 19/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 8.1403 - out_loss: 1.3732 - out_0_loss: 1.0521 - out_1_loss: 1.4387 - out_2_loss: 1.4073 - out_3_loss: 1.4440 - out_4_loss: 1.4250 - out_acc: 0.5589 - out_0_acc: 0.6330 - out_1_acc: 0.4879 - out_2_acc: 0.4978 - out_3_acc: 0.4805 - out_4_acc: 0.4994 - val_loss: 7.8871 - val_out_loss: 1.2238 - val_out_0_loss: 0.9728 - val_out_1_loss: 1.3263 - val_out_2_loss: 1.4280 - val_out_3_loss: 1.3354 - val_out_4_loss: 1.2886 - val_out_acc: 0.6009 - val_out_0_acc: 0.6486 - val_out_1_acc: 0.5206 - val_out_2_acc: 0.4859 - val_out_3_acc: 0.5163 - val_out_4_acc: 0.5380\n",
      "Epoch 20/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 7.9903 - out_loss: 1.3349 - out_0_loss: 1.0257 - out_1_loss: 1.4117 - out_2_loss: 1.4142 - out_3_loss: 1.3969 - out_4_loss: 1.4071 - out_acc: 0.5735 - out_0_acc: 0.6352 - out_1_acc: 0.5050 - out_2_acc: 0.5006 - out_3_acc: 0.4994 - out_4_acc: 0.5071 - val_loss: 7.8176 - val_out_loss: 1.1692 - val_out_0_loss: 1.0135 - val_out_1_loss: 1.3296 - val_out_2_loss: 1.2966 - val_out_3_loss: 1.3997 - val_out_4_loss: 1.2995 - val_out_acc: 0.6334 - val_out_0_acc: 0.6508 - val_out_1_acc: 0.5249 - val_out_2_acc: 0.5336 - val_out_3_acc: 0.5119 - val_out_4_acc: 0.5380\n",
      "Epoch 21/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 8.0140 - out_loss: 1.3182 - out_0_loss: 1.0374 - out_1_loss: 1.4334 - out_2_loss: 1.4259 - out_3_loss: 1.4137 - out_4_loss: 1.3854 - out_acc: 0.5794 - out_0_acc: 0.6246 - out_1_acc: 0.4985 - out_2_acc: 0.5025 - out_3_acc: 0.4892 - out_4_acc: 0.4960 - val_loss: 7.9011 - val_out_loss: 1.1662 - val_out_0_loss: 1.0817 - val_out_1_loss: 1.3406 - val_out_2_loss: 1.3565 - val_out_3_loss: 1.3138 - val_out_4_loss: 1.3294 - val_out_acc: 0.6269 - val_out_0_acc: 0.5965 - val_out_1_acc: 0.4816 - val_out_2_acc: 0.5249 - val_out_3_acc: 0.5163 - val_out_4_acc: 0.5054\n",
      "Epoch 22/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 7.9735 - out_loss: 1.2972 - out_0_loss: 1.0196 - out_1_loss: 1.4266 - out_2_loss: 1.4206 - out_3_loss: 1.3908 - out_4_loss: 1.4186 - out_acc: 0.5862 - out_0_acc: 0.6386 - out_1_acc: 0.4836 - out_2_acc: 0.5000 - out_3_acc: 0.4929 - out_4_acc: 0.4935 - val_loss: 7.9130 - val_out_loss: 1.1893 - val_out_0_loss: 0.9446 - val_out_1_loss: 1.3509 - val_out_2_loss: 1.3943 - val_out_3_loss: 1.3283 - val_out_4_loss: 1.3924 - val_out_acc: 0.6247 - val_out_0_acc: 0.6616 - val_out_1_acc: 0.5033 - val_out_2_acc: 0.4881 - val_out_3_acc: 0.5011 - val_out_4_acc: 0.4989\n",
      "Epoch 23/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 7.7839 - out_loss: 1.2694 - out_0_loss: 0.9952 - out_1_loss: 1.3867 - out_2_loss: 1.3767 - out_3_loss: 1.3711 - out_4_loss: 1.3848 - out_acc: 0.5967 - out_0_acc: 0.6503 - out_1_acc: 0.5003 - out_2_acc: 0.5050 - out_3_acc: 0.5211 - out_4_acc: 0.5028 - val_loss: 7.7217 - val_out_loss: 1.0977 - val_out_0_loss: 1.0868 - val_out_1_loss: 1.3422 - val_out_2_loss: 1.2869 - val_out_3_loss: 1.2750 - val_out_4_loss: 1.3274 - val_out_acc: 0.6334 - val_out_0_acc: 0.6421 - val_out_1_acc: 0.5119 - val_out_2_acc: 0.5336 - val_out_3_acc: 0.5315 - val_out_4_acc: 0.5228\n",
      "Epoch 24/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 63s - loss: 7.6291 - out_loss: 1.2205 - out_0_loss: 0.9679 - out_1_loss: 1.3601 - out_2_loss: 1.3645 - out_3_loss: 1.3465 - out_4_loss: 1.3697 - out_acc: 0.6169 - out_0_acc: 0.6544 - out_1_acc: 0.5146 - out_2_acc: 0.5217 - out_3_acc: 0.5226 - out_4_acc: 0.5226 - val_loss: 7.4838 - val_out_loss: 1.0788 - val_out_0_loss: 0.9529 - val_out_1_loss: 1.3182 - val_out_2_loss: 1.2483 - val_out_3_loss: 1.3278 - val_out_4_loss: 1.2616 - val_out_acc: 0.6377 - val_out_0_acc: 0.6312 - val_out_1_acc: 0.5293 - val_out_2_acc: 0.5423 - val_out_3_acc: 0.5163 - val_out_4_acc: 0.5098\n",
      "Epoch 25/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 7.6390 - out_loss: 1.2264 - out_0_loss: 0.9900 - out_1_loss: 1.3592 - out_2_loss: 1.3659 - out_3_loss: 1.3527 - out_4_loss: 1.3447 - out_acc: 0.6097 - out_0_acc: 0.6485 - out_1_acc: 0.5115 - out_2_acc: 0.5118 - out_3_acc: 0.5220 - out_4_acc: 0.5214 - val_loss: 7.3604 - val_out_loss: 1.0482 - val_out_0_loss: 0.9684 - val_out_1_loss: 1.2689 - val_out_2_loss: 1.2565 - val_out_3_loss: 1.2580 - val_out_4_loss: 1.2692 - val_out_acc: 0.6659 - val_out_0_acc: 0.6573 - val_out_1_acc: 0.5423 - val_out_2_acc: 0.5401 - val_out_3_acc: 0.5640 - val_out_4_acc: 0.5315\n",
      "Epoch 26/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 7.5564 - out_loss: 1.1925 - out_0_loss: 0.9825 - out_1_loss: 1.3477 - out_2_loss: 1.3418 - out_3_loss: 1.3501 - out_4_loss: 1.3419 - out_acc: 0.6206 - out_0_acc: 0.6596 - out_1_acc: 0.5192 - out_2_acc: 0.5180 - out_3_acc: 0.5226 - out_4_acc: 0.5211 - val_loss: 7.7911 - val_out_loss: 1.1300 - val_out_0_loss: 1.1390 - val_out_1_loss: 1.3244 - val_out_2_loss: 1.2248 - val_out_3_loss: 1.3357 - val_out_4_loss: 1.3290 - val_out_acc: 0.6312 - val_out_0_acc: 0.5748 - val_out_1_acc: 0.5011 - val_out_2_acc: 0.5640 - val_out_3_acc: 0.5119 - val_out_4_acc: 0.5011\n",
      "Epoch 27/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 7.3851 - out_loss: 1.1564 - out_0_loss: 0.9373 - out_1_loss: 1.3205 - out_2_loss: 1.3144 - out_3_loss: 1.3347 - out_4_loss: 1.3219 - out_acc: 0.6373 - out_0_acc: 0.6618 - out_1_acc: 0.5180 - out_2_acc: 0.5270 - out_3_acc: 0.5251 - out_4_acc: 0.5276 - val_loss: 7.5254 - val_out_loss: 1.0882 - val_out_0_loss: 1.0484 - val_out_1_loss: 1.2755 - val_out_2_loss: 1.2602 - val_out_3_loss: 1.2768 - val_out_4_loss: 1.2784 - val_out_acc: 0.6551 - val_out_0_acc: 0.6182 - val_out_1_acc: 0.5249 - val_out_2_acc: 0.5510 - val_out_3_acc: 0.5445 - val_out_4_acc: 0.5618\n",
      "Epoch 28/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 7.3638 - out_loss: 1.1391 - out_0_loss: 0.9353 - out_1_loss: 1.3335 - out_2_loss: 1.3164 - out_3_loss: 1.3192 - out_4_loss: 1.3203 - out_acc: 0.6531 - out_0_acc: 0.6758 - out_1_acc: 0.5285 - out_2_acc: 0.5267 - out_3_acc: 0.5344 - out_4_acc: 0.5304 - val_loss: 7.1776 - val_out_loss: 1.0078 - val_out_0_loss: 0.9006 - val_out_1_loss: 1.2857 - val_out_2_loss: 1.2122 - val_out_3_loss: 1.2080 - val_out_4_loss: 1.2792 - val_out_acc: 0.6963 - val_out_0_acc: 0.6746 - val_out_1_acc: 0.5466 - val_out_2_acc: 0.5640 - val_out_3_acc: 0.5879 - val_out_4_acc: 0.5401\n",
      "Epoch 29/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 7.3802 - out_loss: 1.1355 - out_0_loss: 0.9676 - out_1_loss: 1.3362 - out_2_loss: 1.3228 - out_3_loss: 1.3170 - out_4_loss: 1.3011 - out_acc: 0.6414 - out_0_acc: 0.6618 - out_1_acc: 0.5136 - out_2_acc: 0.5294 - out_3_acc: 0.5282 - out_4_acc: 0.5425 - val_loss: 7.0977 - val_out_loss: 0.9845 - val_out_0_loss: 0.9151 - val_out_1_loss: 1.2415 - val_out_2_loss: 1.1994 - val_out_3_loss: 1.2218 - val_out_4_loss: 1.2545 - val_out_acc: 0.6985 - val_out_0_acc: 0.6703 - val_out_1_acc: 0.5531 - val_out_2_acc: 0.5618 - val_out_3_acc: 0.5531 - val_out_4_acc: 0.5163\n",
      "Epoch 30/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 7.2716 - out_loss: 1.1179 - out_0_loss: 0.9350 - out_1_loss: 1.3200 - out_2_loss: 1.3240 - out_3_loss: 1.2757 - out_4_loss: 1.2990 - out_acc: 0.6383 - out_0_acc: 0.6652 - out_1_acc: 0.5304 - out_2_acc: 0.5245 - out_3_acc: 0.5446 - out_4_acc: 0.5270 - val_loss: 7.2294 - val_out_loss: 0.9802 - val_out_0_loss: 0.8650 - val_out_1_loss: 1.2740 - val_out_2_loss: 1.2477 - val_out_3_loss: 1.2949 - val_out_4_loss: 1.2815 - val_out_acc: 0.6963 - val_out_0_acc: 0.6638 - val_out_1_acc: 0.5119 - val_out_2_acc: 0.5813 - val_out_3_acc: 0.5163 - val_out_4_acc: 0.5206\n",
      "Epoch 31/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 7.0913 - out_loss: 1.0700 - out_0_loss: 0.8954 - out_1_loss: 1.2949 - out_2_loss: 1.2675 - out_3_loss: 1.2795 - out_4_loss: 1.2840 - out_acc: 0.6655 - out_0_acc: 0.6736 - out_1_acc: 0.5434 - out_2_acc: 0.5490 - out_3_acc: 0.5515 - out_4_acc: 0.5456 - val_loss: 7.1202 - val_out_loss: 0.9771 - val_out_0_loss: 0.8913 - val_out_1_loss: 1.1918 - val_out_2_loss: 1.2992 - val_out_3_loss: 1.2518 - val_out_4_loss: 1.2272 - val_out_acc: 0.6941 - val_out_0_acc: 0.7007 - val_out_1_acc: 0.5792 - val_out_2_acc: 0.5488 - val_out_3_acc: 0.5336 - val_out_4_acc: 0.5510\n",
      "Epoch 32/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 7.0565 - out_loss: 1.0564 - out_0_loss: 0.8962 - out_1_loss: 1.2775 - out_2_loss: 1.2743 - out_3_loss: 1.2774 - out_4_loss: 1.2748 - out_acc: 0.6689 - out_0_acc: 0.6854 - out_1_acc: 0.5564 - out_2_acc: 0.5415 - out_3_acc: 0.5477 - out_4_acc: 0.5415 - val_loss: 6.8603 - val_out_loss: 0.9097 - val_out_0_loss: 0.9337 - val_out_1_loss: 1.1685 - val_out_2_loss: 1.2535 - val_out_3_loss: 1.1086 - val_out_4_loss: 1.2149 - val_out_acc: 0.6855 - val_out_0_acc: 0.6638 - val_out_1_acc: 0.5358 - val_out_2_acc: 0.5163 - val_out_3_acc: 0.5922 - val_out_4_acc: 0.5358\n",
      "Epoch 33/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 7.0740 - out_loss: 1.0638 - out_0_loss: 0.9145 - out_1_loss: 1.2963 - out_2_loss: 1.2822 - out_3_loss: 1.2613 - out_4_loss: 1.2560 - out_acc: 0.6683 - out_0_acc: 0.6801 - out_1_acc: 0.5434 - out_2_acc: 0.5524 - out_3_acc: 0.5443 - out_4_acc: 0.5487 - val_loss: 6.6253 - val_out_loss: 0.8927 - val_out_0_loss: 0.7822 - val_out_1_loss: 1.1374 - val_out_2_loss: 1.1147 - val_out_3_loss: 1.2649 - val_out_4_loss: 1.1712 - val_out_acc: 0.7245 - val_out_0_acc: 0.6963 - val_out_1_acc: 0.6095 - val_out_2_acc: 0.6074 - val_out_3_acc: 0.5401 - val_out_4_acc: 0.5813\n",
      "Epoch 34/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 6.9958 - out_loss: 1.0420 - out_0_loss: 0.8786 - out_1_loss: 1.2679 - out_2_loss: 1.2959 - out_3_loss: 1.2390 - out_4_loss: 1.2724 - out_acc: 0.6841 - out_0_acc: 0.6841 - out_1_acc: 0.5446 - out_2_acc: 0.5462 - out_3_acc: 0.5549 - out_4_acc: 0.5449 - val_loss: 6.7469 - val_out_loss: 0.9059 - val_out_0_loss: 0.8798 - val_out_1_loss: 1.1619 - val_out_2_loss: 1.1628 - val_out_3_loss: 1.1707 - val_out_4_loss: 1.1987 - val_out_acc: 0.6811 - val_out_0_acc: 0.6573 - val_out_1_acc: 0.5662 - val_out_2_acc: 0.5271 - val_out_3_acc: 0.5575 - val_out_4_acc: 0.5705\n",
      "Epoch 35/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 6.9456 - out_loss: 1.0272 - out_0_loss: 0.8818 - out_1_loss: 1.2680 - out_2_loss: 1.2688 - out_3_loss: 1.2407 - out_4_loss: 1.2590 - out_acc: 0.6754 - out_0_acc: 0.6863 - out_1_acc: 0.5527 - out_2_acc: 0.5515 - out_3_acc: 0.5583 - out_4_acc: 0.5456 - val_loss: 6.9002 - val_out_loss: 0.9222 - val_out_0_loss: 0.9230 - val_out_1_loss: 1.2036 - val_out_2_loss: 1.2057 - val_out_3_loss: 1.2230 - val_out_4_loss: 1.1495 - val_out_acc: 0.6898 - val_out_0_acc: 0.6616 - val_out_1_acc: 0.5553 - val_out_2_acc: 0.5705 - val_out_3_acc: 0.5401 - val_out_4_acc: 0.5597\n",
      "Epoch 36/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 6.9365 - out_loss: 1.0278 - out_0_loss: 0.8858 - out_1_loss: 1.2714 - out_2_loss: 1.2769 - out_3_loss: 1.2223 - out_4_loss: 1.2524 - out_acc: 0.6851 - out_0_acc: 0.6798 - out_1_acc: 0.5490 - out_2_acc: 0.5474 - out_3_acc: 0.5639 - out_4_acc: 0.5521 - val_loss: 7.3533 - val_out_loss: 0.9681 - val_out_0_loss: 0.9630 - val_out_1_loss: 1.4215 - val_out_2_loss: 1.1623 - val_out_3_loss: 1.2931 - val_out_4_loss: 1.2541 - val_out_acc: 0.7050 - val_out_0_acc: 0.6291 - val_out_1_acc: 0.4902 - val_out_2_acc: 0.5965 - val_out_3_acc: 0.5466 - val_out_4_acc: 0.5445\n",
      "Epoch 37/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 6.8547 - out_loss: 0.9940 - out_0_loss: 0.8585 - out_1_loss: 1.2802 - out_2_loss: 1.2466 - out_3_loss: 1.2479 - out_4_loss: 1.2274 - out_acc: 0.6885 - out_0_acc: 0.6931 - out_1_acc: 0.5518 - out_2_acc: 0.5521 - out_3_acc: 0.5552 - out_4_acc: 0.5580 - val_loss: 7.0737 - val_out_loss: 0.9469 - val_out_0_loss: 0.9704 - val_out_1_loss: 1.2439 - val_out_2_loss: 1.2488 - val_out_3_loss: 1.2031 - val_out_4_loss: 1.1806 - val_out_acc: 0.7028 - val_out_0_acc: 0.6551 - val_out_1_acc: 0.5640 - val_out_2_acc: 0.5640 - val_out_3_acc: 0.5748 - val_out_4_acc: 0.5575\n",
      "Epoch 38/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 6.8132 - out_loss: 0.9847 - out_0_loss: 0.8445 - out_1_loss: 1.2508 - out_2_loss: 1.2575 - out_3_loss: 1.2457 - out_4_loss: 1.2301 - out_acc: 0.6931 - out_0_acc: 0.7015 - out_1_acc: 0.5496 - out_2_acc: 0.5542 - out_3_acc: 0.5446 - out_4_acc: 0.5530 - val_loss: 6.6759 - val_out_loss: 0.8937 - val_out_0_loss: 0.8125 - val_out_1_loss: 1.2684 - val_out_2_loss: 1.1777 - val_out_3_loss: 1.1316 - val_out_4_loss: 1.1277 - val_out_acc: 0.7180 - val_out_0_acc: 0.7115 - val_out_1_acc: 0.5380 - val_out_2_acc: 0.5662 - val_out_3_acc: 0.5705 - val_out_4_acc: 0.5879\n",
      "Epoch 39/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 6.8185 - out_loss: 0.9898 - out_0_loss: 0.8556 - out_1_loss: 1.2511 - out_2_loss: 1.2301 - out_3_loss: 1.2393 - out_4_loss: 1.2525 - out_acc: 0.6931 - out_0_acc: 0.6971 - out_1_acc: 0.5629 - out_2_acc: 0.5713 - out_3_acc: 0.5679 - out_4_acc: 0.5511 - val_loss: 6.8119 - val_out_loss: 0.8975 - val_out_0_loss: 0.9151 - val_out_1_loss: 1.2401 - val_out_2_loss: 1.1653 - val_out_3_loss: 1.1556 - val_out_4_loss: 1.1687 - val_out_acc: 0.6876 - val_out_0_acc: 0.6529 - val_out_1_acc: 0.5380 - val_out_2_acc: 0.5879 - val_out_3_acc: 0.5835 - val_out_4_acc: 0.5466\n",
      "Epoch 40/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 6.6224 - out_loss: 0.9447 - out_0_loss: 0.8371 - out_1_loss: 1.2302 - out_2_loss: 1.2024 - out_3_loss: 1.2000 - out_4_loss: 1.2080 - out_acc: 0.7040 - out_0_acc: 0.6965 - out_1_acc: 0.5546 - out_2_acc: 0.5707 - out_3_acc: 0.5728 - out_4_acc: 0.5617 - val_loss: 6.3164 - val_out_loss: 0.8145 - val_out_0_loss: 0.7432 - val_out_1_loss: 1.1377 - val_out_2_loss: 1.1459 - val_out_3_loss: 1.1369 - val_out_4_loss: 1.0882 - val_out_acc: 0.7245 - val_out_0_acc: 0.7050 - val_out_1_acc: 0.5770 - val_out_2_acc: 0.5879 - val_out_3_acc: 0.5727 - val_out_4_acc: 0.5965\n",
      "Epoch 41/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 6.6622 - out_loss: 0.9557 - out_0_loss: 0.8394 - out_1_loss: 1.2103 - out_2_loss: 1.2355 - out_3_loss: 1.2070 - out_4_loss: 1.2142 - out_acc: 0.6990 - out_0_acc: 0.7046 - out_1_acc: 0.5676 - out_2_acc: 0.5577 - out_3_acc: 0.5604 - out_4_acc: 0.5626 - val_loss: 6.4286 - val_out_loss: 0.8193 - val_out_0_loss: 0.8652 - val_out_1_loss: 1.1087 - val_out_2_loss: 1.1907 - val_out_3_loss: 1.0477 - val_out_4_loss: 1.1425 - val_out_acc: 0.7462 - val_out_0_acc: 0.6681 - val_out_1_acc: 0.6182 - val_out_2_acc: 0.5597 - val_out_3_acc: 0.6095 - val_out_4_acc: 0.5987\n",
      "Epoch 42/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 54s - loss: 6.6480 - out_loss: 0.9558 - out_0_loss: 0.8477 - out_1_loss: 1.2154 - out_2_loss: 1.2228 - out_3_loss: 1.2031 - out_4_loss: 1.2032 - out_acc: 0.7021 - out_0_acc: 0.6944 - out_1_acc: 0.5623 - out_2_acc: 0.5654 - out_3_acc: 0.5635 - out_4_acc: 0.5713 - val_loss: 6.2652 - val_out_loss: 0.8119 - val_out_0_loss: 0.7950 - val_out_1_loss: 1.1050 - val_out_2_loss: 1.1005 - val_out_3_loss: 1.1093 - val_out_4_loss: 1.0955 - val_out_acc: 0.7267 - val_out_0_acc: 0.6985 - val_out_1_acc: 0.6009 - val_out_2_acc: 0.6291 - val_out_3_acc: 0.5879 - val_out_4_acc: 0.5922\n",
      "Epoch 43/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 6.6364 - out_loss: 0.9451 - out_0_loss: 0.8386 - out_1_loss: 1.2006 - out_2_loss: 1.2204 - out_3_loss: 1.2034 - out_4_loss: 1.2283 - out_acc: 0.6928 - out_0_acc: 0.6962 - out_1_acc: 0.5728 - out_2_acc: 0.5673 - out_3_acc: 0.5682 - out_4_acc: 0.5688 - val_loss: 6.4494 - val_out_loss: 0.7967 - val_out_0_loss: 0.8348 - val_out_1_loss: 1.1688 - val_out_2_loss: 1.1191 - val_out_3_loss: 1.1500 - val_out_4_loss: 1.1247 - val_out_acc: 0.7332 - val_out_0_acc: 0.6746 - val_out_1_acc: 0.5727 - val_out_2_acc: 0.5575 - val_out_3_acc: 0.5965 - val_out_4_acc: 0.5705\n",
      "Epoch 44/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 6.6013 - out_loss: 0.9454 - out_0_loss: 0.8317 - out_1_loss: 1.2047 - out_2_loss: 1.2309 - out_3_loss: 1.1949 - out_4_loss: 1.1937 - out_acc: 0.7033 - out_0_acc: 0.7037 - out_1_acc: 0.5728 - out_2_acc: 0.5564 - out_3_acc: 0.5657 - out_4_acc: 0.5639 - val_loss: 6.3085 - val_out_loss: 0.8041 - val_out_0_loss: 0.8195 - val_out_1_loss: 1.1186 - val_out_2_loss: 1.0845 - val_out_3_loss: 1.1085 - val_out_4_loss: 1.1237 - val_out_acc: 0.7419 - val_out_0_acc: 0.7093 - val_out_1_acc: 0.5944 - val_out_2_acc: 0.6009 - val_out_3_acc: 0.5813 - val_out_4_acc: 0.5944\n",
      "Epoch 45/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 6.5727 - out_loss: 0.9359 - out_0_loss: 0.8360 - out_1_loss: 1.2029 - out_2_loss: 1.2048 - out_3_loss: 1.2031 - out_4_loss: 1.1901 - out_acc: 0.7012 - out_0_acc: 0.7037 - out_1_acc: 0.5626 - out_2_acc: 0.5632 - out_3_acc: 0.5632 - out_4_acc: 0.5784 - val_loss: 6.3432 - val_out_loss: 0.8086 - val_out_0_loss: 0.7312 - val_out_1_loss: 1.1576 - val_out_2_loss: 1.1166 - val_out_3_loss: 1.1103 - val_out_4_loss: 1.1678 - val_out_acc: 0.7440 - val_out_0_acc: 0.7354 - val_out_1_acc: 0.5748 - val_out_2_acc: 0.5922 - val_out_3_acc: 0.5770 - val_out_4_acc: 0.5683\n",
      "Epoch 46/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 6.4980 - out_loss: 0.9034 - out_0_loss: 0.8067 - out_1_loss: 1.1892 - out_2_loss: 1.2089 - out_3_loss: 1.1887 - out_4_loss: 1.2012 - out_acc: 0.7145 - out_0_acc: 0.7043 - out_1_acc: 0.5697 - out_2_acc: 0.5710 - out_3_acc: 0.5688 - out_4_acc: 0.5759 - val_loss: 6.2373 - val_out_loss: 0.8017 - val_out_0_loss: 0.8063 - val_out_1_loss: 1.0713 - val_out_2_loss: 1.0794 - val_out_3_loss: 1.1420 - val_out_4_loss: 1.0897 - val_out_acc: 0.7332 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.6074 - val_out_2_acc: 0.5987 - val_out_3_acc: 0.5553 - val_out_4_acc: 0.6030\n",
      "Epoch 47/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 6.3843 - out_loss: 0.8746 - out_0_loss: 0.7795 - out_1_loss: 1.2175 - out_2_loss: 1.1699 - out_3_loss: 1.1800 - out_4_loss: 1.1627 - out_acc: 0.7223 - out_0_acc: 0.7176 - out_1_acc: 0.5570 - out_2_acc: 0.5837 - out_3_acc: 0.5648 - out_4_acc: 0.5781 - val_loss: 6.2993 - val_out_loss: 0.8015 - val_out_0_loss: 0.7859 - val_out_1_loss: 1.1729 - val_out_2_loss: 1.1300 - val_out_3_loss: 1.0570 - val_out_4_loss: 1.1026 - val_out_acc: 0.7310 - val_out_0_acc: 0.7375 - val_out_1_acc: 0.5944 - val_out_2_acc: 0.5705 - val_out_3_acc: 0.6269 - val_out_4_acc: 0.5965\n",
      "Epoch 48/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 6.3777 - out_loss: 0.8876 - out_0_loss: 0.7908 - out_1_loss: 1.1808 - out_2_loss: 1.1772 - out_3_loss: 1.1598 - out_4_loss: 1.1815 - out_acc: 0.7154 - out_0_acc: 0.7142 - out_1_acc: 0.5865 - out_2_acc: 0.5806 - out_3_acc: 0.5899 - out_4_acc: 0.5756 - val_loss: 6.4310 - val_out_loss: 0.8481 - val_out_0_loss: 0.8264 - val_out_1_loss: 1.1235 - val_out_2_loss: 1.1156 - val_out_3_loss: 1.0970 - val_out_4_loss: 1.1659 - val_out_acc: 0.7245 - val_out_0_acc: 0.7028 - val_out_1_acc: 0.6009 - val_out_2_acc: 0.5879 - val_out_3_acc: 0.6095 - val_out_4_acc: 0.5640\n",
      "Epoch 49/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 6.3349 - out_loss: 0.8824 - out_0_loss: 0.7793 - out_1_loss: 1.1887 - out_2_loss: 1.1772 - out_3_loss: 1.1458 - out_4_loss: 1.1616 - out_acc: 0.7226 - out_0_acc: 0.7235 - out_1_acc: 0.5790 - out_2_acc: 0.5713 - out_3_acc: 0.5741 - out_4_acc: 0.5778 - val_loss: 6.2208 - val_out_loss: 0.8223 - val_out_0_loss: 0.8376 - val_out_1_loss: 1.0915 - val_out_2_loss: 1.1027 - val_out_3_loss: 1.0519 - val_out_4_loss: 1.0686 - val_out_acc: 0.7115 - val_out_0_acc: 0.6811 - val_out_1_acc: 0.5922 - val_out_2_acc: 0.5987 - val_out_3_acc: 0.6204 - val_out_4_acc: 0.5987\n",
      "Epoch 50/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 6.2628 - out_loss: 0.8758 - out_0_loss: 0.7816 - out_1_loss: 1.1405 - out_2_loss: 1.1615 - out_3_loss: 1.1457 - out_4_loss: 1.1576 - out_acc: 0.7269 - out_0_acc: 0.7157 - out_1_acc: 0.5815 - out_2_acc: 0.5738 - out_3_acc: 0.5911 - out_4_acc: 0.5821 - val_loss: 6.4943 - val_out_loss: 0.8386 - val_out_0_loss: 0.8160 - val_out_1_loss: 1.1987 - val_out_2_loss: 1.1382 - val_out_3_loss: 1.1708 - val_out_4_loss: 1.0749 - val_out_acc: 0.7093 - val_out_0_acc: 0.6681 - val_out_1_acc: 0.5683 - val_out_2_acc: 0.5662 - val_out_3_acc: 0.5683 - val_out_4_acc: 0.5597\n",
      "Epoch 51/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 6.4062 - out_loss: 0.8897 - out_0_loss: 0.8054 - out_1_loss: 1.1969 - out_2_loss: 1.1603 - out_3_loss: 1.1759 - out_4_loss: 1.1781 - out_acc: 0.7102 - out_0_acc: 0.7027 - out_1_acc: 0.5670 - out_2_acc: 0.5834 - out_3_acc: 0.5763 - out_4_acc: 0.5741 - val_loss: 6.4606 - val_out_loss: 0.8510 - val_out_0_loss: 0.9147 - val_out_1_loss: 1.1102 - val_out_2_loss: 1.0482 - val_out_3_loss: 1.1257 - val_out_4_loss: 1.1550 - val_out_acc: 0.7115 - val_out_0_acc: 0.6725 - val_out_1_acc: 0.5965 - val_out_2_acc: 0.6204 - val_out_3_acc: 0.5835 - val_out_4_acc: 0.5987\n",
      "Epoch 52/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 6.3214 - out_loss: 0.8821 - out_0_loss: 0.7868 - out_1_loss: 1.1643 - out_2_loss: 1.1732 - out_3_loss: 1.1419 - out_4_loss: 1.1731 - out_acc: 0.7145 - out_0_acc: 0.7157 - out_1_acc: 0.5846 - out_2_acc: 0.5809 - out_3_acc: 0.5821 - out_4_acc: 0.5840 - val_loss: 7.0144 - val_out_loss: 0.9240 - val_out_0_loss: 1.0004 - val_out_1_loss: 1.3539 - val_out_2_loss: 1.1591 - val_out_3_loss: 1.1408 - val_out_4_loss: 1.1586 - val_out_acc: 0.6703 - val_out_0_acc: 0.6356 - val_out_1_acc: 0.5098 - val_out_2_acc: 0.5792 - val_out_3_acc: 0.5944 - val_out_4_acc: 0.5618\n",
      "Epoch 53/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 6.2455 - out_loss: 0.8503 - out_0_loss: 0.7694 - out_1_loss: 1.1700 - out_2_loss: 1.1478 - out_3_loss: 1.1549 - out_4_loss: 1.1530 - out_acc: 0.7300 - out_0_acc: 0.7219 - out_1_acc: 0.5992 - out_2_acc: 0.5952 - out_3_acc: 0.5821 - out_4_acc: 0.5914 - val_loss: 6.5505 - val_out_loss: 0.8428 - val_out_0_loss: 0.8475 - val_out_1_loss: 1.2077 - val_out_2_loss: 1.1654 - val_out_3_loss: 1.0897 - val_out_4_loss: 1.1382 - val_out_acc: 0.7419 - val_out_0_acc: 0.7158 - val_out_1_acc: 0.5510 - val_out_2_acc: 0.5792 - val_out_3_acc: 0.6204 - val_out_4_acc: 0.5922\n",
      "Epoch 54/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 6.1711 - out_loss: 0.8240 - out_0_loss: 0.7487 - out_1_loss: 1.1586 - out_2_loss: 1.1513 - out_3_loss: 1.1563 - out_4_loss: 1.1323 - out_acc: 0.7347 - out_0_acc: 0.7291 - out_1_acc: 0.5880 - out_2_acc: 0.5921 - out_3_acc: 0.5902 - out_4_acc: 0.5927 - val_loss: 5.8572 - val_out_loss: 0.7223 - val_out_0_loss: 0.7398 - val_out_1_loss: 1.0213 - val_out_2_loss: 1.0268 - val_out_3_loss: 1.0315 - val_out_4_loss: 1.0837 - val_out_acc: 0.7484 - val_out_0_acc: 0.7158 - val_out_1_acc: 0.6204 - val_out_2_acc: 0.6247 - val_out_3_acc: 0.6204 - val_out_4_acc: 0.5987\n",
      "Epoch 55/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 6.1245 - out_loss: 0.8300 - out_0_loss: 0.7625 - out_1_loss: 1.1542 - out_2_loss: 1.1331 - out_3_loss: 1.1256 - out_4_loss: 1.1190 - out_acc: 0.7337 - out_0_acc: 0.7204 - out_1_acc: 0.5815 - out_2_acc: 0.5859 - out_3_acc: 0.5896 - out_4_acc: 0.5927 - val_loss: 6.4213 - val_out_loss: 0.8628 - val_out_0_loss: 0.8578 - val_out_1_loss: 1.0488 - val_out_2_loss: 1.1173 - val_out_3_loss: 1.1251 - val_out_4_loss: 1.1553 - val_out_acc: 0.7093 - val_out_0_acc: 0.6876 - val_out_1_acc: 0.6030 - val_out_2_acc: 0.5705 - val_out_3_acc: 0.5748 - val_out_4_acc: 0.5683\n",
      "Epoch 56/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 6.1710 - out_loss: 0.8317 - out_0_loss: 0.7826 - out_1_loss: 1.1654 - out_2_loss: 1.1333 - out_3_loss: 1.1320 - out_4_loss: 1.1259 - out_acc: 0.7381 - out_0_acc: 0.7300 - out_1_acc: 0.5806 - out_2_acc: 0.5921 - out_3_acc: 0.5908 - out_4_acc: 0.6029 - val_loss: 5.9500 - val_out_loss: 0.7257 - val_out_0_loss: 0.7521 - val_out_1_loss: 1.0771 - val_out_2_loss: 1.0605 - val_out_3_loss: 1.0702 - val_out_4_loss: 1.0290 - val_out_acc: 0.7766 - val_out_0_acc: 0.7202 - val_out_1_acc: 0.6226 - val_out_2_acc: 0.5944 - val_out_3_acc: 0.6182 - val_out_4_acc: 0.6204\n",
      "Epoch 57/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 6.0555 - out_loss: 0.8200 - out_0_loss: 0.7470 - out_1_loss: 1.1307 - out_2_loss: 1.1374 - out_3_loss: 1.1198 - out_4_loss: 1.1005 - out_acc: 0.7362 - out_0_acc: 0.7337 - out_1_acc: 0.5905 - out_2_acc: 0.5893 - out_3_acc: 0.5868 - out_4_acc: 0.5989 - val_loss: 6.1176 - val_out_loss: 0.7757 - val_out_0_loss: 0.7754 - val_out_1_loss: 1.0507 - val_out_2_loss: 1.0654 - val_out_3_loss: 1.0606 - val_out_4_loss: 1.1477 - val_out_acc: 0.7440 - val_out_0_acc: 0.6985 - val_out_1_acc: 0.6334 - val_out_2_acc: 0.6074 - val_out_3_acc: 0.6204 - val_out_4_acc: 0.5662\n",
      "Epoch 58/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 6.0725 - out_loss: 0.8218 - out_0_loss: 0.7573 - out_1_loss: 1.1318 - out_2_loss: 1.1262 - out_3_loss: 1.1025 - out_4_loss: 1.1329 - out_acc: 0.7409 - out_0_acc: 0.7235 - out_1_acc: 0.6001 - out_2_acc: 0.5945 - out_3_acc: 0.6051 - out_4_acc: 0.5964 - val_loss: 6.4981 - val_out_loss: 0.7881 - val_out_0_loss: 0.8845 - val_out_1_loss: 1.1673 - val_out_2_loss: 1.1404 - val_out_3_loss: 1.1902 - val_out_4_loss: 1.0704 - val_out_acc: 0.7289 - val_out_0_acc: 0.6573 - val_out_1_acc: 0.5553 - val_out_2_acc: 0.5683 - val_out_3_acc: 0.5423 - val_out_4_acc: 0.6226\n",
      "Epoch 59/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 6.0658 - out_loss: 0.8181 - out_0_loss: 0.7374 - out_1_loss: 1.1422 - out_2_loss: 1.1173 - out_3_loss: 1.1268 - out_4_loss: 1.1239 - out_acc: 0.7322 - out_0_acc: 0.7309 - out_1_acc: 0.5902 - out_2_acc: 0.5942 - out_3_acc: 0.5976 - out_4_acc: 0.5918 - val_loss: 5.9885 - val_out_loss: 0.7451 - val_out_0_loss: 0.7452 - val_out_1_loss: 1.0585 - val_out_2_loss: 1.0659 - val_out_3_loss: 1.0645 - val_out_4_loss: 1.0723 - val_out_acc: 0.7462 - val_out_0_acc: 0.7137 - val_out_1_acc: 0.6204 - val_out_2_acc: 0.6030 - val_out_3_acc: 0.5857 - val_out_4_acc: 0.5813\n",
      "Epoch 60/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.9906 - out_loss: 0.8005 - out_0_loss: 0.7410 - out_1_loss: 1.1121 - out_2_loss: 1.1142 - out_3_loss: 1.1003 - out_4_loss: 1.1225 - out_acc: 0.7433 - out_0_acc: 0.7381 - out_1_acc: 0.6007 - out_2_acc: 0.5970 - out_3_acc: 0.6048 - out_4_acc: 0.6007 - val_loss: 5.7408 - val_out_loss: 0.6723 - val_out_0_loss: 0.6150 - val_out_1_loss: 1.0396 - val_out_2_loss: 1.0297 - val_out_3_loss: 1.0806 - val_out_4_loss: 1.0765 - val_out_acc: 0.7701 - val_out_0_acc: 0.7462 - val_out_1_acc: 0.6117 - val_out_2_acc: 0.6182 - val_out_3_acc: 0.5965 - val_out_4_acc: 0.5987\n",
      "Epoch 61/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 5.9331 - out_loss: 0.7897 - out_0_loss: 0.7269 - out_1_loss: 1.0934 - out_2_loss: 1.1108 - out_3_loss: 1.0992 - out_4_loss: 1.1132 - out_acc: 0.7464 - out_0_acc: 0.7368 - out_1_acc: 0.6156 - out_2_acc: 0.5983 - out_3_acc: 0.6119 - out_4_acc: 0.6073 - val_loss: 6.2815 - val_out_loss: 0.7875 - val_out_0_loss: 0.8022 - val_out_1_loss: 1.1096 - val_out_2_loss: 1.0952 - val_out_3_loss: 1.1030 - val_out_4_loss: 1.1353 - val_out_acc: 0.7462 - val_out_0_acc: 0.7050 - val_out_1_acc: 0.5879 - val_out_2_acc: 0.5965 - val_out_3_acc: 0.6117 - val_out_4_acc: 0.6030\n",
      "Epoch 62/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 5.9611 - out_loss: 0.7840 - out_0_loss: 0.7339 - out_1_loss: 1.1322 - out_2_loss: 1.1010 - out_3_loss: 1.1031 - out_4_loss: 1.1070 - out_acc: 0.7430 - out_0_acc: 0.7374 - out_1_acc: 0.5856 - out_2_acc: 0.6007 - out_3_acc: 0.5980 - out_4_acc: 0.6054 - val_loss: 5.8379 - val_out_loss: 0.6971 - val_out_0_loss: 0.7397 - val_out_1_loss: 1.0790 - val_out_2_loss: 1.1117 - val_out_3_loss: 0.9617 - val_out_4_loss: 1.0178 - val_out_acc: 0.7787 - val_out_0_acc: 0.7332 - val_out_1_acc: 0.6334 - val_out_2_acc: 0.5987 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6334\n",
      "Epoch 63/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.9321 - out_loss: 0.7968 - out_0_loss: 0.7117 - out_1_loss: 1.1112 - out_2_loss: 1.1101 - out_3_loss: 1.0831 - out_4_loss: 1.1192 - out_acc: 0.7415 - out_0_acc: 0.7440 - out_1_acc: 0.6017 - out_2_acc: 0.5992 - out_3_acc: 0.6073 - out_4_acc: 0.6029 - val_loss: 6.1134 - val_out_loss: 0.7703 - val_out_0_loss: 0.7639 - val_out_1_loss: 1.1405 - val_out_2_loss: 1.0999 - val_out_3_loss: 1.0199 - val_out_4_loss: 1.0770 - val_out_acc: 0.7419 - val_out_0_acc: 0.7115 - val_out_1_acc: 0.5944 - val_out_2_acc: 0.5965 - val_out_3_acc: 0.6139 - val_out_4_acc: 0.5987\n",
      "Epoch 64/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.9508 - out_loss: 0.7816 - out_0_loss: 0.7289 - out_1_loss: 1.1300 - out_2_loss: 1.1063 - out_3_loss: 1.1046 - out_4_loss: 1.0994 - out_acc: 0.7455 - out_0_acc: 0.7396 - out_1_acc: 0.5942 - out_2_acc: 0.6107 - out_3_acc: 0.5911 - out_4_acc: 0.6038 - val_loss: 6.6114 - val_out_loss: 0.8708 - val_out_0_loss: 0.9319 - val_out_1_loss: 1.1732 - val_out_2_loss: 1.1424 - val_out_3_loss: 1.1529 - val_out_4_loss: 1.0784 - val_out_acc: 0.7158 - val_out_0_acc: 0.6811 - val_out_1_acc: 0.5813 - val_out_2_acc: 0.5879 - val_out_3_acc: 0.5987 - val_out_4_acc: 0.6139\n",
      "Epoch 65/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 5.8971 - out_loss: 0.7775 - out_0_loss: 0.7070 - out_1_loss: 1.1076 - out_2_loss: 1.1140 - out_3_loss: 1.1042 - out_4_loss: 1.0869 - out_acc: 0.7520 - out_0_acc: 0.7427 - out_1_acc: 0.5958 - out_2_acc: 0.5976 - out_3_acc: 0.6069 - out_4_acc: 0.6020 - val_loss: 5.7334 - val_out_loss: 0.6859 - val_out_0_loss: 0.7102 - val_out_1_loss: 1.0206 - val_out_2_loss: 1.0644 - val_out_3_loss: 0.9840 - val_out_4_loss: 1.0414 - val_out_acc: 0.7744 - val_out_0_acc: 0.7332 - val_out_1_acc: 0.6356 - val_out_2_acc: 0.5987 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6117\n",
      "Epoch 66/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.8688 - out_loss: 0.7627 - out_0_loss: 0.6994 - out_1_loss: 1.1103 - out_2_loss: 1.1104 - out_3_loss: 1.0856 - out_4_loss: 1.1003 - out_acc: 0.7576 - out_0_acc: 0.7560 - out_1_acc: 0.6100 - out_2_acc: 0.5945 - out_3_acc: 0.6079 - out_4_acc: 0.6011 - val_loss: 5.8934 - val_out_loss: 0.7080 - val_out_0_loss: 0.7017 - val_out_1_loss: 1.0063 - val_out_2_loss: 1.0869 - val_out_3_loss: 1.0726 - val_out_4_loss: 1.0847 - val_out_acc: 0.7722 - val_out_0_acc: 0.7245 - val_out_1_acc: 0.6443 - val_out_2_acc: 0.6204 - val_out_3_acc: 0.6247 - val_out_4_acc: 0.6139\n",
      "Epoch 67/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.8793 - out_loss: 0.7697 - out_0_loss: 0.7196 - out_1_loss: 1.1025 - out_2_loss: 1.0861 - out_3_loss: 1.0883 - out_4_loss: 1.1131 - out_acc: 0.7529 - out_0_acc: 0.7477 - out_1_acc: 0.6032 - out_2_acc: 0.6091 - out_3_acc: 0.6110 - out_4_acc: 0.6032 - val_loss: 5.8050 - val_out_loss: 0.6862 - val_out_0_loss: 0.6663 - val_out_1_loss: 1.0755 - val_out_2_loss: 1.1067 - val_out_3_loss: 0.9991 - val_out_4_loss: 1.0414 - val_out_acc: 0.7657 - val_out_0_acc: 0.7527 - val_out_1_acc: 0.6291 - val_out_2_acc: 0.5944 - val_out_3_acc: 0.5965 - val_out_4_acc: 0.6117\n",
      "Epoch 68/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 5.7129 - out_loss: 0.7475 - out_0_loss: 0.6820 - out_1_loss: 1.0746 - out_2_loss: 1.0719 - out_3_loss: 1.0662 - out_4_loss: 1.0708 - out_acc: 0.7520 - out_0_acc: 0.7471 - out_1_acc: 0.6166 - out_2_acc: 0.6054 - out_3_acc: 0.6116 - out_4_acc: 0.6135 - val_loss: 5.7096 - val_out_loss: 0.6532 - val_out_0_loss: 0.6733 - val_out_1_loss: 1.0664 - val_out_2_loss: 0.9536 - val_out_3_loss: 1.0719 - val_out_4_loss: 1.0652 - val_out_acc: 0.7809 - val_out_0_acc: 0.7462 - val_out_1_acc: 0.5987 - val_out_2_acc: 0.6790 - val_out_3_acc: 0.6161 - val_out_4_acc: 0.5987\n",
      "Epoch 69/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 5.7687 - out_loss: 0.7513 - out_0_loss: 0.6995 - out_1_loss: 1.0814 - out_2_loss: 1.0714 - out_3_loss: 1.0849 - out_4_loss: 1.0803 - out_acc: 0.7554 - out_0_acc: 0.7461 - out_1_acc: 0.6069 - out_2_acc: 0.6162 - out_3_acc: 0.6175 - out_4_acc: 0.6224 - val_loss: 5.8649 - val_out_loss: 0.7225 - val_out_0_loss: 0.7247 - val_out_1_loss: 1.0165 - val_out_2_loss: 1.0374 - val_out_3_loss: 1.0546 - val_out_4_loss: 1.0771 - val_out_acc: 0.7549 - val_out_0_acc: 0.7310 - val_out_1_acc: 0.6291 - val_out_2_acc: 0.6139 - val_out_3_acc: 0.6291 - val_out_4_acc: 0.6052\n",
      "Epoch 70/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.7389 - out_loss: 0.7445 - out_0_loss: 0.7092 - out_1_loss: 1.0762 - out_2_loss: 1.0698 - out_3_loss: 1.0628 - out_4_loss: 1.0765 - out_acc: 0.7554 - out_0_acc: 0.7449 - out_1_acc: 0.6150 - out_2_acc: 0.6048 - out_3_acc: 0.6187 - out_4_acc: 0.6240 - val_loss: 6.1343 - val_out_loss: 0.7523 - val_out_0_loss: 0.7377 - val_out_1_loss: 1.1489 - val_out_2_loss: 1.0810 - val_out_3_loss: 1.0409 - val_out_4_loss: 1.1308 - val_out_acc: 0.7787 - val_out_0_acc: 0.7440 - val_out_1_acc: 0.5813 - val_out_2_acc: 0.5987 - val_out_3_acc: 0.6139 - val_out_4_acc: 0.5748\n",
      "Epoch 71/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.7540 - out_loss: 0.7504 - out_0_loss: 0.6932 - out_1_loss: 1.0834 - out_2_loss: 1.0785 - out_3_loss: 1.0489 - out_4_loss: 1.0997 - out_acc: 0.7560 - out_0_acc: 0.7579 - out_1_acc: 0.6004 - out_2_acc: 0.6184 - out_3_acc: 0.6184 - out_4_acc: 0.6104 - val_loss: 5.8397 - val_out_loss: 0.6941 - val_out_0_loss: 0.7210 - val_out_1_loss: 1.0850 - val_out_2_loss: 0.9865 - val_out_3_loss: 1.0701 - val_out_4_loss: 1.0519 - val_out_acc: 0.7570 - val_out_0_acc: 0.7354 - val_out_1_acc: 0.5813 - val_out_2_acc: 0.6399 - val_out_3_acc: 0.6334 - val_out_4_acc: 0.6161\n",
      "Epoch 72/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.6797 - out_loss: 0.7337 - out_0_loss: 0.6875 - out_1_loss: 1.0707 - out_2_loss: 1.0551 - out_3_loss: 1.0669 - out_4_loss: 1.0659 - out_acc: 0.7576 - out_0_acc: 0.7576 - out_1_acc: 0.6069 - out_2_acc: 0.6181 - out_3_acc: 0.6017 - out_4_acc: 0.6085 - val_loss: 6.2471 - val_out_loss: 0.7700 - val_out_0_loss: 0.7995 - val_out_1_loss: 1.1512 - val_out_2_loss: 1.1136 - val_out_3_loss: 1.0037 - val_out_4_loss: 1.1618 - val_out_acc: 0.7375 - val_out_0_acc: 0.7050 - val_out_1_acc: 0.5857 - val_out_2_acc: 0.5900 - val_out_3_acc: 0.6377 - val_out_4_acc: 0.5813\n",
      "Epoch 73/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.6988 - out_loss: 0.7290 - out_0_loss: 0.6803 - out_1_loss: 1.0762 - out_2_loss: 1.0705 - out_3_loss: 1.0666 - out_4_loss: 1.0762 - out_acc: 0.7579 - out_0_acc: 0.7604 - out_1_acc: 0.6212 - out_2_acc: 0.6097 - out_3_acc: 0.6131 - out_4_acc: 0.6048 - val_loss: 5.3574 - val_out_loss: 0.6109 - val_out_0_loss: 0.6198 - val_out_1_loss: 1.0801 - val_out_2_loss: 0.9081 - val_out_3_loss: 0.9091 - val_out_4_loss: 1.0173 - val_out_acc: 0.7787 - val_out_0_acc: 0.7874 - val_out_1_acc: 0.6117 - val_out_2_acc: 0.6269 - val_out_3_acc: 0.6573 - val_out_4_acc: 0.6182\n",
      "Epoch 74/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 5.6431 - out_loss: 0.7280 - out_0_loss: 0.6888 - out_1_loss: 1.0692 - out_2_loss: 1.0490 - out_3_loss: 1.0437 - out_4_loss: 1.0645 - out_acc: 0.7647 - out_0_acc: 0.7449 - out_1_acc: 0.6234 - out_2_acc: 0.6249 - out_3_acc: 0.6203 - out_4_acc: 0.6116 - val_loss: 5.7258 - val_out_loss: 0.6802 - val_out_0_loss: 0.7044 - val_out_1_loss: 0.9911 - val_out_2_loss: 1.1087 - val_out_3_loss: 0.9577 - val_out_4_loss: 1.0571 - val_out_acc: 0.7831 - val_out_0_acc: 0.7267 - val_out_1_acc: 0.6638 - val_out_2_acc: 0.5857 - val_out_3_acc: 0.6334 - val_out_4_acc: 0.6074\n",
      "Epoch 75/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 5.6555 - out_loss: 0.7130 - out_0_loss: 0.6842 - out_1_loss: 1.0754 - out_2_loss: 1.0417 - out_3_loss: 1.0675 - out_4_loss: 1.0738 - out_acc: 0.7678 - out_0_acc: 0.7622 - out_1_acc: 0.6283 - out_2_acc: 0.6243 - out_3_acc: 0.6135 - out_4_acc: 0.6144 - val_loss: 5.9067 - val_out_loss: 0.7307 - val_out_0_loss: 0.7245 - val_out_1_loss: 1.1708 - val_out_2_loss: 1.0497 - val_out_3_loss: 1.0016 - val_out_4_loss: 0.9956 - val_out_acc: 0.7549 - val_out_0_acc: 0.7462 - val_out_1_acc: 0.5857 - val_out_2_acc: 0.6117 - val_out_3_acc: 0.6291 - val_out_4_acc: 0.6443\n",
      "Epoch 76/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.5745 - out_loss: 0.7153 - out_0_loss: 0.6596 - out_1_loss: 1.0655 - out_2_loss: 1.0583 - out_3_loss: 1.0310 - out_4_loss: 1.0448 - out_acc: 0.7653 - out_0_acc: 0.7644 - out_1_acc: 0.6162 - out_2_acc: 0.6104 - out_3_acc: 0.6262 - out_4_acc: 0.6283 - val_loss: 5.5089 - val_out_loss: 0.6532 - val_out_0_loss: 0.6766 - val_out_1_loss: 1.0487 - val_out_2_loss: 0.9890 - val_out_3_loss: 0.9500 - val_out_4_loss: 0.9733 - val_out_acc: 0.7657 - val_out_0_acc: 0.7332 - val_out_1_acc: 0.6052 - val_out_2_acc: 0.6377 - val_out_3_acc: 0.6573 - val_out_4_acc: 0.6616\n",
      "Epoch 77/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 5.5147 - out_loss: 0.6917 - out_0_loss: 0.6594 - out_1_loss: 1.0340 - out_2_loss: 1.0538 - out_3_loss: 1.0261 - out_4_loss: 1.0496 - out_acc: 0.7691 - out_0_acc: 0.7650 - out_1_acc: 0.6259 - out_2_acc: 0.6181 - out_3_acc: 0.6339 - out_4_acc: 0.6224 - val_loss: 5.9265 - val_out_loss: 0.7208 - val_out_0_loss: 0.7543 - val_out_1_loss: 1.1065 - val_out_2_loss: 1.0829 - val_out_3_loss: 1.0201 - val_out_4_loss: 1.0072 - val_out_acc: 0.7657 - val_out_0_acc: 0.7267 - val_out_1_acc: 0.5965 - val_out_2_acc: 0.6095 - val_out_3_acc: 0.5987 - val_out_4_acc: 0.6269\n",
      "Epoch 78/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 5.5906 - out_loss: 0.7130 - out_0_loss: 0.6772 - out_1_loss: 1.0640 - out_2_loss: 1.0419 - out_3_loss: 1.0462 - out_4_loss: 1.0483 - out_acc: 0.7737 - out_0_acc: 0.7616 - out_1_acc: 0.6215 - out_2_acc: 0.6234 - out_3_acc: 0.6246 - out_4_acc: 0.6243 - val_loss: 6.0538 - val_out_loss: 0.7690 - val_out_0_loss: 0.8913 - val_out_1_loss: 1.0835 - val_out_2_loss: 0.9891 - val_out_3_loss: 1.0572 - val_out_4_loss: 1.0240 - val_out_acc: 0.7223 - val_out_0_acc: 0.6768 - val_out_1_acc: 0.5683 - val_out_2_acc: 0.6508 - val_out_3_acc: 0.5900 - val_out_4_acc: 0.6117\n",
      "Epoch 79/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 5.5321 - out_loss: 0.6931 - out_0_loss: 0.6506 - out_1_loss: 1.0339 - out_2_loss: 1.0453 - out_3_loss: 1.0583 - out_4_loss: 1.0509 - out_acc: 0.7740 - out_0_acc: 0.7653 - out_1_acc: 0.6265 - out_2_acc: 0.6237 - out_3_acc: 0.6107 - out_4_acc: 0.6215 - val_loss: 5.7370 - val_out_loss: 0.6893 - val_out_0_loss: 0.6796 - val_out_1_loss: 1.0750 - val_out_2_loss: 0.9963 - val_out_3_loss: 1.0571 - val_out_4_loss: 1.0127 - val_out_acc: 0.7614 - val_out_0_acc: 0.7419 - val_out_1_acc: 0.5922 - val_out_2_acc: 0.6269 - val_out_3_acc: 0.5944 - val_out_4_acc: 0.6139\n",
      "Epoch 80/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.4873 - out_loss: 0.6895 - out_0_loss: 0.6315 - out_1_loss: 1.0477 - out_2_loss: 1.0571 - out_3_loss: 1.0303 - out_4_loss: 1.0311 - out_acc: 0.7839 - out_0_acc: 0.7746 - out_1_acc: 0.6209 - out_2_acc: 0.6228 - out_3_acc: 0.6259 - out_4_acc: 0.6107 - val_loss: 6.0688 - val_out_loss: 0.7811 - val_out_0_loss: 0.8709 - val_out_1_loss: 1.0146 - val_out_2_loss: 1.0741 - val_out_3_loss: 1.0512 - val_out_4_loss: 1.0367 - val_out_acc: 0.7549 - val_out_0_acc: 0.6551 - val_out_1_acc: 0.6356 - val_out_2_acc: 0.5857 - val_out_3_acc: 0.6182 - val_out_4_acc: 0.6356\n",
      "Epoch 81/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 5.4634 - out_loss: 0.6902 - out_0_loss: 0.6424 - out_1_loss: 1.0362 - out_2_loss: 1.0550 - out_3_loss: 0.9986 - out_4_loss: 1.0411 - out_acc: 0.7759 - out_0_acc: 0.7753 - out_1_acc: 0.6228 - out_2_acc: 0.6153 - out_3_acc: 0.6466 - out_4_acc: 0.6262 - val_loss: 5.9159 - val_out_loss: 0.7340 - val_out_0_loss: 0.7684 - val_out_1_loss: 1.0694 - val_out_2_loss: 0.9937 - val_out_3_loss: 1.0550 - val_out_4_loss: 1.0612 - val_out_acc: 0.7419 - val_out_0_acc: 0.7223 - val_out_1_acc: 0.5835 - val_out_2_acc: 0.6117 - val_out_3_acc: 0.5835 - val_out_4_acc: 0.6161\n",
      "Epoch 82/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 5.4596 - out_loss: 0.6939 - out_0_loss: 0.6513 - out_1_loss: 1.0289 - out_2_loss: 1.0344 - out_3_loss: 1.0030 - out_4_loss: 1.0482 - out_acc: 0.7709 - out_0_acc: 0.7616 - out_1_acc: 0.6302 - out_2_acc: 0.6246 - out_3_acc: 0.6305 - out_4_acc: 0.6215 - val_loss: 5.7355 - val_out_loss: 0.6803 - val_out_0_loss: 0.7070 - val_out_1_loss: 1.0122 - val_out_2_loss: 0.9959 - val_out_3_loss: 1.0729 - val_out_4_loss: 1.0401 - val_out_acc: 0.7527 - val_out_0_acc: 0.7332 - val_out_1_acc: 0.6377 - val_out_2_acc: 0.6377 - val_out_3_acc: 0.5987 - val_out_4_acc: 0.6117\n",
      "Epoch 83/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 5.5800 - out_loss: 0.7173 - out_0_loss: 0.6617 - out_1_loss: 1.0683 - out_2_loss: 1.0269 - out_3_loss: 1.0509 - out_4_loss: 1.0549 - out_acc: 0.7613 - out_0_acc: 0.7619 - out_1_acc: 0.6159 - out_2_acc: 0.6259 - out_3_acc: 0.6113 - out_4_acc: 0.6252 - val_loss: 5.6692 - val_out_loss: 0.6231 - val_out_0_loss: 0.6377 - val_out_1_loss: 1.0755 - val_out_2_loss: 0.9966 - val_out_3_loss: 0.9661 - val_out_4_loss: 1.1458 - val_out_acc: 0.8048 - val_out_0_acc: 0.7549 - val_out_1_acc: 0.6247 - val_out_2_acc: 0.6486 - val_out_3_acc: 0.6443 - val_out_4_acc: 0.6009\n",
      "Epoch 84/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.4625 - out_loss: 0.6683 - out_0_loss: 0.6279 - out_1_loss: 1.0632 - out_2_loss: 1.0227 - out_3_loss: 1.0409 - out_4_loss: 1.0394 - out_acc: 0.7833 - out_0_acc: 0.7706 - out_1_acc: 0.6218 - out_2_acc: 0.6314 - out_3_acc: 0.6327 - out_4_acc: 0.6302 - val_loss: 5.4549 - val_out_loss: 0.5883 - val_out_0_loss: 0.5813 - val_out_1_loss: 1.0341 - val_out_2_loss: 1.0506 - val_out_3_loss: 0.9486 - val_out_4_loss: 1.0362 - val_out_acc: 0.8026 - val_out_0_acc: 0.7679 - val_out_1_acc: 0.6161 - val_out_2_acc: 0.6117 - val_out_3_acc: 0.6161 - val_out_4_acc: 0.6334\n",
      "Epoch 85/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 5.3527 - out_loss: 0.6727 - out_0_loss: 0.6348 - out_1_loss: 1.0049 - out_2_loss: 1.0184 - out_3_loss: 0.9967 - out_4_loss: 1.0253 - out_acc: 0.7818 - out_0_acc: 0.7731 - out_1_acc: 0.6336 - out_2_acc: 0.6330 - out_3_acc: 0.6379 - out_4_acc: 0.6342 - val_loss: 5.7097 - val_out_loss: 0.7064 - val_out_0_loss: 0.7908 - val_out_1_loss: 1.0653 - val_out_2_loss: 1.0171 - val_out_3_loss: 0.9726 - val_out_4_loss: 0.9316 - val_out_acc: 0.7527 - val_out_0_acc: 0.7158 - val_out_1_acc: 0.6226 - val_out_2_acc: 0.6009 - val_out_3_acc: 0.6334 - val_out_4_acc: 0.6616\n",
      "Epoch 86/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 5.3564 - out_loss: 0.6607 - out_0_loss: 0.6231 - out_1_loss: 1.0248 - out_2_loss: 1.0229 - out_3_loss: 1.0158 - out_4_loss: 1.0091 - out_acc: 0.7812 - out_0_acc: 0.7669 - out_1_acc: 0.6305 - out_2_acc: 0.6175 - out_3_acc: 0.6271 - out_4_acc: 0.6296 - val_loss: 5.4221 - val_out_loss: 0.6095 - val_out_0_loss: 0.7055 - val_out_1_loss: 1.0004 - val_out_2_loss: 0.9542 - val_out_3_loss: 0.9432 - val_out_4_loss: 0.9947 - val_out_acc: 0.7961 - val_out_0_acc: 0.7202 - val_out_1_acc: 0.5922 - val_out_2_acc: 0.6573 - val_out_3_acc: 0.6291 - val_out_4_acc: 0.6226\n",
      "Epoch 87/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 5.4518 - out_loss: 0.6761 - out_0_loss: 0.6515 - out_1_loss: 1.0442 - out_2_loss: 1.0414 - out_3_loss: 1.0133 - out_4_loss: 1.0253 - out_acc: 0.7833 - out_0_acc: 0.7759 - out_1_acc: 0.6200 - out_2_acc: 0.6218 - out_3_acc: 0.6314 - out_4_acc: 0.6321 - val_loss: 5.9270 - val_out_loss: 0.7733 - val_out_0_loss: 0.8153 - val_out_1_loss: 1.1560 - val_out_2_loss: 1.0377 - val_out_3_loss: 0.9204 - val_out_4_loss: 0.9898 - val_out_acc: 0.7419 - val_out_0_acc: 0.7050 - val_out_1_acc: 0.5857 - val_out_2_acc: 0.6508 - val_out_3_acc: 0.6486 - val_out_4_acc: 0.6377\n",
      "Epoch 88/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 5.4775 - out_loss: 0.6875 - out_0_loss: 0.6526 - out_1_loss: 1.0452 - out_2_loss: 1.0313 - out_3_loss: 1.0244 - out_4_loss: 1.0366 - out_acc: 0.7802 - out_0_acc: 0.7706 - out_1_acc: 0.6262 - out_2_acc: 0.6252 - out_3_acc: 0.6240 - out_4_acc: 0.6181 - val_loss: 5.4254 - val_out_loss: 0.6272 - val_out_0_loss: 0.7153 - val_out_1_loss: 0.9619 - val_out_2_loss: 0.9868 - val_out_3_loss: 0.9206 - val_out_4_loss: 0.9988 - val_out_acc: 0.7766 - val_out_0_acc: 0.7245 - val_out_1_acc: 0.6226 - val_out_2_acc: 0.6291 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.5965\n",
      "Epoch 89/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 61s - loss: 5.3620 - out_loss: 0.6639 - out_0_loss: 0.6302 - out_1_loss: 1.0340 - out_2_loss: 1.0099 - out_3_loss: 0.9946 - out_4_loss: 1.0293 - out_acc: 0.7818 - out_0_acc: 0.7734 - out_1_acc: 0.6348 - out_2_acc: 0.6286 - out_3_acc: 0.6420 - out_4_acc: 0.6262 - val_loss: 5.0377 - val_out_loss: 0.5643 - val_out_0_loss: 0.5868 - val_out_1_loss: 0.9260 - val_out_2_loss: 0.9209 - val_out_3_loss: 0.9319 - val_out_4_loss: 0.9083 - val_out_acc: 0.8069 - val_out_0_acc: 0.7918 - val_out_1_acc: 0.6768 - val_out_2_acc: 0.6703 - val_out_3_acc: 0.6486 - val_out_4_acc: 0.6443\n",
      "Epoch 90/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 5.2903 - out_loss: 0.6500 - out_0_loss: 0.6180 - out_1_loss: 1.0002 - out_2_loss: 0.9995 - out_3_loss: 1.0094 - out_4_loss: 1.0131 - out_acc: 0.7908 - out_0_acc: 0.7784 - out_1_acc: 0.6352 - out_2_acc: 0.6383 - out_3_acc: 0.6262 - out_4_acc: 0.6426 - val_loss: 5.5975 - val_out_loss: 0.6345 - val_out_0_loss: 0.6739 - val_out_1_loss: 1.0948 - val_out_2_loss: 0.9755 - val_out_3_loss: 0.9831 - val_out_4_loss: 1.0142 - val_out_acc: 0.7809 - val_out_0_acc: 0.7527 - val_out_1_acc: 0.6377 - val_out_2_acc: 0.6443 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6312\n",
      "Epoch 91/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 5.3328 - out_loss: 0.6538 - out_0_loss: 0.6144 - out_1_loss: 1.0059 - out_2_loss: 1.0256 - out_3_loss: 0.9948 - out_4_loss: 1.0382 - out_acc: 0.7939 - out_0_acc: 0.7799 - out_1_acc: 0.6441 - out_2_acc: 0.6178 - out_3_acc: 0.6370 - out_4_acc: 0.6240 - val_loss: 5.4035 - val_out_loss: 0.6263 - val_out_0_loss: 0.6315 - val_out_1_loss: 1.0072 - val_out_2_loss: 1.0346 - val_out_3_loss: 0.9056 - val_out_4_loss: 0.9844 - val_out_acc: 0.7831 - val_out_0_acc: 0.7657 - val_out_1_acc: 0.6312 - val_out_2_acc: 0.6312 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6247\n",
      "Epoch 92/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 5.3138 - out_loss: 0.6647 - out_0_loss: 0.6281 - out_1_loss: 1.0207 - out_2_loss: 1.0088 - out_3_loss: 1.0014 - out_4_loss: 0.9900 - out_acc: 0.7846 - out_0_acc: 0.7743 - out_1_acc: 0.6339 - out_2_acc: 0.6398 - out_3_acc: 0.6361 - out_4_acc: 0.6410 - val_loss: 6.0203 - val_out_loss: 0.7124 - val_out_0_loss: 0.8069 - val_out_1_loss: 1.2608 - val_out_2_loss: 1.0345 - val_out_3_loss: 0.9288 - val_out_4_loss: 1.0386 - val_out_acc: 0.7701 - val_out_0_acc: 0.7289 - val_out_1_acc: 0.5683 - val_out_2_acc: 0.6095 - val_out_3_acc: 0.6594 - val_out_4_acc: 0.6247\n",
      "Epoch 93/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 5.2518 - out_loss: 0.6521 - out_0_loss: 0.6060 - out_1_loss: 1.0083 - out_2_loss: 1.0087 - out_3_loss: 0.9688 - out_4_loss: 1.0080 - out_acc: 0.7867 - out_0_acc: 0.7796 - out_1_acc: 0.6364 - out_2_acc: 0.6290 - out_3_acc: 0.6538 - out_4_acc: 0.6358 - val_loss: 5.3654 - val_out_loss: 0.5964 - val_out_0_loss: 0.6585 - val_out_1_loss: 0.8988 - val_out_2_loss: 1.0310 - val_out_3_loss: 1.0031 - val_out_4_loss: 0.9652 - val_out_acc: 0.7983 - val_out_0_acc: 0.7527 - val_out_1_acc: 0.6920 - val_out_2_acc: 0.6443 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6399\n",
      "Epoch 94/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 5.3197 - out_loss: 0.6554 - out_0_loss: 0.6252 - out_1_loss: 1.0223 - out_2_loss: 1.0041 - out_3_loss: 1.0108 - out_4_loss: 1.0018 - out_acc: 0.7846 - out_0_acc: 0.7799 - out_1_acc: 0.6339 - out_2_acc: 0.6386 - out_3_acc: 0.6379 - out_4_acc: 0.6398 - val_loss: 5.5158 - val_out_loss: 0.6141 - val_out_0_loss: 0.6550 - val_out_1_loss: 0.9061 - val_out_2_loss: 1.1115 - val_out_3_loss: 1.0254 - val_out_4_loss: 0.9854 - val_out_acc: 0.7939 - val_out_0_acc: 0.7809 - val_out_1_acc: 0.6399 - val_out_2_acc: 0.6074 - val_out_3_acc: 0.6161 - val_out_4_acc: 0.6161\n",
      "Epoch 95/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 63s - loss: 5.3027 - out_loss: 0.6440 - out_0_loss: 0.6293 - out_1_loss: 1.0210 - out_2_loss: 0.9971 - out_3_loss: 1.0042 - out_4_loss: 1.0071 - out_acc: 0.7874 - out_0_acc: 0.7709 - out_1_acc: 0.6296 - out_2_acc: 0.6373 - out_3_acc: 0.6435 - out_4_acc: 0.6370 - val_loss: 5.1906 - val_out_loss: 0.5884 - val_out_0_loss: 0.6639 - val_out_1_loss: 0.9236 - val_out_2_loss: 0.9422 - val_out_3_loss: 0.9429 - val_out_4_loss: 0.9242 - val_out_acc: 0.8091 - val_out_0_acc: 0.7505 - val_out_1_acc: 0.6681 - val_out_2_acc: 0.6464 - val_out_3_acc: 0.6508 - val_out_4_acc: 0.6508\n",
      "Epoch 96/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 5.2606 - out_loss: 0.6450 - out_0_loss: 0.6321 - out_1_loss: 0.9972 - out_2_loss: 0.9813 - out_3_loss: 0.9965 - out_4_loss: 1.0086 - out_acc: 0.7920 - out_0_acc: 0.7756 - out_1_acc: 0.6479 - out_2_acc: 0.6476 - out_3_acc: 0.6373 - out_4_acc: 0.6401 - val_loss: 5.4334 - val_out_loss: 0.6529 - val_out_0_loss: 0.7631 - val_out_1_loss: 0.9509 - val_out_2_loss: 0.9174 - val_out_3_loss: 0.9381 - val_out_4_loss: 0.9959 - val_out_acc: 0.7983 - val_out_0_acc: 0.7419 - val_out_1_acc: 0.6681 - val_out_2_acc: 0.6508 - val_out_3_acc: 0.6573 - val_out_4_acc: 0.6551\n",
      "Epoch 97/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 5.2039 - out_loss: 0.6372 - out_0_loss: 0.6080 - out_1_loss: 0.9956 - out_2_loss: 1.0040 - out_3_loss: 0.9669 - out_4_loss: 0.9922 - out_acc: 0.7939 - out_0_acc: 0.7821 - out_1_acc: 0.6404 - out_2_acc: 0.6383 - out_3_acc: 0.6525 - out_4_acc: 0.6407 - val_loss: 5.6254 - val_out_loss: 0.7134 - val_out_0_loss: 0.7471 - val_out_1_loss: 1.0494 - val_out_2_loss: 0.9994 - val_out_3_loss: 0.9427 - val_out_4_loss: 0.9507 - val_out_acc: 0.7592 - val_out_0_acc: 0.7137 - val_out_1_acc: 0.6247 - val_out_2_acc: 0.6443 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6508\n",
      "Epoch 98/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 5.2295 - out_loss: 0.6371 - out_0_loss: 0.6110 - out_1_loss: 1.0109 - out_2_loss: 0.9735 - out_3_loss: 0.9876 - out_4_loss: 1.0094 - out_acc: 0.7926 - out_0_acc: 0.7750 - out_1_acc: 0.6373 - out_2_acc: 0.6383 - out_3_acc: 0.6417 - out_4_acc: 0.6348 - val_loss: 5.2981 - val_out_loss: 0.5766 - val_out_0_loss: 0.5670 - val_out_1_loss: 0.9751 - val_out_2_loss: 0.9962 - val_out_3_loss: 0.9921 - val_out_4_loss: 0.9814 - val_out_acc: 0.7939 - val_out_0_acc: 0.7896 - val_out_1_acc: 0.6226 - val_out_2_acc: 0.6117 - val_out_3_acc: 0.6334 - val_out_4_acc: 0.5965\n",
      "Epoch 99/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 5.2163 - out_loss: 0.6295 - out_0_loss: 0.6200 - out_1_loss: 1.0064 - out_2_loss: 0.9925 - out_3_loss: 0.9872 - out_4_loss: 0.9808 - out_acc: 0.8001 - out_0_acc: 0.7790 - out_1_acc: 0.6358 - out_2_acc: 0.6348 - out_3_acc: 0.6392 - out_4_acc: 0.6507 - val_loss: 5.4503 - val_out_loss: 0.6294 - val_out_0_loss: 0.6912 - val_out_1_loss: 1.0633 - val_out_2_loss: 0.9878 - val_out_3_loss: 0.9393 - val_out_4_loss: 0.9236 - val_out_acc: 0.7809 - val_out_0_acc: 0.7636 - val_out_1_acc: 0.6030 - val_out_2_acc: 0.6464 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6443\n",
      "Epoch 100/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 5.1361 - out_loss: 0.6238 - out_0_loss: 0.6111 - out_1_loss: 0.9891 - out_2_loss: 0.9704 - out_3_loss: 0.9614 - out_4_loss: 0.9802 - out_acc: 0.7982 - out_0_acc: 0.7892 - out_1_acc: 0.6429 - out_2_acc: 0.6451 - out_3_acc: 0.6606 - out_4_acc: 0.6485 - val_loss: 5.3646 - val_out_loss: 0.6257 - val_out_0_loss: 0.7140 - val_out_1_loss: 0.9170 - val_out_2_loss: 1.0129 - val_out_3_loss: 0.9628 - val_out_4_loss: 0.9198 - val_out_acc: 0.7787 - val_out_0_acc: 0.7375 - val_out_1_acc: 0.7050 - val_out_2_acc: 0.6443 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6594\n",
      "Epoch 101/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 61s - loss: 5.1250 - out_loss: 0.6188 - out_0_loss: 0.5880 - out_1_loss: 0.9998 - out_2_loss: 0.9703 - out_3_loss: 0.9731 - out_4_loss: 0.9750 - out_acc: 0.7926 - out_0_acc: 0.7901 - out_1_acc: 0.6401 - out_2_acc: 0.6513 - out_3_acc: 0.6565 - out_4_acc: 0.6463 - val_loss: 5.2030 - val_out_loss: 0.5944 - val_out_0_loss: 0.6319 - val_out_1_loss: 0.9092 - val_out_2_loss: 0.9390 - val_out_3_loss: 0.9095 - val_out_4_loss: 1.0130 - val_out_acc: 0.8113 - val_out_0_acc: 0.7484 - val_out_1_acc: 0.6638 - val_out_2_acc: 0.6573 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6161\n",
      "Epoch 102/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 5.1713 - out_loss: 0.6284 - out_0_loss: 0.6090 - out_1_loss: 0.9828 - out_2_loss: 0.9948 - out_3_loss: 0.9773 - out_4_loss: 0.9791 - out_acc: 0.7945 - out_0_acc: 0.7774 - out_1_acc: 0.6410 - out_2_acc: 0.6386 - out_3_acc: 0.6472 - out_4_acc: 0.6531 - val_loss: 5.5084 - val_out_loss: 0.6052 - val_out_0_loss: 0.7122 - val_out_1_loss: 1.0227 - val_out_2_loss: 0.9140 - val_out_3_loss: 1.0454 - val_out_4_loss: 0.9909 - val_out_acc: 0.7896 - val_out_0_acc: 0.7570 - val_out_1_acc: 0.6139 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6226 - val_out_4_acc: 0.6117\n",
      "Epoch 103/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 61s - loss: 5.2281 - out_loss: 0.6406 - out_0_loss: 0.6214 - out_1_loss: 1.0150 - out_2_loss: 0.9894 - out_3_loss: 0.9652 - out_4_loss: 0.9965 - out_acc: 0.7936 - out_0_acc: 0.7777 - out_1_acc: 0.6404 - out_2_acc: 0.6345 - out_3_acc: 0.6562 - out_4_acc: 0.6410 - val_loss: 5.1301 - val_out_loss: 0.5317 - val_out_0_loss: 0.6009 - val_out_1_loss: 1.1051 - val_out_2_loss: 0.9278 - val_out_3_loss: 0.8690 - val_out_4_loss: 0.8925 - val_out_acc: 0.8308 - val_out_0_acc: 0.8026 - val_out_1_acc: 0.6117 - val_out_2_acc: 0.6443 - val_out_3_acc: 0.7093 - val_out_4_acc: 0.6790\n",
      "Epoch 104/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 60s - loss: 5.0974 - out_loss: 0.5982 - out_0_loss: 0.5800 - out_1_loss: 1.0023 - out_2_loss: 0.9629 - out_3_loss: 0.9730 - out_4_loss: 0.9810 - out_acc: 0.8134 - out_0_acc: 0.7954 - out_1_acc: 0.6268 - out_2_acc: 0.6482 - out_3_acc: 0.6417 - out_4_acc: 0.6491 - val_loss: 5.0098 - val_out_loss: 0.5110 - val_out_0_loss: 0.5436 - val_out_1_loss: 0.9523 - val_out_2_loss: 0.8949 - val_out_3_loss: 0.9008 - val_out_4_loss: 1.0089 - val_out_acc: 0.8330 - val_out_0_acc: 0.8091 - val_out_1_acc: 0.6768 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6074\n",
      "Epoch 105/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 5.1660 - out_loss: 0.6179 - out_0_loss: 0.5967 - out_1_loss: 0.9939 - out_2_loss: 0.9849 - out_3_loss: 0.9906 - out_4_loss: 0.9821 - out_acc: 0.8010 - out_0_acc: 0.7923 - out_1_acc: 0.6541 - out_2_acc: 0.6519 - out_3_acc: 0.6457 - out_4_acc: 0.6429 - val_loss: 5.1422 - val_out_loss: 0.5678 - val_out_0_loss: 0.5979 - val_out_1_loss: 0.9385 - val_out_2_loss: 0.9268 - val_out_3_loss: 0.9316 - val_out_4_loss: 0.9760 - val_out_acc: 0.8048 - val_out_0_acc: 0.7744 - val_out_1_acc: 0.6334 - val_out_2_acc: 0.6399 - val_out_3_acc: 0.6464 - val_out_4_acc: 0.6226\n",
      "Epoch 106/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.1536 - out_loss: 0.6116 - out_0_loss: 0.5864 - out_1_loss: 0.9891 - out_2_loss: 0.9886 - out_3_loss: 0.9829 - out_4_loss: 0.9951 - out_acc: 0.7982 - out_0_acc: 0.7954 - out_1_acc: 0.6342 - out_2_acc: 0.6296 - out_3_acc: 0.6516 - out_4_acc: 0.6448 - val_loss: 5.1950 - val_out_loss: 0.5227 - val_out_0_loss: 0.5760 - val_out_1_loss: 1.0277 - val_out_2_loss: 0.9959 - val_out_3_loss: 0.8860 - val_out_4_loss: 0.9810 - val_out_acc: 0.8156 - val_out_0_acc: 0.7701 - val_out_1_acc: 0.5965 - val_out_2_acc: 0.6464 - val_out_3_acc: 0.6659 - val_out_4_acc: 0.6312\n",
      "Epoch 107/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.0433 - out_loss: 0.5889 - out_0_loss: 0.5734 - out_1_loss: 0.9811 - out_2_loss: 0.9691 - out_3_loss: 0.9666 - out_4_loss: 0.9641 - out_acc: 0.8118 - out_0_acc: 0.7985 - out_1_acc: 0.6479 - out_2_acc: 0.6531 - out_3_acc: 0.6528 - out_4_acc: 0.6466 - val_loss: 5.3171 - val_out_loss: 0.5949 - val_out_0_loss: 0.6366 - val_out_1_loss: 1.0358 - val_out_2_loss: 0.9254 - val_out_3_loss: 0.9377 - val_out_4_loss: 0.9762 - val_out_acc: 0.7874 - val_out_0_acc: 0.7766 - val_out_1_acc: 0.6356 - val_out_2_acc: 0.6443 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6182\n",
      "Epoch 108/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 5.0659 - out_loss: 0.6100 - out_0_loss: 0.5974 - out_1_loss: 0.9722 - out_2_loss: 0.9668 - out_3_loss: 0.9496 - out_4_loss: 0.9700 - out_acc: 0.8069 - out_0_acc: 0.7942 - out_1_acc: 0.6525 - out_2_acc: 0.6503 - out_3_acc: 0.6562 - out_4_acc: 0.6534 - val_loss: 5.5257 - val_out_loss: 0.6471 - val_out_0_loss: 0.6559 - val_out_1_loss: 0.9684 - val_out_2_loss: 0.9886 - val_out_3_loss: 1.0104 - val_out_4_loss: 1.0366 - val_out_acc: 0.7961 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.6573 - val_out_2_acc: 0.6052 - val_out_3_acc: 0.6269 - val_out_4_acc: 0.6161\n",
      "Epoch 109/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 61s - loss: 5.1269 - out_loss: 0.6052 - out_0_loss: 0.5913 - out_1_loss: 1.0028 - out_2_loss: 0.9601 - out_3_loss: 0.9821 - out_4_loss: 0.9855 - out_acc: 0.7973 - out_0_acc: 0.7870 - out_1_acc: 0.6367 - out_2_acc: 0.6507 - out_3_acc: 0.6466 - out_4_acc: 0.6407 - val_loss: 4.9647 - val_out_loss: 0.4959 - val_out_0_loss: 0.5115 - val_out_1_loss: 0.9001 - val_out_2_loss: 0.9296 - val_out_3_loss: 1.0134 - val_out_4_loss: 0.9177 - val_out_acc: 0.8373 - val_out_0_acc: 0.7983 - val_out_1_acc: 0.6399 - val_out_2_acc: 0.6746 - val_out_3_acc: 0.6681 - val_out_4_acc: 0.6529\n",
      "Epoch 110/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 5.0207 - out_loss: 0.5906 - out_0_loss: 0.5726 - out_1_loss: 0.9602 - out_2_loss: 0.9663 - out_3_loss: 0.9539 - out_4_loss: 0.9771 - out_acc: 0.8091 - out_0_acc: 0.7963 - out_1_acc: 0.6516 - out_2_acc: 0.6361 - out_3_acc: 0.6550 - out_4_acc: 0.6460 - val_loss: 5.7286 - val_out_loss: 0.6960 - val_out_0_loss: 0.8025 - val_out_1_loss: 1.0441 - val_out_2_loss: 0.9652 - val_out_3_loss: 0.9808 - val_out_4_loss: 1.0132 - val_out_acc: 0.7614 - val_out_0_acc: 0.6876 - val_out_1_acc: 0.5922 - val_out_2_acc: 0.6161 - val_out_3_acc: 0.6204 - val_out_4_acc: 0.6117\n",
      "Epoch 111/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 5.1262 - out_loss: 0.6179 - out_0_loss: 0.5975 - out_1_loss: 0.9819 - out_2_loss: 0.9698 - out_3_loss: 0.9795 - out_4_loss: 0.9797 - out_acc: 0.8001 - out_0_acc: 0.7815 - out_1_acc: 0.6373 - out_2_acc: 0.6454 - out_3_acc: 0.6460 - out_4_acc: 0.6398 - val_loss: 5.4170 - val_out_loss: 0.6114 - val_out_0_loss: 0.6116 - val_out_1_loss: 0.9959 - val_out_2_loss: 0.9977 - val_out_3_loss: 0.9574 - val_out_4_loss: 1.0285 - val_out_acc: 0.7939 - val_out_0_acc: 0.7787 - val_out_1_acc: 0.6269 - val_out_2_acc: 0.6312 - val_out_3_acc: 0.6421 - val_out_4_acc: 0.6269\n",
      "Epoch 112/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.9200 - out_loss: 0.5714 - out_0_loss: 0.5467 - out_1_loss: 0.9577 - out_2_loss: 0.9439 - out_3_loss: 0.9354 - out_4_loss: 0.9648 - out_acc: 0.8162 - out_0_acc: 0.8044 - out_1_acc: 0.6559 - out_2_acc: 0.6534 - out_3_acc: 0.6553 - out_4_acc: 0.6627 - val_loss: 5.0485 - val_out_loss: 0.5172 - val_out_0_loss: 0.5133 - val_out_1_loss: 1.0100 - val_out_2_loss: 0.9554 - val_out_3_loss: 0.9250 - val_out_4_loss: 0.9278 - val_out_acc: 0.8069 - val_out_0_acc: 0.8069 - val_out_1_acc: 0.6443 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6833 - val_out_4_acc: 0.6464\n",
      "Epoch 113/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.8792 - out_loss: 0.5743 - out_0_loss: 0.5603 - out_1_loss: 0.9225 - out_2_loss: 0.9536 - out_3_loss: 0.9282 - out_4_loss: 0.9403 - out_acc: 0.8112 - out_0_acc: 0.7963 - out_1_acc: 0.6643 - out_2_acc: 0.6522 - out_3_acc: 0.6643 - out_4_acc: 0.6503 - val_loss: 5.0744 - val_out_loss: 0.5388 - val_out_0_loss: 0.6628 - val_out_1_loss: 0.9058 - val_out_2_loss: 0.9443 - val_out_3_loss: 0.9125 - val_out_4_loss: 0.9093 - val_out_acc: 0.8200 - val_out_0_acc: 0.7657 - val_out_1_acc: 0.6551 - val_out_2_acc: 0.6399 - val_out_3_acc: 0.6573 - val_out_4_acc: 0.6464\n",
      "Epoch 114/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.8429 - out_loss: 0.5499 - out_0_loss: 0.5389 - out_1_loss: 0.9489 - out_2_loss: 0.9321 - out_3_loss: 0.9325 - out_4_loss: 0.9407 - out_acc: 0.8224 - out_0_acc: 0.8125 - out_1_acc: 0.6534 - out_2_acc: 0.6606 - out_3_acc: 0.6606 - out_4_acc: 0.6640 - val_loss: 5.0047 - val_out_loss: 0.5477 - val_out_0_loss: 0.5639 - val_out_1_loss: 0.8932 - val_out_2_loss: 0.9680 - val_out_3_loss: 0.9124 - val_out_4_loss: 0.9214 - val_out_acc: 0.8004 - val_out_0_acc: 0.7852 - val_out_1_acc: 0.6616 - val_out_2_acc: 0.6551 - val_out_3_acc: 0.6377 - val_out_4_acc: 0.6594\n",
      "Epoch 115/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.8723 - out_loss: 0.5626 - out_0_loss: 0.5534 - out_1_loss: 0.9390 - out_2_loss: 0.9373 - out_3_loss: 0.9317 - out_4_loss: 0.9483 - out_acc: 0.8165 - out_0_acc: 0.7991 - out_1_acc: 0.6559 - out_2_acc: 0.6553 - out_3_acc: 0.6717 - out_4_acc: 0.6565 - val_loss: 5.2951 - val_out_loss: 0.5819 - val_out_0_loss: 0.6408 - val_out_1_loss: 1.0267 - val_out_2_loss: 0.9261 - val_out_3_loss: 0.9347 - val_out_4_loss: 0.9753 - val_out_acc: 0.8091 - val_out_0_acc: 0.7614 - val_out_1_acc: 0.6161 - val_out_2_acc: 0.6573 - val_out_3_acc: 0.6443 - val_out_4_acc: 0.6182\n",
      "Epoch 116/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.9846 - out_loss: 0.5880 - out_0_loss: 0.5892 - out_1_loss: 0.9901 - out_2_loss: 0.9459 - out_3_loss: 0.9385 - out_4_loss: 0.9329 - out_acc: 0.8140 - out_0_acc: 0.7939 - out_1_acc: 0.6445 - out_2_acc: 0.6634 - out_3_acc: 0.6723 - out_4_acc: 0.6745 - val_loss: 5.4214 - val_out_loss: 0.6384 - val_out_0_loss: 0.7287 - val_out_1_loss: 0.9735 - val_out_2_loss: 0.9551 - val_out_3_loss: 0.9650 - val_out_4_loss: 0.9460 - val_out_acc: 0.7852 - val_out_0_acc: 0.7245 - val_out_1_acc: 0.6356 - val_out_2_acc: 0.6399 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6508\n",
      "Epoch 117/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 59s - loss: 4.8338 - out_loss: 0.5551 - out_0_loss: 0.5440 - out_1_loss: 0.9438 - out_2_loss: 0.9306 - out_3_loss: 0.9260 - out_4_loss: 0.9343 - out_acc: 0.8199 - out_0_acc: 0.8019 - out_1_acc: 0.6584 - out_2_acc: 0.6674 - out_3_acc: 0.6547 - out_4_acc: 0.6631 - val_loss: 5.1928 - val_out_loss: 0.5991 - val_out_0_loss: 0.6209 - val_out_1_loss: 1.0052 - val_out_2_loss: 0.9001 - val_out_3_loss: 0.9488 - val_out_4_loss: 0.9132 - val_out_acc: 0.7961 - val_out_0_acc: 0.7679 - val_out_1_acc: 0.6204 - val_out_2_acc: 0.6659 - val_out_3_acc: 0.6356 - val_out_4_acc: 0.6594\n",
      "Epoch 118/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.8776 - out_loss: 0.5614 - out_0_loss: 0.5728 - out_1_loss: 0.9472 - out_2_loss: 0.9293 - out_3_loss: 0.9368 - out_4_loss: 0.9302 - out_acc: 0.8187 - out_0_acc: 0.7970 - out_1_acc: 0.6615 - out_2_acc: 0.6590 - out_3_acc: 0.6603 - out_4_acc: 0.6593 - val_loss: 5.0403 - val_out_loss: 0.5190 - val_out_0_loss: 0.5794 - val_out_1_loss: 0.9621 - val_out_2_loss: 0.8898 - val_out_3_loss: 0.9442 - val_out_4_loss: 0.9461 - val_out_acc: 0.8156 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.6594 - val_out_2_acc: 0.6421 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6508\n",
      "Epoch 119/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.9433 - out_loss: 0.5643 - out_0_loss: 0.5601 - out_1_loss: 0.9656 - out_2_loss: 0.9444 - out_3_loss: 0.9550 - out_4_loss: 0.9540 - out_acc: 0.8140 - out_0_acc: 0.7936 - out_1_acc: 0.6522 - out_2_acc: 0.6584 - out_3_acc: 0.6541 - out_4_acc: 0.6522 - val_loss: 5.2756 - val_out_loss: 0.6226 - val_out_0_loss: 0.6637 - val_out_1_loss: 0.9371 - val_out_2_loss: 0.9563 - val_out_3_loss: 0.9098 - val_out_4_loss: 0.9774 - val_out_acc: 0.7722 - val_out_0_acc: 0.7440 - val_out_1_acc: 0.6334 - val_out_2_acc: 0.6399 - val_out_3_acc: 0.6334 - val_out_4_acc: 0.6443\n",
      "Epoch 120/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.8405 - out_loss: 0.5653 - out_0_loss: 0.5524 - out_1_loss: 0.9207 - out_2_loss: 0.9396 - out_3_loss: 0.9398 - out_4_loss: 0.9228 - out_acc: 0.8230 - out_0_acc: 0.7998 - out_1_acc: 0.6717 - out_2_acc: 0.6609 - out_3_acc: 0.6655 - out_4_acc: 0.6736 - val_loss: 5.2434 - val_out_loss: 0.5625 - val_out_0_loss: 0.6766 - val_out_1_loss: 1.0253 - val_out_2_loss: 0.9285 - val_out_3_loss: 0.9430 - val_out_4_loss: 0.9000 - val_out_acc: 0.7983 - val_out_0_acc: 0.7419 - val_out_1_acc: 0.6377 - val_out_2_acc: 0.6703 - val_out_3_acc: 0.6681 - val_out_4_acc: 0.6833\n",
      "Epoch 121/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.8600 - out_loss: 0.5552 - out_0_loss: 0.5405 - out_1_loss: 0.9481 - out_2_loss: 0.9192 - out_3_loss: 0.9448 - out_4_loss: 0.9522 - out_acc: 0.8249 - out_0_acc: 0.8056 - out_1_acc: 0.6503 - out_2_acc: 0.6643 - out_3_acc: 0.6668 - out_4_acc: 0.6637 - val_loss: 5.1880 - val_out_loss: 0.6317 - val_out_0_loss: 0.7694 - val_out_1_loss: 0.8477 - val_out_2_loss: 0.8992 - val_out_3_loss: 0.8944 - val_out_4_loss: 0.9403 - val_out_acc: 0.7896 - val_out_0_acc: 0.7245 - val_out_1_acc: 0.7007 - val_out_2_acc: 0.6811 - val_out_3_acc: 0.6508 - val_out_4_acc: 0.6486\n",
      "Epoch 122/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.7886 - out_loss: 0.5590 - out_0_loss: 0.5417 - out_1_loss: 0.9281 - out_2_loss: 0.9070 - out_3_loss: 0.9273 - out_4_loss: 0.9256 - out_acc: 0.8196 - out_0_acc: 0.8047 - out_1_acc: 0.6736 - out_2_acc: 0.6742 - out_3_acc: 0.6717 - out_4_acc: 0.6689 - val_loss: 5.2784 - val_out_loss: 0.6017 - val_out_0_loss: 0.6392 - val_out_1_loss: 0.9413 - val_out_2_loss: 1.0302 - val_out_3_loss: 0.8855 - val_out_4_loss: 0.9716 - val_out_acc: 0.8069 - val_out_0_acc: 0.7701 - val_out_1_acc: 0.6464 - val_out_2_acc: 0.6508 - val_out_3_acc: 0.6659 - val_out_4_acc: 0.6508\n",
      "Epoch 123/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 4.8315 - out_loss: 0.5605 - out_0_loss: 0.5496 - out_1_loss: 0.9433 - out_2_loss: 0.9257 - out_3_loss: 0.9171 - out_4_loss: 0.9354 - out_acc: 0.8171 - out_0_acc: 0.8001 - out_1_acc: 0.6631 - out_2_acc: 0.6652 - out_3_acc: 0.6711 - out_4_acc: 0.6631 - val_loss: 4.8661 - val_out_loss: 0.5335 - val_out_0_loss: 0.5818 - val_out_1_loss: 0.9337 - val_out_2_loss: 0.9294 - val_out_3_loss: 0.8442 - val_out_4_loss: 0.8508 - val_out_acc: 0.8308 - val_out_0_acc: 0.7874 - val_out_1_acc: 0.6486 - val_out_2_acc: 0.6334 - val_out_3_acc: 0.6941 - val_out_4_acc: 0.6941\n",
      "Epoch 124/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.7589 - out_loss: 0.5366 - out_0_loss: 0.5408 - out_1_loss: 0.9080 - out_2_loss: 0.9172 - out_3_loss: 0.9216 - out_4_loss: 0.9347 - out_acc: 0.8221 - out_0_acc: 0.8069 - out_1_acc: 0.6696 - out_2_acc: 0.6596 - out_3_acc: 0.6686 - out_4_acc: 0.6531 - val_loss: 5.0153 - val_out_loss: 0.5163 - val_out_0_loss: 0.5474 - val_out_1_loss: 0.9262 - val_out_2_loss: 0.9126 - val_out_3_loss: 0.8973 - val_out_4_loss: 1.0170 - val_out_acc: 0.8221 - val_out_0_acc: 0.8048 - val_out_1_acc: 0.6573 - val_out_2_acc: 0.6356 - val_out_3_acc: 0.6876 - val_out_4_acc: 0.6377\n",
      "Epoch 125/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 58s - loss: 4.7202 - out_loss: 0.5286 - out_0_loss: 0.5199 - out_1_loss: 0.9142 - out_2_loss: 0.9308 - out_3_loss: 0.8989 - out_4_loss: 0.9277 - out_acc: 0.8264 - out_0_acc: 0.8100 - out_1_acc: 0.6637 - out_2_acc: 0.6581 - out_3_acc: 0.6785 - out_4_acc: 0.6665 - val_loss: 4.8336 - val_out_loss: 0.5294 - val_out_0_loss: 0.5560 - val_out_1_loss: 0.8826 - val_out_2_loss: 0.9237 - val_out_3_loss: 0.8814 - val_out_4_loss: 0.8692 - val_out_acc: 0.8330 - val_out_0_acc: 0.8004 - val_out_1_acc: 0.6768 - val_out_2_acc: 0.6529 - val_out_3_acc: 0.6833 - val_out_4_acc: 0.6855\n",
      "Epoch 126/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 4.8057 - out_loss: 0.5483 - out_0_loss: 0.5431 - out_1_loss: 0.9423 - out_2_loss: 0.9225 - out_3_loss: 0.9234 - out_4_loss: 0.9262 - out_acc: 0.8258 - out_0_acc: 0.8032 - out_1_acc: 0.6600 - out_2_acc: 0.6720 - out_3_acc: 0.6739 - out_4_acc: 0.6748 - val_loss: 4.9040 - val_out_loss: 0.5647 - val_out_0_loss: 0.6744 - val_out_1_loss: 0.9223 - val_out_2_loss: 0.8444 - val_out_3_loss: 0.8807 - val_out_4_loss: 0.8234 - val_out_acc: 0.7983 - val_out_0_acc: 0.7354 - val_out_1_acc: 0.6703 - val_out_2_acc: 0.7028 - val_out_3_acc: 0.6898 - val_out_4_acc: 0.6941\n",
      "Epoch 127/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 4.8688 - out_loss: 0.5540 - out_0_loss: 0.5603 - out_1_loss: 0.9529 - out_2_loss: 0.9405 - out_3_loss: 0.9259 - out_4_loss: 0.9352 - out_acc: 0.8236 - out_0_acc: 0.8025 - out_1_acc: 0.6603 - out_2_acc: 0.6575 - out_3_acc: 0.6624 - out_4_acc: 0.6528 - val_loss: 4.8466 - val_out_loss: 0.5175 - val_out_0_loss: 0.6426 - val_out_1_loss: 0.8711 - val_out_2_loss: 0.8703 - val_out_3_loss: 0.8784 - val_out_4_loss: 0.8749 - val_out_acc: 0.8134 - val_out_0_acc: 0.7440 - val_out_1_acc: 0.6811 - val_out_2_acc: 0.6703 - val_out_3_acc: 0.6638 - val_out_4_acc: 0.6746\n",
      "Epoch 128/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 4.8930 - out_loss: 0.5711 - out_0_loss: 0.5797 - out_1_loss: 0.9350 - out_2_loss: 0.9221 - out_3_loss: 0.9378 - out_4_loss: 0.9473 - out_acc: 0.8236 - out_0_acc: 0.7985 - out_1_acc: 0.6637 - out_2_acc: 0.6674 - out_3_acc: 0.6655 - out_4_acc: 0.6603 - val_loss: 5.0772 - val_out_loss: 0.5763 - val_out_0_loss: 0.7119 - val_out_1_loss: 0.9144 - val_out_2_loss: 0.8948 - val_out_3_loss: 0.9028 - val_out_4_loss: 0.8759 - val_out_acc: 0.7961 - val_out_0_acc: 0.7267 - val_out_1_acc: 0.6443 - val_out_2_acc: 0.6681 - val_out_3_acc: 0.6811 - val_out_4_acc: 0.6421\n",
      "Epoch 129/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 57s - loss: 4.7662 - out_loss: 0.5479 - out_0_loss: 0.5413 - out_1_loss: 0.9161 - out_2_loss: 0.9193 - out_3_loss: 0.9161 - out_4_loss: 0.9256 - out_acc: 0.8196 - out_0_acc: 0.8075 - out_1_acc: 0.6727 - out_2_acc: 0.6692 - out_3_acc: 0.6686 - out_4_acc: 0.6538 - val_loss: 4.9550 - val_out_loss: 0.4919 - val_out_0_loss: 0.5127 - val_out_1_loss: 0.9982 - val_out_2_loss: 0.8811 - val_out_3_loss: 0.9402 - val_out_4_loss: 0.9348 - val_out_acc: 0.8308 - val_out_0_acc: 0.7983 - val_out_1_acc: 0.6399 - val_out_2_acc: 0.6594 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6443\n",
      "Epoch 130/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 4.9109 - out_loss: 0.5721 - out_0_loss: 0.5827 - out_1_loss: 0.9276 - out_2_loss: 0.9509 - out_3_loss: 0.9334 - out_4_loss: 0.9443 - out_acc: 0.8208 - out_0_acc: 0.7967 - out_1_acc: 0.6720 - out_2_acc: 0.6578 - out_3_acc: 0.6708 - out_4_acc: 0.6578 - val_loss: 4.8982 - val_out_loss: 0.5352 - val_out_0_loss: 0.5760 - val_out_1_loss: 0.8948 - val_out_2_loss: 0.9037 - val_out_3_loss: 0.8595 - val_out_4_loss: 0.9352 - val_out_acc: 0.8243 - val_out_0_acc: 0.7961 - val_out_1_acc: 0.6725 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6703 - val_out_4_acc: 0.6529\n",
      "Epoch 131/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 4.7036 - out_loss: 0.5442 - out_0_loss: 0.5529 - out_1_loss: 0.9058 - out_2_loss: 0.9011 - out_3_loss: 0.8949 - out_4_loss: 0.9046 - out_acc: 0.8267 - out_0_acc: 0.8044 - out_1_acc: 0.6733 - out_2_acc: 0.6686 - out_3_acc: 0.6671 - out_4_acc: 0.6689 - val_loss: 4.8013 - val_out_loss: 0.5050 - val_out_0_loss: 0.5612 - val_out_1_loss: 0.9142 - val_out_2_loss: 0.9132 - val_out_3_loss: 0.8385 - val_out_4_loss: 0.8790 - val_out_acc: 0.8178 - val_out_0_acc: 0.7918 - val_out_1_acc: 0.6464 - val_out_2_acc: 0.6312 - val_out_3_acc: 0.7137 - val_out_4_acc: 0.6855\n",
      "Epoch 132/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 56s - loss: 4.8620 - out_loss: 0.5607 - out_0_loss: 0.5599 - out_1_loss: 0.9486 - out_2_loss: 0.9177 - out_3_loss: 0.9216 - out_4_loss: 0.9534 - out_acc: 0.8239 - out_0_acc: 0.8029 - out_1_acc: 0.6479 - out_2_acc: 0.6674 - out_3_acc: 0.6612 - out_4_acc: 0.6500 - val_loss: 5.1327 - val_out_loss: 0.5623 - val_out_0_loss: 0.6175 - val_out_1_loss: 0.9561 - val_out_2_loss: 0.9200 - val_out_3_loss: 0.8656 - val_out_4_loss: 1.0080 - val_out_acc: 0.8113 - val_out_0_acc: 0.7766 - val_out_1_acc: 0.6616 - val_out_2_acc: 0.6616 - val_out_3_acc: 0.6703 - val_out_4_acc: 0.6399\n",
      "Epoch 133/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 4.6889 - out_loss: 0.5201 - out_0_loss: 0.5292 - out_1_loss: 0.9247 - out_2_loss: 0.9193 - out_3_loss: 0.8788 - out_4_loss: 0.9169 - out_acc: 0.8379 - out_0_acc: 0.8094 - out_1_acc: 0.6662 - out_2_acc: 0.6668 - out_3_acc: 0.6767 - out_4_acc: 0.6767 - val_loss: 4.8019 - val_out_loss: 0.5005 - val_out_0_loss: 0.4898 - val_out_1_loss: 0.8895 - val_out_2_loss: 0.8999 - val_out_3_loss: 0.8643 - val_out_4_loss: 0.9679 - val_out_acc: 0.8243 - val_out_0_acc: 0.8048 - val_out_1_acc: 0.6551 - val_out_2_acc: 0.6616 - val_out_3_acc: 0.6811 - val_out_4_acc: 0.6356\n",
      "Epoch 134/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 55s - loss: 4.7206 - out_loss: 0.5455 - out_0_loss: 0.5307 - out_1_loss: 0.9133 - out_2_loss: 0.8708 - out_3_loss: 0.9382 - out_4_loss: 0.9221 - out_acc: 0.8329 - out_0_acc: 0.8109 - out_1_acc: 0.6742 - out_2_acc: 0.6801 - out_3_acc: 0.6643 - out_4_acc: 0.6801 - val_loss: 5.1332 - val_out_loss: 0.6112 - val_out_0_loss: 0.6235 - val_out_1_loss: 0.9090 - val_out_2_loss: 0.9465 - val_out_3_loss: 0.8783 - val_out_4_loss: 0.9615 - val_out_acc: 0.7983 - val_out_0_acc: 0.7722 - val_out_1_acc: 0.6659 - val_out_2_acc: 0.6486 - val_out_3_acc: 0.6638 - val_out_4_acc: 0.6486\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       aerosol       0.26      1.00      0.41        20\n",
      "   altocumulos       0.71      0.88      0.78        88\n",
      "   altostratos       1.00      0.39      0.56        36\n",
      "cieloDespejado       0.81      0.57      0.67        23\n",
      "  cirrocumulos       0.42      0.38      0.40        21\n",
      "        cirros       0.82      0.94      0.88       145\n",
      "  cirrostratos       0.90      0.90      0.90        68\n",
      "       cumulos       0.83      0.77      0.80        71\n",
      "estratocumulos       0.78      0.83      0.80       142\n",
      "      estratos       0.67      0.09      0.16       108\n",
      "     multinube       0.68      0.77      0.72       189\n",
      "  nimbostratos       0.50      0.25      0.33        12\n",
      "\n",
      "      accuracy                           0.72       923\n",
      "     macro avg       0.70      0.65      0.62       923\n",
      "  weighted avg       0.74      0.72      0.69       923\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFMCAYAAABF3IJ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_name = 'cropnetv3'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "                        make_cropnetv3, model_name=model_name,\n",
    "                        model_dir=model_dir, n_outputs=6, batch_size=32)\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'], n_outputs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bQ034zTtDp_",
    "colab_type": "text"
   },
   "source": [
    "#### RandomCropnet v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XOPwT-rts5bD",
    "colab_type": "code",
    "outputId": "17a0e8e8-f92c-4cdd-9350-a3af82c30f49",
    "executionInfo": {
     "status": "error",
     "timestamp": 1.577273865059E12,
     "user_tz": -60.0,
     "elapsed": 1495421.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking train and validation tests\n",
      "Building the network\n",
      "Compiling the network\n",
      "Layers: 365\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_338 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_341 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_344 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_347 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_350 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_353 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_356 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_359 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_362 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_365 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_368 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_371 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_374 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_377 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_380 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_383 ( [(None, None, None,  0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_740 (Conv2D)             (None, 256, 256, 32) 896         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_338[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_341[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_344[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_347[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_350[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_696 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_353[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_700 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_356[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_704 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_359[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_708 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_362[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_712 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_365[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_716 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_368[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_720 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_371[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_724 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_374[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_728 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_377[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_732 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_380[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_736 (Conv2D)             (None, None, None, 4 112         tf_op_layer_strided_slice_383[0][\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_741 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_740[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, None, None, 4 16          conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, None, None, 4 16          conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, None, None, 4 16          conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, None, None, 4 16          conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_644 (BatchN (None, None, None, 4 16          conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_648 (BatchN (None, None, None, 4 16          conv2d_696[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, None, None, 4 16          conv2d_700[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, None, None, 4 16          conv2d_704[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, None, None, 4 16          conv2d_708[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_664 (BatchN (None, None, None, 4 16          conv2d_712[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_668 (BatchN (None, None, None, 4 16          conv2d_716[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_672 (BatchN (None, None, None, 4 16          conv2d_720[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_676 (BatchN (None, None, None, 4 16          conv2d_724[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_680 (BatchN (None, None, None, 4 16          conv2d_728[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_684 (BatchN (None, None, None, 4 16          conv2d_732[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_688 (BatchN (None, None, None, 4 16          conv2d_736[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_592 (MaxPooling2D (None, 128, 128, 32) 0           conv2d_741[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, None, None, 4 0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, None, None, 4 0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, None, None, 4 0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, None, None, 4 0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, None, None, 4 0           batch_normalization_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_648 (Activation)     (None, None, None, 4 0           batch_normalization_648[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_652 (Activation)     (None, None, None, 4 0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_656 (Activation)     (None, None, None, 4 0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_660 (Activation)     (None, None, None, 4 0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_664 (Activation)     (None, None, None, 4 0           batch_normalization_664[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_668 (Activation)     (None, None, None, 4 0           batch_normalization_668[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_672 (Activation)     (None, None, None, 4 0           batch_normalization_672[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_676 (Activation)     (None, None, None, 4 0           batch_normalization_676[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_680 (Activation)     (None, None, None, 4 0           batch_normalization_680[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_684 (Activation)     (None, None, None, 4 0           batch_normalization_684[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_688 (Activation)     (None, None, None, 4 0           batch_normalization_688[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_742 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_592[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_528 (MaxPooling2D (None, None, None, 4 0           activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_532 (MaxPooling2D (None, None, None, 4 0           activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_536 (MaxPooling2D (None, None, None, 4 0           activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_540 (MaxPooling2D (None, None, None, 4 0           activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_544 (MaxPooling2D (None, None, None, 4 0           activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_548 (MaxPooling2D (None, None, None, 4 0           activation_648[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_552 (MaxPooling2D (None, None, None, 4 0           activation_652[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_556 (MaxPooling2D (None, None, None, 4 0           activation_656[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_560 (MaxPooling2D (None, None, None, 4 0           activation_660[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_564 (MaxPooling2D (None, None, None, 4 0           activation_664[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_568 (MaxPooling2D (None, None, None, 4 0           activation_668[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_572 (MaxPooling2D (None, None, None, 4 0           activation_672[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_576 (MaxPooling2D (None, None, None, 4 0           activation_676[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_580 (MaxPooling2D (None, None, None, 4 0           activation_680[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_584 (MaxPooling2D (None, None, None, 4 0           activation_684[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_588 (MaxPooling2D (None, None, None, 4 0           activation_688[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_743 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_742[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, None, None, 4 148         max_pooling2d_528[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, None, None, 4 148         max_pooling2d_532[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, None, None, 4 148         max_pooling2d_536[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, None, None, 4 148         max_pooling2d_540[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, None, None, 4 148         max_pooling2d_544[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_697 (Conv2D)             (None, None, None, 4 148         max_pooling2d_548[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_701 (Conv2D)             (None, None, None, 4 148         max_pooling2d_552[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_705 (Conv2D)             (None, None, None, 4 148         max_pooling2d_556[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_709 (Conv2D)             (None, None, None, 4 148         max_pooling2d_560[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_713 (Conv2D)             (None, None, None, 4 148         max_pooling2d_564[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_717 (Conv2D)             (None, None, None, 4 148         max_pooling2d_568[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_721 (Conv2D)             (None, None, None, 4 148         max_pooling2d_572[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_725 (Conv2D)             (None, None, None, 4 148         max_pooling2d_576[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_729 (Conv2D)             (None, None, None, 4 148         max_pooling2d_580[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_733 (Conv2D)             (None, None, None, 4 148         max_pooling2d_584[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_737 (Conv2D)             (None, None, None, 4 148         max_pooling2d_588[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_593 (MaxPooling2D (None, 64, 64, 64)   0           conv2d_743[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, None, None, 4 16          conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, None, None, 4 16          conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, None, None, 4 16          conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, None, None, 4 16          conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_645 (BatchN (None, None, None, 4 16          conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_649 (BatchN (None, None, None, 4 16          conv2d_697[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, None, None, 4 16          conv2d_701[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, None, None, 4 16          conv2d_705[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, None, None, 4 16          conv2d_709[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_665 (BatchN (None, None, None, 4 16          conv2d_713[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_669 (BatchN (None, None, None, 4 16          conv2d_717[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_673 (BatchN (None, None, None, 4 16          conv2d_721[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_677 (BatchN (None, None, None, 4 16          conv2d_725[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_681 (BatchN (None, None, None, 4 16          conv2d_729[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_685 (BatchN (None, None, None, 4 16          conv2d_733[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_689 (BatchN (None, None, None, 4 16          conv2d_737[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_744 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_593[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, None, None, 4 0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, None, None, 4 0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, None, None, 4 0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, None, None, 4 0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, None, None, 4 0           batch_normalization_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_649 (Activation)     (None, None, None, 4 0           batch_normalization_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_653 (Activation)     (None, None, None, 4 0           batch_normalization_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_657 (Activation)     (None, None, None, 4 0           batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_661 (Activation)     (None, None, None, 4 0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_665 (Activation)     (None, None, None, 4 0           batch_normalization_665[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_669 (Activation)     (None, None, None, 4 0           batch_normalization_669[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_673 (Activation)     (None, None, None, 4 0           batch_normalization_673[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_677 (Activation)     (None, None, None, 4 0           batch_normalization_677[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_681 (Activation)     (None, None, None, 4 0           batch_normalization_681[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_685 (Activation)     (None, None, None, 4 0           batch_normalization_685[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_689 (Activation)     (None, None, None, 4 0           batch_normalization_689[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_745 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_744[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_529 (MaxPooling2D (None, None, None, 4 0           activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_533 (MaxPooling2D (None, None, None, 4 0           activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_537 (MaxPooling2D (None, None, None, 4 0           activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_541 (MaxPooling2D (None, None, None, 4 0           activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_545 (MaxPooling2D (None, None, None, 4 0           activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_549 (MaxPooling2D (None, None, None, 4 0           activation_649[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_553 (MaxPooling2D (None, None, None, 4 0           activation_653[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_557 (MaxPooling2D (None, None, None, 4 0           activation_657[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_561 (MaxPooling2D (None, None, None, 4 0           activation_661[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_565 (MaxPooling2D (None, None, None, 4 0           activation_665[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_569 (MaxPooling2D (None, None, None, 4 0           activation_669[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_573 (MaxPooling2D (None, None, None, 4 0           activation_673[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_577 (MaxPooling2D (None, None, None, 4 0           activation_677[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_581 (MaxPooling2D (None, None, None, 4 0           activation_681[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_585 (MaxPooling2D (None, None, None, 4 0           activation_685[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_589 (MaxPooling2D (None, None, None, 4 0           activation_689[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_594 (MaxPooling2D (None, 32, 32, 128)  0           conv2d_745[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, None, None, 8 296         max_pooling2d_529[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, None, None, 8 296         max_pooling2d_533[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, None, None, 8 296         max_pooling2d_537[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, None, None, 8 296         max_pooling2d_541[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, None, None, 8 296         max_pooling2d_545[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_698 (Conv2D)             (None, None, None, 8 296         max_pooling2d_549[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_702 (Conv2D)             (None, None, None, 8 296         max_pooling2d_553[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_706 (Conv2D)             (None, None, None, 8 296         max_pooling2d_557[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_710 (Conv2D)             (None, None, None, 8 296         max_pooling2d_561[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_714 (Conv2D)             (None, None, None, 8 296         max_pooling2d_565[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_718 (Conv2D)             (None, None, None, 8 296         max_pooling2d_569[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_722 (Conv2D)             (None, None, None, 8 296         max_pooling2d_573[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_726 (Conv2D)             (None, None, None, 8 296         max_pooling2d_577[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_730 (Conv2D)             (None, None, None, 8 296         max_pooling2d_581[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_734 (Conv2D)             (None, None, None, 8 296         max_pooling2d_585[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_738 (Conv2D)             (None, None, None, 8 296         max_pooling2d_589[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_746 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_594[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, None, None, 8 32          conv2d_678[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, None, None, 8 32          conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, None, None, 8 32          conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, None, None, 8 32          conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_646 (BatchN (None, None, None, 8 32          conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, None, None, 8 32          conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, None, None, 8 32          conv2d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, None, None, 8 32          conv2d_706[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_662 (BatchN (None, None, None, 8 32          conv2d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_666 (BatchN (None, None, None, 8 32          conv2d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_670 (BatchN (None, None, None, 8 32          conv2d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_674 (BatchN (None, None, None, 8 32          conv2d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_678 (BatchN (None, None, None, 8 32          conv2d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_682 (BatchN (None, None, None, 8 32          conv2d_730[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_686 (BatchN (None, None, None, 8 32          conv2d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_690 (BatchN (None, None, None, 8 32          conv2d_738[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_747 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_746[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, None, None, 8 0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, None, None, 8 0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, None, None, 8 0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, None, None, 8 0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, None, None, 8 0           batch_normalization_646[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_650 (Activation)     (None, None, None, 8 0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_654 (Activation)     (None, None, None, 8 0           batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_658 (Activation)     (None, None, None, 8 0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_662 (Activation)     (None, None, None, 8 0           batch_normalization_662[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_666 (Activation)     (None, None, None, 8 0           batch_normalization_666[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_670 (Activation)     (None, None, None, 8 0           batch_normalization_670[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_674 (Activation)     (None, None, None, 8 0           batch_normalization_674[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_678 (Activation)     (None, None, None, 8 0           batch_normalization_678[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_682 (Activation)     (None, None, None, 8 0           batch_normalization_682[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_686 (Activation)     (None, None, None, 8 0           batch_normalization_686[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_690 (Activation)     (None, None, None, 8 0           batch_normalization_690[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_748 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_747[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_530 (MaxPooling2D (None, None, None, 8 0           activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_534 (MaxPooling2D (None, None, None, 8 0           activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_538 (MaxPooling2D (None, None, None, 8 0           activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_542 (MaxPooling2D (None, None, None, 8 0           activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_546 (MaxPooling2D (None, None, None, 8 0           activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_550 (MaxPooling2D (None, None, None, 8 0           activation_650[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_554 (MaxPooling2D (None, None, None, 8 0           activation_654[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_558 (MaxPooling2D (None, None, None, 8 0           activation_658[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_562 (MaxPooling2D (None, None, None, 8 0           activation_662[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_566 (MaxPooling2D (None, None, None, 8 0           activation_666[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_570 (MaxPooling2D (None, None, None, 8 0           activation_670[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_574 (MaxPooling2D (None, None, None, 8 0           activation_674[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_578 (MaxPooling2D (None, None, None, 8 0           activation_678[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_582 (MaxPooling2D (None, None, None, 8 0           activation_682[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_586 (MaxPooling2D (None, None, None, 8 0           activation_686[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_590 (MaxPooling2D (None, None, None, 8 0           activation_690[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_595 (MaxPooling2D (None, 16, 16, 256)  0           conv2d_748[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_530[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_534[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_538[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_542[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_546[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_699 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_550[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_703 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_554[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_707 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_558[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_711 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_562[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_715 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_566[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_719 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_570[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_723 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_574[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_727 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_578[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_731 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_582[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_735 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_586[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_739 (Conv2D)             (None, None, None, 1 1168        max_pooling2d_590[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_749 (Conv2D)             (None, 16, 16, 256)  590080      max_pooling2d_595[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, None, None, 1 64          conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, None, None, 1 64          conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, None, None, 1 64          conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_643 (BatchN (None, None, None, 1 64          conv2d_691[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_647 (BatchN (None, None, None, 1 64          conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, None, None, 1 64          conv2d_699[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, None, None, 1 64          conv2d_703[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, None, None, 1 64          conv2d_707[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_663 (BatchN (None, None, None, 1 64          conv2d_711[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_667 (BatchN (None, None, None, 1 64          conv2d_715[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_671 (BatchN (None, None, None, 1 64          conv2d_719[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_675 (BatchN (None, None, None, 1 64          conv2d_723[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_679 (BatchN (None, None, None, 1 64          conv2d_727[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_683 (BatchN (None, None, None, 1 64          conv2d_731[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_687 (BatchN (None, None, None, 1 64          conv2d_735[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_691 (BatchN (None, None, None, 1 64          conv2d_739[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_750 (Conv2D)             (None, 16, 16, 256)  590080      conv2d_749[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, None, None, 1 0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, None, None, 1 0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, None, None, 1 0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, None, None, 1 0           batch_normalization_643[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, None, None, 1 0           batch_normalization_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_651 (Activation)     (None, None, None, 1 0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_655 (Activation)     (None, None, None, 1 0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_659 (Activation)     (None, None, None, 1 0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_663 (Activation)     (None, None, None, 1 0           batch_normalization_663[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_667 (Activation)     (None, None, None, 1 0           batch_normalization_667[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_671 (Activation)     (None, None, None, 1 0           batch_normalization_671[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_675 (Activation)     (None, None, None, 1 0           batch_normalization_675[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_679 (Activation)     (None, None, None, 1 0           batch_normalization_679[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_683 (Activation)     (None, None, None, 1 0           batch_normalization_683[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_687 (Activation)     (None, None, None, 1 0           batch_normalization_687[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_691 (Activation)     (None, None, None, 1 0           batch_normalization_691[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_751 (Conv2D)             (None, 16, 16, 256)  590080      conv2d_750[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_531 (MaxPooling2D (None, None, None, 1 0           activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_535 (MaxPooling2D (None, None, None, 1 0           activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_539 (MaxPooling2D (None, None, None, 1 0           activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_543 (MaxPooling2D (None, None, None, 1 0           activation_643[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_547 (MaxPooling2D (None, None, None, 1 0           activation_647[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_551 (MaxPooling2D (None, None, None, 1 0           activation_651[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_555 (MaxPooling2D (None, None, None, 1 0           activation_655[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_559 (MaxPooling2D (None, None, None, 1 0           activation_659[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_563 (MaxPooling2D (None, None, None, 1 0           activation_663[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_567 (MaxPooling2D (None, None, None, 1 0           activation_667[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_571 (MaxPooling2D (None, None, None, 1 0           activation_671[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_575 (MaxPooling2D (None, None, None, 1 0           activation_675[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_579 (MaxPooling2D (None, None, None, 1 0           activation_679[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_583 (MaxPooling2D (None, None, None, 1 0           activation_683[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_587 (MaxPooling2D (None, None, None, 1 0           activation_687[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_591 (MaxPooling2D (None, None, None, 1 0           activation_691[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_596 (MaxPooling2D (None, 8, 8, 256)    0           conv2d_751[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_126 (G (None, 16)           0           max_pooling2d_531[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_127 (G (None, 16)           0           max_pooling2d_535[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_128 (G (None, 16)           0           max_pooling2d_539[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_129 (G (None, 16)           0           max_pooling2d_543[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_130 (G (None, 16)           0           max_pooling2d_547[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_131 (G (None, 16)           0           max_pooling2d_551[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_132 (G (None, 16)           0           max_pooling2d_555[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_133 (G (None, 16)           0           max_pooling2d_559[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_134 (G (None, 16)           0           max_pooling2d_563[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_135 (G (None, 16)           0           max_pooling2d_567[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_136 (G (None, 16)           0           max_pooling2d_571[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_137 (G (None, 16)           0           max_pooling2d_575[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_138 (G (None, 16)           0           max_pooling2d_579[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_139 (G (None, 16)           0           max_pooling2d_583[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_140 (G (None, 16)           0           max_pooling2d_587[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_141 (G (None, 16)           0           max_pooling2d_591[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_142 (G (None, 256)          0           max_pooling2d_596[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_138 (Dense)               (None, 16)           272         global_average_pooling2d_126[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_139 (Dense)               (None, 16)           272         global_average_pooling2d_127[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_140 (Dense)               (None, 16)           272         global_average_pooling2d_128[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_141 (Dense)               (None, 16)           272         global_average_pooling2d_129[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_142 (Dense)               (None, 16)           272         global_average_pooling2d_130[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_143 (Dense)               (None, 16)           272         global_average_pooling2d_131[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_144 (Dense)               (None, 16)           272         global_average_pooling2d_132[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_145 (Dense)               (None, 16)           272         global_average_pooling2d_133[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_146 (Dense)               (None, 16)           272         global_average_pooling2d_134[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_147 (Dense)               (None, 16)           272         global_average_pooling2d_135[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_148 (Dense)               (None, 16)           272         global_average_pooling2d_136[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_149 (Dense)               (None, 16)           272         global_average_pooling2d_137[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_150 (Dense)               (None, 16)           272         global_average_pooling2d_138[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_151 (Dense)               (None, 16)           272         global_average_pooling2d_139[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_152 (Dense)               (None, 16)           272         global_average_pooling2d_140[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_153 (Dense)               (None, 16)           272         global_average_pooling2d_141[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_154 (Dense)               (None, 256)          65792       global_average_pooling2d_142[0][0\n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_138 (Dropout)           (None, 16)           0           dense_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_139 (Dropout)           (None, 16)           0           dense_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_140 (Dropout)           (None, 16)           0           dense_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_141 (Dropout)           (None, 16)           0           dense_141[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_142 (Dropout)           (None, 16)           0           dense_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_143 (Dropout)           (None, 16)           0           dense_143[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_144 (Dropout)           (None, 16)           0           dense_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_145 (Dropout)           (None, 16)           0           dense_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_146 (Dropout)           (None, 16)           0           dense_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_147 (Dropout)           (None, 16)           0           dense_147[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_148 (Dropout)           (None, 16)           0           dense_148[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_149 (Dropout)           (None, 16)           0           dense_149[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_150 (Dropout)           (None, 16)           0           dense_150[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_151 (Dropout)           (None, 16)           0           dense_151[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_152 (Dropout)           (None, 16)           0           dense_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_153 (Dropout)           (None, 16)           0           dense_153[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_154 (Dropout)           (None, 256)          0           dense_154[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_155 (Dense)               (None, 16)           128         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out_1 (Dense)                   (None, 12)           204         dropout_138[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_2 (Dense)                   (None, 12)           204         dropout_139[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_3 (Dense)                   (None, 12)           204         dropout_140[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_4 (Dense)                   (None, 12)           204         dropout_141[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_5 (Dense)                   (None, 12)           204         dropout_142[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_6 (Dense)                   (None, 12)           204         dropout_143[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_7 (Dense)                   (None, 12)           204         dropout_144[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_8 (Dense)                   (None, 12)           204         dropout_145[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_9 (Dense)                   (None, 12)           204         dropout_146[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_10 (Dense)                  (None, 12)           204         dropout_147[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_11 (Dense)                  (None, 12)           204         dropout_148[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_12 (Dense)                  (None, 12)           204         dropout_149[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_13 (Dense)                  (None, 12)           204         dropout_150[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_14 (Dense)                  (None, 12)           204         dropout_151[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_15 (Dense)                  (None, 12)           204         dropout_152[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_16 (Dense)                  (None, 12)           204         dropout_153[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_0 (Dense)                   (None, 12)           3084        dropout_154[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_155 (Dropout)           (None, 16)           0           dense_155[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 220)          0           out_1[0][0]                      \n",
      "                                                                 out_2[0][0]                      \n",
      "                                                                 out_3[0][0]                      \n",
      "                                                                 out_4[0][0]                      \n",
      "                                                                 out_5[0][0]                      \n",
      "                                                                 out_6[0][0]                      \n",
      "                                                                 out_7[0][0]                      \n",
      "                                                                 out_8[0][0]                      \n",
      "                                                                 out_9[0][0]                      \n",
      "                                                                 out_10[0][0]                     \n",
      "                                                                 out_11[0][0]                     \n",
      "                                                                 out_12[0][0]                     \n",
      "                                                                 out_13[0][0]                     \n",
      "                                                                 out_14[0][0]                     \n",
      "                                                                 out_15[0][0]                     \n",
      "                                                                 out_16[0][0]                     \n",
      "                                                                 out_0[0][0]                      \n",
      "                                                                 dropout_155[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_156 (Dense)               (None, 128)          28288       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_156 (Dropout)           (None, 128)          0           dense_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 12)           1548        dropout_156[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,668,664\n",
      "Trainable params: 3,667,640\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "Fitting the network\n"
     ]
    }
   ],
   "source": [
    "model_name = 'rcropnetv1'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "                        make_rcropnetv1(16), model_name=model_name,\n",
    "                        model_dir=model_dir, n_outputs=18, batch_size=32)\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'], n_outputs=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnH6xSOP0PGQ",
    "colab_type": "text"
   },
   "source": [
    "#### RandomCropnet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "UTPKR0TZ0Vj3",
    "colab_type": "code",
    "outputId": "e436f883-b65e-43c7-db76-2fe64062430d",
    "executionInfo": {
     "status": "error",
     "timestamp": 1.578314923224E12,
     "user_tz": -60.0,
     "elapsed": 31670.0,
     "user": {
      "displayName": "Javier Huertas",
      "photoUrl": "",
      "userId": "08531127118556167809"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking train and validation tests\n",
      "Building the network\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_1\" was not an Input tensor, it was generated by layer input_1.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: input_1:0\n",
      "Compiling the network\n",
      "Layers: 60\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 256, 32) 896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 128, 128, 32) 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 256)  590080      max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           128         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 8, 8, 256)    0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 256)          0           max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 272)          0           dropout[0][0]                    \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          69888       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 256)          3602592     input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_1 (Dense)                   (None, 12)           3084        model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_2 (Dense)                   (None, 12)           3084        model[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_3 (Dense)                   (None, 12)           3084        model[3][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_4 (Dense)                   (None, 12)           3084        model[4][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_5 (Dense)                   (None, 12)           3084        model[5][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_6 (Dense)                   (None, 12)           3084        model[6][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_7 (Dense)                   (None, 12)           3084        model[7][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_8 (Dense)                   (None, 12)           3084        model[8][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_9 (Dense)                   (None, 12)           3084        model[9][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_10 (Dense)                  (None, 12)           3084        model[10][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_11 (Dense)                  (None, 12)           3084        model[11][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_12 (Dense)                  (None, 12)           3084        model[12][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_13 (Dense)                  (None, 12)           3084        model[13][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_14 (Dense)                  (None, 12)           3084        model[14][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_15 (Dense)                  (None, 12)           3084        model[15][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_16 (Dense)                  (None, 12)           3084        model[16][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_17 (Dense)                  (None, 12)           3084        model[17][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_18 (Dense)                  (None, 12)           3084        model[18][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_19 (Dense)                  (None, 12)           3084        model[19][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_20 (Dense)                  (None, 12)           3084        model[20][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_21 (Dense)                  (None, 12)           3084        model[21][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_22 (Dense)                  (None, 12)           3084        model[22][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_23 (Dense)                  (None, 12)           3084        model[23][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_24 (Dense)                  (None, 12)           3084        model[24][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_25 (Dense)                  (None, 12)           3084        model[25][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_26 (Dense)                  (None, 12)           3084        model[26][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_27 (Dense)                  (None, 12)           3084        model[27][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_28 (Dense)                  (None, 12)           3084        model[28][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_29 (Dense)                  (None, 12)           3084        model[29][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_30 (Dense)                  (None, 12)           3084        model[30][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_31 (Dense)                  (None, 12)           3084        model[31][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_32 (Dense)                  (None, 12)           3084        model[32][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_0 (Dense)                   (None, 12)           3084        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out (Average)                   (None, 12)           0           out_1[0][0]                      \n",
      "                                                                 out_2[0][0]                      \n",
      "                                                                 out_3[0][0]                      \n",
      "                                                                 out_4[0][0]                      \n",
      "                                                                 out_5[0][0]                      \n",
      "                                                                 out_6[0][0]                      \n",
      "                                                                 out_7[0][0]                      \n",
      "                                                                 out_8[0][0]                      \n",
      "                                                                 out_9[0][0]                      \n",
      "                                                                 out_10[0][0]                     \n",
      "                                                                 out_11[0][0]                     \n",
      "                                                                 out_12[0][0]                     \n",
      "                                                                 out_13[0][0]                     \n",
      "                                                                 out_14[0][0]                     \n",
      "                                                                 out_15[0][0]                     \n",
      "                                                                 out_16[0][0]                     \n",
      "                                                                 out_17[0][0]                     \n",
      "                                                                 out_18[0][0]                     \n",
      "                                                                 out_19[0][0]                     \n",
      "                                                                 out_20[0][0]                     \n",
      "                                                                 out_21[0][0]                     \n",
      "                                                                 out_22[0][0]                     \n",
      "                                                                 out_23[0][0]                     \n",
      "                                                                 out_24[0][0]                     \n",
      "                                                                 out_25[0][0]                     \n",
      "                                                                 out_26[0][0]                     \n",
      "                                                                 out_27[0][0]                     \n",
      "                                                                 out_28[0][0]                     \n",
      "                                                                 out_29[0][0]                     \n",
      "                                                                 out_30[0][0]                     \n",
      "                                                                 out_31[0][0]                     \n",
      "                                                                 out_32[0][0]                     \n",
      "                                                                 out_0[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 7,306,828\n",
      "Trainable params: 7,306,828\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Fitting the network\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/1000\n",
      "101/100 - 151s - loss: 82.2317 - out_loss: 2.3604 - out_0_loss: 2.1861 - out_1_loss: 2.4016 - out_2_loss: 2.4189 - out_3_loss: 2.4173 - out_4_loss: 2.4160 - out_5_loss: 2.4305 - out_6_loss: 2.4227 - out_7_loss: 2.4310 - out_8_loss: 2.4823 - out_9_loss: 2.4574 - out_10_loss: 2.3906 - out_11_loss: 2.4380 - out_12_loss: 2.4752 - out_13_loss: 2.4007 - out_14_loss: 2.3981 - out_15_loss: 2.4333 - out_16_loss: 2.4717 - out_17_loss: 2.4244 - out_18_loss: 2.4095 - out_19_loss: 2.3910 - out_20_loss: 2.4435 - out_21_loss: 2.4723 - out_22_loss: 2.4028 - out_23_loss: 2.4795 - out_24_loss: 2.4276 - out_25_loss: 2.4484 - out_26_loss: 2.3893 - out_27_loss: 2.4317 - out_28_loss: 2.3851 - out_29_loss: 2.4200 - out_30_loss: 2.4068 - out_31_loss: 2.4301 - out_32_loss: 2.4378 - out_acc: 0.2145 - out_0_acc: 0.2170 - out_1_acc: 0.1596 - out_2_acc: 0.1386 - out_3_acc: 0.1342 - out_4_acc: 0.1472 - out_5_acc: 0.1175 - out_6_acc: 0.1218 - out_7_acc: 0.1286 - out_8_acc: 0.1221 - out_9_acc: 0.1147 - out_10_acc: 0.1482 - out_11_acc: 0.1562 - out_12_acc: 0.1023 - out_13_acc: 0.1373 - out_14_acc: 0.1559 - out_15_acc: 0.1190 - out_16_acc: 0.1011 - out_17_acc: 0.1395 - out_18_acc: 0.1336 - out_19_acc: 0.1671 - out_20_acc: 0.1454 - out_21_acc: 0.1169 - out_22_acc: 0.1243 - out_23_acc: 0.0930 - out_24_acc: 0.1249 - out_25_acc: 0.1305 - out_26_acc: 0.1702 - out_27_acc: 0.1252 - out_28_acc: 0.1739 - out_29_acc: 0.1249 - out_30_acc: 0.1696 - out_31_acc: 0.1193 - out_32_acc: 0.1125 - val_loss: 80.4440 - val_out_loss: 2.2387 - val_out_0_loss: 2.0343 - val_out_1_loss: 2.2397 - val_out_2_loss: 2.2932 - val_out_3_loss: 2.2574 - val_out_4_loss: 2.2482 - val_out_5_loss: 2.2857 - val_out_6_loss: 2.2773 - val_out_7_loss: 2.2775 - val_out_8_loss: 2.3135 - val_out_9_loss: 2.3413 - val_out_10_loss: 2.2336 - val_out_11_loss: 2.2898 - val_out_12_loss: 2.3454 - val_out_13_loss: 2.2610 - val_out_14_loss: 2.2576 - val_out_15_loss: 2.2590 - val_out_16_loss: 2.3346 - val_out_17_loss: 2.2263 - val_out_18_loss: 2.2838 - val_out_19_loss: 2.2517 - val_out_20_loss: 2.2913 - val_out_21_loss: 2.3505 - val_out_22_loss: 2.2468 - val_out_23_loss: 2.3183 - val_out_24_loss: 2.2879 - val_out_25_loss: 2.3153 - val_out_26_loss: 2.2585 - val_out_27_loss: 2.3439 - val_out_28_loss: 2.2140 - val_out_29_loss: 2.2500 - val_out_30_loss: 2.2751 - val_out_31_loss: 2.2914 - val_out_32_loss: 2.2674 - val_out_acc: 0.2061 - val_out_0_acc: 0.2603 - val_out_1_acc: 0.2299 - val_out_2_acc: 0.1562 - val_out_3_acc: 0.2408 - val_out_4_acc: 0.2603 - val_out_5_acc: 0.2234 - val_out_6_acc: 0.1627 - val_out_7_acc: 0.1692 - val_out_8_acc: 0.2039 - val_out_9_acc: 0.1150 - val_out_10_acc: 0.1302 - val_out_11_acc: 0.2061 - val_out_12_acc: 0.1540 - val_out_13_acc: 0.2039 - val_out_14_acc: 0.1518 - val_out_15_acc: 0.2126 - val_out_16_acc: 0.2082 - val_out_17_acc: 0.1562 - val_out_18_acc: 0.2061 - val_out_19_acc: 0.2039 - val_out_20_acc: 0.2082 - val_out_21_acc: 0.2430 - val_out_22_acc: 0.1562 - val_out_23_acc: 0.2234 - val_out_24_acc: 0.2039 - val_out_25_acc: 0.2061 - val_out_26_acc: 0.2061 - val_out_27_acc: 0.0954 - val_out_28_acc: 0.2148 - val_out_29_acc: 0.1670 - val_out_30_acc: 0.2061 - val_out_31_acc: 0.1627 - val_out_32_acc: 0.1822\n",
      "Epoch 2/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 77.7719 - out_loss: 2.1794 - out_0_loss: 1.9492 - out_1_loss: 2.2741 - out_2_loss: 2.3183 - out_3_loss: 2.2767 - out_4_loss: 2.2675 - out_5_loss: 2.2971 - out_6_loss: 2.3287 - out_7_loss: 2.2907 - out_8_loss: 2.3205 - out_9_loss: 2.3247 - out_10_loss: 2.2769 - out_11_loss: 2.3473 - out_12_loss: 2.3258 - out_13_loss: 2.2959 - out_14_loss: 2.2799 - out_15_loss: 2.2609 - out_16_loss: 2.3315 - out_17_loss: 2.2654 - out_18_loss: 2.2994 - out_19_loss: 2.2946 - out_20_loss: 2.3320 - out_21_loss: 2.3335 - out_22_loss: 2.2821 - out_23_loss: 2.3210 - out_24_loss: 2.3075 - out_25_loss: 2.3183 - out_26_loss: 2.2772 - out_27_loss: 2.3251 - out_28_loss: 2.2563 - out_29_loss: 2.3029 - out_30_loss: 2.3046 - out_31_loss: 2.3028 - out_32_loss: 2.3040 - out_acc: 0.2638 - out_0_acc: 0.3286 - out_1_acc: 0.2030 - out_2_acc: 0.1727 - out_3_acc: 0.1981 - out_4_acc: 0.1854 - out_5_acc: 0.1900 - out_6_acc: 0.1782 - out_7_acc: 0.1944 - out_8_acc: 0.1950 - out_9_acc: 0.1742 - out_10_acc: 0.1723 - out_11_acc: 0.2030 - out_12_acc: 0.1844 - out_13_acc: 0.2002 - out_14_acc: 0.1897 - out_15_acc: 0.1990 - out_16_acc: 0.1866 - out_17_acc: 0.1996 - out_18_acc: 0.1875 - out_19_acc: 0.2046 - out_20_acc: 0.1826 - out_21_acc: 0.1922 - out_22_acc: 0.1882 - out_23_acc: 0.2021 - out_24_acc: 0.1705 - out_25_acc: 0.1823 - out_26_acc: 0.2167 - out_27_acc: 0.1727 - out_28_acc: 0.2260 - out_29_acc: 0.1658 - out_30_acc: 0.1953 - out_31_acc: 0.1878 - out_32_acc: 0.1820 - val_loss: 75.3216 - val_out_loss: 2.0955 - val_out_0_loss: 1.7366 - val_out_1_loss: 2.1507 - val_out_2_loss: 2.1663 - val_out_3_loss: 2.1647 - val_out_4_loss: 2.0873 - val_out_5_loss: 2.1137 - val_out_6_loss: 2.1385 - val_out_7_loss: 2.1294 - val_out_8_loss: 2.1530 - val_out_9_loss: 2.1669 - val_out_10_loss: 2.1438 - val_out_11_loss: 2.2000 - val_out_12_loss: 2.1482 - val_out_13_loss: 2.1689 - val_out_14_loss: 2.1100 - val_out_15_loss: 2.1161 - val_out_16_loss: 2.1570 - val_out_17_loss: 2.1322 - val_out_18_loss: 2.1241 - val_out_19_loss: 2.1485 - val_out_20_loss: 2.1528 - val_out_21_loss: 2.1753 - val_out_22_loss: 2.1223 - val_out_23_loss: 2.1341 - val_out_24_loss: 2.1211 - val_out_25_loss: 2.1420 - val_out_26_loss: 2.1148 - val_out_27_loss: 2.1723 - val_out_28_loss: 2.0882 - val_out_29_loss: 2.1645 - val_out_30_loss: 2.1440 - val_out_31_loss: 2.1402 - val_out_32_loss: 2.1171 - val_out_acc: 0.3406 - val_out_0_acc: 0.4078 - val_out_1_acc: 0.2668 - val_out_2_acc: 0.2950 - val_out_3_acc: 0.2625 - val_out_4_acc: 0.2625 - val_out_5_acc: 0.2321 - val_out_6_acc: 0.2842 - val_out_7_acc: 0.2321 - val_out_8_acc: 0.2842 - val_out_9_acc: 0.2777 - val_out_10_acc: 0.1605 - val_out_11_acc: 0.2690 - val_out_12_acc: 0.2646 - val_out_13_acc: 0.2495 - val_out_14_acc: 0.2560 - val_out_15_acc: 0.3254 - val_out_16_acc: 0.2733 - val_out_17_acc: 0.2581 - val_out_18_acc: 0.1974 - val_out_19_acc: 0.2777 - val_out_20_acc: 0.2408 - val_out_21_acc: 0.2777 - val_out_22_acc: 0.2885 - val_out_23_acc: 0.2581 - val_out_24_acc: 0.1952 - val_out_25_acc: 0.2386 - val_out_26_acc: 0.2646 - val_out_27_acc: 0.2950 - val_out_28_acc: 0.2625 - val_out_29_acc: 0.2256 - val_out_30_acc: 0.2039 - val_out_31_acc: 0.2560 - val_out_32_acc: 0.2690\n",
      "Epoch 3/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 74.2633 - out_loss: 2.0715 - out_0_loss: 1.8134 - out_1_loss: 2.1731 - out_2_loss: 2.2043 - out_3_loss: 2.2031 - out_4_loss: 2.1715 - out_5_loss: 2.1730 - out_6_loss: 2.2144 - out_7_loss: 2.1717 - out_8_loss: 2.2056 - out_9_loss: 2.2194 - out_10_loss: 2.2162 - out_11_loss: 2.2253 - out_12_loss: 2.2321 - out_13_loss: 2.1833 - out_14_loss: 2.1728 - out_15_loss: 2.1662 - out_16_loss: 2.2006 - out_17_loss: 2.1709 - out_18_loss: 2.1985 - out_19_loss: 2.1788 - out_20_loss: 2.2569 - out_21_loss: 2.2018 - out_22_loss: 2.1909 - out_23_loss: 2.1984 - out_24_loss: 2.1982 - out_25_loss: 2.2116 - out_26_loss: 2.2226 - out_27_loss: 2.2223 - out_28_loss: 2.1597 - out_29_loss: 2.2232 - out_30_loss: 2.2162 - out_31_loss: 2.1974 - out_32_loss: 2.1986 - out_acc: 0.2973 - out_0_acc: 0.3865 - out_1_acc: 0.2368 - out_2_acc: 0.2219 - out_3_acc: 0.2272 - out_4_acc: 0.2340 - out_5_acc: 0.2443 - out_6_acc: 0.2288 - out_7_acc: 0.2356 - out_8_acc: 0.2402 - out_9_acc: 0.2229 - out_10_acc: 0.2018 - out_11_acc: 0.2387 - out_12_acc: 0.2244 - out_13_acc: 0.2325 - out_14_acc: 0.2393 - out_15_acc: 0.2384 - out_16_acc: 0.2297 - out_17_acc: 0.2347 - out_18_acc: 0.2235 - out_19_acc: 0.2415 - out_20_acc: 0.2126 - out_21_acc: 0.2328 - out_22_acc: 0.2176 - out_23_acc: 0.2275 - out_24_acc: 0.1937 - out_25_acc: 0.2176 - out_26_acc: 0.2204 - out_27_acc: 0.2192 - out_28_acc: 0.2585 - out_29_acc: 0.2058 - out_30_acc: 0.2123 - out_31_acc: 0.2204 - out_32_acc: 0.2092 - val_loss: 71.9172 - val_out_loss: 2.0020 - val_out_0_loss: 1.6565 - val_out_1_loss: 2.0197 - val_out_2_loss: 2.0609 - val_out_3_loss: 2.0470 - val_out_4_loss: 2.0158 - val_out_5_loss: 2.0033 - val_out_6_loss: 2.0426 - val_out_7_loss: 2.0476 - val_out_8_loss: 2.0439 - val_out_9_loss: 2.0477 - val_out_10_loss: 2.0414 - val_out_11_loss: 2.0333 - val_out_12_loss: 2.0531 - val_out_13_loss: 2.0415 - val_out_14_loss: 2.0543 - val_out_15_loss: 2.0323 - val_out_16_loss: 2.0238 - val_out_17_loss: 2.0452 - val_out_18_loss: 2.0192 - val_out_19_loss: 2.0011 - val_out_20_loss: 2.0812 - val_out_21_loss: 2.0466 - val_out_22_loss: 2.0176 - val_out_23_loss: 2.0456 - val_out_24_loss: 2.0539 - val_out_25_loss: 2.0490 - val_out_26_loss: 2.0456 - val_out_27_loss: 2.0527 - val_out_28_loss: 2.0459 - val_out_29_loss: 2.0932 - val_out_30_loss: 2.0596 - val_out_31_loss: 2.0912 - val_out_32_loss: 2.0562 - val_out_acc: 0.2950 - val_out_0_acc: 0.4273 - val_out_1_acc: 0.2755 - val_out_2_acc: 0.2928 - val_out_3_acc: 0.2234 - val_out_4_acc: 0.2798 - val_out_5_acc: 0.3015 - val_out_6_acc: 0.2755 - val_out_7_acc: 0.2646 - val_out_8_acc: 0.2863 - val_out_9_acc: 0.2733 - val_out_10_acc: 0.2625 - val_out_11_acc: 0.3037 - val_out_12_acc: 0.2690 - val_out_13_acc: 0.2668 - val_out_14_acc: 0.2755 - val_out_15_acc: 0.2408 - val_out_16_acc: 0.2733 - val_out_17_acc: 0.2733 - val_out_18_acc: 0.2299 - val_out_19_acc: 0.2842 - val_out_20_acc: 0.2104 - val_out_21_acc: 0.2863 - val_out_22_acc: 0.2950 - val_out_23_acc: 0.2646 - val_out_24_acc: 0.1996 - val_out_25_acc: 0.2148 - val_out_26_acc: 0.2343 - val_out_27_acc: 0.2950 - val_out_28_acc: 0.1931 - val_out_29_acc: 0.2321 - val_out_30_acc: 0.2213 - val_out_31_acc: 0.2278 - val_out_32_acc: 0.2733\n",
      "Epoch 4/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 71.5845 - out_loss: 1.9866 - out_0_loss: 1.7325 - out_1_loss: 2.0815 - out_2_loss: 2.1275 - out_3_loss: 2.1068 - out_4_loss: 2.1033 - out_5_loss: 2.1061 - out_6_loss: 2.1448 - out_7_loss: 2.1131 - out_8_loss: 2.1119 - out_9_loss: 2.1469 - out_10_loss: 2.1393 - out_11_loss: 2.1410 - out_12_loss: 2.1312 - out_13_loss: 2.1107 - out_14_loss: 2.1153 - out_15_loss: 2.0898 - out_16_loss: 2.1227 - out_17_loss: 2.1235 - out_18_loss: 2.1371 - out_19_loss: 2.0726 - out_20_loss: 2.1629 - out_21_loss: 2.1114 - out_22_loss: 2.1070 - out_23_loss: 2.1097 - out_24_loss: 2.1217 - out_25_loss: 2.1167 - out_26_loss: 2.1305 - out_27_loss: 2.1418 - out_28_loss: 2.0968 - out_29_loss: 2.1486 - out_30_loss: 2.1295 - out_31_loss: 2.1380 - out_32_loss: 2.1256 - out_acc: 0.3277 - out_0_acc: 0.4123 - out_1_acc: 0.2740 - out_2_acc: 0.2619 - out_3_acc: 0.2647 - out_4_acc: 0.2412 - out_5_acc: 0.2626 - out_6_acc: 0.2545 - out_7_acc: 0.2595 - out_8_acc: 0.2722 - out_9_acc: 0.2474 - out_10_acc: 0.2285 - out_11_acc: 0.2533 - out_12_acc: 0.2697 - out_13_acc: 0.2622 - out_14_acc: 0.2613 - out_15_acc: 0.2818 - out_16_acc: 0.2579 - out_17_acc: 0.2440 - out_18_acc: 0.2564 - out_19_acc: 0.2833 - out_20_acc: 0.2371 - out_21_acc: 0.2495 - out_22_acc: 0.2647 - out_23_acc: 0.2520 - out_24_acc: 0.2343 - out_25_acc: 0.2405 - out_26_acc: 0.2399 - out_27_acc: 0.2511 - out_28_acc: 0.2657 - out_29_acc: 0.2489 - out_30_acc: 0.2464 - out_31_acc: 0.2405 - out_32_acc: 0.2464 - val_loss: 65.0554 - val_out_loss: 1.8102 - val_out_0_loss: 1.5117 - val_out_1_loss: 1.8165 - val_out_2_loss: 1.8505 - val_out_3_loss: 1.8403 - val_out_4_loss: 1.8228 - val_out_5_loss: 1.8440 - val_out_6_loss: 1.8802 - val_out_7_loss: 1.8245 - val_out_8_loss: 1.8527 - val_out_9_loss: 1.8758 - val_out_10_loss: 1.8741 - val_out_11_loss: 1.8610 - val_out_12_loss: 1.8615 - val_out_13_loss: 1.8288 - val_out_14_loss: 1.8208 - val_out_15_loss: 1.8192 - val_out_16_loss: 1.8532 - val_out_17_loss: 1.8390 - val_out_18_loss: 1.8596 - val_out_19_loss: 1.8160 - val_out_20_loss: 1.8789 - val_out_21_loss: 1.8765 - val_out_22_loss: 1.8390 - val_out_23_loss: 1.8434 - val_out_24_loss: 1.8554 - val_out_25_loss: 1.8621 - val_out_26_loss: 1.8811 - val_out_27_loss: 1.8539 - val_out_28_loss: 1.8497 - val_out_29_loss: 1.8416 - val_out_30_loss: 1.8522 - val_out_31_loss: 1.8440 - val_out_32_loss: 1.8399 - val_out_acc: 0.4121 - val_out_0_acc: 0.4772 - val_out_1_acc: 0.3991 - val_out_2_acc: 0.4143 - val_out_3_acc: 0.3731 - val_out_4_acc: 0.4078 - val_out_5_acc: 0.3753 - val_out_6_acc: 0.3601 - val_out_7_acc: 0.3861 - val_out_8_acc: 0.3254 - val_out_9_acc: 0.3839 - val_out_10_acc: 0.3232 - val_out_11_acc: 0.3861 - val_out_12_acc: 0.3427 - val_out_13_acc: 0.3839 - val_out_14_acc: 0.3839 - val_out_15_acc: 0.3557 - val_out_16_acc: 0.3644 - val_out_17_acc: 0.3601 - val_out_18_acc: 0.3991 - val_out_19_acc: 0.3774 - val_out_20_acc: 0.3557 - val_out_21_acc: 0.3384 - val_out_22_acc: 0.3492 - val_out_23_acc: 0.3926 - val_out_24_acc: 0.3406 - val_out_25_acc: 0.3839 - val_out_26_acc: 0.3384 - val_out_27_acc: 0.3991 - val_out_28_acc: 0.3536 - val_out_29_acc: 0.3427 - val_out_30_acc: 0.3666 - val_out_31_acc: 0.3926 - val_out_32_acc: 0.3492\n",
      "Epoch 5/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 69.0815 - out_loss: 1.9099 - out_0_loss: 1.6364 - out_1_loss: 2.0210 - out_2_loss: 2.0416 - out_3_loss: 2.0450 - out_4_loss: 2.0534 - out_5_loss: 2.0393 - out_6_loss: 2.0599 - out_7_loss: 2.0536 - out_8_loss: 2.0366 - out_9_loss: 2.0531 - out_10_loss: 2.0411 - out_11_loss: 2.0688 - out_12_loss: 2.0599 - out_13_loss: 2.0635 - out_14_loss: 2.0285 - out_15_loss: 2.0308 - out_16_loss: 2.0389 - out_17_loss: 2.0564 - out_18_loss: 2.0462 - out_19_loss: 2.0098 - out_20_loss: 2.0767 - out_21_loss: 2.0673 - out_22_loss: 2.0441 - out_23_loss: 2.0582 - out_24_loss: 2.0594 - out_25_loss: 2.0291 - out_26_loss: 2.0757 - out_27_loss: 2.0460 - out_28_loss: 2.0352 - out_29_loss: 2.0491 - out_30_loss: 2.0677 - out_31_loss: 2.0322 - out_32_loss: 2.0466 - out_acc: 0.3683 - out_0_acc: 0.4492 - out_1_acc: 0.2942 - out_2_acc: 0.3007 - out_3_acc: 0.2839 - out_4_acc: 0.2781 - out_5_acc: 0.2768 - out_6_acc: 0.3016 - out_7_acc: 0.2973 - out_8_acc: 0.2880 - out_9_acc: 0.2886 - out_10_acc: 0.2641 - out_11_acc: 0.2818 - out_12_acc: 0.2957 - out_13_acc: 0.2880 - out_14_acc: 0.3047 - out_15_acc: 0.3001 - out_16_acc: 0.2892 - out_17_acc: 0.2626 - out_18_acc: 0.2948 - out_19_acc: 0.2991 - out_20_acc: 0.2774 - out_21_acc: 0.2920 - out_22_acc: 0.2932 - out_23_acc: 0.2750 - out_24_acc: 0.2731 - out_25_acc: 0.2914 - out_26_acc: 0.2787 - out_27_acc: 0.2929 - out_28_acc: 0.2932 - out_29_acc: 0.3001 - out_30_acc: 0.2855 - out_31_acc: 0.2883 - out_32_acc: 0.2920 - val_loss: 67.8402 - val_out_loss: 1.8947 - val_out_0_loss: 1.5211 - val_out_1_loss: 1.9055 - val_out_2_loss: 1.9414 - val_out_3_loss: 1.9400 - val_out_4_loss: 1.9384 - val_out_5_loss: 1.9265 - val_out_6_loss: 1.9422 - val_out_7_loss: 1.9176 - val_out_8_loss: 1.9183 - val_out_9_loss: 1.9487 - val_out_10_loss: 1.9459 - val_out_11_loss: 1.9309 - val_out_12_loss: 1.9460 - val_out_13_loss: 1.9146 - val_out_14_loss: 1.9092 - val_out_15_loss: 1.9049 - val_out_16_loss: 1.9323 - val_out_17_loss: 1.9354 - val_out_18_loss: 1.9137 - val_out_19_loss: 1.8936 - val_out_20_loss: 1.9611 - val_out_21_loss: 1.9327 - val_out_22_loss: 1.9234 - val_out_23_loss: 1.9127 - val_out_24_loss: 1.9298 - val_out_25_loss: 1.9310 - val_out_26_loss: 1.9448 - val_out_27_loss: 1.9544 - val_out_28_loss: 1.9192 - val_out_29_loss: 1.9315 - val_out_30_loss: 1.9361 - val_out_31_loss: 1.9280 - val_out_32_loss: 1.9290 - val_out_acc: 0.3948 - val_out_0_acc: 0.4751 - val_out_1_acc: 0.3753 - val_out_2_acc: 0.3991 - val_out_3_acc: 0.3666 - val_out_4_acc: 0.3449 - val_out_5_acc: 0.3796 - val_out_6_acc: 0.3536 - val_out_7_acc: 0.3905 - val_out_8_acc: 0.3601 - val_out_9_acc: 0.3774 - val_out_10_acc: 0.3514 - val_out_11_acc: 0.3731 - val_out_12_acc: 0.3557 - val_out_13_acc: 0.3471 - val_out_14_acc: 0.3948 - val_out_15_acc: 0.3731 - val_out_16_acc: 0.3774 - val_out_17_acc: 0.3601 - val_out_18_acc: 0.4121 - val_out_19_acc: 0.3623 - val_out_20_acc: 0.3536 - val_out_21_acc: 0.3926 - val_out_22_acc: 0.3731 - val_out_23_acc: 0.3731 - val_out_24_acc: 0.3709 - val_out_25_acc: 0.3709 - val_out_26_acc: 0.3579 - val_out_27_acc: 0.3666 - val_out_28_acc: 0.3774 - val_out_29_acc: 0.3753 - val_out_30_acc: 0.3926 - val_out_31_acc: 0.3861 - val_out_32_acc: 0.3601\n",
      "Epoch 6/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 66.7807 - out_loss: 1.8420 - out_0_loss: 1.5276 - out_1_loss: 1.9426 - out_2_loss: 1.9714 - out_3_loss: 1.9866 - out_4_loss: 1.9925 - out_5_loss: 1.9712 - out_6_loss: 2.0063 - out_7_loss: 1.9851 - out_8_loss: 1.9817 - out_9_loss: 1.9837 - out_10_loss: 1.9848 - out_11_loss: 1.9945 - out_12_loss: 1.9851 - out_13_loss: 1.9894 - out_14_loss: 1.9600 - out_15_loss: 1.9496 - out_16_loss: 1.9681 - out_17_loss: 1.9881 - out_18_loss: 1.9783 - out_19_loss: 1.9684 - out_20_loss: 2.0239 - out_21_loss: 1.9906 - out_22_loss: 1.9672 - out_23_loss: 1.9863 - out_24_loss: 1.9868 - out_25_loss: 1.9624 - out_26_loss: 1.9898 - out_27_loss: 1.9995 - out_28_loss: 1.9853 - out_29_loss: 1.9854 - out_30_loss: 2.0087 - out_31_loss: 1.9770 - out_32_loss: 1.9611 - out_acc: 0.3859 - out_0_acc: 0.4678 - out_1_acc: 0.3366 - out_2_acc: 0.3277 - out_3_acc: 0.3066 - out_4_acc: 0.3001 - out_5_acc: 0.3115 - out_6_acc: 0.3199 - out_7_acc: 0.3177 - out_8_acc: 0.3146 - out_9_acc: 0.3224 - out_10_acc: 0.2951 - out_11_acc: 0.3233 - out_12_acc: 0.3143 - out_13_acc: 0.3140 - out_14_acc: 0.3208 - out_15_acc: 0.3273 - out_16_acc: 0.3208 - out_17_acc: 0.3004 - out_18_acc: 0.3301 - out_19_acc: 0.3205 - out_20_acc: 0.3029 - out_21_acc: 0.3125 - out_22_acc: 0.3246 - out_23_acc: 0.3125 - out_24_acc: 0.2905 - out_25_acc: 0.3233 - out_26_acc: 0.2967 - out_27_acc: 0.3044 - out_28_acc: 0.3122 - out_29_acc: 0.3066 - out_30_acc: 0.3100 - out_31_acc: 0.3047 - out_32_acc: 0.3184 - val_loss: 65.7390 - val_out_loss: 1.8355 - val_out_0_loss: 1.4134 - val_out_1_loss: 1.8410 - val_out_2_loss: 1.8655 - val_out_3_loss: 1.8686 - val_out_4_loss: 1.8582 - val_out_5_loss: 1.8700 - val_out_6_loss: 1.8957 - val_out_7_loss: 1.8536 - val_out_8_loss: 1.8704 - val_out_9_loss: 1.8998 - val_out_10_loss: 1.8885 - val_out_11_loss: 1.8813 - val_out_12_loss: 1.8909 - val_out_13_loss: 1.8685 - val_out_14_loss: 1.8522 - val_out_15_loss: 1.8562 - val_out_16_loss: 1.8752 - val_out_17_loss: 1.8844 - val_out_18_loss: 1.8539 - val_out_19_loss: 1.8411 - val_out_20_loss: 1.9210 - val_out_21_loss: 1.8699 - val_out_22_loss: 1.8550 - val_out_23_loss: 1.8650 - val_out_24_loss: 1.8543 - val_out_25_loss: 1.8756 - val_out_26_loss: 1.9061 - val_out_27_loss: 1.8891 - val_out_28_loss: 1.8479 - val_out_29_loss: 1.8944 - val_out_30_loss: 1.8727 - val_out_31_loss: 1.8685 - val_out_32_loss: 1.8535 - val_out_acc: 0.3861 - val_out_0_acc: 0.4881 - val_out_1_acc: 0.3970 - val_out_2_acc: 0.4013 - val_out_3_acc: 0.3666 - val_out_4_acc: 0.3731 - val_out_5_acc: 0.4013 - val_out_6_acc: 0.3688 - val_out_7_acc: 0.3839 - val_out_8_acc: 0.3753 - val_out_9_acc: 0.3623 - val_out_10_acc: 0.3861 - val_out_11_acc: 0.3709 - val_out_12_acc: 0.3970 - val_out_13_acc: 0.3861 - val_out_14_acc: 0.3926 - val_out_15_acc: 0.3839 - val_out_16_acc: 0.3861 - val_out_17_acc: 0.3883 - val_out_18_acc: 0.3839 - val_out_19_acc: 0.3796 - val_out_20_acc: 0.3926 - val_out_21_acc: 0.3796 - val_out_22_acc: 0.3948 - val_out_23_acc: 0.3623 - val_out_24_acc: 0.3905 - val_out_25_acc: 0.3926 - val_out_26_acc: 0.4056 - val_out_27_acc: 0.3709 - val_out_28_acc: 0.3970 - val_out_29_acc: 0.3796 - val_out_30_acc: 0.3709 - val_out_31_acc: 0.3926 - val_out_32_acc: 0.3883\n",
      "Epoch 7/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 64.3319 - out_loss: 1.7735 - out_0_loss: 1.4919 - out_1_loss: 1.8848 - out_2_loss: 1.8976 - out_3_loss: 1.9121 - out_4_loss: 1.9335 - out_5_loss: 1.9151 - out_6_loss: 1.9229 - out_7_loss: 1.9120 - out_8_loss: 1.9118 - out_9_loss: 1.9260 - out_10_loss: 1.9181 - out_11_loss: 1.9247 - out_12_loss: 1.8948 - out_13_loss: 1.9119 - out_14_loss: 1.9083 - out_15_loss: 1.8921 - out_16_loss: 1.8982 - out_17_loss: 1.9198 - out_18_loss: 1.8946 - out_19_loss: 1.8811 - out_20_loss: 1.9353 - out_21_loss: 1.9120 - out_22_loss: 1.8768 - out_23_loss: 1.9060 - out_24_loss: 1.9187 - out_25_loss: 1.8745 - out_26_loss: 1.9137 - out_27_loss: 1.9236 - out_28_loss: 1.9034 - out_29_loss: 1.9135 - out_30_loss: 1.9166 - out_31_loss: 1.9035 - out_32_loss: 1.9093 - out_acc: 0.4017 - out_0_acc: 0.4864 - out_1_acc: 0.3400 - out_2_acc: 0.3500 - out_3_acc: 0.3326 - out_4_acc: 0.3053 - out_5_acc: 0.3264 - out_6_acc: 0.3500 - out_7_acc: 0.3450 - out_8_acc: 0.3255 - out_9_acc: 0.3360 - out_10_acc: 0.3218 - out_11_acc: 0.3351 - out_12_acc: 0.3524 - out_13_acc: 0.3295 - out_14_acc: 0.3369 - out_15_acc: 0.3472 - out_16_acc: 0.3308 - out_17_acc: 0.3227 - out_18_acc: 0.3497 - out_19_acc: 0.3524 - out_20_acc: 0.3227 - out_21_acc: 0.3314 - out_22_acc: 0.3540 - out_23_acc: 0.3326 - out_24_acc: 0.3187 - out_25_acc: 0.3475 - out_26_acc: 0.3304 - out_27_acc: 0.3298 - out_28_acc: 0.3416 - out_29_acc: 0.3360 - out_30_acc: 0.3255 - out_31_acc: 0.3323 - out_32_acc: 0.3335 - val_loss: 59.6548 - val_out_loss: 1.6641 - val_out_0_loss: 1.3255 - val_out_1_loss: 1.6793 - val_out_2_loss: 1.6951 - val_out_3_loss: 1.7064 - val_out_4_loss: 1.7089 - val_out_5_loss: 1.7048 - val_out_6_loss: 1.7183 - val_out_7_loss: 1.6771 - val_out_8_loss: 1.6934 - val_out_9_loss: 1.7212 - val_out_10_loss: 1.7058 - val_out_11_loss: 1.7152 - val_out_12_loss: 1.6905 - val_out_13_loss: 1.6949 - val_out_14_loss: 1.6713 - val_out_15_loss: 1.6691 - val_out_16_loss: 1.6992 - val_out_17_loss: 1.7020 - val_out_18_loss: 1.6731 - val_out_19_loss: 1.6772 - val_out_20_loss: 1.7177 - val_out_21_loss: 1.6988 - val_out_22_loss: 1.6795 - val_out_23_loss: 1.7031 - val_out_24_loss: 1.7083 - val_out_25_loss: 1.6975 - val_out_26_loss: 1.7043 - val_out_27_loss: 1.7102 - val_out_28_loss: 1.6943 - val_out_29_loss: 1.7075 - val_out_30_loss: 1.6966 - val_out_31_loss: 1.6954 - val_out_32_loss: 1.6878 - val_out_acc: 0.4490 - val_out_0_acc: 0.5358 - val_out_1_acc: 0.4403 - val_out_2_acc: 0.4577 - val_out_3_acc: 0.4360 - val_out_4_acc: 0.4273 - val_out_5_acc: 0.4534 - val_out_6_acc: 0.4469 - val_out_7_acc: 0.4577 - val_out_8_acc: 0.4338 - val_out_9_acc: 0.4360 - val_out_10_acc: 0.4252 - val_out_11_acc: 0.4273 - val_out_12_acc: 0.4490 - val_out_13_acc: 0.4360 - val_out_14_acc: 0.4490 - val_out_15_acc: 0.4642 - val_out_16_acc: 0.4555 - val_out_17_acc: 0.4599 - val_out_18_acc: 0.4599 - val_out_19_acc: 0.4577 - val_out_20_acc: 0.4403 - val_out_21_acc: 0.4447 - val_out_22_acc: 0.4469 - val_out_23_acc: 0.4208 - val_out_24_acc: 0.4295 - val_out_25_acc: 0.4425 - val_out_26_acc: 0.4447 - val_out_27_acc: 0.4295 - val_out_28_acc: 0.4447 - val_out_29_acc: 0.4447 - val_out_30_acc: 0.4425 - val_out_31_acc: 0.4360 - val_out_32_acc: 0.4512\n",
      "Epoch 8/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 62.9910 - out_loss: 1.7309 - out_0_loss: 1.4629 - out_1_loss: 1.8451 - out_2_loss: 1.8668 - out_3_loss: 1.8721 - out_4_loss: 1.8921 - out_5_loss: 1.8554 - out_6_loss: 1.8809 - out_7_loss: 1.8649 - out_8_loss: 1.8678 - out_9_loss: 1.8842 - out_10_loss: 1.8814 - out_11_loss: 1.8743 - out_12_loss: 1.8568 - out_13_loss: 1.8661 - out_14_loss: 1.8792 - out_15_loss: 1.8474 - out_16_loss: 1.8664 - out_17_loss: 1.8714 - out_18_loss: 1.8549 - out_19_loss: 1.8433 - out_20_loss: 1.8965 - out_21_loss: 1.8745 - out_22_loss: 1.8383 - out_23_loss: 1.8759 - out_24_loss: 1.8776 - out_25_loss: 1.8513 - out_26_loss: 1.8819 - out_27_loss: 1.8804 - out_28_loss: 1.8654 - out_29_loss: 1.8865 - out_30_loss: 1.8639 - out_31_loss: 1.8759 - out_32_loss: 1.8582 - out_acc: 0.4250 - out_0_acc: 0.4985 - out_1_acc: 0.3748 - out_2_acc: 0.3782 - out_3_acc: 0.3590 - out_4_acc: 0.3301 - out_5_acc: 0.3779 - out_6_acc: 0.3565 - out_7_acc: 0.3745 - out_8_acc: 0.3555 - out_9_acc: 0.3555 - out_10_acc: 0.3540 - out_11_acc: 0.3462 - out_12_acc: 0.3695 - out_13_acc: 0.3500 - out_14_acc: 0.3611 - out_15_acc: 0.3664 - out_16_acc: 0.3652 - out_17_acc: 0.3614 - out_18_acc: 0.3720 - out_19_acc: 0.3745 - out_20_acc: 0.3537 - out_21_acc: 0.3599 - out_22_acc: 0.3732 - out_23_acc: 0.3614 - out_24_acc: 0.3456 - out_25_acc: 0.3766 - out_26_acc: 0.3543 - out_27_acc: 0.3509 - out_28_acc: 0.3673 - out_29_acc: 0.3568 - out_30_acc: 0.3633 - out_31_acc: 0.3453 - out_32_acc: 0.3667 - val_loss: 59.7291 - val_out_loss: 1.6597 - val_out_0_loss: 1.3049 - val_out_1_loss: 1.7047 - val_out_2_loss: 1.7123 - val_out_3_loss: 1.6940 - val_out_4_loss: 1.7175 - val_out_5_loss: 1.7057 - val_out_6_loss: 1.7086 - val_out_7_loss: 1.6809 - val_out_8_loss: 1.6860 - val_out_9_loss: 1.7135 - val_out_10_loss: 1.7082 - val_out_11_loss: 1.7121 - val_out_12_loss: 1.7118 - val_out_13_loss: 1.7129 - val_out_14_loss: 1.7026 - val_out_15_loss: 1.6848 - val_out_16_loss: 1.6957 - val_out_17_loss: 1.6956 - val_out_18_loss: 1.6946 - val_out_19_loss: 1.6905 - val_out_20_loss: 1.7095 - val_out_21_loss: 1.7170 - val_out_22_loss: 1.6691 - val_out_23_loss: 1.6880 - val_out_24_loss: 1.7112 - val_out_25_loss: 1.6930 - val_out_26_loss: 1.7002 - val_out_27_loss: 1.6991 - val_out_28_loss: 1.7166 - val_out_29_loss: 1.6896 - val_out_30_loss: 1.6984 - val_out_31_loss: 1.6952 - val_out_32_loss: 1.6815 - val_out_acc: 0.4165 - val_out_0_acc: 0.5293 - val_out_1_acc: 0.4121 - val_out_2_acc: 0.4078 - val_out_3_acc: 0.4121 - val_out_4_acc: 0.3861 - val_out_5_acc: 0.4100 - val_out_6_acc: 0.4056 - val_out_7_acc: 0.4100 - val_out_8_acc: 0.4230 - val_out_9_acc: 0.4143 - val_out_10_acc: 0.4078 - val_out_11_acc: 0.3883 - val_out_12_acc: 0.4187 - val_out_13_acc: 0.3926 - val_out_14_acc: 0.4252 - val_out_15_acc: 0.4230 - val_out_16_acc: 0.4121 - val_out_17_acc: 0.4273 - val_out_18_acc: 0.4165 - val_out_19_acc: 0.4056 - val_out_20_acc: 0.4100 - val_out_21_acc: 0.4100 - val_out_22_acc: 0.4187 - val_out_23_acc: 0.4013 - val_out_24_acc: 0.3948 - val_out_25_acc: 0.4143 - val_out_26_acc: 0.4056 - val_out_27_acc: 0.3991 - val_out_28_acc: 0.4056 - val_out_29_acc: 0.4013 - val_out_30_acc: 0.4035 - val_out_31_acc: 0.4187 - val_out_32_acc: 0.4143\n",
      "Epoch 9/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 61.2150 - out_loss: 1.6762 - out_0_loss: 1.4062 - out_1_loss: 1.7974 - out_2_loss: 1.8036 - out_3_loss: 1.8180 - out_4_loss: 1.8215 - out_5_loss: 1.8110 - out_6_loss: 1.8231 - out_7_loss: 1.8241 - out_8_loss: 1.8059 - out_9_loss: 1.8296 - out_10_loss: 1.8301 - out_11_loss: 1.8377 - out_12_loss: 1.8024 - out_13_loss: 1.8055 - out_14_loss: 1.8143 - out_15_loss: 1.7982 - out_16_loss: 1.8040 - out_17_loss: 1.8229 - out_18_loss: 1.8076 - out_19_loss: 1.7937 - out_20_loss: 1.8497 - out_21_loss: 1.8186 - out_22_loss: 1.8093 - out_23_loss: 1.8278 - out_24_loss: 1.8450 - out_25_loss: 1.8040 - out_26_loss: 1.8166 - out_27_loss: 1.8189 - out_28_loss: 1.8101 - out_29_loss: 1.8259 - out_30_loss: 1.8124 - out_31_loss: 1.8182 - out_32_loss: 1.8255 - out_acc: 0.4330 - out_0_acc: 0.5071 - out_1_acc: 0.3782 - out_2_acc: 0.3859 - out_3_acc: 0.3670 - out_4_acc: 0.3689 - out_5_acc: 0.3741 - out_6_acc: 0.3813 - out_7_acc: 0.3714 - out_8_acc: 0.3831 - out_9_acc: 0.3717 - out_10_acc: 0.3673 - out_11_acc: 0.3648 - out_12_acc: 0.3946 - out_13_acc: 0.3714 - out_14_acc: 0.3726 - out_15_acc: 0.3841 - out_16_acc: 0.3779 - out_17_acc: 0.3614 - out_18_acc: 0.3769 - out_19_acc: 0.3986 - out_20_acc: 0.3596 - out_21_acc: 0.3834 - out_22_acc: 0.3807 - out_23_acc: 0.3655 - out_24_acc: 0.3599 - out_25_acc: 0.3791 - out_26_acc: 0.3828 - out_27_acc: 0.3738 - out_28_acc: 0.3819 - out_29_acc: 0.3698 - out_30_acc: 0.3831 - out_31_acc: 0.3741 - out_32_acc: 0.3636 - val_loss: 58.2766 - val_out_loss: 1.6231 - val_out_0_loss: 1.2618 - val_out_1_loss: 1.6537 - val_out_2_loss: 1.6548 - val_out_3_loss: 1.6718 - val_out_4_loss: 1.6695 - val_out_5_loss: 1.6607 - val_out_6_loss: 1.6839 - val_out_7_loss: 1.6490 - val_out_8_loss: 1.6497 - val_out_9_loss: 1.6819 - val_out_10_loss: 1.6705 - val_out_11_loss: 1.6625 - val_out_12_loss: 1.6531 - val_out_13_loss: 1.6351 - val_out_14_loss: 1.6519 - val_out_15_loss: 1.6621 - val_out_16_loss: 1.6564 - val_out_17_loss: 1.6537 - val_out_18_loss: 1.6295 - val_out_19_loss: 1.6459 - val_out_20_loss: 1.6656 - val_out_21_loss: 1.6850 - val_out_22_loss: 1.6377 - val_out_23_loss: 1.6649 - val_out_24_loss: 1.6724 - val_out_25_loss: 1.6627 - val_out_26_loss: 1.6589 - val_out_27_loss: 1.6585 - val_out_28_loss: 1.6603 - val_out_29_loss: 1.6578 - val_out_30_loss: 1.6636 - val_out_31_loss: 1.6529 - val_out_32_loss: 1.6486 - val_out_acc: 0.4317 - val_out_0_acc: 0.5770 - val_out_1_acc: 0.4382 - val_out_2_acc: 0.4338 - val_out_3_acc: 0.4187 - val_out_4_acc: 0.4165 - val_out_5_acc: 0.4338 - val_out_6_acc: 0.4252 - val_out_7_acc: 0.4317 - val_out_8_acc: 0.4295 - val_out_9_acc: 0.4317 - val_out_10_acc: 0.4360 - val_out_11_acc: 0.4338 - val_out_12_acc: 0.4447 - val_out_13_acc: 0.4447 - val_out_14_acc: 0.4273 - val_out_15_acc: 0.4317 - val_out_16_acc: 0.4208 - val_out_17_acc: 0.4469 - val_out_18_acc: 0.4447 - val_out_19_acc: 0.4360 - val_out_20_acc: 0.4295 - val_out_21_acc: 0.4273 - val_out_22_acc: 0.4273 - val_out_23_acc: 0.4295 - val_out_24_acc: 0.4273 - val_out_25_acc: 0.4295 - val_out_26_acc: 0.4317 - val_out_27_acc: 0.4230 - val_out_28_acc: 0.4360 - val_out_29_acc: 0.4360 - val_out_30_acc: 0.4317 - val_out_31_acc: 0.4295 - val_out_32_acc: 0.4403\n",
      "Epoch 10/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 59.9967 - out_loss: 1.6395 - out_0_loss: 1.3707 - out_1_loss: 1.7615 - out_2_loss: 1.7889 - out_3_loss: 1.7689 - out_4_loss: 1.8081 - out_5_loss: 1.7791 - out_6_loss: 1.7980 - out_7_loss: 1.7944 - out_8_loss: 1.7850 - out_9_loss: 1.7865 - out_10_loss: 1.7789 - out_11_loss: 1.7798 - out_12_loss: 1.7837 - out_13_loss: 1.7677 - out_14_loss: 1.7842 - out_15_loss: 1.7707 - out_16_loss: 1.7687 - out_17_loss: 1.7925 - out_18_loss: 1.7732 - out_19_loss: 1.7673 - out_20_loss: 1.8087 - out_21_loss: 1.8053 - out_22_loss: 1.7641 - out_23_loss: 1.7675 - out_24_loss: 1.7764 - out_25_loss: 1.7696 - out_26_loss: 1.7664 - out_27_loss: 1.7964 - out_28_loss: 1.7890 - out_29_loss: 1.7869 - out_30_loss: 1.7950 - out_31_loss: 1.7653 - out_32_loss: 1.7585 - out_acc: 0.4420 - out_0_acc: 0.5270 - out_1_acc: 0.3924 - out_2_acc: 0.3819 - out_3_acc: 0.3853 - out_4_acc: 0.3717 - out_5_acc: 0.3859 - out_6_acc: 0.3735 - out_7_acc: 0.3803 - out_8_acc: 0.3800 - out_9_acc: 0.3884 - out_10_acc: 0.3906 - out_11_acc: 0.3853 - out_12_acc: 0.3927 - out_13_acc: 0.3915 - out_14_acc: 0.3788 - out_15_acc: 0.4005 - out_16_acc: 0.3896 - out_17_acc: 0.3741 - out_18_acc: 0.3971 - out_19_acc: 0.4042 - out_20_acc: 0.3785 - out_21_acc: 0.3797 - out_22_acc: 0.3949 - out_23_acc: 0.3844 - out_24_acc: 0.3856 - out_25_acc: 0.4055 - out_26_acc: 0.3865 - out_27_acc: 0.3751 - out_28_acc: 0.3906 - out_29_acc: 0.3859 - out_30_acc: 0.3816 - out_31_acc: 0.3878 - out_32_acc: 0.4014 - val_loss: 54.7795 - val_out_loss: 1.5197 - val_out_0_loss: 1.1506 - val_out_1_loss: 1.5458 - val_out_2_loss: 1.5615 - val_out_3_loss: 1.5585 - val_out_4_loss: 1.5711 - val_out_5_loss: 1.5655 - val_out_6_loss: 1.5604 - val_out_7_loss: 1.5353 - val_out_8_loss: 1.5528 - val_out_9_loss: 1.5717 - val_out_10_loss: 1.5709 - val_out_11_loss: 1.5662 - val_out_12_loss: 1.5622 - val_out_13_loss: 1.5598 - val_out_14_loss: 1.5581 - val_out_15_loss: 1.5519 - val_out_16_loss: 1.5504 - val_out_17_loss: 1.5640 - val_out_18_loss: 1.5357 - val_out_19_loss: 1.5521 - val_out_20_loss: 1.5971 - val_out_21_loss: 1.5707 - val_out_22_loss: 1.5490 - val_out_23_loss: 1.5560 - val_out_24_loss: 1.5514 - val_out_25_loss: 1.5655 - val_out_26_loss: 1.5803 - val_out_27_loss: 1.5716 - val_out_28_loss: 1.5692 - val_out_29_loss: 1.5676 - val_out_30_loss: 1.5594 - val_out_31_loss: 1.5489 - val_out_32_loss: 1.5604 - val_out_acc: 0.4816 - val_out_0_acc: 0.5813 - val_out_1_acc: 0.4642 - val_out_2_acc: 0.4707 - val_out_3_acc: 0.4620 - val_out_4_acc: 0.4664 - val_out_5_acc: 0.4729 - val_out_6_acc: 0.4859 - val_out_7_acc: 0.4924 - val_out_8_acc: 0.4794 - val_out_9_acc: 0.4772 - val_out_10_acc: 0.4751 - val_out_11_acc: 0.4707 - val_out_12_acc: 0.4794 - val_out_13_acc: 0.4599 - val_out_14_acc: 0.4707 - val_out_15_acc: 0.4729 - val_out_16_acc: 0.4794 - val_out_17_acc: 0.4924 - val_out_18_acc: 0.4881 - val_out_19_acc: 0.4816 - val_out_20_acc: 0.4816 - val_out_21_acc: 0.4707 - val_out_22_acc: 0.4772 - val_out_23_acc: 0.4794 - val_out_24_acc: 0.4664 - val_out_25_acc: 0.4837 - val_out_26_acc: 0.4816 - val_out_27_acc: 0.4534 - val_out_28_acc: 0.4794 - val_out_29_acc: 0.4729 - val_out_30_acc: 0.4664 - val_out_31_acc: 0.4729 - val_out_32_acc: 0.4685\n",
      "Epoch 11/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 58.9519 - out_loss: 1.6098 - out_0_loss: 1.3552 - out_1_loss: 1.7278 - out_2_loss: 1.7513 - out_3_loss: 1.7493 - out_4_loss: 1.7650 - out_5_loss: 1.7514 - out_6_loss: 1.7461 - out_7_loss: 1.7620 - out_8_loss: 1.7301 - out_9_loss: 1.7500 - out_10_loss: 1.7487 - out_11_loss: 1.7665 - out_12_loss: 1.7490 - out_13_loss: 1.7284 - out_14_loss: 1.7619 - out_15_loss: 1.7495 - out_16_loss: 1.7465 - out_17_loss: 1.7481 - out_18_loss: 1.7283 - out_19_loss: 1.7439 - out_20_loss: 1.7494 - out_21_loss: 1.7615 - out_22_loss: 1.7546 - out_23_loss: 1.7555 - out_24_loss: 1.7495 - out_25_loss: 1.7375 - out_26_loss: 1.7629 - out_27_loss: 1.7693 - out_28_loss: 1.7466 - out_29_loss: 1.7659 - out_30_loss: 1.7428 - out_31_loss: 1.7527 - out_32_loss: 1.7349 - out_acc: 0.4526 - out_0_acc: 0.5257 - out_1_acc: 0.3958 - out_2_acc: 0.4020 - out_3_acc: 0.3918 - out_4_acc: 0.3831 - out_5_acc: 0.3980 - out_6_acc: 0.3993 - out_7_acc: 0.3955 - out_8_acc: 0.3918 - out_9_acc: 0.3974 - out_10_acc: 0.3958 - out_11_acc: 0.3847 - out_12_acc: 0.4017 - out_13_acc: 0.4051 - out_14_acc: 0.3996 - out_15_acc: 0.3958 - out_16_acc: 0.3993 - out_17_acc: 0.4039 - out_18_acc: 0.4079 - out_19_acc: 0.4008 - out_20_acc: 0.4008 - out_21_acc: 0.3983 - out_22_acc: 0.3958 - out_23_acc: 0.3903 - out_24_acc: 0.3924 - out_25_acc: 0.4008 - out_26_acc: 0.3887 - out_27_acc: 0.3881 - out_28_acc: 0.3946 - out_29_acc: 0.3927 - out_30_acc: 0.4055 - out_31_acc: 0.3958 - out_32_acc: 0.4030 - val_loss: 57.7781 - val_out_loss: 1.6109 - val_out_0_loss: 1.2358 - val_out_1_loss: 1.6262 - val_out_2_loss: 1.6597 - val_out_3_loss: 1.6420 - val_out_4_loss: 1.6523 - val_out_5_loss: 1.6428 - val_out_6_loss: 1.6485 - val_out_7_loss: 1.6314 - val_out_8_loss: 1.6455 - val_out_9_loss: 1.6639 - val_out_10_loss: 1.6534 - val_out_11_loss: 1.6538 - val_out_12_loss: 1.6487 - val_out_13_loss: 1.6402 - val_out_14_loss: 1.6570 - val_out_15_loss: 1.6465 - val_out_16_loss: 1.6397 - val_out_17_loss: 1.6498 - val_out_18_loss: 1.6195 - val_out_19_loss: 1.6311 - val_out_20_loss: 1.6602 - val_out_21_loss: 1.6642 - val_out_22_loss: 1.6382 - val_out_23_loss: 1.6281 - val_out_24_loss: 1.6590 - val_out_25_loss: 1.6385 - val_out_26_loss: 1.6498 - val_out_27_loss: 1.6319 - val_out_28_loss: 1.6514 - val_out_29_loss: 1.6432 - val_out_30_loss: 1.6515 - val_out_31_loss: 1.6402 - val_out_32_loss: 1.6360 - val_out_acc: 0.4924 - val_out_0_acc: 0.5879 - val_out_1_acc: 0.4772 - val_out_2_acc: 0.4685 - val_out_3_acc: 0.4664 - val_out_4_acc: 0.4751 - val_out_5_acc: 0.4707 - val_out_6_acc: 0.4881 - val_out_7_acc: 0.4837 - val_out_8_acc: 0.4946 - val_out_9_acc: 0.4772 - val_out_10_acc: 0.4599 - val_out_11_acc: 0.4577 - val_out_12_acc: 0.4707 - val_out_13_acc: 0.4772 - val_out_14_acc: 0.4859 - val_out_15_acc: 0.4859 - val_out_16_acc: 0.4664 - val_out_17_acc: 0.4794 - val_out_18_acc: 0.4794 - val_out_19_acc: 0.4555 - val_out_20_acc: 0.4751 - val_out_21_acc: 0.4555 - val_out_22_acc: 0.4620 - val_out_23_acc: 0.4772 - val_out_24_acc: 0.4642 - val_out_25_acc: 0.4707 - val_out_26_acc: 0.4729 - val_out_27_acc: 0.4751 - val_out_28_acc: 0.4490 - val_out_29_acc: 0.5033 - val_out_30_acc: 0.4599 - val_out_31_acc: 0.4729 - val_out_32_acc: 0.4794\n",
      "Epoch 12/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 56.3722 - out_loss: 1.5323 - out_0_loss: 1.2423 - out_1_loss: 1.6533 - out_2_loss: 1.6611 - out_3_loss: 1.6662 - out_4_loss: 1.7037 - out_5_loss: 1.6758 - out_6_loss: 1.6783 - out_7_loss: 1.6715 - out_8_loss: 1.6755 - out_9_loss: 1.6782 - out_10_loss: 1.6810 - out_11_loss: 1.6687 - out_12_loss: 1.6750 - out_13_loss: 1.6683 - out_14_loss: 1.6671 - out_15_loss: 1.6962 - out_16_loss: 1.6723 - out_17_loss: 1.6982 - out_18_loss: 1.6570 - out_19_loss: 1.6679 - out_20_loss: 1.6934 - out_21_loss: 1.6757 - out_22_loss: 1.6825 - out_23_loss: 1.6637 - out_24_loss: 1.6764 - out_25_loss: 1.6694 - out_26_loss: 1.6822 - out_27_loss: 1.6927 - out_28_loss: 1.6787 - out_29_loss: 1.6632 - out_30_loss: 1.6687 - out_31_loss: 1.6602 - out_32_loss: 1.6751 - out_acc: 0.4768 - out_0_acc: 0.5595 - out_1_acc: 0.4309 - out_2_acc: 0.4361 - out_3_acc: 0.4197 - out_4_acc: 0.4042 - out_5_acc: 0.4222 - out_6_acc: 0.4073 - out_7_acc: 0.4250 - out_8_acc: 0.4272 - out_9_acc: 0.4247 - out_10_acc: 0.4182 - out_11_acc: 0.4216 - out_12_acc: 0.4287 - out_13_acc: 0.4172 - out_14_acc: 0.4194 - out_15_acc: 0.4188 - out_16_acc: 0.4191 - out_17_acc: 0.4082 - out_18_acc: 0.4430 - out_19_acc: 0.4306 - out_20_acc: 0.4210 - out_21_acc: 0.4281 - out_22_acc: 0.4203 - out_23_acc: 0.4241 - out_24_acc: 0.4241 - out_25_acc: 0.4423 - out_26_acc: 0.4154 - out_27_acc: 0.4117 - out_28_acc: 0.4206 - out_29_acc: 0.4151 - out_30_acc: 0.4188 - out_31_acc: 0.4265 - out_32_acc: 0.4241 - val_loss: 54.1947 - val_out_loss: 1.5088 - val_out_0_loss: 1.1580 - val_out_1_loss: 1.5351 - val_out_2_loss: 1.5549 - val_out_3_loss: 1.5339 - val_out_4_loss: 1.5613 - val_out_5_loss: 1.5416 - val_out_6_loss: 1.5330 - val_out_7_loss: 1.5330 - val_out_8_loss: 1.5469 - val_out_9_loss: 1.5554 - val_out_10_loss: 1.5450 - val_out_11_loss: 1.5479 - val_out_12_loss: 1.5522 - val_out_13_loss: 1.5424 - val_out_14_loss: 1.5286 - val_out_15_loss: 1.5399 - val_out_16_loss: 1.5407 - val_out_17_loss: 1.5467 - val_out_18_loss: 1.5294 - val_out_19_loss: 1.5490 - val_out_20_loss: 1.5614 - val_out_21_loss: 1.5438 - val_out_22_loss: 1.5288 - val_out_23_loss: 1.5398 - val_out_24_loss: 1.5383 - val_out_25_loss: 1.5552 - val_out_26_loss: 1.5451 - val_out_27_loss: 1.5506 - val_out_28_loss: 1.5438 - val_out_29_loss: 1.5431 - val_out_30_loss: 1.5435 - val_out_31_loss: 1.5391 - val_out_32_loss: 1.5333 - val_out_acc: 0.4729 - val_out_0_acc: 0.6204 - val_out_1_acc: 0.4620 - val_out_2_acc: 0.4794 - val_out_3_acc: 0.4685 - val_out_4_acc: 0.4685 - val_out_5_acc: 0.4902 - val_out_6_acc: 0.4794 - val_out_7_acc: 0.4772 - val_out_8_acc: 0.4881 - val_out_9_acc: 0.4794 - val_out_10_acc: 0.4685 - val_out_11_acc: 0.4599 - val_out_12_acc: 0.4664 - val_out_13_acc: 0.4707 - val_out_14_acc: 0.4881 - val_out_15_acc: 0.4751 - val_out_16_acc: 0.4794 - val_out_17_acc: 0.4772 - val_out_18_acc: 0.4881 - val_out_19_acc: 0.4707 - val_out_20_acc: 0.4772 - val_out_21_acc: 0.4664 - val_out_22_acc: 0.4664 - val_out_23_acc: 0.4685 - val_out_24_acc: 0.4859 - val_out_25_acc: 0.4664 - val_out_26_acc: 0.4751 - val_out_27_acc: 0.4794 - val_out_28_acc: 0.4685 - val_out_29_acc: 0.4555 - val_out_30_acc: 0.4685 - val_out_31_acc: 0.4794 - val_out_32_acc: 0.4729\n",
      "Epoch 13/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 55.4556 - out_loss: 1.5045 - out_0_loss: 1.2924 - out_1_loss: 1.6281 - out_2_loss: 1.6453 - out_3_loss: 1.6581 - out_4_loss: 1.6608 - out_5_loss: 1.6392 - out_6_loss: 1.6341 - out_7_loss: 1.6429 - out_8_loss: 1.6430 - out_9_loss: 1.6672 - out_10_loss: 1.6371 - out_11_loss: 1.6565 - out_12_loss: 1.6356 - out_13_loss: 1.6302 - out_14_loss: 1.6299 - out_15_loss: 1.6620 - out_16_loss: 1.6676 - out_17_loss: 1.6556 - out_18_loss: 1.6483 - out_19_loss: 1.6344 - out_20_loss: 1.6638 - out_21_loss: 1.6559 - out_22_loss: 1.6425 - out_23_loss: 1.6482 - out_24_loss: 1.6625 - out_25_loss: 1.6369 - out_26_loss: 1.6341 - out_27_loss: 1.6555 - out_28_loss: 1.6555 - out_29_loss: 1.6320 - out_30_loss: 1.6254 - out_31_loss: 1.6381 - out_32_loss: 1.6325 - out_acc: 0.4851 - out_0_acc: 0.5558 - out_1_acc: 0.4380 - out_2_acc: 0.4392 - out_3_acc: 0.4318 - out_4_acc: 0.4191 - out_5_acc: 0.4383 - out_6_acc: 0.4383 - out_7_acc: 0.4392 - out_8_acc: 0.4312 - out_9_acc: 0.4303 - out_10_acc: 0.4284 - out_11_acc: 0.4296 - out_12_acc: 0.4458 - out_13_acc: 0.4259 - out_14_acc: 0.4451 - out_15_acc: 0.4352 - out_16_acc: 0.4296 - out_17_acc: 0.4272 - out_18_acc: 0.4377 - out_19_acc: 0.4380 - out_20_acc: 0.4315 - out_21_acc: 0.4340 - out_22_acc: 0.4315 - out_23_acc: 0.4427 - out_24_acc: 0.4200 - out_25_acc: 0.4451 - out_26_acc: 0.4408 - out_27_acc: 0.4228 - out_28_acc: 0.4268 - out_29_acc: 0.4306 - out_30_acc: 0.4417 - out_31_acc: 0.4355 - out_32_acc: 0.4433 - val_loss: 50.1505 - val_out_loss: 1.3946 - val_out_0_loss: 1.1440 - val_out_1_loss: 1.4009 - val_out_2_loss: 1.4397 - val_out_3_loss: 1.4246 - val_out_4_loss: 1.4407 - val_out_5_loss: 1.4247 - val_out_6_loss: 1.4137 - val_out_7_loss: 1.4123 - val_out_8_loss: 1.4257 - val_out_9_loss: 1.4468 - val_out_10_loss: 1.4239 - val_out_11_loss: 1.4281 - val_out_12_loss: 1.4222 - val_out_13_loss: 1.4277 - val_out_14_loss: 1.4141 - val_out_15_loss: 1.4119 - val_out_16_loss: 1.4231 - val_out_17_loss: 1.4427 - val_out_18_loss: 1.4179 - val_out_19_loss: 1.4361 - val_out_20_loss: 1.4331 - val_out_21_loss: 1.4333 - val_out_22_loss: 1.4132 - val_out_23_loss: 1.4215 - val_out_24_loss: 1.4223 - val_out_25_loss: 1.4247 - val_out_26_loss: 1.4313 - val_out_27_loss: 1.4306 - val_out_28_loss: 1.4405 - val_out_29_loss: 1.4275 - val_out_30_loss: 1.4248 - val_out_31_loss: 1.4303 - val_out_32_loss: 1.4171 - val_out_acc: 0.5076 - val_out_0_acc: 0.5879 - val_out_1_acc: 0.5098 - val_out_2_acc: 0.4924 - val_out_3_acc: 0.4989 - val_out_4_acc: 0.5098 - val_out_5_acc: 0.5011 - val_out_6_acc: 0.5098 - val_out_7_acc: 0.5033 - val_out_8_acc: 0.5033 - val_out_9_acc: 0.4946 - val_out_10_acc: 0.5184 - val_out_11_acc: 0.5054 - val_out_12_acc: 0.5163 - val_out_13_acc: 0.4946 - val_out_14_acc: 0.4902 - val_out_15_acc: 0.5076 - val_out_16_acc: 0.5141 - val_out_17_acc: 0.5076 - val_out_18_acc: 0.5033 - val_out_19_acc: 0.5011 - val_out_20_acc: 0.5076 - val_out_21_acc: 0.5098 - val_out_22_acc: 0.5076 - val_out_23_acc: 0.5054 - val_out_24_acc: 0.5076 - val_out_25_acc: 0.5163 - val_out_26_acc: 0.5098 - val_out_27_acc: 0.5119 - val_out_28_acc: 0.4967 - val_out_29_acc: 0.4989 - val_out_30_acc: 0.5033 - val_out_31_acc: 0.5033 - val_out_32_acc: 0.5163\n",
      "Epoch 14/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 54.4150 - out_loss: 1.4734 - out_0_loss: 1.2234 - out_1_loss: 1.5945 - out_2_loss: 1.6198 - out_3_loss: 1.6200 - out_4_loss: 1.6309 - out_5_loss: 1.6126 - out_6_loss: 1.6280 - out_7_loss: 1.6159 - out_8_loss: 1.6031 - out_9_loss: 1.6262 - out_10_loss: 1.6128 - out_11_loss: 1.6141 - out_12_loss: 1.6139 - out_13_loss: 1.6240 - out_14_loss: 1.5936 - out_15_loss: 1.6133 - out_16_loss: 1.6247 - out_17_loss: 1.6219 - out_18_loss: 1.6019 - out_19_loss: 1.6177 - out_20_loss: 1.6083 - out_21_loss: 1.6351 - out_22_loss: 1.6200 - out_23_loss: 1.6154 - out_24_loss: 1.6328 - out_25_loss: 1.6044 - out_26_loss: 1.6091 - out_27_loss: 1.6140 - out_28_loss: 1.6256 - out_29_loss: 1.6084 - out_30_loss: 1.6116 - out_31_loss: 1.6119 - out_32_loss: 1.6326 - out_acc: 0.5009 - out_0_acc: 0.5759 - out_1_acc: 0.4489 - out_2_acc: 0.4399 - out_3_acc: 0.4454 - out_4_acc: 0.4321 - out_5_acc: 0.4374 - out_6_acc: 0.4358 - out_7_acc: 0.4324 - out_8_acc: 0.4495 - out_9_acc: 0.4476 - out_10_acc: 0.4414 - out_11_acc: 0.4383 - out_12_acc: 0.4523 - out_13_acc: 0.4396 - out_14_acc: 0.4538 - out_15_acc: 0.4547 - out_16_acc: 0.4386 - out_17_acc: 0.4318 - out_18_acc: 0.4563 - out_19_acc: 0.4467 - out_20_acc: 0.4507 - out_21_acc: 0.4380 - out_22_acc: 0.4349 - out_23_acc: 0.4461 - out_24_acc: 0.4383 - out_25_acc: 0.4507 - out_26_acc: 0.4411 - out_27_acc: 0.4442 - out_28_acc: 0.4402 - out_29_acc: 0.4516 - out_30_acc: 0.4430 - out_31_acc: 0.4476 - out_32_acc: 0.4392 - val_loss: 49.8836 - val_out_loss: 1.3890 - val_out_0_loss: 1.1662 - val_out_1_loss: 1.4112 - val_out_2_loss: 1.4252 - val_out_3_loss: 1.4140 - val_out_4_loss: 1.4159 - val_out_5_loss: 1.4211 - val_out_6_loss: 1.4160 - val_out_7_loss: 1.4117 - val_out_8_loss: 1.4217 - val_out_9_loss: 1.4311 - val_out_10_loss: 1.4256 - val_out_11_loss: 1.4234 - val_out_12_loss: 1.4201 - val_out_13_loss: 1.4138 - val_out_14_loss: 1.4147 - val_out_15_loss: 1.4134 - val_out_16_loss: 1.4071 - val_out_17_loss: 1.4253 - val_out_18_loss: 1.4032 - val_out_19_loss: 1.4109 - val_out_20_loss: 1.4311 - val_out_21_loss: 1.4207 - val_out_22_loss: 1.4081 - val_out_23_loss: 1.4181 - val_out_24_loss: 1.4183 - val_out_25_loss: 1.4116 - val_out_26_loss: 1.4259 - val_out_27_loss: 1.4227 - val_out_28_loss: 1.4214 - val_out_29_loss: 1.4146 - val_out_30_loss: 1.4180 - val_out_31_loss: 1.4090 - val_out_32_loss: 1.4089 - val_out_acc: 0.5119 - val_out_0_acc: 0.5965 - val_out_1_acc: 0.5098 - val_out_2_acc: 0.4946 - val_out_3_acc: 0.5011 - val_out_4_acc: 0.5184 - val_out_5_acc: 0.5206 - val_out_6_acc: 0.5076 - val_out_7_acc: 0.5033 - val_out_8_acc: 0.4989 - val_out_9_acc: 0.5206 - val_out_10_acc: 0.4859 - val_out_11_acc: 0.5011 - val_out_12_acc: 0.5033 - val_out_13_acc: 0.5119 - val_out_14_acc: 0.4989 - val_out_15_acc: 0.5163 - val_out_16_acc: 0.4989 - val_out_17_acc: 0.5184 - val_out_18_acc: 0.5163 - val_out_19_acc: 0.5054 - val_out_20_acc: 0.5011 - val_out_21_acc: 0.5011 - val_out_22_acc: 0.5011 - val_out_23_acc: 0.5228 - val_out_24_acc: 0.5119 - val_out_25_acc: 0.5141 - val_out_26_acc: 0.4946 - val_out_27_acc: 0.4967 - val_out_28_acc: 0.4989 - val_out_29_acc: 0.5249 - val_out_30_acc: 0.5076 - val_out_31_acc: 0.5141 - val_out_32_acc: 0.5076\n",
      "Epoch 15/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 53.7584 - out_loss: 1.4588 - out_0_loss: 1.1970 - out_1_loss: 1.5825 - out_2_loss: 1.5987 - out_3_loss: 1.6059 - out_4_loss: 1.5890 - out_5_loss: 1.5922 - out_6_loss: 1.6120 - out_7_loss: 1.6101 - out_8_loss: 1.5864 - out_9_loss: 1.5982 - out_10_loss: 1.5869 - out_11_loss: 1.5978 - out_12_loss: 1.6012 - out_13_loss: 1.5951 - out_14_loss: 1.5933 - out_15_loss: 1.6125 - out_16_loss: 1.6011 - out_17_loss: 1.5903 - out_18_loss: 1.6212 - out_19_loss: 1.5830 - out_20_loss: 1.5927 - out_21_loss: 1.6235 - out_22_loss: 1.5864 - out_23_loss: 1.5903 - out_24_loss: 1.6108 - out_25_loss: 1.5911 - out_26_loss: 1.5910 - out_27_loss: 1.6009 - out_28_loss: 1.6004 - out_29_loss: 1.5939 - out_30_loss: 1.5862 - out_31_loss: 1.5953 - out_32_loss: 1.5828 - out_acc: 0.4978 - out_0_acc: 0.5834 - out_1_acc: 0.4585 - out_2_acc: 0.4470 - out_3_acc: 0.4485 - out_4_acc: 0.4476 - out_5_acc: 0.4538 - out_6_acc: 0.4479 - out_7_acc: 0.4547 - out_8_acc: 0.4532 - out_9_acc: 0.4557 - out_10_acc: 0.4489 - out_11_acc: 0.4538 - out_12_acc: 0.4538 - out_13_acc: 0.4529 - out_14_acc: 0.4501 - out_15_acc: 0.4423 - out_16_acc: 0.4340 - out_17_acc: 0.4482 - out_18_acc: 0.4423 - out_19_acc: 0.4554 - out_20_acc: 0.4414 - out_21_acc: 0.4476 - out_22_acc: 0.4504 - out_23_acc: 0.4439 - out_24_acc: 0.4405 - out_25_acc: 0.4575 - out_26_acc: 0.4461 - out_27_acc: 0.4482 - out_28_acc: 0.4479 - out_29_acc: 0.4507 - out_30_acc: 0.4544 - out_31_acc: 0.4485 - out_32_acc: 0.4625 - val_loss: 51.4267 - val_out_loss: 1.4314 - val_out_0_loss: 1.1493 - val_out_1_loss: 1.4506 - val_out_2_loss: 1.4741 - val_out_3_loss: 1.4608 - val_out_4_loss: 1.4877 - val_out_5_loss: 1.4678 - val_out_6_loss: 1.4583 - val_out_7_loss: 1.4607 - val_out_8_loss: 1.4666 - val_out_9_loss: 1.4748 - val_out_10_loss: 1.4706 - val_out_11_loss: 1.4731 - val_out_12_loss: 1.4571 - val_out_13_loss: 1.4582 - val_out_14_loss: 1.4485 - val_out_15_loss: 1.4588 - val_out_16_loss: 1.4595 - val_out_17_loss: 1.4820 - val_out_18_loss: 1.4484 - val_out_19_loss: 1.4601 - val_out_20_loss: 1.4714 - val_out_21_loss: 1.4639 - val_out_22_loss: 1.4612 - val_out_23_loss: 1.4549 - val_out_24_loss: 1.4625 - val_out_25_loss: 1.4620 - val_out_26_loss: 1.4683 - val_out_27_loss: 1.4632 - val_out_28_loss: 1.4606 - val_out_29_loss: 1.4616 - val_out_30_loss: 1.4503 - val_out_31_loss: 1.4595 - val_out_32_loss: 1.4532 - val_out_acc: 0.4967 - val_out_0_acc: 0.6030 - val_out_1_acc: 0.4794 - val_out_2_acc: 0.5033 - val_out_3_acc: 0.4664 - val_out_4_acc: 0.4881 - val_out_5_acc: 0.4772 - val_out_6_acc: 0.4946 - val_out_7_acc: 0.4967 - val_out_8_acc: 0.4967 - val_out_9_acc: 0.4967 - val_out_10_acc: 0.4707 - val_out_11_acc: 0.4816 - val_out_12_acc: 0.5033 - val_out_13_acc: 0.4816 - val_out_14_acc: 0.4881 - val_out_15_acc: 0.4946 - val_out_16_acc: 0.4946 - val_out_17_acc: 0.4989 - val_out_18_acc: 0.4946 - val_out_19_acc: 0.4924 - val_out_20_acc: 0.5033 - val_out_21_acc: 0.4946 - val_out_22_acc: 0.4751 - val_out_23_acc: 0.4794 - val_out_24_acc: 0.4924 - val_out_25_acc: 0.4751 - val_out_26_acc: 0.4837 - val_out_27_acc: 0.4881 - val_out_28_acc: 0.4989 - val_out_29_acc: 0.4924 - val_out_30_acc: 0.4816 - val_out_31_acc: 0.4946 - val_out_32_acc: 0.4924\n",
      "Epoch 16/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 52.9323 - out_loss: 1.4328 - out_0_loss: 1.2114 - out_1_loss: 1.5537 - out_2_loss: 1.5720 - out_3_loss: 1.5579 - out_4_loss: 1.5859 - out_5_loss: 1.5852 - out_6_loss: 1.5780 - out_7_loss: 1.5630 - out_8_loss: 1.5673 - out_9_loss: 1.5617 - out_10_loss: 1.5855 - out_11_loss: 1.5810 - out_12_loss: 1.5725 - out_13_loss: 1.5692 - out_14_loss: 1.5698 - out_15_loss: 1.5781 - out_16_loss: 1.5742 - out_17_loss: 1.5684 - out_18_loss: 1.5783 - out_19_loss: 1.5617 - out_20_loss: 1.5786 - out_21_loss: 1.5703 - out_22_loss: 1.5708 - out_23_loss: 1.5713 - out_24_loss: 1.5732 - out_25_loss: 1.5631 - out_26_loss: 1.5741 - out_27_loss: 1.5756 - out_28_loss: 1.5731 - out_29_loss: 1.5602 - out_30_loss: 1.5812 - out_31_loss: 1.5721 - out_32_loss: 1.5610 - out_acc: 0.5124 - out_0_acc: 0.5809 - out_1_acc: 0.4675 - out_2_acc: 0.4616 - out_3_acc: 0.4585 - out_4_acc: 0.4495 - out_5_acc: 0.4535 - out_6_acc: 0.4464 - out_7_acc: 0.4616 - out_8_acc: 0.4647 - out_9_acc: 0.4532 - out_10_acc: 0.4445 - out_11_acc: 0.4637 - out_12_acc: 0.4544 - out_13_acc: 0.4566 - out_14_acc: 0.4554 - out_15_acc: 0.4647 - out_16_acc: 0.4603 - out_17_acc: 0.4675 - out_18_acc: 0.4650 - out_19_acc: 0.4637 - out_20_acc: 0.4588 - out_21_acc: 0.4616 - out_22_acc: 0.4476 - out_23_acc: 0.4616 - out_24_acc: 0.4538 - out_25_acc: 0.4619 - out_26_acc: 0.4557 - out_27_acc: 0.4547 - out_28_acc: 0.4622 - out_29_acc: 0.4699 - out_30_acc: 0.4662 - out_31_acc: 0.4572 - out_32_acc: 0.4566 - val_loss: 52.9730 - val_out_loss: 1.4725 - val_out_0_loss: 1.2674 - val_out_1_loss: 1.4855 - val_out_2_loss: 1.5175 - val_out_3_loss: 1.4911 - val_out_4_loss: 1.4964 - val_out_5_loss: 1.5068 - val_out_6_loss: 1.4993 - val_out_7_loss: 1.5012 - val_out_8_loss: 1.5162 - val_out_9_loss: 1.5238 - val_out_10_loss: 1.5113 - val_out_11_loss: 1.5073 - val_out_12_loss: 1.5037 - val_out_13_loss: 1.5101 - val_out_14_loss: 1.4993 - val_out_15_loss: 1.5110 - val_out_16_loss: 1.5070 - val_out_17_loss: 1.5060 - val_out_18_loss: 1.4949 - val_out_19_loss: 1.5101 - val_out_20_loss: 1.5072 - val_out_21_loss: 1.5011 - val_out_22_loss: 1.4981 - val_out_23_loss: 1.4953 - val_out_24_loss: 1.4938 - val_out_25_loss: 1.5084 - val_out_26_loss: 1.5089 - val_out_27_loss: 1.5058 - val_out_28_loss: 1.5135 - val_out_29_loss: 1.5025 - val_out_30_loss: 1.5060 - val_out_31_loss: 1.4958 - val_out_32_loss: 1.5013 - val_out_acc: 0.4837 - val_out_0_acc: 0.5748 - val_out_1_acc: 0.4837 - val_out_2_acc: 0.4751 - val_out_3_acc: 0.4772 - val_out_4_acc: 0.4859 - val_out_5_acc: 0.4924 - val_out_6_acc: 0.4794 - val_out_7_acc: 0.4837 - val_out_8_acc: 0.4794 - val_out_9_acc: 0.4837 - val_out_10_acc: 0.4794 - val_out_11_acc: 0.4837 - val_out_12_acc: 0.5011 - val_out_13_acc: 0.4816 - val_out_14_acc: 0.4902 - val_out_15_acc: 0.4924 - val_out_16_acc: 0.4816 - val_out_17_acc: 0.4946 - val_out_18_acc: 0.4881 - val_out_19_acc: 0.4859 - val_out_20_acc: 0.4816 - val_out_21_acc: 0.4816 - val_out_22_acc: 0.4924 - val_out_23_acc: 0.4816 - val_out_24_acc: 0.4924 - val_out_25_acc: 0.4967 - val_out_26_acc: 0.4837 - val_out_27_acc: 0.4837 - val_out_28_acc: 0.4881 - val_out_29_acc: 0.4859 - val_out_30_acc: 0.4794 - val_out_31_acc: 0.4946 - val_out_32_acc: 0.4859\n",
      "Epoch 17/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 52.2675 - out_loss: 1.4152 - out_0_loss: 1.1728 - out_1_loss: 1.5307 - out_2_loss: 1.5493 - out_3_loss: 1.5408 - out_4_loss: 1.5640 - out_5_loss: 1.5548 - out_6_loss: 1.5491 - out_7_loss: 1.5646 - out_8_loss: 1.5373 - out_9_loss: 1.5572 - out_10_loss: 1.5594 - out_11_loss: 1.5465 - out_12_loss: 1.5530 - out_13_loss: 1.5360 - out_14_loss: 1.5405 - out_15_loss: 1.5619 - out_16_loss: 1.5530 - out_17_loss: 1.5599 - out_18_loss: 1.5456 - out_19_loss: 1.5469 - out_20_loss: 1.5603 - out_21_loss: 1.5517 - out_22_loss: 1.5506 - out_23_loss: 1.5571 - out_24_loss: 1.5693 - out_25_loss: 1.5475 - out_26_loss: 1.5497 - out_27_loss: 1.5640 - out_28_loss: 1.5474 - out_29_loss: 1.5481 - out_30_loss: 1.5539 - out_31_loss: 1.5729 - out_32_loss: 1.5566 - out_acc: 0.5096 - out_0_acc: 0.5967 - out_1_acc: 0.4721 - out_2_acc: 0.4693 - out_3_acc: 0.4619 - out_4_acc: 0.4594 - out_5_acc: 0.4656 - out_6_acc: 0.4613 - out_7_acc: 0.4551 - out_8_acc: 0.4684 - out_9_acc: 0.4597 - out_10_acc: 0.4675 - out_11_acc: 0.4625 - out_12_acc: 0.4650 - out_13_acc: 0.4743 - out_14_acc: 0.4644 - out_15_acc: 0.4659 - out_16_acc: 0.4662 - out_17_acc: 0.4572 - out_18_acc: 0.4678 - out_19_acc: 0.4687 - out_20_acc: 0.4563 - out_21_acc: 0.4690 - out_22_acc: 0.4619 - out_23_acc: 0.4563 - out_24_acc: 0.4516 - out_25_acc: 0.4622 - out_26_acc: 0.4706 - out_27_acc: 0.4572 - out_28_acc: 0.4560 - out_29_acc: 0.4740 - out_30_acc: 0.4681 - out_31_acc: 0.4520 - out_32_acc: 0.4647 - val_loss: 49.2334 - val_out_loss: 1.3699 - val_out_0_loss: 1.0834 - val_out_1_loss: 1.3738 - val_out_2_loss: 1.4156 - val_out_3_loss: 1.4010 - val_out_4_loss: 1.4127 - val_out_5_loss: 1.4038 - val_out_6_loss: 1.3955 - val_out_7_loss: 1.3936 - val_out_8_loss: 1.4093 - val_out_9_loss: 1.4215 - val_out_10_loss: 1.4029 - val_out_11_loss: 1.4072 - val_out_12_loss: 1.3990 - val_out_13_loss: 1.4036 - val_out_14_loss: 1.3920 - val_out_15_loss: 1.4055 - val_out_16_loss: 1.3995 - val_out_17_loss: 1.4203 - val_out_18_loss: 1.3938 - val_out_19_loss: 1.4027 - val_out_20_loss: 1.4106 - val_out_21_loss: 1.3821 - val_out_22_loss: 1.3899 - val_out_23_loss: 1.3959 - val_out_24_loss: 1.4058 - val_out_25_loss: 1.3856 - val_out_26_loss: 1.3921 - val_out_27_loss: 1.4072 - val_out_28_loss: 1.4107 - val_out_29_loss: 1.3918 - val_out_30_loss: 1.4051 - val_out_31_loss: 1.4032 - val_out_32_loss: 1.3981 - val_out_acc: 0.4989 - val_out_0_acc: 0.6334 - val_out_1_acc: 0.5033 - val_out_2_acc: 0.5076 - val_out_3_acc: 0.4946 - val_out_4_acc: 0.4967 - val_out_5_acc: 0.4924 - val_out_6_acc: 0.5076 - val_out_7_acc: 0.4946 - val_out_8_acc: 0.4989 - val_out_9_acc: 0.4946 - val_out_10_acc: 0.5098 - val_out_11_acc: 0.4902 - val_out_12_acc: 0.4924 - val_out_13_acc: 0.4881 - val_out_14_acc: 0.4989 - val_out_15_acc: 0.4924 - val_out_16_acc: 0.5011 - val_out_17_acc: 0.5011 - val_out_18_acc: 0.5033 - val_out_19_acc: 0.4967 - val_out_20_acc: 0.4902 - val_out_21_acc: 0.5054 - val_out_22_acc: 0.4967 - val_out_23_acc: 0.5011 - val_out_24_acc: 0.4902 - val_out_25_acc: 0.4967 - val_out_26_acc: 0.4816 - val_out_27_acc: 0.5163 - val_out_28_acc: 0.4989 - val_out_29_acc: 0.5033 - val_out_30_acc: 0.4837 - val_out_31_acc: 0.4881 - val_out_32_acc: 0.5011\n",
      "Epoch 18/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 50.6283 - out_loss: 1.3682 - out_0_loss: 1.1089 - out_1_loss: 1.4989 - out_2_loss: 1.5196 - out_3_loss: 1.5031 - out_4_loss: 1.5176 - out_5_loss: 1.5026 - out_6_loss: 1.5089 - out_7_loss: 1.4958 - out_8_loss: 1.4960 - out_9_loss: 1.5053 - out_10_loss: 1.4892 - out_11_loss: 1.5267 - out_12_loss: 1.4873 - out_13_loss: 1.4959 - out_14_loss: 1.5182 - out_15_loss: 1.5152 - out_16_loss: 1.4940 - out_17_loss: 1.5067 - out_18_loss: 1.4966 - out_19_loss: 1.4991 - out_20_loss: 1.5153 - out_21_loss: 1.5159 - out_22_loss: 1.5012 - out_23_loss: 1.5181 - out_24_loss: 1.5185 - out_25_loss: 1.5155 - out_26_loss: 1.5044 - out_27_loss: 1.4987 - out_28_loss: 1.5029 - out_29_loss: 1.5070 - out_30_loss: 1.4909 - out_31_loss: 1.4984 - out_32_loss: 1.4878 - out_acc: 0.5167 - out_0_acc: 0.6178 - out_1_acc: 0.4746 - out_2_acc: 0.4730 - out_3_acc: 0.4737 - out_4_acc: 0.4709 - out_5_acc: 0.4671 - out_6_acc: 0.4755 - out_7_acc: 0.4833 - out_8_acc: 0.4817 - out_9_acc: 0.4724 - out_10_acc: 0.4795 - out_11_acc: 0.4551 - out_12_acc: 0.4864 - out_13_acc: 0.4851 - out_14_acc: 0.4699 - out_15_acc: 0.4653 - out_16_acc: 0.4681 - out_17_acc: 0.4768 - out_18_acc: 0.4802 - out_19_acc: 0.4768 - out_20_acc: 0.4687 - out_21_acc: 0.4718 - out_22_acc: 0.4752 - out_23_acc: 0.4814 - out_24_acc: 0.4619 - out_25_acc: 0.4721 - out_26_acc: 0.4743 - out_27_acc: 0.4721 - out_28_acc: 0.4758 - out_29_acc: 0.4613 - out_30_acc: 0.4913 - out_31_acc: 0.4768 - out_32_acc: 0.4789 - val_loss: 47.7600 - val_out_loss: 1.3247 - val_out_0_loss: 1.2358 - val_out_1_loss: 1.3317 - val_out_2_loss: 1.3597 - val_out_3_loss: 1.3541 - val_out_4_loss: 1.3691 - val_out_5_loss: 1.3538 - val_out_6_loss: 1.3518 - val_out_7_loss: 1.3487 - val_out_8_loss: 1.3520 - val_out_9_loss: 1.3615 - val_out_10_loss: 1.3569 - val_out_11_loss: 1.3640 - val_out_12_loss: 1.3460 - val_out_13_loss: 1.3545 - val_out_14_loss: 1.3430 - val_out_15_loss: 1.3631 - val_out_16_loss: 1.3479 - val_out_17_loss: 1.3499 - val_out_18_loss: 1.3425 - val_out_19_loss: 1.3537 - val_out_20_loss: 1.3643 - val_out_21_loss: 1.3580 - val_out_22_loss: 1.3540 - val_out_23_loss: 1.3415 - val_out_24_loss: 1.3546 - val_out_25_loss: 1.3592 - val_out_26_loss: 1.3550 - val_out_27_loss: 1.3636 - val_out_28_loss: 1.3559 - val_out_29_loss: 1.3443 - val_out_30_loss: 1.3474 - val_out_31_loss: 1.3549 - val_out_32_loss: 1.3527 - val_out_acc: 0.5380 - val_out_0_acc: 0.5727 - val_out_1_acc: 0.5575 - val_out_2_acc: 0.5401 - val_out_3_acc: 0.5380 - val_out_4_acc: 0.5336 - val_out_5_acc: 0.5466 - val_out_6_acc: 0.5358 - val_out_7_acc: 0.5401 - val_out_8_acc: 0.5336 - val_out_9_acc: 0.5358 - val_out_10_acc: 0.5423 - val_out_11_acc: 0.5163 - val_out_12_acc: 0.5315 - val_out_13_acc: 0.5293 - val_out_14_acc: 0.5380 - val_out_15_acc: 0.5249 - val_out_16_acc: 0.5488 - val_out_17_acc: 0.5466 - val_out_18_acc: 0.5358 - val_out_19_acc: 0.5315 - val_out_20_acc: 0.5293 - val_out_21_acc: 0.5293 - val_out_22_acc: 0.5336 - val_out_23_acc: 0.5380 - val_out_24_acc: 0.5380 - val_out_25_acc: 0.5336 - val_out_26_acc: 0.5358 - val_out_27_acc: 0.5315 - val_out_28_acc: 0.5315 - val_out_29_acc: 0.5423 - val_out_30_acc: 0.5380 - val_out_31_acc: 0.5401 - val_out_32_acc: 0.5380\n",
      "Epoch 19/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 49.4968 - out_loss: 1.3330 - out_0_loss: 1.1389 - out_1_loss: 1.4480 - out_2_loss: 1.4826 - out_3_loss: 1.4750 - out_4_loss: 1.4925 - out_5_loss: 1.4660 - out_6_loss: 1.4613 - out_7_loss: 1.4671 - out_8_loss: 1.4733 - out_9_loss: 1.4589 - out_10_loss: 1.4743 - out_11_loss: 1.4818 - out_12_loss: 1.4788 - out_13_loss: 1.4446 - out_14_loss: 1.4736 - out_15_loss: 1.4758 - out_16_loss: 1.4730 - out_17_loss: 1.4691 - out_18_loss: 1.4875 - out_19_loss: 1.4601 - out_20_loss: 1.4607 - out_21_loss: 1.4778 - out_22_loss: 1.4678 - out_23_loss: 1.4542 - out_24_loss: 1.4758 - out_25_loss: 1.4571 - out_26_loss: 1.4785 - out_27_loss: 1.4573 - out_28_loss: 1.4754 - out_29_loss: 1.4750 - out_30_loss: 1.4798 - out_31_loss: 1.4726 - out_32_loss: 1.4496 - out_acc: 0.5356 - out_0_acc: 0.6138 - out_1_acc: 0.4963 - out_2_acc: 0.4715 - out_3_acc: 0.4808 - out_4_acc: 0.4833 - out_5_acc: 0.4932 - out_6_acc: 0.4814 - out_7_acc: 0.4935 - out_8_acc: 0.4861 - out_9_acc: 0.4969 - out_10_acc: 0.4941 - out_11_acc: 0.4771 - out_12_acc: 0.4876 - out_13_acc: 0.4864 - out_14_acc: 0.4895 - out_15_acc: 0.4892 - out_16_acc: 0.4864 - out_17_acc: 0.4892 - out_18_acc: 0.4861 - out_19_acc: 0.4957 - out_20_acc: 0.4913 - out_21_acc: 0.4882 - out_22_acc: 0.4994 - out_23_acc: 0.4888 - out_24_acc: 0.4888 - out_25_acc: 0.4981 - out_26_acc: 0.4885 - out_27_acc: 0.4892 - out_28_acc: 0.4870 - out_29_acc: 0.4885 - out_30_acc: 0.4904 - out_31_acc: 0.4910 - out_32_acc: 0.4851 - val_loss: 47.1791 - val_out_loss: 1.3036 - val_out_0_loss: 1.1409 - val_out_1_loss: 1.3330 - val_out_2_loss: 1.3438 - val_out_3_loss: 1.3377 - val_out_4_loss: 1.3329 - val_out_5_loss: 1.3285 - val_out_6_loss: 1.3350 - val_out_7_loss: 1.3313 - val_out_8_loss: 1.3476 - val_out_9_loss: 1.3521 - val_out_10_loss: 1.3432 - val_out_11_loss: 1.3529 - val_out_12_loss: 1.3381 - val_out_13_loss: 1.3312 - val_out_14_loss: 1.3341 - val_out_15_loss: 1.3459 - val_out_16_loss: 1.3421 - val_out_17_loss: 1.3429 - val_out_18_loss: 1.3315 - val_out_19_loss: 1.3514 - val_out_20_loss: 1.3495 - val_out_21_loss: 1.3350 - val_out_22_loss: 1.3324 - val_out_23_loss: 1.3359 - val_out_24_loss: 1.3408 - val_out_25_loss: 1.3422 - val_out_26_loss: 1.3394 - val_out_27_loss: 1.3410 - val_out_28_loss: 1.3375 - val_out_29_loss: 1.3375 - val_out_30_loss: 1.3476 - val_out_31_loss: 1.3350 - val_out_32_loss: 1.3383 - val_out_acc: 0.5141 - val_out_0_acc: 0.6074 - val_out_1_acc: 0.5163 - val_out_2_acc: 0.5011 - val_out_3_acc: 0.5076 - val_out_4_acc: 0.5163 - val_out_5_acc: 0.5184 - val_out_6_acc: 0.5076 - val_out_7_acc: 0.5163 - val_out_8_acc: 0.5098 - val_out_9_acc: 0.5206 - val_out_10_acc: 0.5141 - val_out_11_acc: 0.5054 - val_out_12_acc: 0.5119 - val_out_13_acc: 0.5228 - val_out_14_acc: 0.5249 - val_out_15_acc: 0.5054 - val_out_16_acc: 0.5184 - val_out_17_acc: 0.5098 - val_out_18_acc: 0.5249 - val_out_19_acc: 0.5054 - val_out_20_acc: 0.4989 - val_out_21_acc: 0.5141 - val_out_22_acc: 0.5119 - val_out_23_acc: 0.5249 - val_out_24_acc: 0.5119 - val_out_25_acc: 0.5098 - val_out_26_acc: 0.4989 - val_out_27_acc: 0.5054 - val_out_28_acc: 0.5163 - val_out_29_acc: 0.5184 - val_out_30_acc: 0.5033 - val_out_31_acc: 0.5141 - val_out_32_acc: 0.5163\n",
      "Epoch 20/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 49.2988 - out_loss: 1.3273 - out_0_loss: 1.1091 - out_1_loss: 1.4490 - out_2_loss: 1.4774 - out_3_loss: 1.4547 - out_4_loss: 1.4716 - out_5_loss: 1.4728 - out_6_loss: 1.4622 - out_7_loss: 1.4531 - out_8_loss: 1.4510 - out_9_loss: 1.4775 - out_10_loss: 1.4723 - out_11_loss: 1.4767 - out_12_loss: 1.4512 - out_13_loss: 1.4361 - out_14_loss: 1.4551 - out_15_loss: 1.4674 - out_16_loss: 1.4670 - out_17_loss: 1.4749 - out_18_loss: 1.4716 - out_19_loss: 1.4646 - out_20_loss: 1.4746 - out_21_loss: 1.4815 - out_22_loss: 1.4672 - out_23_loss: 1.4721 - out_24_loss: 1.4680 - out_25_loss: 1.4557 - out_26_loss: 1.4616 - out_27_loss: 1.4702 - out_28_loss: 1.4704 - out_29_loss: 1.4584 - out_30_loss: 1.4596 - out_31_loss: 1.4478 - out_32_loss: 1.4690 - out_acc: 0.5350 - out_0_acc: 0.6249 - out_1_acc: 0.5009 - out_2_acc: 0.4954 - out_3_acc: 0.4882 - out_4_acc: 0.4817 - out_5_acc: 0.4817 - out_6_acc: 0.4929 - out_7_acc: 0.4960 - out_8_acc: 0.5015 - out_9_acc: 0.4861 - out_10_acc: 0.4823 - out_11_acc: 0.4771 - out_12_acc: 0.4944 - out_13_acc: 0.4972 - out_14_acc: 0.4882 - out_15_acc: 0.4944 - out_16_acc: 0.4916 - out_17_acc: 0.4966 - out_18_acc: 0.4870 - out_19_acc: 0.4907 - out_20_acc: 0.4755 - out_21_acc: 0.4944 - out_22_acc: 0.4857 - out_23_acc: 0.4830 - out_24_acc: 0.4929 - out_25_acc: 0.4972 - out_26_acc: 0.4923 - out_27_acc: 0.4864 - out_28_acc: 0.4978 - out_29_acc: 0.4938 - out_30_acc: 0.4851 - out_31_acc: 0.5025 - out_32_acc: 0.4913 - val_loss: 43.8019 - val_out_loss: 1.2218 - val_out_0_loss: 1.0313 - val_out_1_loss: 1.2258 - val_out_2_loss: 1.2514 - val_out_3_loss: 1.2369 - val_out_4_loss: 1.2400 - val_out_5_loss: 1.2491 - val_out_6_loss: 1.2399 - val_out_7_loss: 1.2321 - val_out_8_loss: 1.2485 - val_out_9_loss: 1.2568 - val_out_10_loss: 1.2563 - val_out_11_loss: 1.2573 - val_out_12_loss: 1.2392 - val_out_13_loss: 1.2356 - val_out_14_loss: 1.2360 - val_out_15_loss: 1.2540 - val_out_16_loss: 1.2475 - val_out_17_loss: 1.2474 - val_out_18_loss: 1.2341 - val_out_19_loss: 1.2487 - val_out_20_loss: 1.2492 - val_out_21_loss: 1.2447 - val_out_22_loss: 1.2417 - val_out_23_loss: 1.2392 - val_out_24_loss: 1.2407 - val_out_25_loss: 1.2577 - val_out_26_loss: 1.2466 - val_out_27_loss: 1.2453 - val_out_28_loss: 1.2470 - val_out_29_loss: 1.2433 - val_out_30_loss: 1.2332 - val_out_31_loss: 1.2469 - val_out_32_loss: 1.2428 - val_out_acc: 0.5857 - val_out_0_acc: 0.6421 - val_out_1_acc: 0.6009 - val_out_2_acc: 0.5792 - val_out_3_acc: 0.5662 - val_out_4_acc: 0.5900 - val_out_5_acc: 0.5879 - val_out_6_acc: 0.5879 - val_out_7_acc: 0.5813 - val_out_8_acc: 0.5835 - val_out_9_acc: 0.5879 - val_out_10_acc: 0.5727 - val_out_11_acc: 0.5813 - val_out_12_acc: 0.5835 - val_out_13_acc: 0.5857 - val_out_14_acc: 0.5835 - val_out_15_acc: 0.5727 - val_out_16_acc: 0.5835 - val_out_17_acc: 0.5879 - val_out_18_acc: 0.5922 - val_out_19_acc: 0.5900 - val_out_20_acc: 0.5813 - val_out_21_acc: 0.5857 - val_out_22_acc: 0.5727 - val_out_23_acc: 0.5857 - val_out_24_acc: 0.5900 - val_out_25_acc: 0.5748 - val_out_26_acc: 0.5662 - val_out_27_acc: 0.5770 - val_out_28_acc: 0.5748 - val_out_29_acc: 0.5965 - val_out_30_acc: 0.5813 - val_out_31_acc: 0.5683 - val_out_32_acc: 0.5770\n",
      "Epoch 21/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 48.4615 - out_loss: 1.3047 - out_0_loss: 1.0927 - out_1_loss: 1.4306 - out_2_loss: 1.4396 - out_3_loss: 1.4367 - out_4_loss: 1.4411 - out_5_loss: 1.4314 - out_6_loss: 1.4326 - out_7_loss: 1.4383 - out_8_loss: 1.4346 - out_9_loss: 1.4415 - out_10_loss: 1.4392 - out_11_loss: 1.4455 - out_12_loss: 1.4278 - out_13_loss: 1.4399 - out_14_loss: 1.4309 - out_15_loss: 1.4420 - out_16_loss: 1.4385 - out_17_loss: 1.4393 - out_18_loss: 1.4378 - out_19_loss: 1.4381 - out_20_loss: 1.4591 - out_21_loss: 1.4356 - out_22_loss: 1.4345 - out_23_loss: 1.4328 - out_24_loss: 1.4360 - out_25_loss: 1.4376 - out_26_loss: 1.4399 - out_27_loss: 1.4491 - out_28_loss: 1.4533 - out_29_loss: 1.4302 - out_30_loss: 1.4333 - out_31_loss: 1.4630 - out_32_loss: 1.4544 - out_acc: 0.5425 - out_0_acc: 0.6190 - out_1_acc: 0.4988 - out_2_acc: 0.4997 - out_3_acc: 0.4864 - out_4_acc: 0.4963 - out_5_acc: 0.5003 - out_6_acc: 0.4941 - out_7_acc: 0.4941 - out_8_acc: 0.5012 - out_9_acc: 0.4882 - out_10_acc: 0.4919 - out_11_acc: 0.5022 - out_12_acc: 0.5108 - out_13_acc: 0.4932 - out_14_acc: 0.4873 - out_15_acc: 0.4975 - out_16_acc: 0.4978 - out_17_acc: 0.4950 - out_18_acc: 0.5003 - out_19_acc: 0.4935 - out_20_acc: 0.4845 - out_21_acc: 0.5053 - out_22_acc: 0.4910 - out_23_acc: 0.4963 - out_24_acc: 0.4910 - out_25_acc: 0.5034 - out_26_acc: 0.4907 - out_27_acc: 0.4938 - out_28_acc: 0.4935 - out_29_acc: 0.4969 - out_30_acc: 0.4969 - out_31_acc: 0.4923 - out_32_acc: 0.4892 - val_loss: 46.2236 - val_out_loss: 1.2856 - val_out_0_loss: 1.0068 - val_out_1_loss: 1.3025 - val_out_2_loss: 1.3240 - val_out_3_loss: 1.3064 - val_out_4_loss: 1.3187 - val_out_5_loss: 1.3143 - val_out_6_loss: 1.3093 - val_out_7_loss: 1.3050 - val_out_8_loss: 1.3111 - val_out_9_loss: 1.3262 - val_out_10_loss: 1.3222 - val_out_11_loss: 1.3322 - val_out_12_loss: 1.3129 - val_out_13_loss: 1.3112 - val_out_14_loss: 1.3067 - val_out_15_loss: 1.3159 - val_out_16_loss: 1.3237 - val_out_17_loss: 1.3106 - val_out_18_loss: 1.3219 - val_out_19_loss: 1.3174 - val_out_20_loss: 1.3261 - val_out_21_loss: 1.3152 - val_out_22_loss: 1.3160 - val_out_23_loss: 1.3025 - val_out_24_loss: 1.3135 - val_out_25_loss: 1.3271 - val_out_26_loss: 1.3169 - val_out_27_loss: 1.3149 - val_out_28_loss: 1.3157 - val_out_29_loss: 1.3174 - val_out_30_loss: 1.3179 - val_out_31_loss: 1.3112 - val_out_32_loss: 1.3150 - val_out_acc: 0.5163 - val_out_0_acc: 0.6638 - val_out_1_acc: 0.5228 - val_out_2_acc: 0.5163 - val_out_3_acc: 0.5228 - val_out_4_acc: 0.5228 - val_out_5_acc: 0.5141 - val_out_6_acc: 0.5119 - val_out_7_acc: 0.4946 - val_out_8_acc: 0.5184 - val_out_9_acc: 0.5098 - val_out_10_acc: 0.5141 - val_out_11_acc: 0.4946 - val_out_12_acc: 0.5184 - val_out_13_acc: 0.5141 - val_out_14_acc: 0.5206 - val_out_15_acc: 0.5033 - val_out_16_acc: 0.5206 - val_out_17_acc: 0.5054 - val_out_18_acc: 0.5141 - val_out_19_acc: 0.5033 - val_out_20_acc: 0.5033 - val_out_21_acc: 0.5054 - val_out_22_acc: 0.5033 - val_out_23_acc: 0.5228 - val_out_24_acc: 0.5054 - val_out_25_acc: 0.5119 - val_out_26_acc: 0.5098 - val_out_27_acc: 0.5228 - val_out_28_acc: 0.5119 - val_out_29_acc: 0.5141 - val_out_30_acc: 0.5054 - val_out_31_acc: 0.5228 - val_out_32_acc: 0.5163\n",
      "Epoch 22/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 48.2964 - out_loss: 1.3015 - out_0_loss: 1.0676 - out_1_loss: 1.4312 - out_2_loss: 1.4444 - out_3_loss: 1.4401 - out_4_loss: 1.4436 - out_5_loss: 1.4310 - out_6_loss: 1.4341 - out_7_loss: 1.4408 - out_8_loss: 1.4418 - out_9_loss: 1.4394 - out_10_loss: 1.4304 - out_11_loss: 1.4341 - out_12_loss: 1.4398 - out_13_loss: 1.4304 - out_14_loss: 1.4408 - out_15_loss: 1.4389 - out_16_loss: 1.4426 - out_17_loss: 1.4389 - out_18_loss: 1.4445 - out_19_loss: 1.4174 - out_20_loss: 1.4448 - out_21_loss: 1.4314 - out_22_loss: 1.4359 - out_23_loss: 1.4201 - out_24_loss: 1.4207 - out_25_loss: 1.4303 - out_26_loss: 1.4379 - out_27_loss: 1.4286 - out_28_loss: 1.4446 - out_29_loss: 1.4234 - out_30_loss: 1.4482 - out_31_loss: 1.4328 - out_32_loss: 1.4248 - out_acc: 0.5350 - out_0_acc: 0.6293 - out_1_acc: 0.5028 - out_2_acc: 0.4752 - out_3_acc: 0.4817 - out_4_acc: 0.4802 - out_5_acc: 0.5077 - out_6_acc: 0.4892 - out_7_acc: 0.4882 - out_8_acc: 0.4898 - out_9_acc: 0.4876 - out_10_acc: 0.4879 - out_11_acc: 0.4932 - out_12_acc: 0.4904 - out_13_acc: 0.4861 - out_14_acc: 0.4947 - out_15_acc: 0.4892 - out_16_acc: 0.4876 - out_17_acc: 0.4845 - out_18_acc: 0.4916 - out_19_acc: 0.4935 - out_20_acc: 0.4857 - out_21_acc: 0.4916 - out_22_acc: 0.4938 - out_23_acc: 0.4901 - out_24_acc: 0.4960 - out_25_acc: 0.4913 - out_26_acc: 0.4885 - out_27_acc: 0.4923 - out_28_acc: 0.4808 - out_29_acc: 0.4823 - out_30_acc: 0.4975 - out_31_acc: 0.4947 - out_32_acc: 0.4864 - val_loss: 45.0313 - val_out_loss: 1.2482 - val_out_0_loss: 0.9919 - val_out_1_loss: 1.2619 - val_out_2_loss: 1.2843 - val_out_3_loss: 1.2737 - val_out_4_loss: 1.2891 - val_out_5_loss: 1.2755 - val_out_6_loss: 1.2878 - val_out_7_loss: 1.2783 - val_out_8_loss: 1.2791 - val_out_9_loss: 1.2890 - val_out_10_loss: 1.2870 - val_out_11_loss: 1.2907 - val_out_12_loss: 1.2838 - val_out_13_loss: 1.2808 - val_out_14_loss: 1.2721 - val_out_15_loss: 1.2747 - val_out_16_loss: 1.2827 - val_out_17_loss: 1.2926 - val_out_18_loss: 1.2793 - val_out_19_loss: 1.2866 - val_out_20_loss: 1.2877 - val_out_21_loss: 1.2826 - val_out_22_loss: 1.2805 - val_out_23_loss: 1.2843 - val_out_24_loss: 1.2851 - val_out_25_loss: 1.2863 - val_out_26_loss: 1.2838 - val_out_27_loss: 1.2951 - val_out_28_loss: 1.2823 - val_out_29_loss: 1.2728 - val_out_30_loss: 1.2680 - val_out_31_loss: 1.2707 - val_out_32_loss: 1.2808 - val_out_acc: 0.5531 - val_out_0_acc: 0.6703 - val_out_1_acc: 0.5597 - val_out_2_acc: 0.5466 - val_out_3_acc: 0.5597 - val_out_4_acc: 0.5445 - val_out_5_acc: 0.5488 - val_out_6_acc: 0.5401 - val_out_7_acc: 0.5445 - val_out_8_acc: 0.5531 - val_out_9_acc: 0.5575 - val_out_10_acc: 0.5510 - val_out_11_acc: 0.5380 - val_out_12_acc: 0.5466 - val_out_13_acc: 0.5466 - val_out_14_acc: 0.5445 - val_out_15_acc: 0.5575 - val_out_16_acc: 0.5510 - val_out_17_acc: 0.5466 - val_out_18_acc: 0.5531 - val_out_19_acc: 0.5466 - val_out_20_acc: 0.5336 - val_out_21_acc: 0.5423 - val_out_22_acc: 0.5575 - val_out_23_acc: 0.5293 - val_out_24_acc: 0.5466 - val_out_25_acc: 0.5488 - val_out_26_acc: 0.5315 - val_out_27_acc: 0.5401 - val_out_28_acc: 0.5531 - val_out_29_acc: 0.5466 - val_out_30_acc: 0.5531 - val_out_31_acc: 0.5618 - val_out_32_acc: 0.5597\n",
      "Epoch 23/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 47.2547 - out_loss: 1.2694 - out_0_loss: 1.0275 - out_1_loss: 1.4000 - out_2_loss: 1.4165 - out_3_loss: 1.3997 - out_4_loss: 1.4021 - out_5_loss: 1.3956 - out_6_loss: 1.4168 - out_7_loss: 1.4118 - out_8_loss: 1.3948 - out_9_loss: 1.4040 - out_10_loss: 1.4221 - out_11_loss: 1.4286 - out_12_loss: 1.3907 - out_13_loss: 1.4019 - out_14_loss: 1.3966 - out_15_loss: 1.4085 - out_16_loss: 1.4181 - out_17_loss: 1.4130 - out_18_loss: 1.4061 - out_19_loss: 1.4063 - out_20_loss: 1.4313 - out_21_loss: 1.3941 - out_22_loss: 1.4163 - out_23_loss: 1.3706 - out_24_loss: 1.3975 - out_25_loss: 1.4032 - out_26_loss: 1.3981 - out_27_loss: 1.3901 - out_28_loss: 1.4110 - out_29_loss: 1.3950 - out_30_loss: 1.3917 - out_31_loss: 1.4001 - out_32_loss: 1.4255 - out_acc: 0.5465 - out_0_acc: 0.6479 - out_1_acc: 0.5059 - out_2_acc: 0.5096 - out_3_acc: 0.4910 - out_4_acc: 0.4969 - out_5_acc: 0.5146 - out_6_acc: 0.4895 - out_7_acc: 0.5062 - out_8_acc: 0.5096 - out_9_acc: 0.5062 - out_10_acc: 0.4985 - out_11_acc: 0.4919 - out_12_acc: 0.5115 - out_13_acc: 0.5022 - out_14_acc: 0.4988 - out_15_acc: 0.5006 - out_16_acc: 0.4935 - out_17_acc: 0.4957 - out_18_acc: 0.5084 - out_19_acc: 0.5056 - out_20_acc: 0.4969 - out_21_acc: 0.5177 - out_22_acc: 0.4975 - out_23_acc: 0.5152 - out_24_acc: 0.5077 - out_25_acc: 0.5161 - out_26_acc: 0.5074 - out_27_acc: 0.5087 - out_28_acc: 0.5059 - out_29_acc: 0.5015 - out_30_acc: 0.5170 - out_31_acc: 0.5025 - out_32_acc: 0.4901 - val_loss: 43.9209 - val_out_loss: 1.2180 - val_out_0_loss: 0.9828 - val_out_1_loss: 1.2371 - val_out_2_loss: 1.2574 - val_out_3_loss: 1.2525 - val_out_4_loss: 1.2460 - val_out_5_loss: 1.2585 - val_out_6_loss: 1.2403 - val_out_7_loss: 1.2371 - val_out_8_loss: 1.2547 - val_out_9_loss: 1.2612 - val_out_10_loss: 1.2528 - val_out_11_loss: 1.2704 - val_out_12_loss: 1.2467 - val_out_13_loss: 1.2472 - val_out_14_loss: 1.2435 - val_out_15_loss: 1.2559 - val_out_16_loss: 1.2432 - val_out_17_loss: 1.2618 - val_out_18_loss: 1.2404 - val_out_19_loss: 1.2562 - val_out_20_loss: 1.2515 - val_out_21_loss: 1.2381 - val_out_22_loss: 1.2404 - val_out_23_loss: 1.2410 - val_out_24_loss: 1.2567 - val_out_25_loss: 1.2591 - val_out_26_loss: 1.2538 - val_out_27_loss: 1.2437 - val_out_28_loss: 1.2529 - val_out_29_loss: 1.2462 - val_out_30_loss: 1.2402 - val_out_31_loss: 1.2518 - val_out_32_loss: 1.2432 - val_out_acc: 0.5662 - val_out_0_acc: 0.6746 - val_out_1_acc: 0.5553 - val_out_2_acc: 0.5510 - val_out_3_acc: 0.5531 - val_out_4_acc: 0.5575 - val_out_5_acc: 0.5575 - val_out_6_acc: 0.5597 - val_out_7_acc: 0.5575 - val_out_8_acc: 0.5597 - val_out_9_acc: 0.5575 - val_out_10_acc: 0.5445 - val_out_11_acc: 0.5380 - val_out_12_acc: 0.5423 - val_out_13_acc: 0.5575 - val_out_14_acc: 0.5510 - val_out_15_acc: 0.5597 - val_out_16_acc: 0.5597 - val_out_17_acc: 0.5553 - val_out_18_acc: 0.5553 - val_out_19_acc: 0.5510 - val_out_20_acc: 0.5640 - val_out_21_acc: 0.5640 - val_out_22_acc: 0.5553 - val_out_23_acc: 0.5488 - val_out_24_acc: 0.5553 - val_out_25_acc: 0.5488 - val_out_26_acc: 0.5380 - val_out_27_acc: 0.5618 - val_out_28_acc: 0.5597 - val_out_29_acc: 0.5488 - val_out_30_acc: 0.5553 - val_out_31_acc: 0.5531 - val_out_32_acc: 0.5531\n",
      "Epoch 24/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 46.0706 - out_loss: 1.2372 - out_0_loss: 1.0263 - out_1_loss: 1.3566 - out_2_loss: 1.3869 - out_3_loss: 1.3636 - out_4_loss: 1.3818 - out_5_loss: 1.3515 - out_6_loss: 1.3738 - out_7_loss: 1.3804 - out_8_loss: 1.3444 - out_9_loss: 1.3735 - out_10_loss: 1.3666 - out_11_loss: 1.3722 - out_12_loss: 1.3632 - out_13_loss: 1.3571 - out_14_loss: 1.3705 - out_15_loss: 1.3708 - out_16_loss: 1.3735 - out_17_loss: 1.3753 - out_18_loss: 1.3772 - out_19_loss: 1.3830 - out_20_loss: 1.3852 - out_21_loss: 1.3653 - out_22_loss: 1.3704 - out_23_loss: 1.3561 - out_24_loss: 1.3662 - out_25_loss: 1.3586 - out_26_loss: 1.3662 - out_27_loss: 1.3663 - out_28_loss: 1.3853 - out_29_loss: 1.3626 - out_30_loss: 1.3640 - out_31_loss: 1.3595 - out_32_loss: 1.3798 - out_acc: 0.5648 - out_0_acc: 0.6457 - out_1_acc: 0.5232 - out_2_acc: 0.5087 - out_3_acc: 0.5105 - out_4_acc: 0.4981 - out_5_acc: 0.5201 - out_6_acc: 0.5121 - out_7_acc: 0.5087 - out_8_acc: 0.5260 - out_9_acc: 0.5167 - out_10_acc: 0.5186 - out_11_acc: 0.5108 - out_12_acc: 0.5208 - out_13_acc: 0.5217 - out_14_acc: 0.5143 - out_15_acc: 0.5239 - out_16_acc: 0.5071 - out_17_acc: 0.5099 - out_18_acc: 0.5068 - out_19_acc: 0.5167 - out_20_acc: 0.5108 - out_21_acc: 0.5099 - out_22_acc: 0.5189 - out_23_acc: 0.5263 - out_24_acc: 0.5189 - out_25_acc: 0.5152 - out_26_acc: 0.5155 - out_27_acc: 0.5161 - out_28_acc: 0.5071 - out_29_acc: 0.5174 - out_30_acc: 0.5133 - out_31_acc: 0.5257 - out_32_acc: 0.5152 - val_loss: 45.1345 - val_out_loss: 1.2445 - val_out_0_loss: 0.8877 - val_out_1_loss: 1.2746 - val_out_2_loss: 1.2902 - val_out_3_loss: 1.2848 - val_out_4_loss: 1.2897 - val_out_5_loss: 1.2940 - val_out_6_loss: 1.2903 - val_out_7_loss: 1.2861 - val_out_8_loss: 1.2780 - val_out_9_loss: 1.2908 - val_out_10_loss: 1.2904 - val_out_11_loss: 1.2978 - val_out_12_loss: 1.2772 - val_out_13_loss: 1.2874 - val_out_14_loss: 1.2863 - val_out_15_loss: 1.2921 - val_out_16_loss: 1.2821 - val_out_17_loss: 1.2834 - val_out_18_loss: 1.2787 - val_out_19_loss: 1.2901 - val_out_20_loss: 1.2944 - val_out_21_loss: 1.2858 - val_out_22_loss: 1.2861 - val_out_23_loss: 1.2812 - val_out_24_loss: 1.2981 - val_out_25_loss: 1.2982 - val_out_26_loss: 1.2938 - val_out_27_loss: 1.2847 - val_out_28_loss: 1.2838 - val_out_29_loss: 1.2931 - val_out_30_loss: 1.2889 - val_out_31_loss: 1.3054 - val_out_32_loss: 1.2782 - val_out_acc: 0.5618 - val_out_0_acc: 0.6790 - val_out_1_acc: 0.5618 - val_out_2_acc: 0.5510 - val_out_3_acc: 0.5531 - val_out_4_acc: 0.5510 - val_out_5_acc: 0.5466 - val_out_6_acc: 0.5575 - val_out_7_acc: 0.5380 - val_out_8_acc: 0.5553 - val_out_9_acc: 0.5531 - val_out_10_acc: 0.5423 - val_out_11_acc: 0.5445 - val_out_12_acc: 0.5553 - val_out_13_acc: 0.5488 - val_out_14_acc: 0.5488 - val_out_15_acc: 0.5445 - val_out_16_acc: 0.5531 - val_out_17_acc: 0.5662 - val_out_18_acc: 0.5510 - val_out_19_acc: 0.5510 - val_out_20_acc: 0.5488 - val_out_21_acc: 0.5358 - val_out_22_acc: 0.5575 - val_out_23_acc: 0.5531 - val_out_24_acc: 0.5488 - val_out_25_acc: 0.5401 - val_out_26_acc: 0.5466 - val_out_27_acc: 0.5510 - val_out_28_acc: 0.5488 - val_out_29_acc: 0.5315 - val_out_30_acc: 0.5466 - val_out_31_acc: 0.5510 - val_out_32_acc: 0.5640\n",
      "Epoch 25/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 45.8716 - out_loss: 1.2334 - out_0_loss: 1.0289 - out_1_loss: 1.3551 - out_2_loss: 1.3562 - out_3_loss: 1.3379 - out_4_loss: 1.3761 - out_5_loss: 1.3719 - out_6_loss: 1.3614 - out_7_loss: 1.3772 - out_8_loss: 1.3572 - out_9_loss: 1.3555 - out_10_loss: 1.3575 - out_11_loss: 1.3695 - out_12_loss: 1.3666 - out_13_loss: 1.3505 - out_14_loss: 1.3583 - out_15_loss: 1.3705 - out_16_loss: 1.3668 - out_17_loss: 1.3486 - out_18_loss: 1.3692 - out_19_loss: 1.3683 - out_20_loss: 1.3555 - out_21_loss: 1.3584 - out_22_loss: 1.3808 - out_23_loss: 1.3555 - out_24_loss: 1.3728 - out_25_loss: 1.3676 - out_26_loss: 1.3778 - out_27_loss: 1.3515 - out_28_loss: 1.3723 - out_29_loss: 1.3586 - out_30_loss: 1.3402 - out_31_loss: 1.3690 - out_32_loss: 1.3747 - out_acc: 0.5601 - out_0_acc: 0.6485 - out_1_acc: 0.5186 - out_2_acc: 0.5183 - out_3_acc: 0.5195 - out_4_acc: 0.5118 - out_5_acc: 0.5223 - out_6_acc: 0.5174 - out_7_acc: 0.5183 - out_8_acc: 0.5133 - out_9_acc: 0.5220 - out_10_acc: 0.5211 - out_11_acc: 0.5155 - out_12_acc: 0.5180 - out_13_acc: 0.5214 - out_14_acc: 0.5136 - out_15_acc: 0.5143 - out_16_acc: 0.5186 - out_17_acc: 0.5226 - out_18_acc: 0.5149 - out_19_acc: 0.5211 - out_20_acc: 0.5108 - out_21_acc: 0.5186 - out_22_acc: 0.5149 - out_23_acc: 0.5201 - out_24_acc: 0.5186 - out_25_acc: 0.5263 - out_26_acc: 0.5099 - out_27_acc: 0.5108 - out_28_acc: 0.5205 - out_29_acc: 0.5099 - out_30_acc: 0.5307 - out_31_acc: 0.5124 - out_32_acc: 0.5108 - val_loss: 42.9097 - val_out_loss: 1.1937 - val_out_0_loss: 0.8832 - val_out_1_loss: 1.2155 - val_out_2_loss: 1.2185 - val_out_3_loss: 1.2201 - val_out_4_loss: 1.2289 - val_out_5_loss: 1.2238 - val_out_6_loss: 1.2268 - val_out_7_loss: 1.2195 - val_out_8_loss: 1.2253 - val_out_9_loss: 1.2314 - val_out_10_loss: 1.2206 - val_out_11_loss: 1.2284 - val_out_12_loss: 1.2189 - val_out_13_loss: 1.2243 - val_out_14_loss: 1.2170 - val_out_15_loss: 1.2271 - val_out_16_loss: 1.2266 - val_out_17_loss: 1.2188 - val_out_18_loss: 1.2201 - val_out_19_loss: 1.2312 - val_out_20_loss: 1.2310 - val_out_21_loss: 1.2185 - val_out_22_loss: 1.2238 - val_out_23_loss: 1.2219 - val_out_24_loss: 1.2235 - val_out_25_loss: 1.2241 - val_out_26_loss: 1.2255 - val_out_27_loss: 1.2193 - val_out_28_loss: 1.2291 - val_out_29_loss: 1.2110 - val_out_30_loss: 1.2170 - val_out_31_loss: 1.2218 - val_out_32_loss: 1.2249 - val_out_acc: 0.5597 - val_out_0_acc: 0.6963 - val_out_1_acc: 0.5510 - val_out_2_acc: 0.5488 - val_out_3_acc: 0.5531 - val_out_4_acc: 0.5531 - val_out_5_acc: 0.5597 - val_out_6_acc: 0.5488 - val_out_7_acc: 0.5618 - val_out_8_acc: 0.5445 - val_out_9_acc: 0.5488 - val_out_10_acc: 0.5488 - val_out_11_acc: 0.5445 - val_out_12_acc: 0.5640 - val_out_13_acc: 0.5488 - val_out_14_acc: 0.5618 - val_out_15_acc: 0.5510 - val_out_16_acc: 0.5662 - val_out_17_acc: 0.5531 - val_out_18_acc: 0.5553 - val_out_19_acc: 0.5510 - val_out_20_acc: 0.5510 - val_out_21_acc: 0.5553 - val_out_22_acc: 0.5575 - val_out_23_acc: 0.5597 - val_out_24_acc: 0.5597 - val_out_25_acc: 0.5553 - val_out_26_acc: 0.5597 - val_out_27_acc: 0.5510 - val_out_28_acc: 0.5510 - val_out_29_acc: 0.5488 - val_out_30_acc: 0.5531 - val_out_31_acc: 0.5466 - val_out_32_acc: 0.5466\n",
      "Epoch 26/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 44.9660 - out_loss: 1.2014 - out_0_loss: 0.9873 - out_1_loss: 1.3258 - out_2_loss: 1.3477 - out_3_loss: 1.3338 - out_4_loss: 1.3443 - out_5_loss: 1.3245 - out_6_loss: 1.3349 - out_7_loss: 1.3413 - out_8_loss: 1.3295 - out_9_loss: 1.3178 - out_10_loss: 1.3331 - out_11_loss: 1.3603 - out_12_loss: 1.3375 - out_13_loss: 1.3229 - out_14_loss: 1.3285 - out_15_loss: 1.3465 - out_16_loss: 1.3366 - out_17_loss: 1.3315 - out_18_loss: 1.3351 - out_19_loss: 1.3399 - out_20_loss: 1.3421 - out_21_loss: 1.3372 - out_22_loss: 1.3419 - out_23_loss: 1.3309 - out_24_loss: 1.3372 - out_25_loss: 1.3446 - out_26_loss: 1.3341 - out_27_loss: 1.3409 - out_28_loss: 1.3428 - out_29_loss: 1.3326 - out_30_loss: 1.3287 - out_31_loss: 1.3405 - out_32_loss: 1.3522 - out_acc: 0.5722 - out_0_acc: 0.6652 - out_1_acc: 0.5369 - out_2_acc: 0.5192 - out_3_acc: 0.5298 - out_4_acc: 0.5223 - out_5_acc: 0.5319 - out_6_acc: 0.5282 - out_7_acc: 0.5260 - out_8_acc: 0.5214 - out_9_acc: 0.5319 - out_10_acc: 0.5313 - out_11_acc: 0.5136 - out_12_acc: 0.5298 - out_13_acc: 0.5356 - out_14_acc: 0.5248 - out_15_acc: 0.5279 - out_16_acc: 0.5245 - out_17_acc: 0.5288 - out_18_acc: 0.5276 - out_19_acc: 0.5335 - out_20_acc: 0.5192 - out_21_acc: 0.5341 - out_22_acc: 0.5232 - out_23_acc: 0.5214 - out_24_acc: 0.5288 - out_25_acc: 0.5183 - out_26_acc: 0.5276 - out_27_acc: 0.5260 - out_28_acc: 0.5285 - out_29_acc: 0.5260 - out_30_acc: 0.5236 - out_31_acc: 0.5223 - out_32_acc: 0.5180 - val_loss: 43.8554 - val_out_loss: 1.2187 - val_out_0_loss: 0.9224 - val_out_1_loss: 1.2489 - val_out_2_loss: 1.2461 - val_out_3_loss: 1.2424 - val_out_4_loss: 1.2521 - val_out_5_loss: 1.2503 - val_out_6_loss: 1.2539 - val_out_7_loss: 1.2381 - val_out_8_loss: 1.2562 - val_out_9_loss: 1.2572 - val_out_10_loss: 1.2495 - val_out_11_loss: 1.2528 - val_out_12_loss: 1.2434 - val_out_13_loss: 1.2527 - val_out_14_loss: 1.2515 - val_out_15_loss: 1.2519 - val_out_16_loss: 1.2498 - val_out_17_loss: 1.2546 - val_out_18_loss: 1.2405 - val_out_19_loss: 1.2527 - val_out_20_loss: 1.2608 - val_out_21_loss: 1.2393 - val_out_22_loss: 1.2473 - val_out_23_loss: 1.2443 - val_out_24_loss: 1.2528 - val_out_25_loss: 1.2521 - val_out_26_loss: 1.2428 - val_out_27_loss: 1.2471 - val_out_28_loss: 1.2578 - val_out_29_loss: 1.2443 - val_out_30_loss: 1.2479 - val_out_31_loss: 1.2453 - val_out_32_loss: 1.2516 - val_out_acc: 0.5879 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.5727 - val_out_2_acc: 0.5727 - val_out_3_acc: 0.5792 - val_out_4_acc: 0.5683 - val_out_5_acc: 0.5640 - val_out_6_acc: 0.5770 - val_out_7_acc: 0.5770 - val_out_8_acc: 0.5640 - val_out_9_acc: 0.5727 - val_out_10_acc: 0.5748 - val_out_11_acc: 0.5640 - val_out_12_acc: 0.5857 - val_out_13_acc: 0.5813 - val_out_14_acc: 0.5792 - val_out_15_acc: 0.5705 - val_out_16_acc: 0.5748 - val_out_17_acc: 0.5683 - val_out_18_acc: 0.5813 - val_out_19_acc: 0.5705 - val_out_20_acc: 0.5597 - val_out_21_acc: 0.5770 - val_out_22_acc: 0.5727 - val_out_23_acc: 0.5835 - val_out_24_acc: 0.5770 - val_out_25_acc: 0.5748 - val_out_26_acc: 0.5683 - val_out_27_acc: 0.5792 - val_out_28_acc: 0.5662 - val_out_29_acc: 0.5727 - val_out_30_acc: 0.5792 - val_out_31_acc: 0.5813 - val_out_32_acc: 0.5662\n",
      "Epoch 27/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 43.9065 - out_loss: 1.1731 - out_0_loss: 1.0012 - out_1_loss: 1.3062 - out_2_loss: 1.2949 - out_3_loss: 1.2950 - out_4_loss: 1.3189 - out_5_loss: 1.3047 - out_6_loss: 1.3025 - out_7_loss: 1.3025 - out_8_loss: 1.3092 - out_9_loss: 1.2967 - out_10_loss: 1.2889 - out_11_loss: 1.3178 - out_12_loss: 1.2964 - out_13_loss: 1.3058 - out_14_loss: 1.2998 - out_15_loss: 1.3202 - out_16_loss: 1.3001 - out_17_loss: 1.2797 - out_18_loss: 1.3241 - out_19_loss: 1.2991 - out_20_loss: 1.3078 - out_21_loss: 1.2953 - out_22_loss: 1.3148 - out_23_loss: 1.3040 - out_24_loss: 1.3027 - out_25_loss: 1.3187 - out_26_loss: 1.3062 - out_27_loss: 1.2845 - out_28_loss: 1.3149 - out_29_loss: 1.3055 - out_30_loss: 1.2987 - out_31_loss: 1.3125 - out_32_loss: 1.3041 - out_acc: 0.5890 - out_0_acc: 0.6550 - out_1_acc: 0.5391 - out_2_acc: 0.5409 - out_3_acc: 0.5446 - out_4_acc: 0.5378 - out_5_acc: 0.5360 - out_6_acc: 0.5298 - out_7_acc: 0.5378 - out_8_acc: 0.5443 - out_9_acc: 0.5335 - out_10_acc: 0.5443 - out_11_acc: 0.5372 - out_12_acc: 0.5329 - out_13_acc: 0.5347 - out_14_acc: 0.5434 - out_15_acc: 0.5338 - out_16_acc: 0.5397 - out_17_acc: 0.5456 - out_18_acc: 0.5322 - out_19_acc: 0.5350 - out_20_acc: 0.5260 - out_21_acc: 0.5387 - out_22_acc: 0.5229 - out_23_acc: 0.5356 - out_24_acc: 0.5369 - out_25_acc: 0.5322 - out_26_acc: 0.5400 - out_27_acc: 0.5409 - out_28_acc: 0.5335 - out_29_acc: 0.5428 - out_30_acc: 0.5403 - out_31_acc: 0.5282 - out_32_acc: 0.5415 - val_loss: 42.0049 - val_out_loss: 1.1705 - val_out_0_loss: 0.9552 - val_out_1_loss: 1.1861 - val_out_2_loss: 1.1955 - val_out_3_loss: 1.1917 - val_out_4_loss: 1.1983 - val_out_5_loss: 1.1925 - val_out_6_loss: 1.1884 - val_out_7_loss: 1.1847 - val_out_8_loss: 1.1912 - val_out_9_loss: 1.1942 - val_out_10_loss: 1.1961 - val_out_11_loss: 1.2000 - val_out_12_loss: 1.1968 - val_out_13_loss: 1.1943 - val_out_14_loss: 1.1956 - val_out_15_loss: 1.1951 - val_out_16_loss: 1.1948 - val_out_17_loss: 1.1930 - val_out_18_loss: 1.1873 - val_out_19_loss: 1.1916 - val_out_20_loss: 1.2049 - val_out_21_loss: 1.1901 - val_out_22_loss: 1.1983 - val_out_23_loss: 1.1931 - val_out_24_loss: 1.2020 - val_out_25_loss: 1.1986 - val_out_26_loss: 1.2002 - val_out_27_loss: 1.1922 - val_out_28_loss: 1.1925 - val_out_29_loss: 1.1948 - val_out_30_loss: 1.1889 - val_out_31_loss: 1.2007 - val_out_32_loss: 1.1931 - val_out_acc: 0.6161 - val_out_0_acc: 0.6746 - val_out_1_acc: 0.6117 - val_out_2_acc: 0.6074 - val_out_3_acc: 0.6074 - val_out_4_acc: 0.6095 - val_out_5_acc: 0.6030 - val_out_6_acc: 0.6095 - val_out_7_acc: 0.6117 - val_out_8_acc: 0.6095 - val_out_9_acc: 0.6052 - val_out_10_acc: 0.6030 - val_out_11_acc: 0.6074 - val_out_12_acc: 0.6052 - val_out_13_acc: 0.6074 - val_out_14_acc: 0.6030 - val_out_15_acc: 0.6009 - val_out_16_acc: 0.6182 - val_out_17_acc: 0.6009 - val_out_18_acc: 0.6074 - val_out_19_acc: 0.6117 - val_out_20_acc: 0.6074 - val_out_21_acc: 0.6095 - val_out_22_acc: 0.6052 - val_out_23_acc: 0.6117 - val_out_24_acc: 0.6095 - val_out_25_acc: 0.6117 - val_out_26_acc: 0.6030 - val_out_27_acc: 0.6161 - val_out_28_acc: 0.6052 - val_out_29_acc: 0.6052 - val_out_30_acc: 0.6074 - val_out_31_acc: 0.6095 - val_out_32_acc: 0.6052\n",
      "Epoch 28/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 44.0459 - out_loss: 1.1791 - out_0_loss: 0.9952 - out_1_loss: 1.2987 - out_2_loss: 1.3232 - out_3_loss: 1.3207 - out_4_loss: 1.3108 - out_5_loss: 1.3040 - out_6_loss: 1.3109 - out_7_loss: 1.2970 - out_8_loss: 1.2945 - out_9_loss: 1.3092 - out_10_loss: 1.2988 - out_11_loss: 1.3177 - out_12_loss: 1.2966 - out_13_loss: 1.3016 - out_14_loss: 1.3041 - out_15_loss: 1.3297 - out_16_loss: 1.3331 - out_17_loss: 1.3090 - out_18_loss: 1.3116 - out_19_loss: 1.2976 - out_20_loss: 1.3089 - out_21_loss: 1.3056 - out_22_loss: 1.3075 - out_23_loss: 1.2971 - out_24_loss: 1.3237 - out_25_loss: 1.3101 - out_26_loss: 1.3186 - out_27_loss: 1.3000 - out_28_loss: 1.3046 - out_29_loss: 1.2982 - out_30_loss: 1.3092 - out_31_loss: 1.3027 - out_32_loss: 1.3165 - out_acc: 0.5828 - out_0_acc: 0.6643 - out_1_acc: 0.5369 - out_2_acc: 0.5270 - out_3_acc: 0.5440 - out_4_acc: 0.5356 - out_5_acc: 0.5434 - out_6_acc: 0.5363 - out_7_acc: 0.5356 - out_8_acc: 0.5375 - out_9_acc: 0.5372 - out_10_acc: 0.5279 - out_11_acc: 0.5279 - out_12_acc: 0.5400 - out_13_acc: 0.5279 - out_14_acc: 0.5356 - out_15_acc: 0.5325 - out_16_acc: 0.5288 - out_17_acc: 0.5229 - out_18_acc: 0.5301 - out_19_acc: 0.5443 - out_20_acc: 0.5387 - out_21_acc: 0.5356 - out_22_acc: 0.5329 - out_23_acc: 0.5313 - out_24_acc: 0.5279 - out_25_acc: 0.5307 - out_26_acc: 0.5310 - out_27_acc: 0.5329 - out_28_acc: 0.5422 - out_29_acc: 0.5412 - out_30_acc: 0.5350 - out_31_acc: 0.5409 - out_32_acc: 0.5198 - val_loss: 39.7520 - val_out_loss: 1.1105 - val_out_0_loss: 0.8606 - val_out_1_loss: 1.1248 - val_out_2_loss: 1.1334 - val_out_3_loss: 1.1322 - val_out_4_loss: 1.1334 - val_out_5_loss: 1.1302 - val_out_6_loss: 1.1320 - val_out_7_loss: 1.1296 - val_out_8_loss: 1.1334 - val_out_9_loss: 1.1343 - val_out_10_loss: 1.1368 - val_out_11_loss: 1.1491 - val_out_12_loss: 1.1297 - val_out_13_loss: 1.1276 - val_out_14_loss: 1.1292 - val_out_15_loss: 1.1372 - val_out_16_loss: 1.1251 - val_out_17_loss: 1.1301 - val_out_18_loss: 1.1202 - val_out_19_loss: 1.1376 - val_out_20_loss: 1.1357 - val_out_21_loss: 1.1278 - val_out_22_loss: 1.1340 - val_out_23_loss: 1.1247 - val_out_24_loss: 1.1261 - val_out_25_loss: 1.1338 - val_out_26_loss: 1.1347 - val_out_27_loss: 1.1324 - val_out_28_loss: 1.1351 - val_out_29_loss: 1.1285 - val_out_30_loss: 1.1268 - val_out_31_loss: 1.1304 - val_out_32_loss: 1.1315 - val_out_acc: 0.5835 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.5900 - val_out_2_acc: 0.5857 - val_out_3_acc: 0.5770 - val_out_4_acc: 0.5835 - val_out_5_acc: 0.5748 - val_out_6_acc: 0.5813 - val_out_7_acc: 0.5813 - val_out_8_acc: 0.5922 - val_out_9_acc: 0.5792 - val_out_10_acc: 0.5857 - val_out_11_acc: 0.5705 - val_out_12_acc: 0.5813 - val_out_13_acc: 0.5944 - val_out_14_acc: 0.5900 - val_out_15_acc: 0.5748 - val_out_16_acc: 0.6095 - val_out_17_acc: 0.5900 - val_out_18_acc: 0.6052 - val_out_19_acc: 0.5748 - val_out_20_acc: 0.5683 - val_out_21_acc: 0.5770 - val_out_22_acc: 0.5835 - val_out_23_acc: 0.5813 - val_out_24_acc: 0.6074 - val_out_25_acc: 0.5813 - val_out_26_acc: 0.5770 - val_out_27_acc: 0.5879 - val_out_28_acc: 0.5813 - val_out_29_acc: 0.5813 - val_out_30_acc: 0.5900 - val_out_31_acc: 0.5900 - val_out_32_acc: 0.5770\n",
      "Epoch 29/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 42.7737 - out_loss: 1.1429 - out_0_loss: 0.9524 - out_1_loss: 1.2727 - out_2_loss: 1.2745 - out_3_loss: 1.2711 - out_4_loss: 1.2634 - out_5_loss: 1.2654 - out_6_loss: 1.2696 - out_7_loss: 1.2710 - out_8_loss: 1.2728 - out_9_loss: 1.2689 - out_10_loss: 1.2730 - out_11_loss: 1.2891 - out_12_loss: 1.2823 - out_13_loss: 1.2668 - out_14_loss: 1.2676 - out_15_loss: 1.2589 - out_16_loss: 1.2659 - out_17_loss: 1.2751 - out_18_loss: 1.2824 - out_19_loss: 1.2672 - out_20_loss: 1.2793 - out_21_loss: 1.2601 - out_22_loss: 1.2813 - out_23_loss: 1.2666 - out_24_loss: 1.2582 - out_25_loss: 1.2661 - out_26_loss: 1.2791 - out_27_loss: 1.2774 - out_28_loss: 1.2747 - out_29_loss: 1.2790 - out_30_loss: 1.2668 - out_31_loss: 1.2735 - out_32_loss: 1.2585 - out_acc: 0.5933 - out_0_acc: 0.6720 - out_1_acc: 0.5446 - out_2_acc: 0.5474 - out_3_acc: 0.5381 - out_4_acc: 0.5499 - out_5_acc: 0.5515 - out_6_acc: 0.5487 - out_7_acc: 0.5418 - out_8_acc: 0.5440 - out_9_acc: 0.5521 - out_10_acc: 0.5415 - out_11_acc: 0.5360 - out_12_acc: 0.5468 - out_13_acc: 0.5415 - out_14_acc: 0.5434 - out_15_acc: 0.5549 - out_16_acc: 0.5437 - out_17_acc: 0.5437 - out_18_acc: 0.5412 - out_19_acc: 0.5456 - out_20_acc: 0.5409 - out_21_acc: 0.5496 - out_22_acc: 0.5406 - out_23_acc: 0.5462 - out_24_acc: 0.5459 - out_25_acc: 0.5453 - out_26_acc: 0.5366 - out_27_acc: 0.5422 - out_28_acc: 0.5431 - out_29_acc: 0.5325 - out_30_acc: 0.5521 - out_31_acc: 0.5449 - out_32_acc: 0.5375 - val_loss: 43.7826 - val_out_loss: 1.2198 - val_out_0_loss: 0.9236 - val_out_1_loss: 1.2423 - val_out_2_loss: 1.2487 - val_out_3_loss: 1.2414 - val_out_4_loss: 1.2372 - val_out_5_loss: 1.2518 - val_out_6_loss: 1.2429 - val_out_7_loss: 1.2449 - val_out_8_loss: 1.2399 - val_out_9_loss: 1.2536 - val_out_10_loss: 1.2441 - val_out_11_loss: 1.2610 - val_out_12_loss: 1.2496 - val_out_13_loss: 1.2602 - val_out_14_loss: 1.2471 - val_out_15_loss: 1.2489 - val_out_16_loss: 1.2573 - val_out_17_loss: 1.2501 - val_out_18_loss: 1.2402 - val_out_19_loss: 1.2492 - val_out_20_loss: 1.2516 - val_out_21_loss: 1.2364 - val_out_22_loss: 1.2414 - val_out_23_loss: 1.2462 - val_out_24_loss: 1.2515 - val_out_25_loss: 1.2596 - val_out_26_loss: 1.2528 - val_out_27_loss: 1.2398 - val_out_28_loss: 1.2467 - val_out_29_loss: 1.2369 - val_out_30_loss: 1.2363 - val_out_31_loss: 1.2485 - val_out_32_loss: 1.2481 - val_out_acc: 0.5792 - val_out_0_acc: 0.6768 - val_out_1_acc: 0.5683 - val_out_2_acc: 0.5662 - val_out_3_acc: 0.5705 - val_out_4_acc: 0.5770 - val_out_5_acc: 0.5662 - val_out_6_acc: 0.5705 - val_out_7_acc: 0.5748 - val_out_8_acc: 0.5683 - val_out_9_acc: 0.5662 - val_out_10_acc: 0.5705 - val_out_11_acc: 0.5662 - val_out_12_acc: 0.5705 - val_out_13_acc: 0.5662 - val_out_14_acc: 0.5683 - val_out_15_acc: 0.5640 - val_out_16_acc: 0.5618 - val_out_17_acc: 0.5792 - val_out_18_acc: 0.5792 - val_out_19_acc: 0.5727 - val_out_20_acc: 0.5662 - val_out_21_acc: 0.5705 - val_out_22_acc: 0.5705 - val_out_23_acc: 0.5662 - val_out_24_acc: 0.5727 - val_out_25_acc: 0.5597 - val_out_26_acc: 0.5770 - val_out_27_acc: 0.5727 - val_out_28_acc: 0.5748 - val_out_29_acc: 0.5770 - val_out_30_acc: 0.5748 - val_out_31_acc: 0.5597 - val_out_32_acc: 0.5748\n",
      "Epoch 30/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 42.9174 - out_loss: 1.1517 - out_0_loss: 0.9669 - out_1_loss: 1.2533 - out_2_loss: 1.2949 - out_3_loss: 1.2890 - out_4_loss: 1.2712 - out_5_loss: 1.2734 - out_6_loss: 1.2781 - out_7_loss: 1.2690 - out_8_loss: 1.2678 - out_9_loss: 1.2627 - out_10_loss: 1.2825 - out_11_loss: 1.2826 - out_12_loss: 1.2687 - out_13_loss: 1.2712 - out_14_loss: 1.2703 - out_15_loss: 1.2877 - out_16_loss: 1.2815 - out_17_loss: 1.2744 - out_18_loss: 1.2729 - out_19_loss: 1.2665 - out_20_loss: 1.2797 - out_21_loss: 1.2738 - out_22_loss: 1.2807 - out_23_loss: 1.2755 - out_24_loss: 1.2709 - out_25_loss: 1.2817 - out_26_loss: 1.2714 - out_27_loss: 1.2850 - out_28_loss: 1.2812 - out_29_loss: 1.2632 - out_30_loss: 1.2595 - out_31_loss: 1.2781 - out_32_loss: 1.2802 - out_acc: 0.5949 - out_0_acc: 0.6696 - out_1_acc: 0.5539 - out_2_acc: 0.5347 - out_3_acc: 0.5480 - out_4_acc: 0.5524 - out_5_acc: 0.5508 - out_6_acc: 0.5480 - out_7_acc: 0.5533 - out_8_acc: 0.5558 - out_9_acc: 0.5536 - out_10_acc: 0.5471 - out_11_acc: 0.5477 - out_12_acc: 0.5539 - out_13_acc: 0.5549 - out_14_acc: 0.5539 - out_15_acc: 0.5412 - out_16_acc: 0.5493 - out_17_acc: 0.5440 - out_18_acc: 0.5583 - out_19_acc: 0.5493 - out_20_acc: 0.5524 - out_21_acc: 0.5496 - out_22_acc: 0.5468 - out_23_acc: 0.5477 - out_24_acc: 0.5443 - out_25_acc: 0.5490 - out_26_acc: 0.5490 - out_27_acc: 0.5502 - out_28_acc: 0.5437 - out_29_acc: 0.5611 - out_30_acc: 0.5521 - out_31_acc: 0.5524 - out_32_acc: 0.5387 - val_loss: 41.5052 - val_out_loss: 1.1484 - val_out_0_loss: 0.8928 - val_out_1_loss: 1.1767 - val_out_2_loss: 1.1833 - val_out_3_loss: 1.1679 - val_out_4_loss: 1.1843 - val_out_5_loss: 1.1836 - val_out_6_loss: 1.1825 - val_out_7_loss: 1.1866 - val_out_8_loss: 1.1862 - val_out_9_loss: 1.1838 - val_out_10_loss: 1.1901 - val_out_11_loss: 1.1804 - val_out_12_loss: 1.1900 - val_out_13_loss: 1.1851 - val_out_14_loss: 1.1847 - val_out_15_loss: 1.1796 - val_out_16_loss: 1.1796 - val_out_17_loss: 1.1789 - val_out_18_loss: 1.1692 - val_out_19_loss: 1.1870 - val_out_20_loss: 1.1871 - val_out_21_loss: 1.1806 - val_out_22_loss: 1.1716 - val_out_23_loss: 1.1782 - val_out_24_loss: 1.1801 - val_out_25_loss: 1.1889 - val_out_26_loss: 1.1897 - val_out_27_loss: 1.1789 - val_out_28_loss: 1.1868 - val_out_29_loss: 1.1817 - val_out_30_loss: 1.1840 - val_out_31_loss: 1.1708 - val_out_32_loss: 1.1836 - val_out_acc: 0.5857 - val_out_0_acc: 0.6920 - val_out_1_acc: 0.5900 - val_out_2_acc: 0.5922 - val_out_3_acc: 0.5748 - val_out_4_acc: 0.5879 - val_out_5_acc: 0.5835 - val_out_6_acc: 0.5813 - val_out_7_acc: 0.5900 - val_out_8_acc: 0.5900 - val_out_9_acc: 0.5944 - val_out_10_acc: 0.5857 - val_out_11_acc: 0.5792 - val_out_12_acc: 0.5813 - val_out_13_acc: 0.5900 - val_out_14_acc: 0.5879 - val_out_15_acc: 0.5748 - val_out_16_acc: 0.5879 - val_out_17_acc: 0.5879 - val_out_18_acc: 0.5900 - val_out_19_acc: 0.5835 - val_out_20_acc: 0.5770 - val_out_21_acc: 0.5792 - val_out_22_acc: 0.5900 - val_out_23_acc: 0.5879 - val_out_24_acc: 0.5770 - val_out_25_acc: 0.5835 - val_out_26_acc: 0.5770 - val_out_27_acc: 0.5792 - val_out_28_acc: 0.5748 - val_out_29_acc: 0.5879 - val_out_30_acc: 0.5813 - val_out_31_acc: 0.5792 - val_out_32_acc: 0.5900\n",
      "Epoch 31/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 42.5941 - out_loss: 1.1425 - out_0_loss: 0.9425 - out_1_loss: 1.2596 - out_2_loss: 1.2548 - out_3_loss: 1.2613 - out_4_loss: 1.2668 - out_5_loss: 1.2629 - out_6_loss: 1.2667 - out_7_loss: 1.2621 - out_8_loss: 1.2559 - out_9_loss: 1.2658 - out_10_loss: 1.2594 - out_11_loss: 1.2752 - out_12_loss: 1.2651 - out_13_loss: 1.2636 - out_14_loss: 1.2695 - out_15_loss: 1.2707 - out_16_loss: 1.2585 - out_17_loss: 1.2769 - out_18_loss: 1.2840 - out_19_loss: 1.2680 - out_20_loss: 1.2712 - out_21_loss: 1.2760 - out_22_loss: 1.2683 - out_23_loss: 1.2621 - out_24_loss: 1.2607 - out_25_loss: 1.2718 - out_26_loss: 1.2766 - out_27_loss: 1.2704 - out_28_loss: 1.2688 - out_29_loss: 1.2606 - out_30_loss: 1.2640 - out_31_loss: 1.2492 - out_32_loss: 1.2626 - out_acc: 0.5887 - out_0_acc: 0.6683 - out_1_acc: 0.5468 - out_2_acc: 0.5524 - out_3_acc: 0.5372 - out_4_acc: 0.5418 - out_5_acc: 0.5515 - out_6_acc: 0.5400 - out_7_acc: 0.5561 - out_8_acc: 0.5462 - out_9_acc: 0.5443 - out_10_acc: 0.5403 - out_11_acc: 0.5394 - out_12_acc: 0.5443 - out_13_acc: 0.5437 - out_14_acc: 0.5422 - out_15_acc: 0.5372 - out_16_acc: 0.5459 - out_17_acc: 0.5468 - out_18_acc: 0.5381 - out_19_acc: 0.5394 - out_20_acc: 0.5391 - out_21_acc: 0.5363 - out_22_acc: 0.5480 - out_23_acc: 0.5412 - out_24_acc: 0.5387 - out_25_acc: 0.5487 - out_26_acc: 0.5322 - out_27_acc: 0.5422 - out_28_acc: 0.5428 - out_29_acc: 0.5418 - out_30_acc: 0.5415 - out_31_acc: 0.5422 - out_32_acc: 0.5465 - val_loss: 40.4330 - val_out_loss: 1.1221 - val_out_0_loss: 0.9966 - val_out_1_loss: 1.1448 - val_out_2_loss: 1.1411 - val_out_3_loss: 1.1501 - val_out_4_loss: 1.1489 - val_out_5_loss: 1.1460 - val_out_6_loss: 1.1469 - val_out_7_loss: 1.1444 - val_out_8_loss: 1.1453 - val_out_9_loss: 1.1406 - val_out_10_loss: 1.1475 - val_out_11_loss: 1.1600 - val_out_12_loss: 1.1457 - val_out_13_loss: 1.1427 - val_out_14_loss: 1.1479 - val_out_15_loss: 1.1480 - val_out_16_loss: 1.1435 - val_out_17_loss: 1.1439 - val_out_18_loss: 1.1446 - val_out_19_loss: 1.1472 - val_out_20_loss: 1.1485 - val_out_21_loss: 1.1429 - val_out_22_loss: 1.1533 - val_out_23_loss: 1.1444 - val_out_24_loss: 1.1546 - val_out_25_loss: 1.1542 - val_out_26_loss: 1.1527 - val_out_27_loss: 1.1483 - val_out_28_loss: 1.1468 - val_out_29_loss: 1.1412 - val_out_30_loss: 1.1439 - val_out_31_loss: 1.1527 - val_out_32_loss: 1.1513 - val_out_acc: 0.6182 - val_out_0_acc: 0.6573 - val_out_1_acc: 0.6139 - val_out_2_acc: 0.5987 - val_out_3_acc: 0.5987 - val_out_4_acc: 0.6117 - val_out_5_acc: 0.6074 - val_out_6_acc: 0.6052 - val_out_7_acc: 0.6009 - val_out_8_acc: 0.6095 - val_out_9_acc: 0.6161 - val_out_10_acc: 0.6139 - val_out_11_acc: 0.5944 - val_out_12_acc: 0.6052 - val_out_13_acc: 0.6226 - val_out_14_acc: 0.6095 - val_out_15_acc: 0.6161 - val_out_16_acc: 0.6139 - val_out_17_acc: 0.6117 - val_out_18_acc: 0.6074 - val_out_19_acc: 0.6161 - val_out_20_acc: 0.6009 - val_out_21_acc: 0.5965 - val_out_22_acc: 0.6052 - val_out_23_acc: 0.6030 - val_out_24_acc: 0.6052 - val_out_25_acc: 0.5965 - val_out_26_acc: 0.6052 - val_out_27_acc: 0.6095 - val_out_28_acc: 0.6117 - val_out_29_acc: 0.6161 - val_out_30_acc: 0.6139 - val_out_31_acc: 0.6139 - val_out_32_acc: 0.6009\n",
      "Epoch 32/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 42.5877 - out_loss: 1.1381 - out_0_loss: 0.9313 - out_1_loss: 1.2532 - out_2_loss: 1.2570 - out_3_loss: 1.2828 - out_4_loss: 1.2830 - out_5_loss: 1.2650 - out_6_loss: 1.2761 - out_7_loss: 1.2722 - out_8_loss: 1.2543 - out_9_loss: 1.2554 - out_10_loss: 1.2663 - out_11_loss: 1.2697 - out_12_loss: 1.2720 - out_13_loss: 1.2583 - out_14_loss: 1.2615 - out_15_loss: 1.2778 - out_16_loss: 1.2799 - out_17_loss: 1.2596 - out_18_loss: 1.2595 - out_19_loss: 1.2578 - out_20_loss: 1.2707 - out_21_loss: 1.2638 - out_22_loss: 1.2649 - out_23_loss: 1.2638 - out_24_loss: 1.2688 - out_25_loss: 1.2720 - out_26_loss: 1.2598 - out_27_loss: 1.2672 - out_28_loss: 1.2710 - out_29_loss: 1.2646 - out_30_loss: 1.2560 - out_31_loss: 1.2538 - out_32_loss: 1.2803 - out_acc: 0.5945 - out_0_acc: 0.6754 - out_1_acc: 0.5567 - out_2_acc: 0.5533 - out_3_acc: 0.5443 - out_4_acc: 0.5412 - out_5_acc: 0.5412 - out_6_acc: 0.5415 - out_7_acc: 0.5428 - out_8_acc: 0.5449 - out_9_acc: 0.5515 - out_10_acc: 0.5415 - out_11_acc: 0.5465 - out_12_acc: 0.5446 - out_13_acc: 0.5539 - out_14_acc: 0.5598 - out_15_acc: 0.5434 - out_16_acc: 0.5403 - out_17_acc: 0.5505 - out_18_acc: 0.5493 - out_19_acc: 0.5499 - out_20_acc: 0.5440 - out_21_acc: 0.5474 - out_22_acc: 0.5496 - out_23_acc: 0.5453 - out_24_acc: 0.5511 - out_25_acc: 0.5400 - out_26_acc: 0.5496 - out_27_acc: 0.5471 - out_28_acc: 0.5474 - out_29_acc: 0.5418 - out_30_acc: 0.5549 - out_31_acc: 0.5539 - out_32_acc: 0.5391 - val_loss: 40.5110 - val_out_loss: 1.1260 - val_out_0_loss: 0.9387 - val_out_1_loss: 1.1506 - val_out_2_loss: 1.1458 - val_out_3_loss: 1.1547 - val_out_4_loss: 1.1496 - val_out_5_loss: 1.1494 - val_out_6_loss: 1.1539 - val_out_7_loss: 1.1497 - val_out_8_loss: 1.1461 - val_out_9_loss: 1.1523 - val_out_10_loss: 1.1476 - val_out_11_loss: 1.1650 - val_out_12_loss: 1.1432 - val_out_13_loss: 1.1535 - val_out_14_loss: 1.1478 - val_out_15_loss: 1.1483 - val_out_16_loss: 1.1509 - val_out_17_loss: 1.1511 - val_out_18_loss: 1.1491 - val_out_19_loss: 1.1583 - val_out_20_loss: 1.1543 - val_out_21_loss: 1.1430 - val_out_22_loss: 1.1464 - val_out_23_loss: 1.1533 - val_out_24_loss: 1.1540 - val_out_25_loss: 1.1595 - val_out_26_loss: 1.1485 - val_out_27_loss: 1.1511 - val_out_28_loss: 1.1616 - val_out_29_loss: 1.1493 - val_out_30_loss: 1.1498 - val_out_31_loss: 1.1566 - val_out_32_loss: 1.1486 - val_out_acc: 0.6030 - val_out_0_acc: 0.6790 - val_out_1_acc: 0.5965 - val_out_2_acc: 0.5965 - val_out_3_acc: 0.6009 - val_out_4_acc: 0.6074 - val_out_5_acc: 0.6030 - val_out_6_acc: 0.5922 - val_out_7_acc: 0.5857 - val_out_8_acc: 0.6009 - val_out_9_acc: 0.5987 - val_out_10_acc: 0.6009 - val_out_11_acc: 0.5879 - val_out_12_acc: 0.5965 - val_out_13_acc: 0.5987 - val_out_14_acc: 0.5965 - val_out_15_acc: 0.6052 - val_out_16_acc: 0.6074 - val_out_17_acc: 0.5965 - val_out_18_acc: 0.6009 - val_out_19_acc: 0.5922 - val_out_20_acc: 0.5922 - val_out_21_acc: 0.6052 - val_out_22_acc: 0.6139 - val_out_23_acc: 0.6030 - val_out_24_acc: 0.5944 - val_out_25_acc: 0.5922 - val_out_26_acc: 0.5922 - val_out_27_acc: 0.5965 - val_out_28_acc: 0.6009 - val_out_29_acc: 0.5987 - val_out_30_acc: 0.5987 - val_out_31_acc: 0.5879 - val_out_32_acc: 0.6095\n",
      "Epoch 33/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 41.8266 - out_loss: 1.1207 - out_0_loss: 0.9329 - out_1_loss: 1.2252 - out_2_loss: 1.2352 - out_3_loss: 1.2393 - out_4_loss: 1.2481 - out_5_loss: 1.2353 - out_6_loss: 1.2371 - out_7_loss: 1.2724 - out_8_loss: 1.2389 - out_9_loss: 1.2316 - out_10_loss: 1.2510 - out_11_loss: 1.2483 - out_12_loss: 1.2475 - out_13_loss: 1.2371 - out_14_loss: 1.2419 - out_15_loss: 1.2338 - out_16_loss: 1.2455 - out_17_loss: 1.2264 - out_18_loss: 1.2468 - out_19_loss: 1.2430 - out_20_loss: 1.2584 - out_21_loss: 1.2439 - out_22_loss: 1.2610 - out_23_loss: 1.2403 - out_24_loss: 1.2473 - out_25_loss: 1.2625 - out_26_loss: 1.2425 - out_27_loss: 1.2354 - out_28_loss: 1.2505 - out_29_loss: 1.2190 - out_30_loss: 1.2480 - out_31_loss: 1.2435 - out_32_loss: 1.2360 - out_acc: 0.6057 - out_0_acc: 0.6792 - out_1_acc: 0.5620 - out_2_acc: 0.5601 - out_3_acc: 0.5552 - out_4_acc: 0.5484 - out_5_acc: 0.5561 - out_6_acc: 0.5527 - out_7_acc: 0.5375 - out_8_acc: 0.5567 - out_9_acc: 0.5629 - out_10_acc: 0.5515 - out_11_acc: 0.5502 - out_12_acc: 0.5583 - out_13_acc: 0.5614 - out_14_acc: 0.5530 - out_15_acc: 0.5620 - out_16_acc: 0.5546 - out_17_acc: 0.5626 - out_18_acc: 0.5440 - out_19_acc: 0.5558 - out_20_acc: 0.5422 - out_21_acc: 0.5555 - out_22_acc: 0.5484 - out_23_acc: 0.5623 - out_24_acc: 0.5530 - out_25_acc: 0.5502 - out_26_acc: 0.5418 - out_27_acc: 0.5549 - out_28_acc: 0.5505 - out_29_acc: 0.5651 - out_30_acc: 0.5580 - out_31_acc: 0.5583 - out_32_acc: 0.5620 - val_loss: 40.5770 - val_out_loss: 1.1275 - val_out_0_loss: 0.8611 - val_out_1_loss: 1.1447 - val_out_2_loss: 1.1538 - val_out_3_loss: 1.1598 - val_out_4_loss: 1.1590 - val_out_5_loss: 1.1536 - val_out_6_loss: 1.1536 - val_out_7_loss: 1.1480 - val_out_8_loss: 1.1595 - val_out_9_loss: 1.1545 - val_out_10_loss: 1.1635 - val_out_11_loss: 1.1639 - val_out_12_loss: 1.1590 - val_out_13_loss: 1.1571 - val_out_14_loss: 1.1541 - val_out_15_loss: 1.1570 - val_out_16_loss: 1.1598 - val_out_17_loss: 1.1520 - val_out_18_loss: 1.1507 - val_out_19_loss: 1.1566 - val_out_20_loss: 1.1620 - val_out_21_loss: 1.1490 - val_out_22_loss: 1.1547 - val_out_23_loss: 1.1512 - val_out_24_loss: 1.1575 - val_out_25_loss: 1.1569 - val_out_26_loss: 1.1629 - val_out_27_loss: 1.1515 - val_out_28_loss: 1.1607 - val_out_29_loss: 1.1518 - val_out_30_loss: 1.1512 - val_out_31_loss: 1.1611 - val_out_32_loss: 1.1518 - val_out_acc: 0.5922 - val_out_0_acc: 0.7028 - val_out_1_acc: 0.5900 - val_out_2_acc: 0.5835 - val_out_3_acc: 0.5705 - val_out_4_acc: 0.5879 - val_out_5_acc: 0.5792 - val_out_6_acc: 0.5683 - val_out_7_acc: 0.5792 - val_out_8_acc: 0.5770 - val_out_9_acc: 0.5792 - val_out_10_acc: 0.5727 - val_out_11_acc: 0.5770 - val_out_12_acc: 0.5770 - val_out_13_acc: 0.5727 - val_out_14_acc: 0.5770 - val_out_15_acc: 0.5857 - val_out_16_acc: 0.5879 - val_out_17_acc: 0.5835 - val_out_18_acc: 0.5944 - val_out_19_acc: 0.5835 - val_out_20_acc: 0.5727 - val_out_21_acc: 0.5922 - val_out_22_acc: 0.5835 - val_out_23_acc: 0.5792 - val_out_24_acc: 0.5900 - val_out_25_acc: 0.5944 - val_out_26_acc: 0.5727 - val_out_27_acc: 0.5748 - val_out_28_acc: 0.5770 - val_out_29_acc: 0.5792 - val_out_30_acc: 0.5748 - val_out_31_acc: 0.5813 - val_out_32_acc: 0.5857\n",
      "Epoch 34/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 41.3372 - out_loss: 1.1027 - out_0_loss: 0.8938 - out_1_loss: 1.2142 - out_2_loss: 1.2304 - out_3_loss: 1.2376 - out_4_loss: 1.2569 - out_5_loss: 1.2349 - out_6_loss: 1.2266 - out_7_loss: 1.2331 - out_8_loss: 1.2227 - out_9_loss: 1.2230 - out_10_loss: 1.2266 - out_11_loss: 1.2359 - out_12_loss: 1.2239 - out_13_loss: 1.2271 - out_14_loss: 1.2229 - out_15_loss: 1.2324 - out_16_loss: 1.2183 - out_17_loss: 1.2057 - out_18_loss: 1.2363 - out_19_loss: 1.2301 - out_20_loss: 1.2363 - out_21_loss: 1.2426 - out_22_loss: 1.2280 - out_23_loss: 1.2362 - out_24_loss: 1.2312 - out_25_loss: 1.2364 - out_26_loss: 1.2223 - out_27_loss: 1.2363 - out_28_loss: 1.2267 - out_29_loss: 1.2200 - out_30_loss: 1.2334 - out_31_loss: 1.2242 - out_32_loss: 1.2284 - out_acc: 0.6156 - out_0_acc: 0.6937 - out_1_acc: 0.5728 - out_2_acc: 0.5645 - out_3_acc: 0.5555 - out_4_acc: 0.5533 - out_5_acc: 0.5580 - out_6_acc: 0.5626 - out_7_acc: 0.5654 - out_8_acc: 0.5604 - out_9_acc: 0.5744 - out_10_acc: 0.5623 - out_11_acc: 0.5645 - out_12_acc: 0.5611 - out_13_acc: 0.5629 - out_14_acc: 0.5716 - out_15_acc: 0.5645 - out_16_acc: 0.5670 - out_17_acc: 0.5626 - out_18_acc: 0.5539 - out_19_acc: 0.5598 - out_20_acc: 0.5598 - out_21_acc: 0.5558 - out_22_acc: 0.5604 - out_23_acc: 0.5496 - out_24_acc: 0.5629 - out_25_acc: 0.5660 - out_26_acc: 0.5688 - out_27_acc: 0.5567 - out_28_acc: 0.5539 - out_29_acc: 0.5617 - out_30_acc: 0.5663 - out_31_acc: 0.5701 - out_32_acc: 0.5601 - val_loss: 39.0702 - val_out_loss: 1.0892 - val_out_0_loss: 0.8212 - val_out_1_loss: 1.1089 - val_out_2_loss: 1.1073 - val_out_3_loss: 1.1101 - val_out_4_loss: 1.1122 - val_out_5_loss: 1.1079 - val_out_6_loss: 1.1114 - val_out_7_loss: 1.1135 - val_out_8_loss: 1.1107 - val_out_9_loss: 1.1152 - val_out_10_loss: 1.1127 - val_out_11_loss: 1.1220 - val_out_12_loss: 1.1079 - val_out_13_loss: 1.1173 - val_out_14_loss: 1.1081 - val_out_15_loss: 1.1154 - val_out_16_loss: 1.1124 - val_out_17_loss: 1.1200 - val_out_18_loss: 1.1120 - val_out_19_loss: 1.1166 - val_out_20_loss: 1.1163 - val_out_21_loss: 1.1048 - val_out_22_loss: 1.1108 - val_out_23_loss: 1.1128 - val_out_24_loss: 1.1130 - val_out_25_loss: 1.1146 - val_out_26_loss: 1.1205 - val_out_27_loss: 1.1090 - val_out_28_loss: 1.1197 - val_out_29_loss: 1.1148 - val_out_30_loss: 1.1076 - val_out_31_loss: 1.1085 - val_out_32_loss: 1.1194 - val_out_acc: 0.6074 - val_out_0_acc: 0.7375 - val_out_1_acc: 0.6009 - val_out_2_acc: 0.6009 - val_out_3_acc: 0.5987 - val_out_4_acc: 0.5944 - val_out_5_acc: 0.6052 - val_out_6_acc: 0.5987 - val_out_7_acc: 0.5987 - val_out_8_acc: 0.5922 - val_out_9_acc: 0.5965 - val_out_10_acc: 0.5944 - val_out_11_acc: 0.5922 - val_out_12_acc: 0.5965 - val_out_13_acc: 0.5879 - val_out_14_acc: 0.6030 - val_out_15_acc: 0.6095 - val_out_16_acc: 0.5900 - val_out_17_acc: 0.5965 - val_out_18_acc: 0.5922 - val_out_19_acc: 0.5900 - val_out_20_acc: 0.5900 - val_out_21_acc: 0.5965 - val_out_22_acc: 0.6030 - val_out_23_acc: 0.5987 - val_out_24_acc: 0.5944 - val_out_25_acc: 0.5922 - val_out_26_acc: 0.5987 - val_out_27_acc: 0.5987 - val_out_28_acc: 0.5944 - val_out_29_acc: 0.5987 - val_out_30_acc: 0.6095 - val_out_31_acc: 0.5900 - val_out_32_acc: 0.5900\n",
      "Epoch 35/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 41.0302 - out_loss: 1.0998 - out_0_loss: 0.9034 - out_1_loss: 1.2023 - out_2_loss: 1.2203 - out_3_loss: 1.2119 - out_4_loss: 1.2271 - out_5_loss: 1.2123 - out_6_loss: 1.2208 - out_7_loss: 1.2286 - out_8_loss: 1.2212 - out_9_loss: 1.2110 - out_10_loss: 1.2119 - out_11_loss: 1.2294 - out_12_loss: 1.2097 - out_13_loss: 1.2198 - out_14_loss: 1.2255 - out_15_loss: 1.2316 - out_16_loss: 1.2114 - out_17_loss: 1.2320 - out_18_loss: 1.2163 - out_19_loss: 1.2091 - out_20_loss: 1.2280 - out_21_loss: 1.2133 - out_22_loss: 1.2183 - out_23_loss: 1.2098 - out_24_loss: 1.2227 - out_25_loss: 1.2400 - out_26_loss: 1.2299 - out_27_loss: 1.2175 - out_28_loss: 1.2129 - out_29_loss: 1.2102 - out_30_loss: 1.2180 - out_31_loss: 1.2310 - out_32_loss: 1.2233 - out_acc: 0.6085 - out_0_acc: 0.6857 - out_1_acc: 0.5654 - out_2_acc: 0.5614 - out_3_acc: 0.5654 - out_4_acc: 0.5561 - out_5_acc: 0.5756 - out_6_acc: 0.5580 - out_7_acc: 0.5518 - out_8_acc: 0.5645 - out_9_acc: 0.5694 - out_10_acc: 0.5626 - out_11_acc: 0.5518 - out_12_acc: 0.5626 - out_13_acc: 0.5626 - out_14_acc: 0.5527 - out_15_acc: 0.5564 - out_16_acc: 0.5666 - out_17_acc: 0.5577 - out_18_acc: 0.5620 - out_19_acc: 0.5629 - out_20_acc: 0.5524 - out_21_acc: 0.5580 - out_22_acc: 0.5552 - out_23_acc: 0.5611 - out_24_acc: 0.5626 - out_25_acc: 0.5530 - out_26_acc: 0.5617 - out_27_acc: 0.5666 - out_28_acc: 0.5639 - out_29_acc: 0.5679 - out_30_acc: 0.5648 - out_31_acc: 0.5564 - out_32_acc: 0.5595 - val_loss: 42.0772 - val_out_loss: 1.1714 - val_out_0_loss: 0.8976 - val_out_1_loss: 1.1930 - val_out_2_loss: 1.1959 - val_out_3_loss: 1.1900 - val_out_4_loss: 1.2086 - val_out_5_loss: 1.1946 - val_out_6_loss: 1.1889 - val_out_7_loss: 1.1983 - val_out_8_loss: 1.1992 - val_out_9_loss: 1.2002 - val_out_10_loss: 1.1910 - val_out_11_loss: 1.2075 - val_out_12_loss: 1.1982 - val_out_13_loss: 1.2056 - val_out_14_loss: 1.2019 - val_out_15_loss: 1.1910 - val_out_16_loss: 1.1972 - val_out_17_loss: 1.1974 - val_out_18_loss: 1.1912 - val_out_19_loss: 1.2002 - val_out_20_loss: 1.2090 - val_out_21_loss: 1.1938 - val_out_22_loss: 1.1996 - val_out_23_loss: 1.1985 - val_out_24_loss: 1.2088 - val_out_25_loss: 1.1929 - val_out_26_loss: 1.2025 - val_out_27_loss: 1.1992 - val_out_28_loss: 1.1984 - val_out_29_loss: 1.2032 - val_out_30_loss: 1.1987 - val_out_31_loss: 1.1920 - val_out_32_loss: 1.1965 - val_out_acc: 0.5553 - val_out_0_acc: 0.6855 - val_out_1_acc: 0.5488 - val_out_2_acc: 0.5423 - val_out_3_acc: 0.5445 - val_out_4_acc: 0.5358 - val_out_5_acc: 0.5640 - val_out_6_acc: 0.5575 - val_out_7_acc: 0.5553 - val_out_8_acc: 0.5445 - val_out_9_acc: 0.5445 - val_out_10_acc: 0.5618 - val_out_11_acc: 0.5510 - val_out_12_acc: 0.5423 - val_out_13_acc: 0.5531 - val_out_14_acc: 0.5597 - val_out_15_acc: 0.5640 - val_out_16_acc: 0.5466 - val_out_17_acc: 0.5531 - val_out_18_acc: 0.5531 - val_out_19_acc: 0.5466 - val_out_20_acc: 0.5423 - val_out_21_acc: 0.5466 - val_out_22_acc: 0.5510 - val_out_23_acc: 0.5423 - val_out_24_acc: 0.5423 - val_out_25_acc: 0.5553 - val_out_26_acc: 0.5531 - val_out_27_acc: 0.5531 - val_out_28_acc: 0.5575 - val_out_29_acc: 0.5466 - val_out_30_acc: 0.5510 - val_out_31_acc: 0.5553 - val_out_32_acc: 0.5466\n",
      "Epoch 36/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 40.6361 - out_loss: 1.0875 - out_0_loss: 0.9003 - out_1_loss: 1.1940 - out_2_loss: 1.2190 - out_3_loss: 1.1966 - out_4_loss: 1.1937 - out_5_loss: 1.2077 - out_6_loss: 1.2059 - out_7_loss: 1.2047 - out_8_loss: 1.2025 - out_9_loss: 1.2055 - out_10_loss: 1.2066 - out_11_loss: 1.2168 - out_12_loss: 1.2081 - out_13_loss: 1.2267 - out_14_loss: 1.2089 - out_15_loss: 1.2073 - out_16_loss: 1.2004 - out_17_loss: 1.2093 - out_18_loss: 1.1952 - out_19_loss: 1.2055 - out_20_loss: 1.2182 - out_21_loss: 1.2174 - out_22_loss: 1.2095 - out_23_loss: 1.2219 - out_24_loss: 1.2294 - out_25_loss: 1.2038 - out_26_loss: 1.2170 - out_27_loss: 1.2050 - out_28_loss: 1.1994 - out_29_loss: 1.2006 - out_30_loss: 1.2069 - out_31_loss: 1.2015 - out_32_loss: 1.2030 - out_acc: 0.6116 - out_0_acc: 0.6761 - out_1_acc: 0.5688 - out_2_acc: 0.5682 - out_3_acc: 0.5701 - out_4_acc: 0.5769 - out_5_acc: 0.5697 - out_6_acc: 0.5685 - out_7_acc: 0.5595 - out_8_acc: 0.5676 - out_9_acc: 0.5691 - out_10_acc: 0.5595 - out_11_acc: 0.5688 - out_12_acc: 0.5620 - out_13_acc: 0.5704 - out_14_acc: 0.5756 - out_15_acc: 0.5738 - out_16_acc: 0.5725 - out_17_acc: 0.5685 - out_18_acc: 0.5781 - out_19_acc: 0.5623 - out_20_acc: 0.5651 - out_21_acc: 0.5608 - out_22_acc: 0.5617 - out_23_acc: 0.5620 - out_24_acc: 0.5657 - out_25_acc: 0.5645 - out_26_acc: 0.5598 - out_27_acc: 0.5673 - out_28_acc: 0.5666 - out_29_acc: 0.5787 - out_30_acc: 0.5673 - out_31_acc: 0.5725 - out_32_acc: 0.5673 - val_loss: 40.3390 - val_out_loss: 1.1030 - val_out_0_loss: 0.8550 - val_out_1_loss: 1.1503 - val_out_2_loss: 1.1334 - val_out_3_loss: 1.1414 - val_out_4_loss: 1.1490 - val_out_5_loss: 1.1282 - val_out_6_loss: 1.1449 - val_out_7_loss: 1.1427 - val_out_8_loss: 1.1579 - val_out_9_loss: 1.1602 - val_out_10_loss: 1.1509 - val_out_11_loss: 1.1674 - val_out_12_loss: 1.1496 - val_out_13_loss: 1.1406 - val_out_14_loss: 1.1501 - val_out_15_loss: 1.1365 - val_out_16_loss: 1.1496 - val_out_17_loss: 1.1589 - val_out_18_loss: 1.1408 - val_out_19_loss: 1.1468 - val_out_20_loss: 1.1591 - val_out_21_loss: 1.1481 - val_out_22_loss: 1.1505 - val_out_23_loss: 1.1553 - val_out_24_loss: 1.1608 - val_out_25_loss: 1.1543 - val_out_26_loss: 1.1404 - val_out_27_loss: 1.1475 - val_out_28_loss: 1.1608 - val_out_29_loss: 1.1555 - val_out_30_loss: 1.1590 - val_out_31_loss: 1.1405 - val_out_32_loss: 1.1532 - val_out_acc: 0.6204 - val_out_0_acc: 0.7419 - val_out_1_acc: 0.6139 - val_out_2_acc: 0.6182 - val_out_3_acc: 0.6074 - val_out_4_acc: 0.6117 - val_out_5_acc: 0.6139 - val_out_6_acc: 0.6095 - val_out_7_acc: 0.6182 - val_out_8_acc: 0.6095 - val_out_9_acc: 0.6204 - val_out_10_acc: 0.6204 - val_out_11_acc: 0.6095 - val_out_12_acc: 0.6074 - val_out_13_acc: 0.6009 - val_out_14_acc: 0.6161 - val_out_15_acc: 0.6117 - val_out_16_acc: 0.6009 - val_out_17_acc: 0.6139 - val_out_18_acc: 0.6161 - val_out_19_acc: 0.6182 - val_out_20_acc: 0.6095 - val_out_21_acc: 0.6161 - val_out_22_acc: 0.6204 - val_out_23_acc: 0.6139 - val_out_24_acc: 0.6161 - val_out_25_acc: 0.6161 - val_out_26_acc: 0.6226 - val_out_27_acc: 0.6182 - val_out_28_acc: 0.6074 - val_out_29_acc: 0.6139 - val_out_30_acc: 0.6161 - val_out_31_acc: 0.6030 - val_out_32_acc: 0.6226\n",
      "Epoch 37/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 39.9589 - out_loss: 1.0644 - out_0_loss: 0.8974 - out_1_loss: 1.1833 - out_2_loss: 1.1986 - out_3_loss: 1.1762 - out_4_loss: 1.1929 - out_5_loss: 1.1898 - out_6_loss: 1.1854 - out_7_loss: 1.2032 - out_8_loss: 1.1937 - out_9_loss: 1.2008 - out_10_loss: 1.1768 - out_11_loss: 1.1952 - out_12_loss: 1.1960 - out_13_loss: 1.1784 - out_14_loss: 1.1862 - out_15_loss: 1.1935 - out_16_loss: 1.1824 - out_17_loss: 1.1850 - out_18_loss: 1.2010 - out_19_loss: 1.1761 - out_20_loss: 1.1935 - out_21_loss: 1.1829 - out_22_loss: 1.1903 - out_23_loss: 1.1795 - out_24_loss: 1.1814 - out_25_loss: 1.1909 - out_26_loss: 1.1864 - out_27_loss: 1.1921 - out_28_loss: 1.1814 - out_29_loss: 1.1860 - out_30_loss: 1.1823 - out_31_loss: 1.1771 - out_32_loss: 1.1786 - out_acc: 0.6259 - out_0_acc: 0.6838 - out_1_acc: 0.5787 - out_2_acc: 0.5759 - out_3_acc: 0.5772 - out_4_acc: 0.5725 - out_5_acc: 0.5778 - out_6_acc: 0.5694 - out_7_acc: 0.5697 - out_8_acc: 0.5800 - out_9_acc: 0.5775 - out_10_acc: 0.5856 - out_11_acc: 0.5614 - out_12_acc: 0.5663 - out_13_acc: 0.5787 - out_14_acc: 0.5790 - out_15_acc: 0.5763 - out_16_acc: 0.5769 - out_17_acc: 0.5719 - out_18_acc: 0.5673 - out_19_acc: 0.5759 - out_20_acc: 0.5840 - out_21_acc: 0.5831 - out_22_acc: 0.5728 - out_23_acc: 0.5769 - out_24_acc: 0.5856 - out_25_acc: 0.5803 - out_26_acc: 0.5735 - out_27_acc: 0.5794 - out_28_acc: 0.5735 - out_29_acc: 0.5697 - out_30_acc: 0.5852 - out_31_acc: 0.5812 - out_32_acc: 0.5722 - val_loss: 42.1124 - val_out_loss: 1.1701 - val_out_0_loss: 0.8989 - val_out_1_loss: 1.1986 - val_out_2_loss: 1.1989 - val_out_3_loss: 1.2073 - val_out_4_loss: 1.1991 - val_out_5_loss: 1.1966 - val_out_6_loss: 1.1943 - val_out_7_loss: 1.1989 - val_out_8_loss: 1.2012 - val_out_9_loss: 1.2054 - val_out_10_loss: 1.1972 - val_out_11_loss: 1.2018 - val_out_12_loss: 1.1944 - val_out_13_loss: 1.2016 - val_out_14_loss: 1.1926 - val_out_15_loss: 1.1928 - val_out_16_loss: 1.2012 - val_out_17_loss: 1.2044 - val_out_18_loss: 1.1981 - val_out_19_loss: 1.2054 - val_out_20_loss: 1.2002 - val_out_21_loss: 1.1912 - val_out_22_loss: 1.2006 - val_out_23_loss: 1.2046 - val_out_24_loss: 1.1937 - val_out_25_loss: 1.2024 - val_out_26_loss: 1.2056 - val_out_27_loss: 1.1971 - val_out_28_loss: 1.2014 - val_out_29_loss: 1.1940 - val_out_30_loss: 1.1882 - val_out_31_loss: 1.2003 - val_out_32_loss: 1.2071 - val_out_acc: 0.5510 - val_out_0_acc: 0.6768 - val_out_1_acc: 0.5423 - val_out_2_acc: 0.5510 - val_out_3_acc: 0.5380 - val_out_4_acc: 0.5445 - val_out_5_acc: 0.5510 - val_out_6_acc: 0.5575 - val_out_7_acc: 0.5358 - val_out_8_acc: 0.5531 - val_out_9_acc: 0.5466 - val_out_10_acc: 0.5445 - val_out_11_acc: 0.5553 - val_out_12_acc: 0.5466 - val_out_13_acc: 0.5380 - val_out_14_acc: 0.5466 - val_out_15_acc: 0.5553 - val_out_16_acc: 0.5336 - val_out_17_acc: 0.5380 - val_out_18_acc: 0.5423 - val_out_19_acc: 0.5358 - val_out_20_acc: 0.5510 - val_out_21_acc: 0.5445 - val_out_22_acc: 0.5336 - val_out_23_acc: 0.5510 - val_out_24_acc: 0.5553 - val_out_25_acc: 0.5445 - val_out_26_acc: 0.5401 - val_out_27_acc: 0.5488 - val_out_28_acc: 0.5445 - val_out_29_acc: 0.5510 - val_out_30_acc: 0.5445 - val_out_31_acc: 0.5423 - val_out_32_acc: 0.5423\n",
      "Epoch 38/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 39.7489 - out_loss: 1.0587 - out_0_loss: 0.8378 - out_1_loss: 1.1682 - out_2_loss: 1.1779 - out_3_loss: 1.1786 - out_4_loss: 1.2061 - out_5_loss: 1.1910 - out_6_loss: 1.1783 - out_7_loss: 1.2002 - out_8_loss: 1.1852 - out_9_loss: 1.1678 - out_10_loss: 1.1856 - out_11_loss: 1.1927 - out_12_loss: 1.1758 - out_13_loss: 1.1831 - out_14_loss: 1.1715 - out_15_loss: 1.1890 - out_16_loss: 1.1814 - out_17_loss: 1.1926 - out_18_loss: 1.1717 - out_19_loss: 1.1913 - out_20_loss: 1.1879 - out_21_loss: 1.1831 - out_22_loss: 1.1822 - out_23_loss: 1.1846 - out_24_loss: 1.1720 - out_25_loss: 1.1829 - out_26_loss: 1.1771 - out_27_loss: 1.1919 - out_28_loss: 1.1943 - out_29_loss: 1.1705 - out_30_loss: 1.1735 - out_31_loss: 1.1808 - out_32_loss: 1.1835 - out_acc: 0.6190 - out_0_acc: 0.7232 - out_1_acc: 0.5747 - out_2_acc: 0.5725 - out_3_acc: 0.5682 - out_4_acc: 0.5660 - out_5_acc: 0.5670 - out_6_acc: 0.5732 - out_7_acc: 0.5697 - out_8_acc: 0.5722 - out_9_acc: 0.5741 - out_10_acc: 0.5831 - out_11_acc: 0.5673 - out_12_acc: 0.5778 - out_13_acc: 0.5750 - out_14_acc: 0.5772 - out_15_acc: 0.5688 - out_16_acc: 0.5719 - out_17_acc: 0.5670 - out_18_acc: 0.5794 - out_19_acc: 0.5790 - out_20_acc: 0.5635 - out_21_acc: 0.5732 - out_22_acc: 0.5725 - out_23_acc: 0.5697 - out_24_acc: 0.5701 - out_25_acc: 0.5673 - out_26_acc: 0.5763 - out_27_acc: 0.5620 - out_28_acc: 0.5794 - out_29_acc: 0.5759 - out_30_acc: 0.5728 - out_31_acc: 0.5732 - out_32_acc: 0.5728 - val_loss: 37.4476 - val_out_loss: 1.0429 - val_out_0_loss: 0.8457 - val_out_1_loss: 1.0571 - val_out_2_loss: 1.0604 - val_out_3_loss: 1.0646 - val_out_4_loss: 1.0639 - val_out_5_loss: 1.0588 - val_out_6_loss: 1.0670 - val_out_7_loss: 1.0648 - val_out_8_loss: 1.0575 - val_out_9_loss: 1.0611 - val_out_10_loss: 1.0714 - val_out_11_loss: 1.0698 - val_out_12_loss: 1.0625 - val_out_13_loss: 1.0688 - val_out_14_loss: 1.0652 - val_out_15_loss: 1.0583 - val_out_16_loss: 1.0681 - val_out_17_loss: 1.0631 - val_out_18_loss: 1.0591 - val_out_19_loss: 1.0642 - val_out_20_loss: 1.0728 - val_out_21_loss: 1.0643 - val_out_22_loss: 1.0633 - val_out_23_loss: 1.0724 - val_out_24_loss: 1.0662 - val_out_25_loss: 1.0709 - val_out_26_loss: 1.0625 - val_out_27_loss: 1.0650 - val_out_28_loss: 1.0669 - val_out_29_loss: 1.0689 - val_out_30_loss: 1.0650 - val_out_31_loss: 1.0643 - val_out_32_loss: 1.0685 - val_out_acc: 0.6399 - val_out_0_acc: 0.6833 - val_out_1_acc: 0.6399 - val_out_2_acc: 0.6291 - val_out_3_acc: 0.6334 - val_out_4_acc: 0.6421 - val_out_5_acc: 0.6334 - val_out_6_acc: 0.6312 - val_out_7_acc: 0.6312 - val_out_8_acc: 0.6356 - val_out_9_acc: 0.6486 - val_out_10_acc: 0.6291 - val_out_11_acc: 0.6399 - val_out_12_acc: 0.6269 - val_out_13_acc: 0.6334 - val_out_14_acc: 0.6508 - val_out_15_acc: 0.6464 - val_out_16_acc: 0.6356 - val_out_17_acc: 0.6399 - val_out_18_acc: 0.6464 - val_out_19_acc: 0.6291 - val_out_20_acc: 0.6226 - val_out_21_acc: 0.6312 - val_out_22_acc: 0.6334 - val_out_23_acc: 0.6204 - val_out_24_acc: 0.6464 - val_out_25_acc: 0.6334 - val_out_26_acc: 0.6312 - val_out_27_acc: 0.6312 - val_out_28_acc: 0.6334 - val_out_29_acc: 0.6377 - val_out_30_acc: 0.6443 - val_out_31_acc: 0.6377 - val_out_32_acc: 0.6247\n",
      "Epoch 39/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 39.1692 - out_loss: 1.0448 - out_0_loss: 0.8615 - out_1_loss: 1.1624 - out_2_loss: 1.1710 - out_3_loss: 1.1688 - out_4_loss: 1.1675 - out_5_loss: 1.1670 - out_6_loss: 1.1631 - out_7_loss: 1.1697 - out_8_loss: 1.1564 - out_9_loss: 1.1632 - out_10_loss: 1.1456 - out_11_loss: 1.1904 - out_12_loss: 1.1414 - out_13_loss: 1.1587 - out_14_loss: 1.1654 - out_15_loss: 1.1759 - out_16_loss: 1.1540 - out_17_loss: 1.1679 - out_18_loss: 1.1521 - out_19_loss: 1.1604 - out_20_loss: 1.1637 - out_21_loss: 1.1599 - out_22_loss: 1.1684 - out_23_loss: 1.1719 - out_24_loss: 1.1838 - out_25_loss: 1.1690 - out_26_loss: 1.1533 - out_27_loss: 1.1719 - out_28_loss: 1.1614 - out_29_loss: 1.1622 - out_30_loss: 1.1713 - out_31_loss: 1.1563 - out_32_loss: 1.1690 - out_acc: 0.6311 - out_0_acc: 0.6984 - out_1_acc: 0.5852 - out_2_acc: 0.5719 - out_3_acc: 0.5744 - out_4_acc: 0.5908 - out_5_acc: 0.5877 - out_6_acc: 0.5843 - out_7_acc: 0.5800 - out_8_acc: 0.5865 - out_9_acc: 0.5781 - out_10_acc: 0.6007 - out_11_acc: 0.5738 - out_12_acc: 0.6007 - out_13_acc: 0.5843 - out_14_acc: 0.5945 - out_15_acc: 0.5766 - out_16_acc: 0.5741 - out_17_acc: 0.5803 - out_18_acc: 0.5797 - out_19_acc: 0.5893 - out_20_acc: 0.5825 - out_21_acc: 0.5856 - out_22_acc: 0.5654 - out_23_acc: 0.5759 - out_24_acc: 0.5759 - out_25_acc: 0.5787 - out_26_acc: 0.5840 - out_27_acc: 0.5837 - out_28_acc: 0.5837 - out_29_acc: 0.5890 - out_30_acc: 0.5887 - out_31_acc: 0.5911 - out_32_acc: 0.5806 - val_loss: 41.5232 - val_out_loss: 1.1440 - val_out_0_loss: 0.8621 - val_out_1_loss: 1.1866 - val_out_2_loss: 1.1679 - val_out_3_loss: 1.1802 - val_out_4_loss: 1.1885 - val_out_5_loss: 1.1744 - val_out_6_loss: 1.1841 - val_out_7_loss: 1.1910 - val_out_8_loss: 1.1914 - val_out_9_loss: 1.1848 - val_out_10_loss: 1.1851 - val_out_11_loss: 1.1945 - val_out_12_loss: 1.1815 - val_out_13_loss: 1.1950 - val_out_14_loss: 1.1881 - val_out_15_loss: 1.1932 - val_out_16_loss: 1.1761 - val_out_17_loss: 1.1768 - val_out_18_loss: 1.1825 - val_out_19_loss: 1.1969 - val_out_20_loss: 1.1808 - val_out_21_loss: 1.1679 - val_out_22_loss: 1.1872 - val_out_23_loss: 1.1875 - val_out_24_loss: 1.1879 - val_out_25_loss: 1.1857 - val_out_26_loss: 1.1881 - val_out_27_loss: 1.1676 - val_out_28_loss: 1.1818 - val_out_29_loss: 1.1846 - val_out_30_loss: 1.1773 - val_out_31_loss: 1.1771 - val_out_32_loss: 1.1815 - val_out_acc: 0.5640 - val_out_0_acc: 0.6963 - val_out_1_acc: 0.5553 - val_out_2_acc: 0.5683 - val_out_3_acc: 0.5488 - val_out_4_acc: 0.5618 - val_out_5_acc: 0.5727 - val_out_6_acc: 0.5597 - val_out_7_acc: 0.5531 - val_out_8_acc: 0.5575 - val_out_9_acc: 0.5553 - val_out_10_acc: 0.5597 - val_out_11_acc: 0.5466 - val_out_12_acc: 0.5618 - val_out_13_acc: 0.5488 - val_out_14_acc: 0.5510 - val_out_15_acc: 0.5575 - val_out_16_acc: 0.5597 - val_out_17_acc: 0.5618 - val_out_18_acc: 0.5618 - val_out_19_acc: 0.5597 - val_out_20_acc: 0.5575 - val_out_21_acc: 0.5553 - val_out_22_acc: 0.5575 - val_out_23_acc: 0.5553 - val_out_24_acc: 0.5575 - val_out_25_acc: 0.5662 - val_out_26_acc: 0.5662 - val_out_27_acc: 0.5597 - val_out_28_acc: 0.5575 - val_out_29_acc: 0.5510 - val_out_30_acc: 0.5618 - val_out_31_acc: 0.5575 - val_out_32_acc: 0.5597\n",
      "Epoch 40/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 39.2148 - out_loss: 1.0446 - out_0_loss: 0.8386 - out_1_loss: 1.1506 - out_2_loss: 1.1617 - out_3_loss: 1.1751 - out_4_loss: 1.1600 - out_5_loss: 1.1628 - out_6_loss: 1.1628 - out_7_loss: 1.1867 - out_8_loss: 1.1781 - out_9_loss: 1.1689 - out_10_loss: 1.1688 - out_11_loss: 1.1727 - out_12_loss: 1.1680 - out_13_loss: 1.1652 - out_14_loss: 1.1669 - out_15_loss: 1.1749 - out_16_loss: 1.1654 - out_17_loss: 1.1589 - out_18_loss: 1.1632 - out_19_loss: 1.1593 - out_20_loss: 1.1699 - out_21_loss: 1.1604 - out_22_loss: 1.1638 - out_23_loss: 1.1603 - out_24_loss: 1.1673 - out_25_loss: 1.1679 - out_26_loss: 1.1584 - out_27_loss: 1.1628 - out_28_loss: 1.1598 - out_29_loss: 1.1640 - out_30_loss: 1.1620 - out_31_loss: 1.1827 - out_32_loss: 1.1827 - out_acc: 0.6321 - out_0_acc: 0.7049 - out_1_acc: 0.5868 - out_2_acc: 0.5812 - out_3_acc: 0.5725 - out_4_acc: 0.5846 - out_5_acc: 0.5856 - out_6_acc: 0.5803 - out_7_acc: 0.5685 - out_8_acc: 0.5809 - out_9_acc: 0.5856 - out_10_acc: 0.5806 - out_11_acc: 0.5763 - out_12_acc: 0.5883 - out_13_acc: 0.5778 - out_14_acc: 0.5741 - out_15_acc: 0.5809 - out_16_acc: 0.5818 - out_17_acc: 0.5887 - out_18_acc: 0.5877 - out_19_acc: 0.5877 - out_20_acc: 0.5887 - out_21_acc: 0.5772 - out_22_acc: 0.5933 - out_23_acc: 0.5828 - out_24_acc: 0.5856 - out_25_acc: 0.5902 - out_26_acc: 0.5880 - out_27_acc: 0.5806 - out_28_acc: 0.5812 - out_29_acc: 0.5753 - out_30_acc: 0.5871 - out_31_acc: 0.5759 - out_32_acc: 0.5732 - val_loss: 36.8957 - val_out_loss: 1.0258 - val_out_0_loss: 0.7887 - val_out_1_loss: 1.0441 - val_out_2_loss: 1.0444 - val_out_3_loss: 1.0523 - val_out_4_loss: 1.0527 - val_out_5_loss: 1.0493 - val_out_6_loss: 1.0460 - val_out_7_loss: 1.0511 - val_out_8_loss: 1.0522 - val_out_9_loss: 1.0508 - val_out_10_loss: 1.0541 - val_out_11_loss: 1.0559 - val_out_12_loss: 1.0469 - val_out_13_loss: 1.0511 - val_out_14_loss: 1.0571 - val_out_15_loss: 1.0516 - val_out_16_loss: 1.0448 - val_out_17_loss: 1.0513 - val_out_18_loss: 1.0486 - val_out_19_loss: 1.0578 - val_out_20_loss: 1.0593 - val_out_21_loss: 1.0455 - val_out_22_loss: 1.0490 - val_out_23_loss: 1.0479 - val_out_24_loss: 1.0501 - val_out_25_loss: 1.0553 - val_out_26_loss: 1.0502 - val_out_27_loss: 1.0488 - val_out_28_loss: 1.0528 - val_out_29_loss: 1.0493 - val_out_30_loss: 1.0440 - val_out_31_loss: 1.0530 - val_out_32_loss: 1.0535 - val_out_acc: 0.6204 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.6204 - val_out_2_acc: 0.6204 - val_out_3_acc: 0.6182 - val_out_4_acc: 0.6095 - val_out_5_acc: 0.6204 - val_out_6_acc: 0.6182 - val_out_7_acc: 0.6161 - val_out_8_acc: 0.6161 - val_out_9_acc: 0.6074 - val_out_10_acc: 0.6204 - val_out_11_acc: 0.6161 - val_out_12_acc: 0.6226 - val_out_13_acc: 0.6182 - val_out_14_acc: 0.6095 - val_out_15_acc: 0.6226 - val_out_16_acc: 0.6161 - val_out_17_acc: 0.6052 - val_out_18_acc: 0.6052 - val_out_19_acc: 0.6095 - val_out_20_acc: 0.6117 - val_out_21_acc: 0.6095 - val_out_22_acc: 0.6139 - val_out_23_acc: 0.6052 - val_out_24_acc: 0.6204 - val_out_25_acc: 0.6247 - val_out_26_acc: 0.6139 - val_out_27_acc: 0.6095 - val_out_28_acc: 0.6074 - val_out_29_acc: 0.6052 - val_out_30_acc: 0.6139 - val_out_31_acc: 0.6204 - val_out_32_acc: 0.6095\n",
      "Epoch 41/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 38.7656 - out_loss: 1.0341 - out_0_loss: 0.8670 - out_1_loss: 1.1583 - out_2_loss: 1.1548 - out_3_loss: 1.1567 - out_4_loss: 1.1676 - out_5_loss: 1.1524 - out_6_loss: 1.1449 - out_7_loss: 1.1607 - out_8_loss: 1.1508 - out_9_loss: 1.1603 - out_10_loss: 1.1537 - out_11_loss: 1.1478 - out_12_loss: 1.1579 - out_13_loss: 1.1407 - out_14_loss: 1.1470 - out_15_loss: 1.1531 - out_16_loss: 1.1544 - out_17_loss: 1.1526 - out_18_loss: 1.1591 - out_19_loss: 1.1572 - out_20_loss: 1.1591 - out_21_loss: 1.1421 - out_22_loss: 1.1490 - out_23_loss: 1.1411 - out_24_loss: 1.1373 - out_25_loss: 1.1584 - out_26_loss: 1.1486 - out_27_loss: 1.1334 - out_28_loss: 1.1591 - out_29_loss: 1.1586 - out_30_loss: 1.1519 - out_31_loss: 1.1419 - out_32_loss: 1.1540 - out_acc: 0.6339 - out_0_acc: 0.6971 - out_1_acc: 0.5890 - out_2_acc: 0.5890 - out_3_acc: 0.5902 - out_4_acc: 0.5769 - out_5_acc: 0.5846 - out_6_acc: 0.5849 - out_7_acc: 0.5840 - out_8_acc: 0.5945 - out_9_acc: 0.5738 - out_10_acc: 0.5883 - out_11_acc: 0.5834 - out_12_acc: 0.5797 - out_13_acc: 0.5874 - out_14_acc: 0.5856 - out_15_acc: 0.5859 - out_16_acc: 0.5862 - out_17_acc: 0.5902 - out_18_acc: 0.5831 - out_19_acc: 0.5797 - out_20_acc: 0.5809 - out_21_acc: 0.5821 - out_22_acc: 0.5921 - out_23_acc: 0.5862 - out_24_acc: 0.5887 - out_25_acc: 0.5862 - out_26_acc: 0.5856 - out_27_acc: 0.5890 - out_28_acc: 0.5871 - out_29_acc: 0.5753 - out_30_acc: 0.5939 - out_31_acc: 0.5880 - out_32_acc: 0.5837 - val_loss: 38.3992 - val_out_loss: 1.0644 - val_out_0_loss: 0.7971 - val_out_1_loss: 1.0876 - val_out_2_loss: 1.0940 - val_out_3_loss: 1.0955 - val_out_4_loss: 1.0954 - val_out_5_loss: 1.0939 - val_out_6_loss: 1.0956 - val_out_7_loss: 1.0844 - val_out_8_loss: 1.0902 - val_out_9_loss: 1.0911 - val_out_10_loss: 1.0957 - val_out_11_loss: 1.0941 - val_out_12_loss: 1.0907 - val_out_13_loss: 1.0986 - val_out_14_loss: 1.0908 - val_out_15_loss: 1.0901 - val_out_16_loss: 1.0953 - val_out_17_loss: 1.0954 - val_out_18_loss: 1.0875 - val_out_19_loss: 1.0982 - val_out_20_loss: 1.0989 - val_out_21_loss: 1.0791 - val_out_22_loss: 1.0991 - val_out_23_loss: 1.1009 - val_out_24_loss: 1.0992 - val_out_25_loss: 1.0984 - val_out_26_loss: 1.1034 - val_out_27_loss: 1.0992 - val_out_28_loss: 1.0967 - val_out_29_loss: 1.0914 - val_out_30_loss: 1.0956 - val_out_31_loss: 1.0989 - val_out_32_loss: 1.0930 - val_out_acc: 0.6161 - val_out_0_acc: 0.7072 - val_out_1_acc: 0.5922 - val_out_2_acc: 0.6052 - val_out_3_acc: 0.5922 - val_out_4_acc: 0.5965 - val_out_5_acc: 0.5987 - val_out_6_acc: 0.6074 - val_out_7_acc: 0.5944 - val_out_8_acc: 0.5944 - val_out_9_acc: 0.6009 - val_out_10_acc: 0.5987 - val_out_11_acc: 0.5987 - val_out_12_acc: 0.5987 - val_out_13_acc: 0.5944 - val_out_14_acc: 0.5944 - val_out_15_acc: 0.5944 - val_out_16_acc: 0.5770 - val_out_17_acc: 0.6074 - val_out_18_acc: 0.5987 - val_out_19_acc: 0.5922 - val_out_20_acc: 0.5944 - val_out_21_acc: 0.6052 - val_out_22_acc: 0.5879 - val_out_23_acc: 0.5813 - val_out_24_acc: 0.6052 - val_out_25_acc: 0.5900 - val_out_26_acc: 0.5987 - val_out_27_acc: 0.5900 - val_out_28_acc: 0.6030 - val_out_29_acc: 0.6030 - val_out_30_acc: 0.5987 - val_out_31_acc: 0.5900 - val_out_32_acc: 0.6052\n",
      "Epoch 42/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 38.2397 - out_loss: 1.0187 - out_0_loss: 0.8446 - out_1_loss: 1.1458 - out_2_loss: 1.1297 - out_3_loss: 1.1365 - out_4_loss: 1.1415 - out_5_loss: 1.1423 - out_6_loss: 1.1431 - out_7_loss: 1.1371 - out_8_loss: 1.1411 - out_9_loss: 1.1330 - out_10_loss: 1.1309 - out_11_loss: 1.1336 - out_12_loss: 1.1342 - out_13_loss: 1.1455 - out_14_loss: 1.1316 - out_15_loss: 1.1368 - out_16_loss: 1.1366 - out_17_loss: 1.1462 - out_18_loss: 1.1386 - out_19_loss: 1.1347 - out_20_loss: 1.1473 - out_21_loss: 1.1369 - out_22_loss: 1.1432 - out_23_loss: 1.1357 - out_24_loss: 1.1247 - out_25_loss: 1.1388 - out_26_loss: 1.1370 - out_27_loss: 1.1287 - out_28_loss: 1.1389 - out_29_loss: 1.1183 - out_30_loss: 1.1462 - out_31_loss: 1.1174 - out_32_loss: 1.1445 - out_acc: 0.6327 - out_0_acc: 0.6866 - out_1_acc: 0.5831 - out_2_acc: 0.5809 - out_3_acc: 0.5918 - out_4_acc: 0.5849 - out_5_acc: 0.5849 - out_6_acc: 0.5809 - out_7_acc: 0.5908 - out_8_acc: 0.5902 - out_9_acc: 0.5924 - out_10_acc: 0.5952 - out_11_acc: 0.5964 - out_12_acc: 0.5924 - out_13_acc: 0.5843 - out_14_acc: 0.5890 - out_15_acc: 0.5930 - out_16_acc: 0.5865 - out_17_acc: 0.5834 - out_18_acc: 0.5840 - out_19_acc: 0.5871 - out_20_acc: 0.5846 - out_21_acc: 0.5902 - out_22_acc: 0.5784 - out_23_acc: 0.5952 - out_24_acc: 0.5964 - out_25_acc: 0.5871 - out_26_acc: 0.5899 - out_27_acc: 0.5887 - out_28_acc: 0.5725 - out_29_acc: 0.5930 - out_30_acc: 0.5812 - out_31_acc: 0.5939 - out_32_acc: 0.5856 - val_loss: 35.1081 - val_out_loss: 0.9793 - val_out_0_loss: 0.8078 - val_out_1_loss: 0.9923 - val_out_2_loss: 0.9909 - val_out_3_loss: 0.9922 - val_out_4_loss: 1.0075 - val_out_5_loss: 0.9909 - val_out_6_loss: 0.9975 - val_out_7_loss: 0.9927 - val_out_8_loss: 0.9927 - val_out_9_loss: 1.0026 - val_out_10_loss: 0.9982 - val_out_11_loss: 0.9987 - val_out_12_loss: 1.0011 - val_out_13_loss: 1.0021 - val_out_14_loss: 1.0027 - val_out_15_loss: 1.0005 - val_out_16_loss: 0.9910 - val_out_17_loss: 0.9966 - val_out_18_loss: 0.9955 - val_out_19_loss: 1.0034 - val_out_20_loss: 1.0016 - val_out_21_loss: 0.9878 - val_out_22_loss: 0.9996 - val_out_23_loss: 0.9978 - val_out_24_loss: 0.9907 - val_out_25_loss: 1.0014 - val_out_26_loss: 1.0030 - val_out_27_loss: 1.0040 - val_out_28_loss: 1.0010 - val_out_29_loss: 0.9997 - val_out_30_loss: 0.9983 - val_out_31_loss: 0.9960 - val_out_32_loss: 1.0009 - val_out_acc: 0.6464 - val_out_0_acc: 0.7137 - val_out_1_acc: 0.6421 - val_out_2_acc: 0.6486 - val_out_3_acc: 0.6421 - val_out_4_acc: 0.6421 - val_out_5_acc: 0.6443 - val_out_6_acc: 0.6377 - val_out_7_acc: 0.6399 - val_out_8_acc: 0.6421 - val_out_9_acc: 0.6399 - val_out_10_acc: 0.6443 - val_out_11_acc: 0.6464 - val_out_12_acc: 0.6421 - val_out_13_acc: 0.6464 - val_out_14_acc: 0.6443 - val_out_15_acc: 0.6443 - val_out_16_acc: 0.6377 - val_out_17_acc: 0.6443 - val_out_18_acc: 0.6486 - val_out_19_acc: 0.6399 - val_out_20_acc: 0.6399 - val_out_21_acc: 0.6443 - val_out_22_acc: 0.6399 - val_out_23_acc: 0.6399 - val_out_24_acc: 0.6421 - val_out_25_acc: 0.6464 - val_out_26_acc: 0.6399 - val_out_27_acc: 0.6377 - val_out_28_acc: 0.6529 - val_out_29_acc: 0.6334 - val_out_30_acc: 0.6464 - val_out_31_acc: 0.6486 - val_out_32_acc: 0.6443\n",
      "Epoch 43/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 37.6864 - out_loss: 1.0036 - out_0_loss: 0.8505 - out_1_loss: 1.1114 - out_2_loss: 1.1192 - out_3_loss: 1.1209 - out_4_loss: 1.1123 - out_5_loss: 1.1143 - out_6_loss: 1.1240 - out_7_loss: 1.1087 - out_8_loss: 1.1269 - out_9_loss: 1.1201 - out_10_loss: 1.1204 - out_11_loss: 1.1256 - out_12_loss: 1.1281 - out_13_loss: 1.1064 - out_14_loss: 1.1190 - out_15_loss: 1.1258 - out_16_loss: 1.1256 - out_17_loss: 1.1152 - out_18_loss: 1.1339 - out_19_loss: 1.1205 - out_20_loss: 1.1181 - out_21_loss: 1.1146 - out_22_loss: 1.1377 - out_23_loss: 1.1211 - out_24_loss: 1.1077 - out_25_loss: 1.1356 - out_26_loss: 1.1302 - out_27_loss: 1.1156 - out_28_loss: 1.1215 - out_29_loss: 1.1227 - out_30_loss: 1.1110 - out_31_loss: 1.1098 - out_32_loss: 1.1084 - out_acc: 0.6472 - out_0_acc: 0.7086 - out_1_acc: 0.6023 - out_2_acc: 0.6045 - out_3_acc: 0.5964 - out_4_acc: 0.5933 - out_5_acc: 0.6023 - out_6_acc: 0.5964 - out_7_acc: 0.6066 - out_8_acc: 0.5924 - out_9_acc: 0.5883 - out_10_acc: 0.5914 - out_11_acc: 0.5945 - out_12_acc: 0.5927 - out_13_acc: 0.6020 - out_14_acc: 0.6042 - out_15_acc: 0.5995 - out_16_acc: 0.5942 - out_17_acc: 0.5930 - out_18_acc: 0.5967 - out_19_acc: 0.6032 - out_20_acc: 0.5980 - out_21_acc: 0.5936 - out_22_acc: 0.5939 - out_23_acc: 0.5936 - out_24_acc: 0.6035 - out_25_acc: 0.5908 - out_26_acc: 0.5986 - out_27_acc: 0.5964 - out_28_acc: 0.5905 - out_29_acc: 0.6007 - out_30_acc: 0.5958 - out_31_acc: 0.6029 - out_32_acc: 0.6060 - val_loss: 36.7182 - val_out_loss: 1.0215 - val_out_0_loss: 0.7561 - val_out_1_loss: 1.0417 - val_out_2_loss: 1.0430 - val_out_3_loss: 1.0402 - val_out_4_loss: 1.0506 - val_out_5_loss: 1.0416 - val_out_6_loss: 1.0451 - val_out_7_loss: 1.0497 - val_out_8_loss: 1.0487 - val_out_9_loss: 1.0516 - val_out_10_loss: 1.0486 - val_out_11_loss: 1.0517 - val_out_12_loss: 1.0471 - val_out_13_loss: 1.0496 - val_out_14_loss: 1.0454 - val_out_15_loss: 1.0435 - val_out_16_loss: 1.0455 - val_out_17_loss: 1.0432 - val_out_18_loss: 1.0425 - val_out_19_loss: 1.0595 - val_out_20_loss: 1.0536 - val_out_21_loss: 1.0313 - val_out_22_loss: 1.0417 - val_out_23_loss: 1.0416 - val_out_24_loss: 1.0522 - val_out_25_loss: 1.0502 - val_out_26_loss: 1.0530 - val_out_27_loss: 1.0406 - val_out_28_loss: 1.0448 - val_out_29_loss: 1.0464 - val_out_30_loss: 1.0542 - val_out_31_loss: 1.0488 - val_out_32_loss: 1.0399 - val_out_acc: 0.5987 - val_out_0_acc: 0.7223 - val_out_1_acc: 0.6030 - val_out_2_acc: 0.6052 - val_out_3_acc: 0.6095 - val_out_4_acc: 0.5987 - val_out_5_acc: 0.6074 - val_out_6_acc: 0.5944 - val_out_7_acc: 0.6052 - val_out_8_acc: 0.5987 - val_out_9_acc: 0.5987 - val_out_10_acc: 0.5944 - val_out_11_acc: 0.5944 - val_out_12_acc: 0.5987 - val_out_13_acc: 0.6030 - val_out_14_acc: 0.6074 - val_out_15_acc: 0.6009 - val_out_16_acc: 0.5987 - val_out_17_acc: 0.6052 - val_out_18_acc: 0.6030 - val_out_19_acc: 0.5900 - val_out_20_acc: 0.5922 - val_out_21_acc: 0.6052 - val_out_22_acc: 0.6009 - val_out_23_acc: 0.6052 - val_out_24_acc: 0.5987 - val_out_25_acc: 0.6009 - val_out_26_acc: 0.6030 - val_out_27_acc: 0.6052 - val_out_28_acc: 0.6009 - val_out_29_acc: 0.6117 - val_out_30_acc: 0.6009 - val_out_31_acc: 0.5944 - val_out_32_acc: 0.5965\n",
      "Epoch 44/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 105s - loss: 38.2847 - out_loss: 1.0181 - out_0_loss: 0.8142 - out_1_loss: 1.1266 - out_2_loss: 1.1246 - out_3_loss: 1.1407 - out_4_loss: 1.1417 - out_5_loss: 1.1249 - out_6_loss: 1.1453 - out_7_loss: 1.1545 - out_8_loss: 1.1373 - out_9_loss: 1.1226 - out_10_loss: 1.1487 - out_11_loss: 1.1292 - out_12_loss: 1.1420 - out_13_loss: 1.1359 - out_14_loss: 1.1418 - out_15_loss: 1.1403 - out_16_loss: 1.1248 - out_17_loss: 1.1455 - out_18_loss: 1.1529 - out_19_loss: 1.1418 - out_20_loss: 1.1406 - out_21_loss: 1.1347 - out_22_loss: 1.1547 - out_23_loss: 1.1373 - out_24_loss: 1.1449 - out_25_loss: 1.1447 - out_26_loss: 1.1542 - out_27_loss: 1.1210 - out_28_loss: 1.1413 - out_29_loss: 1.1303 - out_30_loss: 1.1456 - out_31_loss: 1.1289 - out_32_loss: 1.1530 - out_acc: 0.6376 - out_0_acc: 0.7148 - out_1_acc: 0.6014 - out_2_acc: 0.6060 - out_3_acc: 0.5849 - out_4_acc: 0.5899 - out_5_acc: 0.6051 - out_6_acc: 0.5890 - out_7_acc: 0.5976 - out_8_acc: 0.5849 - out_9_acc: 0.6014 - out_10_acc: 0.5877 - out_11_acc: 0.6017 - out_12_acc: 0.5859 - out_13_acc: 0.5899 - out_14_acc: 0.5859 - out_15_acc: 0.5967 - out_16_acc: 0.6023 - out_17_acc: 0.5908 - out_18_acc: 0.5846 - out_19_acc: 0.5914 - out_20_acc: 0.5893 - out_21_acc: 0.5945 - out_22_acc: 0.5880 - out_23_acc: 0.5945 - out_24_acc: 0.6045 - out_25_acc: 0.5874 - out_26_acc: 0.5893 - out_27_acc: 0.5995 - out_28_acc: 0.5914 - out_29_acc: 0.6051 - out_30_acc: 0.5949 - out_31_acc: 0.5880 - out_32_acc: 0.5828 - val_loss: 35.4603 - val_out_loss: 0.9851 - val_out_0_loss: 0.7325 - val_out_1_loss: 1.0042 - val_out_2_loss: 1.0112 - val_out_3_loss: 1.0073 - val_out_4_loss: 1.0065 - val_out_5_loss: 1.0111 - val_out_6_loss: 1.0100 - val_out_7_loss: 1.0077 - val_out_8_loss: 1.0115 - val_out_9_loss: 1.0101 - val_out_10_loss: 1.0133 - val_out_11_loss: 1.0143 - val_out_12_loss: 1.0060 - val_out_13_loss: 1.0104 - val_out_14_loss: 1.0098 - val_out_15_loss: 1.0106 - val_out_16_loss: 1.0116 - val_out_17_loss: 1.0100 - val_out_18_loss: 1.0103 - val_out_19_loss: 1.0220 - val_out_20_loss: 1.0129 - val_out_21_loss: 1.0081 - val_out_22_loss: 1.0051 - val_out_23_loss: 1.0128 - val_out_24_loss: 1.0144 - val_out_25_loss: 1.0143 - val_out_26_loss: 1.0135 - val_out_27_loss: 1.0081 - val_out_28_loss: 1.0109 - val_out_29_loss: 1.0061 - val_out_30_loss: 1.0113 - val_out_31_loss: 1.0096 - val_out_32_loss: 1.0142 - val_out_acc: 0.6681 - val_out_0_acc: 0.7310 - val_out_1_acc: 0.6594 - val_out_2_acc: 0.6616 - val_out_3_acc: 0.6573 - val_out_4_acc: 0.6551 - val_out_5_acc: 0.6573 - val_out_6_acc: 0.6616 - val_out_7_acc: 0.6616 - val_out_8_acc: 0.6703 - val_out_9_acc: 0.6551 - val_out_10_acc: 0.6616 - val_out_11_acc: 0.6529 - val_out_12_acc: 0.6573 - val_out_13_acc: 0.6659 - val_out_14_acc: 0.6638 - val_out_15_acc: 0.6638 - val_out_16_acc: 0.6638 - val_out_17_acc: 0.6616 - val_out_18_acc: 0.6594 - val_out_19_acc: 0.6594 - val_out_20_acc: 0.6594 - val_out_21_acc: 0.6659 - val_out_22_acc: 0.6616 - val_out_23_acc: 0.6594 - val_out_24_acc: 0.6616 - val_out_25_acc: 0.6594 - val_out_26_acc: 0.6573 - val_out_27_acc: 0.6681 - val_out_28_acc: 0.6594 - val_out_29_acc: 0.6573 - val_out_30_acc: 0.6594 - val_out_31_acc: 0.6616 - val_out_32_acc: 0.6681\n",
      "Epoch 45/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 38.3959 - out_loss: 1.0192 - out_0_loss: 0.8067 - out_1_loss: 1.1422 - out_2_loss: 1.1424 - out_3_loss: 1.1402 - out_4_loss: 1.1302 - out_5_loss: 1.1430 - out_6_loss: 1.1362 - out_7_loss: 1.1378 - out_8_loss: 1.1439 - out_9_loss: 1.1294 - out_10_loss: 1.1292 - out_11_loss: 1.1437 - out_12_loss: 1.1537 - out_13_loss: 1.1425 - out_14_loss: 1.1368 - out_15_loss: 1.1352 - out_16_loss: 1.1477 - out_17_loss: 1.1381 - out_18_loss: 1.1474 - out_19_loss: 1.1432 - out_20_loss: 1.1416 - out_21_loss: 1.1500 - out_22_loss: 1.1396 - out_23_loss: 1.1438 - out_24_loss: 1.1397 - out_25_loss: 1.1435 - out_26_loss: 1.1523 - out_27_loss: 1.1488 - out_28_loss: 1.1336 - out_29_loss: 1.1610 - out_30_loss: 1.1601 - out_31_loss: 1.1481 - out_32_loss: 1.1450 - out_acc: 0.6379 - out_0_acc: 0.7188 - out_1_acc: 0.5893 - out_2_acc: 0.5967 - out_3_acc: 0.5843 - out_4_acc: 0.5970 - out_5_acc: 0.5849 - out_6_acc: 0.5890 - out_7_acc: 0.5939 - out_8_acc: 0.5930 - out_9_acc: 0.6007 - out_10_acc: 0.5961 - out_11_acc: 0.5983 - out_12_acc: 0.5825 - out_13_acc: 0.5874 - out_14_acc: 0.5980 - out_15_acc: 0.5986 - out_16_acc: 0.5800 - out_17_acc: 0.5880 - out_18_acc: 0.5846 - out_19_acc: 0.5852 - out_20_acc: 0.5787 - out_21_acc: 0.5896 - out_22_acc: 0.5970 - out_23_acc: 0.5908 - out_24_acc: 0.5976 - out_25_acc: 0.5825 - out_26_acc: 0.5871 - out_27_acc: 0.5769 - out_28_acc: 0.5980 - out_29_acc: 0.5849 - out_30_acc: 0.5883 - out_31_acc: 0.5815 - out_32_acc: 0.5930 - val_loss: 35.4867 - val_out_loss: 0.9889 - val_out_0_loss: 0.7678 - val_out_1_loss: 1.0105 - val_out_2_loss: 1.0076 - val_out_3_loss: 1.0029 - val_out_4_loss: 1.0119 - val_out_5_loss: 1.0049 - val_out_6_loss: 1.0081 - val_out_7_loss: 1.0073 - val_out_8_loss: 1.0113 - val_out_9_loss: 1.0107 - val_out_10_loss: 1.0140 - val_out_11_loss: 1.0120 - val_out_12_loss: 1.0119 - val_out_13_loss: 1.0140 - val_out_14_loss: 1.0148 - val_out_15_loss: 1.0143 - val_out_16_loss: 1.0105 - val_out_17_loss: 1.0051 - val_out_18_loss: 1.0041 - val_out_19_loss: 1.0129 - val_out_20_loss: 1.0124 - val_out_21_loss: 1.0060 - val_out_22_loss: 1.0111 - val_out_23_loss: 1.0059 - val_out_24_loss: 1.0121 - val_out_25_loss: 1.0142 - val_out_26_loss: 1.0084 - val_out_27_loss: 1.0084 - val_out_28_loss: 1.0133 - val_out_29_loss: 1.0117 - val_out_30_loss: 1.0088 - val_out_31_loss: 1.0104 - val_out_32_loss: 1.0138 - val_out_acc: 0.6790 - val_out_0_acc: 0.7397 - val_out_1_acc: 0.6768 - val_out_2_acc: 0.6811 - val_out_3_acc: 0.6725 - val_out_4_acc: 0.6703 - val_out_5_acc: 0.6659 - val_out_6_acc: 0.6703 - val_out_7_acc: 0.6725 - val_out_8_acc: 0.6746 - val_out_9_acc: 0.6681 - val_out_10_acc: 0.6746 - val_out_11_acc: 0.6725 - val_out_12_acc: 0.6725 - val_out_13_acc: 0.6768 - val_out_14_acc: 0.6703 - val_out_15_acc: 0.6703 - val_out_16_acc: 0.6703 - val_out_17_acc: 0.6703 - val_out_18_acc: 0.6790 - val_out_19_acc: 0.6746 - val_out_20_acc: 0.6703 - val_out_21_acc: 0.6768 - val_out_22_acc: 0.6616 - val_out_23_acc: 0.6703 - val_out_24_acc: 0.6703 - val_out_25_acc: 0.6659 - val_out_26_acc: 0.6703 - val_out_27_acc: 0.6703 - val_out_28_acc: 0.6681 - val_out_29_acc: 0.6746 - val_out_30_acc: 0.6703 - val_out_31_acc: 0.6725 - val_out_32_acc: 0.6703\n",
      "Epoch 46/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 37.3370 - out_loss: 0.9942 - out_0_loss: 0.8232 - out_1_loss: 1.1113 - out_2_loss: 1.1070 - out_3_loss: 1.1158 - out_4_loss: 1.1037 - out_5_loss: 1.1070 - out_6_loss: 1.1064 - out_7_loss: 1.1145 - out_8_loss: 1.0982 - out_9_loss: 1.0977 - out_10_loss: 1.1188 - out_11_loss: 1.1092 - out_12_loss: 1.1160 - out_13_loss: 1.1071 - out_14_loss: 1.1225 - out_15_loss: 1.1127 - out_16_loss: 1.1225 - out_17_loss: 1.1182 - out_18_loss: 1.1132 - out_19_loss: 1.1060 - out_20_loss: 1.1162 - out_21_loss: 1.0895 - out_22_loss: 1.1303 - out_23_loss: 1.1012 - out_24_loss: 1.1146 - out_25_loss: 1.1025 - out_26_loss: 1.1093 - out_27_loss: 1.1241 - out_28_loss: 1.1045 - out_29_loss: 1.0971 - out_30_loss: 1.1016 - out_31_loss: 1.1254 - out_32_loss: 1.0954 - out_acc: 0.6587 - out_0_acc: 0.7161 - out_1_acc: 0.6104 - out_2_acc: 0.6066 - out_3_acc: 0.6088 - out_4_acc: 0.6054 - out_5_acc: 0.6138 - out_6_acc: 0.6144 - out_7_acc: 0.6051 - out_8_acc: 0.6097 - out_9_acc: 0.6153 - out_10_acc: 0.6035 - out_11_acc: 0.6023 - out_12_acc: 0.6020 - out_13_acc: 0.6073 - out_14_acc: 0.5998 - out_15_acc: 0.6057 - out_16_acc: 0.6073 - out_17_acc: 0.6063 - out_18_acc: 0.6063 - out_19_acc: 0.6073 - out_20_acc: 0.5986 - out_21_acc: 0.6169 - out_22_acc: 0.6048 - out_23_acc: 0.6119 - out_24_acc: 0.5970 - out_25_acc: 0.6038 - out_26_acc: 0.6066 - out_27_acc: 0.6038 - out_28_acc: 0.6045 - out_29_acc: 0.6141 - out_30_acc: 0.6175 - out_31_acc: 0.6069 - out_32_acc: 0.6079 - val_loss: 32.4631 - val_out_loss: 0.9017 - val_out_0_loss: 0.7024 - val_out_1_loss: 0.9189 - val_out_2_loss: 0.9242 - val_out_3_loss: 0.9194 - val_out_4_loss: 0.9261 - val_out_5_loss: 0.9230 - val_out_6_loss: 0.9220 - val_out_7_loss: 0.9215 - val_out_8_loss: 0.9229 - val_out_9_loss: 0.9230 - val_out_10_loss: 0.9268 - val_out_11_loss: 0.9315 - val_out_12_loss: 0.9191 - val_out_13_loss: 0.9290 - val_out_14_loss: 0.9243 - val_out_15_loss: 0.9242 - val_out_16_loss: 0.9254 - val_out_17_loss: 0.9201 - val_out_18_loss: 0.9200 - val_out_19_loss: 0.9308 - val_out_20_loss: 0.9256 - val_out_21_loss: 0.9184 - val_out_22_loss: 0.9240 - val_out_23_loss: 0.9246 - val_out_24_loss: 0.9275 - val_out_25_loss: 0.9316 - val_out_26_loss: 0.9281 - val_out_27_loss: 0.9187 - val_out_28_loss: 0.9275 - val_out_29_loss: 0.9254 - val_out_30_loss: 0.9231 - val_out_31_loss: 0.9246 - val_out_32_loss: 0.9228 - val_out_acc: 0.6811 - val_out_0_acc: 0.7397 - val_out_1_acc: 0.6811 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6703 - val_out_4_acc: 0.6833 - val_out_5_acc: 0.6746 - val_out_6_acc: 0.6876 - val_out_7_acc: 0.6746 - val_out_8_acc: 0.6725 - val_out_9_acc: 0.6833 - val_out_10_acc: 0.6833 - val_out_11_acc: 0.6855 - val_out_12_acc: 0.6681 - val_out_13_acc: 0.6703 - val_out_14_acc: 0.6768 - val_out_15_acc: 0.6768 - val_out_16_acc: 0.6833 - val_out_17_acc: 0.6855 - val_out_18_acc: 0.6833 - val_out_19_acc: 0.6746 - val_out_20_acc: 0.6790 - val_out_21_acc: 0.6746 - val_out_22_acc: 0.6746 - val_out_23_acc: 0.6703 - val_out_24_acc: 0.6898 - val_out_25_acc: 0.6790 - val_out_26_acc: 0.6681 - val_out_27_acc: 0.6876 - val_out_28_acc: 0.6768 - val_out_29_acc: 0.6681 - val_out_30_acc: 0.6855 - val_out_31_acc: 0.6811 - val_out_32_acc: 0.6811\n",
      "Epoch 47/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 35.7940 - out_loss: 0.9498 - out_0_loss: 0.7892 - out_1_loss: 1.0550 - out_2_loss: 1.0741 - out_3_loss: 1.0529 - out_4_loss: 1.0741 - out_5_loss: 1.0470 - out_6_loss: 1.0738 - out_7_loss: 1.0822 - out_8_loss: 1.0598 - out_9_loss: 1.0619 - out_10_loss: 1.0626 - out_11_loss: 1.0634 - out_12_loss: 1.0719 - out_13_loss: 1.0712 - out_14_loss: 1.0654 - out_15_loss: 1.0669 - out_16_loss: 1.0648 - out_17_loss: 1.0755 - out_18_loss: 1.0616 - out_19_loss: 1.0718 - out_20_loss: 1.0707 - out_21_loss: 1.0734 - out_22_loss: 1.0581 - out_23_loss: 1.0566 - out_24_loss: 1.0854 - out_25_loss: 1.0566 - out_26_loss: 1.0498 - out_27_loss: 1.0517 - out_28_loss: 1.0620 - out_29_loss: 1.0588 - out_30_loss: 1.0534 - out_31_loss: 1.0521 - out_32_loss: 1.0705 - out_acc: 0.6739 - out_0_acc: 0.7247 - out_1_acc: 0.6144 - out_2_acc: 0.6197 - out_3_acc: 0.6221 - out_4_acc: 0.6097 - out_5_acc: 0.6373 - out_6_acc: 0.6184 - out_7_acc: 0.6104 - out_8_acc: 0.6262 - out_9_acc: 0.6193 - out_10_acc: 0.6271 - out_11_acc: 0.6178 - out_12_acc: 0.6237 - out_13_acc: 0.6255 - out_14_acc: 0.6175 - out_15_acc: 0.6200 - out_16_acc: 0.6184 - out_17_acc: 0.6184 - out_18_acc: 0.6054 - out_19_acc: 0.6138 - out_20_acc: 0.6181 - out_21_acc: 0.6104 - out_22_acc: 0.6200 - out_23_acc: 0.6181 - out_24_acc: 0.6162 - out_25_acc: 0.6277 - out_26_acc: 0.6283 - out_27_acc: 0.6305 - out_28_acc: 0.6317 - out_29_acc: 0.6305 - out_30_acc: 0.6246 - out_31_acc: 0.6286 - out_32_acc: 0.6184 - val_loss: 35.0717 - val_out_loss: 0.9696 - val_out_0_loss: 0.7461 - val_out_1_loss: 0.9977 - val_out_2_loss: 0.9913 - val_out_3_loss: 0.9939 - val_out_4_loss: 0.9976 - val_out_5_loss: 0.9945 - val_out_6_loss: 0.9982 - val_out_7_loss: 0.9981 - val_out_8_loss: 0.9977 - val_out_9_loss: 1.0035 - val_out_10_loss: 1.0089 - val_out_11_loss: 1.0081 - val_out_12_loss: 0.9979 - val_out_13_loss: 1.0074 - val_out_14_loss: 0.9959 - val_out_15_loss: 1.0025 - val_out_16_loss: 0.9971 - val_out_17_loss: 0.9961 - val_out_18_loss: 0.9963 - val_out_19_loss: 1.0047 - val_out_20_loss: 1.0051 - val_out_21_loss: 0.9895 - val_out_22_loss: 0.9916 - val_out_23_loss: 1.0026 - val_out_24_loss: 0.9978 - val_out_25_loss: 1.0040 - val_out_26_loss: 0.9952 - val_out_27_loss: 0.9974 - val_out_28_loss: 0.9986 - val_out_29_loss: 0.9975 - val_out_30_loss: 0.9923 - val_out_31_loss: 1.0055 - val_out_32_loss: 1.0032 - val_out_acc: 0.6659 - val_out_0_acc: 0.7570 - val_out_1_acc: 0.6508 - val_out_2_acc: 0.6573 - val_out_3_acc: 0.6573 - val_out_4_acc: 0.6508 - val_out_5_acc: 0.6551 - val_out_6_acc: 0.6551 - val_out_7_acc: 0.6551 - val_out_8_acc: 0.6573 - val_out_9_acc: 0.6551 - val_out_10_acc: 0.6508 - val_out_11_acc: 0.6529 - val_out_12_acc: 0.6464 - val_out_13_acc: 0.6464 - val_out_14_acc: 0.6486 - val_out_15_acc: 0.6508 - val_out_16_acc: 0.6529 - val_out_17_acc: 0.6551 - val_out_18_acc: 0.6464 - val_out_19_acc: 0.6529 - val_out_20_acc: 0.6529 - val_out_21_acc: 0.6638 - val_out_22_acc: 0.6659 - val_out_23_acc: 0.6486 - val_out_24_acc: 0.6616 - val_out_25_acc: 0.6616 - val_out_26_acc: 0.6508 - val_out_27_acc: 0.6508 - val_out_28_acc: 0.6529 - val_out_29_acc: 0.6529 - val_out_30_acc: 0.6486 - val_out_31_acc: 0.6443 - val_out_32_acc: 0.6443\n",
      "Epoch 48/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 36.2845 - out_loss: 0.9636 - out_0_loss: 0.7973 - out_1_loss: 1.0687 - out_2_loss: 1.0731 - out_3_loss: 1.0810 - out_4_loss: 1.0565 - out_5_loss: 1.0642 - out_6_loss: 1.0766 - out_7_loss: 1.0789 - out_8_loss: 1.0769 - out_9_loss: 1.0820 - out_10_loss: 1.0756 - out_11_loss: 1.0887 - out_12_loss: 1.0823 - out_13_loss: 1.0829 - out_14_loss: 1.0867 - out_15_loss: 1.0798 - out_16_loss: 1.0885 - out_17_loss: 1.0770 - out_18_loss: 1.0914 - out_19_loss: 1.0888 - out_20_loss: 1.0724 - out_21_loss: 1.0731 - out_22_loss: 1.0918 - out_23_loss: 1.0878 - out_24_loss: 1.0756 - out_25_loss: 1.0769 - out_26_loss: 1.0737 - out_27_loss: 1.0811 - out_28_loss: 1.0795 - out_29_loss: 1.0926 - out_30_loss: 1.0790 - out_31_loss: 1.0810 - out_32_loss: 1.0592 - out_acc: 0.6627 - out_0_acc: 0.7241 - out_1_acc: 0.6203 - out_2_acc: 0.6187 - out_3_acc: 0.6166 - out_4_acc: 0.6237 - out_5_acc: 0.6302 - out_6_acc: 0.6153 - out_7_acc: 0.6141 - out_8_acc: 0.6234 - out_9_acc: 0.6224 - out_10_acc: 0.6187 - out_11_acc: 0.6110 - out_12_acc: 0.6221 - out_13_acc: 0.6141 - out_14_acc: 0.6153 - out_15_acc: 0.6215 - out_16_acc: 0.6110 - out_17_acc: 0.6107 - out_18_acc: 0.6153 - out_19_acc: 0.6088 - out_20_acc: 0.6215 - out_21_acc: 0.6212 - out_22_acc: 0.6175 - out_23_acc: 0.6144 - out_24_acc: 0.6128 - out_25_acc: 0.6255 - out_26_acc: 0.6321 - out_27_acc: 0.6218 - out_28_acc: 0.6181 - out_29_acc: 0.6203 - out_30_acc: 0.6135 - out_31_acc: 0.6327 - out_32_acc: 0.6259 - val_loss: 34.1200 - val_out_loss: 0.9480 - val_out_0_loss: 0.7143 - val_out_1_loss: 0.9682 - val_out_2_loss: 0.9688 - val_out_3_loss: 0.9691 - val_out_4_loss: 0.9718 - val_out_5_loss: 0.9697 - val_out_6_loss: 0.9705 - val_out_7_loss: 0.9769 - val_out_8_loss: 0.9766 - val_out_9_loss: 0.9691 - val_out_10_loss: 0.9709 - val_out_11_loss: 0.9769 - val_out_12_loss: 0.9689 - val_out_13_loss: 0.9748 - val_out_14_loss: 0.9718 - val_out_15_loss: 0.9735 - val_out_16_loss: 0.9752 - val_out_17_loss: 0.9697 - val_out_18_loss: 0.9652 - val_out_19_loss: 0.9795 - val_out_20_loss: 0.9773 - val_out_21_loss: 0.9647 - val_out_22_loss: 0.9706 - val_out_23_loss: 0.9715 - val_out_24_loss: 0.9711 - val_out_25_loss: 0.9839 - val_out_26_loss: 0.9782 - val_out_27_loss: 0.9726 - val_out_28_loss: 0.9733 - val_out_29_loss: 0.9699 - val_out_30_loss: 0.9673 - val_out_31_loss: 0.9672 - val_out_32_loss: 0.9726 - val_out_acc: 0.6464 - val_out_0_acc: 0.7419 - val_out_1_acc: 0.6334 - val_out_2_acc: 0.6356 - val_out_3_acc: 0.6421 - val_out_4_acc: 0.6312 - val_out_5_acc: 0.6399 - val_out_6_acc: 0.6464 - val_out_7_acc: 0.6312 - val_out_8_acc: 0.6399 - val_out_9_acc: 0.6356 - val_out_10_acc: 0.6486 - val_out_11_acc: 0.6399 - val_out_12_acc: 0.6356 - val_out_13_acc: 0.6464 - val_out_14_acc: 0.6356 - val_out_15_acc: 0.6377 - val_out_16_acc: 0.6377 - val_out_17_acc: 0.6377 - val_out_18_acc: 0.6399 - val_out_19_acc: 0.6334 - val_out_20_acc: 0.6312 - val_out_21_acc: 0.6443 - val_out_22_acc: 0.6377 - val_out_23_acc: 0.6508 - val_out_24_acc: 0.6356 - val_out_25_acc: 0.6356 - val_out_26_acc: 0.6356 - val_out_27_acc: 0.6399 - val_out_28_acc: 0.6399 - val_out_29_acc: 0.6421 - val_out_30_acc: 0.6356 - val_out_31_acc: 0.6312 - val_out_32_acc: 0.6443\n",
      "Epoch 49/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 36.0391 - out_loss: 0.9550 - out_0_loss: 0.7879 - out_1_loss: 1.0648 - out_2_loss: 1.0673 - out_3_loss: 1.0666 - out_4_loss: 1.0567 - out_5_loss: 1.0749 - out_6_loss: 1.0751 - out_7_loss: 1.0708 - out_8_loss: 1.0677 - out_9_loss: 1.0735 - out_10_loss: 1.0693 - out_11_loss: 1.0683 - out_12_loss: 1.0659 - out_13_loss: 1.0589 - out_14_loss: 1.0718 - out_15_loss: 1.0677 - out_16_loss: 1.0823 - out_17_loss: 1.0718 - out_18_loss: 1.0949 - out_19_loss: 1.0854 - out_20_loss: 1.0935 - out_21_loss: 1.0789 - out_22_loss: 1.0614 - out_23_loss: 1.0815 - out_24_loss: 1.0728 - out_25_loss: 1.0667 - out_26_loss: 1.0684 - out_27_loss: 1.0727 - out_28_loss: 1.0674 - out_29_loss: 1.0757 - out_30_loss: 1.0716 - out_31_loss: 1.0655 - out_32_loss: 1.0665 - out_acc: 0.6637 - out_0_acc: 0.7179 - out_1_acc: 0.6122 - out_2_acc: 0.6131 - out_3_acc: 0.6159 - out_4_acc: 0.6252 - out_5_acc: 0.6135 - out_6_acc: 0.6138 - out_7_acc: 0.6166 - out_8_acc: 0.6228 - out_9_acc: 0.6181 - out_10_acc: 0.6234 - out_11_acc: 0.6131 - out_12_acc: 0.6178 - out_13_acc: 0.6240 - out_14_acc: 0.6153 - out_15_acc: 0.6147 - out_16_acc: 0.6122 - out_17_acc: 0.5992 - out_18_acc: 0.6088 - out_19_acc: 0.6141 - out_20_acc: 0.6023 - out_21_acc: 0.6094 - out_22_acc: 0.6042 - out_23_acc: 0.6175 - out_24_acc: 0.6131 - out_25_acc: 0.6268 - out_26_acc: 0.6200 - out_27_acc: 0.6066 - out_28_acc: 0.6187 - out_29_acc: 0.6141 - out_30_acc: 0.6246 - out_31_acc: 0.6079 - out_32_acc: 0.6193 - val_loss: 35.8047 - val_out_loss: 0.9921 - val_out_0_loss: 0.7636 - val_out_1_loss: 1.0136 - val_out_2_loss: 1.0168 - val_out_3_loss: 1.0192 - val_out_4_loss: 1.0227 - val_out_5_loss: 1.0183 - val_out_6_loss: 1.0171 - val_out_7_loss: 1.0264 - val_out_8_loss: 1.0185 - val_out_9_loss: 1.0144 - val_out_10_loss: 1.0205 - val_out_11_loss: 1.0220 - val_out_12_loss: 1.0184 - val_out_13_loss: 1.0178 - val_out_14_loss: 1.0181 - val_out_15_loss: 1.0223 - val_out_16_loss: 1.0214 - val_out_17_loss: 1.0186 - val_out_18_loss: 1.0143 - val_out_19_loss: 1.0296 - val_out_20_loss: 1.0253 - val_out_21_loss: 1.0109 - val_out_22_loss: 1.0228 - val_out_23_loss: 1.0205 - val_out_24_loss: 1.0099 - val_out_25_loss: 1.0291 - val_out_26_loss: 1.0290 - val_out_27_loss: 1.0258 - val_out_28_loss: 1.0271 - val_out_29_loss: 1.0117 - val_out_30_loss: 1.0140 - val_out_31_loss: 1.0210 - val_out_32_loss: 1.0148 - val_out_acc: 0.6312 - val_out_0_acc: 0.7397 - val_out_1_acc: 0.6334 - val_out_2_acc: 0.6356 - val_out_3_acc: 0.6269 - val_out_4_acc: 0.6247 - val_out_5_acc: 0.6291 - val_out_6_acc: 0.6291 - val_out_7_acc: 0.6269 - val_out_8_acc: 0.6421 - val_out_9_acc: 0.6334 - val_out_10_acc: 0.6312 - val_out_11_acc: 0.6312 - val_out_12_acc: 0.6204 - val_out_13_acc: 0.6377 - val_out_14_acc: 0.6356 - val_out_15_acc: 0.6334 - val_out_16_acc: 0.6161 - val_out_17_acc: 0.6312 - val_out_18_acc: 0.6269 - val_out_19_acc: 0.6247 - val_out_20_acc: 0.6204 - val_out_21_acc: 0.6334 - val_out_22_acc: 0.6291 - val_out_23_acc: 0.6204 - val_out_24_acc: 0.6312 - val_out_25_acc: 0.6226 - val_out_26_acc: 0.6204 - val_out_27_acc: 0.6226 - val_out_28_acc: 0.6269 - val_out_29_acc: 0.6356 - val_out_30_acc: 0.6356 - val_out_31_acc: 0.6312 - val_out_32_acc: 0.6334\n",
      "Epoch 50/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 35.6016 - out_loss: 0.9465 - out_0_loss: 0.7789 - out_1_loss: 1.0547 - out_2_loss: 1.0620 - out_3_loss: 1.0627 - out_4_loss: 1.0773 - out_5_loss: 1.0707 - out_6_loss: 1.0675 - out_7_loss: 1.0656 - out_8_loss: 1.0487 - out_9_loss: 1.0616 - out_10_loss: 1.0605 - out_11_loss: 1.0558 - out_12_loss: 1.0653 - out_13_loss: 1.0580 - out_14_loss: 1.0505 - out_15_loss: 1.0569 - out_16_loss: 1.0598 - out_17_loss: 1.0538 - out_18_loss: 1.0438 - out_19_loss: 1.0528 - out_20_loss: 1.0538 - out_21_loss: 1.0434 - out_22_loss: 1.0716 - out_23_loss: 1.0571 - out_24_loss: 1.0533 - out_25_loss: 1.0605 - out_26_loss: 1.0630 - out_27_loss: 1.0517 - out_28_loss: 1.0493 - out_29_loss: 1.0588 - out_30_loss: 1.0498 - out_31_loss: 1.0603 - out_32_loss: 1.0756 - out_acc: 0.6829 - out_0_acc: 0.7238 - out_1_acc: 0.6407 - out_2_acc: 0.6277 - out_3_acc: 0.6259 - out_4_acc: 0.6252 - out_5_acc: 0.6178 - out_6_acc: 0.6234 - out_7_acc: 0.6271 - out_8_acc: 0.6339 - out_9_acc: 0.6302 - out_10_acc: 0.6215 - out_11_acc: 0.6389 - out_12_acc: 0.6336 - out_13_acc: 0.6218 - out_14_acc: 0.6280 - out_15_acc: 0.6280 - out_16_acc: 0.6243 - out_17_acc: 0.6271 - out_18_acc: 0.6240 - out_19_acc: 0.6308 - out_20_acc: 0.6317 - out_21_acc: 0.6274 - out_22_acc: 0.6234 - out_23_acc: 0.6249 - out_24_acc: 0.6277 - out_25_acc: 0.6265 - out_26_acc: 0.6249 - out_27_acc: 0.6305 - out_28_acc: 0.6361 - out_29_acc: 0.6234 - out_30_acc: 0.6302 - out_31_acc: 0.6141 - out_32_acc: 0.6215 - val_loss: 34.7687 - val_out_loss: 0.9631 - val_out_0_loss: 0.7811 - val_out_1_loss: 0.9840 - val_out_2_loss: 0.9859 - val_out_3_loss: 0.9849 - val_out_4_loss: 0.9928 - val_out_5_loss: 0.9837 - val_out_6_loss: 0.9979 - val_out_7_loss: 0.9897 - val_out_8_loss: 0.9898 - val_out_9_loss: 0.9878 - val_out_10_loss: 0.9836 - val_out_11_loss: 0.9938 - val_out_12_loss: 0.9865 - val_out_13_loss: 0.9963 - val_out_14_loss: 0.9900 - val_out_15_loss: 0.9828 - val_out_16_loss: 0.9869 - val_out_17_loss: 0.9910 - val_out_18_loss: 0.9919 - val_out_19_loss: 0.9965 - val_out_20_loss: 0.9919 - val_out_21_loss: 0.9835 - val_out_22_loss: 0.9855 - val_out_23_loss: 0.9906 - val_out_24_loss: 0.9896 - val_out_25_loss: 0.9836 - val_out_26_loss: 0.9958 - val_out_27_loss: 0.9915 - val_out_28_loss: 0.9923 - val_out_29_loss: 0.9880 - val_out_30_loss: 0.9836 - val_out_31_loss: 0.9879 - val_out_32_loss: 0.9887 - val_out_acc: 0.6573 - val_out_0_acc: 0.7028 - val_out_1_acc: 0.6529 - val_out_2_acc: 0.6616 - val_out_3_acc: 0.6529 - val_out_4_acc: 0.6421 - val_out_5_acc: 0.6486 - val_out_6_acc: 0.6421 - val_out_7_acc: 0.6529 - val_out_8_acc: 0.6573 - val_out_9_acc: 0.6421 - val_out_10_acc: 0.6508 - val_out_11_acc: 0.6573 - val_out_12_acc: 0.6638 - val_out_13_acc: 0.6508 - val_out_14_acc: 0.6551 - val_out_15_acc: 0.6464 - val_out_16_acc: 0.6464 - val_out_17_acc: 0.6486 - val_out_18_acc: 0.6486 - val_out_19_acc: 0.6421 - val_out_20_acc: 0.6551 - val_out_21_acc: 0.6486 - val_out_22_acc: 0.6529 - val_out_23_acc: 0.6508 - val_out_24_acc: 0.6573 - val_out_25_acc: 0.6551 - val_out_26_acc: 0.6464 - val_out_27_acc: 0.6464 - val_out_28_acc: 0.6508 - val_out_29_acc: 0.6551 - val_out_30_acc: 0.6551 - val_out_31_acc: 0.6551 - val_out_32_acc: 0.6659\n",
      "Epoch 51/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 36.7034 - out_loss: 0.9740 - out_0_loss: 0.8071 - out_1_loss: 1.0947 - out_2_loss: 1.0815 - out_3_loss: 1.0938 - out_4_loss: 1.0936 - out_5_loss: 1.0871 - out_6_loss: 1.0806 - out_7_loss: 1.1065 - out_8_loss: 1.0797 - out_9_loss: 1.1011 - out_10_loss: 1.0880 - out_11_loss: 1.1048 - out_12_loss: 1.0951 - out_13_loss: 1.0839 - out_14_loss: 1.0941 - out_15_loss: 1.0811 - out_16_loss: 1.0939 - out_17_loss: 1.0892 - out_18_loss: 1.0852 - out_19_loss: 1.0915 - out_20_loss: 1.0966 - out_21_loss: 1.0961 - out_22_loss: 1.0902 - out_23_loss: 1.1082 - out_24_loss: 1.0888 - out_25_loss: 1.0998 - out_26_loss: 1.0844 - out_27_loss: 1.0806 - out_28_loss: 1.0922 - out_29_loss: 1.0863 - out_30_loss: 1.0963 - out_31_loss: 1.0851 - out_32_loss: 1.0922 - out_acc: 0.6593 - out_0_acc: 0.7099 - out_1_acc: 0.6128 - out_2_acc: 0.6206 - out_3_acc: 0.6178 - out_4_acc: 0.6156 - out_5_acc: 0.6116 - out_6_acc: 0.6131 - out_7_acc: 0.6073 - out_8_acc: 0.6156 - out_9_acc: 0.6131 - out_10_acc: 0.6172 - out_11_acc: 0.6060 - out_12_acc: 0.6138 - out_13_acc: 0.6224 - out_14_acc: 0.6150 - out_15_acc: 0.6125 - out_16_acc: 0.6156 - out_17_acc: 0.6162 - out_18_acc: 0.6131 - out_19_acc: 0.6162 - out_20_acc: 0.6181 - out_21_acc: 0.6218 - out_22_acc: 0.6141 - out_23_acc: 0.5995 - out_24_acc: 0.6193 - out_25_acc: 0.6150 - out_26_acc: 0.6094 - out_27_acc: 0.6221 - out_28_acc: 0.6153 - out_29_acc: 0.6166 - out_30_acc: 0.6156 - out_31_acc: 0.6224 - out_32_acc: 0.6203 - val_loss: 31.2909 - val_out_loss: 0.8694 - val_out_0_loss: 0.6732 - val_out_1_loss: 0.8894 - val_out_2_loss: 0.8904 - val_out_3_loss: 0.8964 - val_out_4_loss: 0.8886 - val_out_5_loss: 0.8869 - val_out_6_loss: 0.8874 - val_out_7_loss: 0.8889 - val_out_8_loss: 0.8870 - val_out_9_loss: 0.8875 - val_out_10_loss: 0.8992 - val_out_11_loss: 0.9002 - val_out_12_loss: 0.8981 - val_out_13_loss: 0.8918 - val_out_14_loss: 0.8882 - val_out_15_loss: 0.8856 - val_out_16_loss: 0.8908 - val_out_17_loss: 0.8865 - val_out_18_loss: 0.8887 - val_out_19_loss: 0.8902 - val_out_20_loss: 0.8940 - val_out_21_loss: 0.8912 - val_out_22_loss: 0.8844 - val_out_23_loss: 0.8901 - val_out_24_loss: 0.8893 - val_out_25_loss: 0.8967 - val_out_26_loss: 0.8932 - val_out_27_loss: 0.8846 - val_out_28_loss: 0.8920 - val_out_29_loss: 0.8957 - val_out_30_loss: 0.8875 - val_out_31_loss: 0.8904 - val_out_32_loss: 0.8989 - val_out_acc: 0.6638 - val_out_0_acc: 0.7744 - val_out_1_acc: 0.6594 - val_out_2_acc: 0.6616 - val_out_3_acc: 0.6573 - val_out_4_acc: 0.6681 - val_out_5_acc: 0.6551 - val_out_6_acc: 0.6659 - val_out_7_acc: 0.6486 - val_out_8_acc: 0.6529 - val_out_9_acc: 0.6681 - val_out_10_acc: 0.6551 - val_out_11_acc: 0.6573 - val_out_12_acc: 0.6594 - val_out_13_acc: 0.6616 - val_out_14_acc: 0.6594 - val_out_15_acc: 0.6725 - val_out_16_acc: 0.6638 - val_out_17_acc: 0.6594 - val_out_18_acc: 0.6659 - val_out_19_acc: 0.6594 - val_out_20_acc: 0.6486 - val_out_21_acc: 0.6594 - val_out_22_acc: 0.6551 - val_out_23_acc: 0.6638 - val_out_24_acc: 0.6616 - val_out_25_acc: 0.6573 - val_out_26_acc: 0.6594 - val_out_27_acc: 0.6659 - val_out_28_acc: 0.6638 - val_out_29_acc: 0.6616 - val_out_30_acc: 0.6594 - val_out_31_acc: 0.6551 - val_out_32_acc: 0.6616\n",
      "Epoch 52/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 34.9879 - out_loss: 0.9241 - out_0_loss: 0.7793 - out_1_loss: 1.0416 - out_2_loss: 1.0333 - out_3_loss: 1.0374 - out_4_loss: 1.0513 - out_5_loss: 1.0385 - out_6_loss: 1.0358 - out_7_loss: 1.0498 - out_8_loss: 1.0382 - out_9_loss: 1.0423 - out_10_loss: 1.0444 - out_11_loss: 1.0397 - out_12_loss: 1.0263 - out_13_loss: 1.0343 - out_14_loss: 1.0595 - out_15_loss: 1.0589 - out_16_loss: 1.0451 - out_17_loss: 1.0365 - out_18_loss: 1.0305 - out_19_loss: 1.0488 - out_20_loss: 1.0416 - out_21_loss: 1.0384 - out_22_loss: 1.0369 - out_23_loss: 1.0360 - out_24_loss: 1.0365 - out_25_loss: 1.0367 - out_26_loss: 1.0332 - out_27_loss: 1.0341 - out_28_loss: 1.0388 - out_29_loss: 1.0427 - out_30_loss: 1.0384 - out_31_loss: 1.0365 - out_32_loss: 1.0431 - out_acc: 0.6844 - out_0_acc: 0.7288 - out_1_acc: 0.6336 - out_2_acc: 0.6314 - out_3_acc: 0.6308 - out_4_acc: 0.6262 - out_5_acc: 0.6327 - out_6_acc: 0.6299 - out_7_acc: 0.6383 - out_8_acc: 0.6336 - out_9_acc: 0.6311 - out_10_acc: 0.6383 - out_11_acc: 0.6277 - out_12_acc: 0.6358 - out_13_acc: 0.6311 - out_14_acc: 0.6252 - out_15_acc: 0.6302 - out_16_acc: 0.6271 - out_17_acc: 0.6305 - out_18_acc: 0.6414 - out_19_acc: 0.6255 - out_20_acc: 0.6404 - out_21_acc: 0.6432 - out_22_acc: 0.6379 - out_23_acc: 0.6302 - out_24_acc: 0.6290 - out_25_acc: 0.6280 - out_26_acc: 0.6333 - out_27_acc: 0.6249 - out_28_acc: 0.6358 - out_29_acc: 0.6370 - out_30_acc: 0.6293 - out_31_acc: 0.6435 - out_32_acc: 0.6324 - val_loss: 34.4700 - val_out_loss: 0.9515 - val_out_0_loss: 0.8820 - val_out_1_loss: 0.9712 - val_out_2_loss: 0.9762 - val_out_3_loss: 0.9718 - val_out_4_loss: 0.9811 - val_out_5_loss: 0.9784 - val_out_6_loss: 0.9823 - val_out_7_loss: 0.9863 - val_out_8_loss: 0.9840 - val_out_9_loss: 0.9725 - val_out_10_loss: 0.9773 - val_out_11_loss: 0.9803 - val_out_12_loss: 0.9703 - val_out_13_loss: 0.9840 - val_out_14_loss: 0.9705 - val_out_15_loss: 0.9846 - val_out_16_loss: 0.9826 - val_out_17_loss: 0.9762 - val_out_18_loss: 0.9743 - val_out_19_loss: 0.9880 - val_out_20_loss: 0.9856 - val_out_21_loss: 0.9663 - val_out_22_loss: 0.9773 - val_out_23_loss: 0.9755 - val_out_24_loss: 0.9757 - val_out_25_loss: 0.9771 - val_out_26_loss: 0.9717 - val_out_27_loss: 0.9728 - val_out_28_loss: 0.9791 - val_out_29_loss: 0.9743 - val_out_30_loss: 0.9731 - val_out_31_loss: 0.9746 - val_out_32_loss: 0.9769 - val_out_acc: 0.6334 - val_out_0_acc: 0.6898 - val_out_1_acc: 0.6312 - val_out_2_acc: 0.6269 - val_out_3_acc: 0.6356 - val_out_4_acc: 0.6291 - val_out_5_acc: 0.6377 - val_out_6_acc: 0.6269 - val_out_7_acc: 0.6312 - val_out_8_acc: 0.6377 - val_out_9_acc: 0.6334 - val_out_10_acc: 0.6334 - val_out_11_acc: 0.6356 - val_out_12_acc: 0.6377 - val_out_13_acc: 0.6269 - val_out_14_acc: 0.6269 - val_out_15_acc: 0.6356 - val_out_16_acc: 0.6247 - val_out_17_acc: 0.6226 - val_out_18_acc: 0.6291 - val_out_19_acc: 0.6247 - val_out_20_acc: 0.6356 - val_out_21_acc: 0.6334 - val_out_22_acc: 0.6312 - val_out_23_acc: 0.6247 - val_out_24_acc: 0.6399 - val_out_25_acc: 0.6247 - val_out_26_acc: 0.6291 - val_out_27_acc: 0.6269 - val_out_28_acc: 0.6312 - val_out_29_acc: 0.6312 - val_out_30_acc: 0.6334 - val_out_31_acc: 0.6291 - val_out_32_acc: 0.6312\n",
      "Epoch 53/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 34.4528 - out_loss: 0.9144 - out_0_loss: 0.7441 - out_1_loss: 1.0124 - out_2_loss: 1.0290 - out_3_loss: 1.0270 - out_4_loss: 1.0362 - out_5_loss: 1.0122 - out_6_loss: 1.0115 - out_7_loss: 1.0330 - out_8_loss: 1.0255 - out_9_loss: 1.0213 - out_10_loss: 1.0291 - out_11_loss: 1.0186 - out_12_loss: 1.0157 - out_13_loss: 1.0241 - out_14_loss: 1.0236 - out_15_loss: 1.0343 - out_16_loss: 1.0323 - out_17_loss: 1.0279 - out_18_loss: 1.0207 - out_19_loss: 1.0139 - out_20_loss: 1.0374 - out_21_loss: 1.0364 - out_22_loss: 1.0343 - out_23_loss: 1.0199 - out_24_loss: 1.0263 - out_25_loss: 1.0269 - out_26_loss: 1.0231 - out_27_loss: 1.0166 - out_28_loss: 1.0343 - out_29_loss: 1.0245 - out_30_loss: 1.0294 - out_31_loss: 1.0172 - out_32_loss: 1.0196 - out_acc: 0.6745 - out_0_acc: 0.7446 - out_1_acc: 0.6376 - out_2_acc: 0.6324 - out_3_acc: 0.6249 - out_4_acc: 0.6299 - out_5_acc: 0.6454 - out_6_acc: 0.6392 - out_7_acc: 0.6339 - out_8_acc: 0.6296 - out_9_acc: 0.6345 - out_10_acc: 0.6448 - out_11_acc: 0.6324 - out_12_acc: 0.6286 - out_13_acc: 0.6376 - out_14_acc: 0.6274 - out_15_acc: 0.6367 - out_16_acc: 0.6317 - out_17_acc: 0.6398 - out_18_acc: 0.6296 - out_19_acc: 0.6404 - out_20_acc: 0.6268 - out_21_acc: 0.6286 - out_22_acc: 0.6324 - out_23_acc: 0.6342 - out_24_acc: 0.6361 - out_25_acc: 0.6339 - out_26_acc: 0.6407 - out_27_acc: 0.6308 - out_28_acc: 0.6311 - out_29_acc: 0.6370 - out_30_acc: 0.6342 - out_31_acc: 0.6339 - out_32_acc: 0.6333 - val_loss: 37.8453 - val_out_loss: 1.0355 - val_out_0_loss: 0.7469 - val_out_1_loss: 1.0792 - val_out_2_loss: 1.0776 - val_out_3_loss: 1.0795 - val_out_4_loss: 1.0705 - val_out_5_loss: 1.0816 - val_out_6_loss: 1.0725 - val_out_7_loss: 1.0809 - val_out_8_loss: 1.0672 - val_out_9_loss: 1.0777 - val_out_10_loss: 1.0866 - val_out_11_loss: 1.0821 - val_out_12_loss: 1.0869 - val_out_13_loss: 1.0929 - val_out_14_loss: 1.0796 - val_out_15_loss: 1.0750 - val_out_16_loss: 1.0705 - val_out_17_loss: 1.0763 - val_out_18_loss: 1.0862 - val_out_19_loss: 1.0841 - val_out_20_loss: 1.0880 - val_out_21_loss: 1.0776 - val_out_22_loss: 1.0744 - val_out_23_loss: 1.0794 - val_out_24_loss: 1.0786 - val_out_25_loss: 1.0749 - val_out_26_loss: 1.0849 - val_out_27_loss: 1.0821 - val_out_28_loss: 1.0801 - val_out_29_loss: 1.0867 - val_out_30_loss: 1.0733 - val_out_31_loss: 1.0864 - val_out_32_loss: 1.0916 - val_out_acc: 0.6551 - val_out_0_acc: 0.7289 - val_out_1_acc: 0.6443 - val_out_2_acc: 0.6464 - val_out_3_acc: 0.6464 - val_out_4_acc: 0.6464 - val_out_5_acc: 0.6573 - val_out_6_acc: 0.6573 - val_out_7_acc: 0.6508 - val_out_8_acc: 0.6573 - val_out_9_acc: 0.6464 - val_out_10_acc: 0.6551 - val_out_11_acc: 0.6486 - val_out_12_acc: 0.6464 - val_out_13_acc: 0.6486 - val_out_14_acc: 0.6594 - val_out_15_acc: 0.6486 - val_out_16_acc: 0.6464 - val_out_17_acc: 0.6529 - val_out_18_acc: 0.6464 - val_out_19_acc: 0.6486 - val_out_20_acc: 0.6508 - val_out_21_acc: 0.6551 - val_out_22_acc: 0.6464 - val_out_23_acc: 0.6529 - val_out_24_acc: 0.6486 - val_out_25_acc: 0.6486 - val_out_26_acc: 0.6464 - val_out_27_acc: 0.6486 - val_out_28_acc: 0.6508 - val_out_29_acc: 0.6443 - val_out_30_acc: 0.6508 - val_out_31_acc: 0.6464 - val_out_32_acc: 0.6616\n",
      "Epoch 54/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 35.4518 - out_loss: 0.9371 - out_0_loss: 0.7572 - out_1_loss: 1.0433 - out_2_loss: 1.0582 - out_3_loss: 1.0579 - out_4_loss: 1.0536 - out_5_loss: 1.0461 - out_6_loss: 1.0624 - out_7_loss: 1.0691 - out_8_loss: 1.0645 - out_9_loss: 1.0688 - out_10_loss: 1.0359 - out_11_loss: 1.0435 - out_12_loss: 1.0649 - out_13_loss: 1.0563 - out_14_loss: 1.0684 - out_15_loss: 1.0563 - out_16_loss: 1.0368 - out_17_loss: 1.0578 - out_18_loss: 1.0629 - out_19_loss: 1.0383 - out_20_loss: 1.0501 - out_21_loss: 1.0451 - out_22_loss: 1.0621 - out_23_loss: 1.0512 - out_24_loss: 1.0556 - out_25_loss: 1.0621 - out_26_loss: 1.0437 - out_27_loss: 1.0555 - out_28_loss: 1.0552 - out_29_loss: 1.0648 - out_30_loss: 1.0660 - out_31_loss: 1.0461 - out_32_loss: 1.0550 - out_acc: 0.6751 - out_0_acc: 0.7371 - out_1_acc: 0.6398 - out_2_acc: 0.6221 - out_3_acc: 0.6302 - out_4_acc: 0.6265 - out_5_acc: 0.6293 - out_6_acc: 0.6302 - out_7_acc: 0.6228 - out_8_acc: 0.6259 - out_9_acc: 0.6246 - out_10_acc: 0.6321 - out_11_acc: 0.6305 - out_12_acc: 0.6231 - out_13_acc: 0.6274 - out_14_acc: 0.6249 - out_15_acc: 0.6352 - out_16_acc: 0.6283 - out_17_acc: 0.6218 - out_18_acc: 0.6234 - out_19_acc: 0.6389 - out_20_acc: 0.6265 - out_21_acc: 0.6321 - out_22_acc: 0.6150 - out_23_acc: 0.6252 - out_24_acc: 0.6243 - out_25_acc: 0.6255 - out_26_acc: 0.6330 - out_27_acc: 0.6224 - out_28_acc: 0.6249 - out_29_acc: 0.6317 - out_30_acc: 0.6274 - out_31_acc: 0.6293 - out_32_acc: 0.6255 - val_loss: 33.8257 - val_out_loss: 0.9302 - val_out_0_loss: 0.7024 - val_out_1_loss: 0.9604 - val_out_2_loss: 0.9637 - val_out_3_loss: 0.9670 - val_out_4_loss: 0.9618 - val_out_5_loss: 0.9641 - val_out_6_loss: 0.9660 - val_out_7_loss: 0.9612 - val_out_8_loss: 0.9627 - val_out_9_loss: 0.9638 - val_out_10_loss: 0.9594 - val_out_11_loss: 0.9693 - val_out_12_loss: 0.9589 - val_out_13_loss: 0.9611 - val_out_14_loss: 0.9690 - val_out_15_loss: 0.9627 - val_out_16_loss: 0.9624 - val_out_17_loss: 0.9628 - val_out_18_loss: 0.9638 - val_out_19_loss: 0.9750 - val_out_20_loss: 0.9672 - val_out_21_loss: 0.9600 - val_out_22_loss: 0.9633 - val_out_23_loss: 0.9673 - val_out_24_loss: 0.9648 - val_out_25_loss: 0.9681 - val_out_26_loss: 0.9669 - val_out_27_loss: 0.9597 - val_out_28_loss: 0.9626 - val_out_29_loss: 0.9693 - val_out_30_loss: 0.9621 - val_out_31_loss: 0.9620 - val_out_32_loss: 0.9655 - val_out_acc: 0.6746 - val_out_0_acc: 0.7440 - val_out_1_acc: 0.6703 - val_out_2_acc: 0.6659 - val_out_3_acc: 0.6681 - val_out_4_acc: 0.6746 - val_out_5_acc: 0.6638 - val_out_6_acc: 0.6768 - val_out_7_acc: 0.6681 - val_out_8_acc: 0.6768 - val_out_9_acc: 0.6659 - val_out_10_acc: 0.6725 - val_out_11_acc: 0.6703 - val_out_12_acc: 0.6725 - val_out_13_acc: 0.6768 - val_out_14_acc: 0.6725 - val_out_15_acc: 0.6725 - val_out_16_acc: 0.6638 - val_out_17_acc: 0.6725 - val_out_18_acc: 0.6725 - val_out_19_acc: 0.6616 - val_out_20_acc: 0.6659 - val_out_21_acc: 0.6725 - val_out_22_acc: 0.6681 - val_out_23_acc: 0.6703 - val_out_24_acc: 0.6746 - val_out_25_acc: 0.6659 - val_out_26_acc: 0.6681 - val_out_27_acc: 0.6681 - val_out_28_acc: 0.6703 - val_out_29_acc: 0.6681 - val_out_30_acc: 0.6703 - val_out_31_acc: 0.6681 - val_out_32_acc: 0.6703\n",
      "Epoch 55/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 33.9992 - out_loss: 0.8987 - out_0_loss: 0.7541 - out_1_loss: 1.0176 - out_2_loss: 1.0225 - out_3_loss: 0.9996 - out_4_loss: 1.0267 - out_5_loss: 1.0093 - out_6_loss: 1.0218 - out_7_loss: 1.0141 - out_8_loss: 1.0072 - out_9_loss: 0.9964 - out_10_loss: 1.0129 - out_11_loss: 1.0177 - out_12_loss: 0.9999 - out_13_loss: 1.0099 - out_14_loss: 1.0045 - out_15_loss: 1.0093 - out_16_loss: 0.9989 - out_17_loss: 1.0146 - out_18_loss: 1.0238 - out_19_loss: 0.9947 - out_20_loss: 1.0227 - out_21_loss: 0.9985 - out_22_loss: 1.0197 - out_23_loss: 1.0120 - out_24_loss: 1.0178 - out_25_loss: 1.0178 - out_26_loss: 1.0088 - out_27_loss: 1.0173 - out_28_loss: 1.0009 - out_29_loss: 1.0149 - out_30_loss: 1.0174 - out_31_loss: 0.9934 - out_32_loss: 1.0036 - out_acc: 0.6804 - out_0_acc: 0.7260 - out_1_acc: 0.6271 - out_2_acc: 0.6348 - out_3_acc: 0.6398 - out_4_acc: 0.6392 - out_5_acc: 0.6429 - out_6_acc: 0.6293 - out_7_acc: 0.6308 - out_8_acc: 0.6395 - out_9_acc: 0.6383 - out_10_acc: 0.6352 - out_11_acc: 0.6274 - out_12_acc: 0.6485 - out_13_acc: 0.6392 - out_14_acc: 0.6355 - out_15_acc: 0.6392 - out_16_acc: 0.6485 - out_17_acc: 0.6345 - out_18_acc: 0.6330 - out_19_acc: 0.6410 - out_20_acc: 0.6383 - out_21_acc: 0.6420 - out_22_acc: 0.6308 - out_23_acc: 0.6358 - out_24_acc: 0.6352 - out_25_acc: 0.6367 - out_26_acc: 0.6345 - out_27_acc: 0.6345 - out_28_acc: 0.6469 - out_29_acc: 0.6364 - out_30_acc: 0.6389 - out_31_acc: 0.6491 - out_32_acc: 0.6352 - val_loss: 33.3700 - val_out_loss: 0.9217 - val_out_0_loss: 0.7213 - val_out_1_loss: 0.9482 - val_out_2_loss: 0.9508 - val_out_3_loss: 0.9461 - val_out_4_loss: 0.9543 - val_out_5_loss: 0.9469 - val_out_6_loss: 0.9466 - val_out_7_loss: 0.9536 - val_out_8_loss: 0.9525 - val_out_9_loss: 0.9422 - val_out_10_loss: 0.9510 - val_out_11_loss: 0.9523 - val_out_12_loss: 0.9499 - val_out_13_loss: 0.9522 - val_out_14_loss: 0.9514 - val_out_15_loss: 0.9517 - val_out_16_loss: 0.9468 - val_out_17_loss: 0.9462 - val_out_18_loss: 0.9487 - val_out_19_loss: 0.9548 - val_out_20_loss: 0.9525 - val_out_21_loss: 0.9523 - val_out_22_loss: 0.9545 - val_out_23_loss: 0.9454 - val_out_24_loss: 0.9536 - val_out_25_loss: 0.9507 - val_out_26_loss: 0.9550 - val_out_27_loss: 0.9444 - val_out_28_loss: 0.9532 - val_out_29_loss: 0.9470 - val_out_30_loss: 0.9516 - val_out_31_loss: 0.9526 - val_out_32_loss: 0.9471 - val_out_acc: 0.6594 - val_out_0_acc: 0.7289 - val_out_1_acc: 0.6508 - val_out_2_acc: 0.6594 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6616 - val_out_5_acc: 0.6573 - val_out_6_acc: 0.6659 - val_out_7_acc: 0.6508 - val_out_8_acc: 0.6573 - val_out_9_acc: 0.6486 - val_out_10_acc: 0.6551 - val_out_11_acc: 0.6573 - val_out_12_acc: 0.6464 - val_out_13_acc: 0.6508 - val_out_14_acc: 0.6529 - val_out_15_acc: 0.6508 - val_out_16_acc: 0.6529 - val_out_17_acc: 0.6573 - val_out_18_acc: 0.6551 - val_out_19_acc: 0.6594 - val_out_20_acc: 0.6594 - val_out_21_acc: 0.6573 - val_out_22_acc: 0.6508 - val_out_23_acc: 0.6638 - val_out_24_acc: 0.6529 - val_out_25_acc: 0.6659 - val_out_26_acc: 0.6508 - val_out_27_acc: 0.6573 - val_out_28_acc: 0.6594 - val_out_29_acc: 0.6551 - val_out_30_acc: 0.6529 - val_out_31_acc: 0.6573 - val_out_32_acc: 0.6638\n",
      "Epoch 56/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 34.7222 - out_loss: 0.9212 - out_0_loss: 0.7486 - out_1_loss: 1.0435 - out_2_loss: 1.0232 - out_3_loss: 1.0343 - out_4_loss: 1.0328 - out_5_loss: 1.0315 - out_6_loss: 1.0335 - out_7_loss: 1.0434 - out_8_loss: 1.0348 - out_9_loss: 1.0305 - out_10_loss: 1.0308 - out_11_loss: 1.0437 - out_12_loss: 1.0245 - out_13_loss: 1.0283 - out_14_loss: 1.0229 - out_15_loss: 1.0461 - out_16_loss: 1.0289 - out_17_loss: 1.0433 - out_18_loss: 1.0255 - out_19_loss: 1.0356 - out_20_loss: 1.0277 - out_21_loss: 1.0359 - out_22_loss: 1.0278 - out_23_loss: 1.0284 - out_24_loss: 1.0378 - out_25_loss: 1.0450 - out_26_loss: 1.0240 - out_27_loss: 1.0249 - out_28_loss: 1.0252 - out_29_loss: 1.0304 - out_30_loss: 1.0326 - out_31_loss: 1.0367 - out_32_loss: 1.0390 - out_acc: 0.6801 - out_0_acc: 0.7353 - out_1_acc: 0.6296 - out_2_acc: 0.6324 - out_3_acc: 0.6283 - out_4_acc: 0.6286 - out_5_acc: 0.6345 - out_6_acc: 0.6373 - out_7_acc: 0.6209 - out_8_acc: 0.6286 - out_9_acc: 0.6293 - out_10_acc: 0.6342 - out_11_acc: 0.6252 - out_12_acc: 0.6296 - out_13_acc: 0.6401 - out_14_acc: 0.6302 - out_15_acc: 0.6246 - out_16_acc: 0.6358 - out_17_acc: 0.6308 - out_18_acc: 0.6398 - out_19_acc: 0.6339 - out_20_acc: 0.6302 - out_21_acc: 0.6268 - out_22_acc: 0.6283 - out_23_acc: 0.6317 - out_24_acc: 0.6283 - out_25_acc: 0.6166 - out_26_acc: 0.6358 - out_27_acc: 0.6348 - out_28_acc: 0.6345 - out_29_acc: 0.6274 - out_30_acc: 0.6361 - out_31_acc: 0.6221 - out_32_acc: 0.6317 - val_loss: 30.6907 - val_out_loss: 0.8509 - val_out_0_loss: 0.6695 - val_out_1_loss: 0.8729 - val_out_2_loss: 0.8757 - val_out_3_loss: 0.8734 - val_out_4_loss: 0.8736 - val_out_5_loss: 0.8754 - val_out_6_loss: 0.8726 - val_out_7_loss: 0.8759 - val_out_8_loss: 0.8684 - val_out_9_loss: 0.8705 - val_out_10_loss: 0.8736 - val_out_11_loss: 0.8750 - val_out_12_loss: 0.8732 - val_out_13_loss: 0.8774 - val_out_14_loss: 0.8788 - val_out_15_loss: 0.8709 - val_out_16_loss: 0.8682 - val_out_17_loss: 0.8760 - val_out_18_loss: 0.8717 - val_out_19_loss: 0.8732 - val_out_20_loss: 0.8768 - val_out_21_loss: 0.8712 - val_out_22_loss: 0.8750 - val_out_23_loss: 0.8706 - val_out_24_loss: 0.8748 - val_out_25_loss: 0.8757 - val_out_26_loss: 0.8729 - val_out_27_loss: 0.8730 - val_out_28_loss: 0.8755 - val_out_29_loss: 0.8777 - val_out_30_loss: 0.8736 - val_out_31_loss: 0.8684 - val_out_32_loss: 0.8739 - val_out_acc: 0.7007 - val_out_0_acc: 0.7592 - val_out_1_acc: 0.6876 - val_out_2_acc: 0.6876 - val_out_3_acc: 0.6963 - val_out_4_acc: 0.6855 - val_out_5_acc: 0.6963 - val_out_6_acc: 0.6941 - val_out_7_acc: 0.6941 - val_out_8_acc: 0.6855 - val_out_9_acc: 0.6920 - val_out_10_acc: 0.7007 - val_out_11_acc: 0.6833 - val_out_12_acc: 0.6920 - val_out_13_acc: 0.6941 - val_out_14_acc: 0.6833 - val_out_15_acc: 0.6833 - val_out_16_acc: 0.6898 - val_out_17_acc: 0.6876 - val_out_18_acc: 0.6898 - val_out_19_acc: 0.6855 - val_out_20_acc: 0.6920 - val_out_21_acc: 0.6876 - val_out_22_acc: 0.6898 - val_out_23_acc: 0.6876 - val_out_24_acc: 0.6876 - val_out_25_acc: 0.6855 - val_out_26_acc: 0.6855 - val_out_27_acc: 0.6833 - val_out_28_acc: 0.6790 - val_out_29_acc: 0.6941 - val_out_30_acc: 0.7050 - val_out_31_acc: 0.6876 - val_out_32_acc: 0.6985\n",
      "Epoch 57/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 34.6650 - out_loss: 0.9175 - out_0_loss: 0.7260 - out_1_loss: 1.0315 - out_2_loss: 1.0346 - out_3_loss: 1.0343 - out_4_loss: 1.0324 - out_5_loss: 1.0293 - out_6_loss: 1.0219 - out_7_loss: 1.0281 - out_8_loss: 1.0399 - out_9_loss: 1.0353 - out_10_loss: 1.0332 - out_11_loss: 1.0533 - out_12_loss: 1.0400 - out_13_loss: 1.0239 - out_14_loss: 1.0378 - out_15_loss: 1.0285 - out_16_loss: 1.0372 - out_17_loss: 1.0354 - out_18_loss: 1.0334 - out_19_loss: 1.0300 - out_20_loss: 1.0337 - out_21_loss: 1.0365 - out_22_loss: 1.0181 - out_23_loss: 1.0284 - out_24_loss: 1.0088 - out_25_loss: 1.0371 - out_26_loss: 1.0379 - out_27_loss: 1.0279 - out_28_loss: 1.0507 - out_29_loss: 1.0244 - out_30_loss: 1.0191 - out_31_loss: 1.0374 - out_32_loss: 1.0212 - out_acc: 0.6754 - out_0_acc: 0.7353 - out_1_acc: 0.6339 - out_2_acc: 0.6389 - out_3_acc: 0.6302 - out_4_acc: 0.6373 - out_5_acc: 0.6370 - out_6_acc: 0.6420 - out_7_acc: 0.6262 - out_8_acc: 0.6358 - out_9_acc: 0.6305 - out_10_acc: 0.6296 - out_11_acc: 0.6358 - out_12_acc: 0.6271 - out_13_acc: 0.6336 - out_14_acc: 0.6336 - out_15_acc: 0.6321 - out_16_acc: 0.6274 - out_17_acc: 0.6364 - out_18_acc: 0.6379 - out_19_acc: 0.6308 - out_20_acc: 0.6311 - out_21_acc: 0.6358 - out_22_acc: 0.6414 - out_23_acc: 0.6426 - out_24_acc: 0.6420 - out_25_acc: 0.6255 - out_26_acc: 0.6296 - out_27_acc: 0.6383 - out_28_acc: 0.6293 - out_29_acc: 0.6410 - out_30_acc: 0.6358 - out_31_acc: 0.6293 - out_32_acc: 0.6417 - val_loss: 30.7439 - val_out_loss: 0.8558 - val_out_0_loss: 0.6797 - val_out_1_loss: 0.8723 - val_out_2_loss: 0.8725 - val_out_3_loss: 0.8749 - val_out_4_loss: 0.8735 - val_out_5_loss: 0.8710 - val_out_6_loss: 0.8710 - val_out_7_loss: 0.8783 - val_out_8_loss: 0.8735 - val_out_9_loss: 0.8721 - val_out_10_loss: 0.8797 - val_out_11_loss: 0.8807 - val_out_12_loss: 0.8765 - val_out_13_loss: 0.8772 - val_out_14_loss: 0.8784 - val_out_15_loss: 0.8704 - val_out_16_loss: 0.8732 - val_out_17_loss: 0.8747 - val_out_18_loss: 0.8737 - val_out_19_loss: 0.8753 - val_out_20_loss: 0.8788 - val_out_21_loss: 0.8725 - val_out_22_loss: 0.8735 - val_out_23_loss: 0.8741 - val_out_24_loss: 0.8721 - val_out_25_loss: 0.8807 - val_out_26_loss: 0.8768 - val_out_27_loss: 0.8710 - val_out_28_loss: 0.8765 - val_out_29_loss: 0.8752 - val_out_30_loss: 0.8736 - val_out_31_loss: 0.8736 - val_out_32_loss: 0.8741 - val_out_acc: 0.7007 - val_out_0_acc: 0.7310 - val_out_1_acc: 0.6920 - val_out_2_acc: 0.6855 - val_out_3_acc: 0.6985 - val_out_4_acc: 0.6941 - val_out_5_acc: 0.7028 - val_out_6_acc: 0.6855 - val_out_7_acc: 0.6920 - val_out_8_acc: 0.6876 - val_out_9_acc: 0.6920 - val_out_10_acc: 0.6855 - val_out_11_acc: 0.6876 - val_out_12_acc: 0.6963 - val_out_13_acc: 0.6855 - val_out_14_acc: 0.6855 - val_out_15_acc: 0.6898 - val_out_16_acc: 0.6920 - val_out_17_acc: 0.7028 - val_out_18_acc: 0.6876 - val_out_19_acc: 0.7007 - val_out_20_acc: 0.6855 - val_out_21_acc: 0.6920 - val_out_22_acc: 0.7007 - val_out_23_acc: 0.6985 - val_out_24_acc: 0.6941 - val_out_25_acc: 0.6833 - val_out_26_acc: 0.7050 - val_out_27_acc: 0.6811 - val_out_28_acc: 0.6963 - val_out_29_acc: 0.7050 - val_out_30_acc: 0.6898 - val_out_31_acc: 0.6898 - val_out_32_acc: 0.6876\n",
      "Epoch 58/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 34.0346 - out_loss: 0.8984 - out_0_loss: 0.7275 - out_1_loss: 1.0062 - out_2_loss: 1.0206 - out_3_loss: 1.0258 - out_4_loss: 1.0271 - out_5_loss: 1.0129 - out_6_loss: 1.0125 - out_7_loss: 1.0224 - out_8_loss: 1.0010 - out_9_loss: 1.0068 - out_10_loss: 1.0183 - out_11_loss: 1.0140 - out_12_loss: 1.0171 - out_13_loss: 1.0175 - out_14_loss: 1.0111 - out_15_loss: 1.0110 - out_16_loss: 1.0099 - out_17_loss: 0.9972 - out_18_loss: 1.0158 - out_19_loss: 1.0234 - out_20_loss: 1.0072 - out_21_loss: 1.0108 - out_22_loss: 1.0095 - out_23_loss: 0.9968 - out_24_loss: 1.0157 - out_25_loss: 1.0246 - out_26_loss: 1.0164 - out_27_loss: 1.0046 - out_28_loss: 1.0100 - out_29_loss: 1.0023 - out_30_loss: 1.0046 - out_31_loss: 1.0191 - out_32_loss: 1.0165 - out_acc: 0.6903 - out_0_acc: 0.7384 - out_1_acc: 0.6420 - out_2_acc: 0.6324 - out_3_acc: 0.6407 - out_4_acc: 0.6327 - out_5_acc: 0.6376 - out_6_acc: 0.6364 - out_7_acc: 0.6317 - out_8_acc: 0.6457 - out_9_acc: 0.6472 - out_10_acc: 0.6435 - out_11_acc: 0.6429 - out_12_acc: 0.6389 - out_13_acc: 0.6454 - out_14_acc: 0.6479 - out_15_acc: 0.6392 - out_16_acc: 0.6373 - out_17_acc: 0.6445 - out_18_acc: 0.6463 - out_19_acc: 0.6342 - out_20_acc: 0.6370 - out_21_acc: 0.6370 - out_22_acc: 0.6522 - out_23_acc: 0.6494 - out_24_acc: 0.6361 - out_25_acc: 0.6398 - out_26_acc: 0.6423 - out_27_acc: 0.6451 - out_28_acc: 0.6463 - out_29_acc: 0.6503 - out_30_acc: 0.6386 - out_31_acc: 0.6355 - out_32_acc: 0.6441 - val_loss: 32.8815 - val_out_loss: 0.9151 - val_out_0_loss: 0.7569 - val_out_1_loss: 0.9316 - val_out_2_loss: 0.9331 - val_out_3_loss: 0.9340 - val_out_4_loss: 0.9396 - val_out_5_loss: 0.9293 - val_out_6_loss: 0.9383 - val_out_7_loss: 0.9319 - val_out_8_loss: 0.9274 - val_out_9_loss: 0.9295 - val_out_10_loss: 0.9371 - val_out_11_loss: 0.9360 - val_out_12_loss: 0.9345 - val_out_13_loss: 0.9397 - val_out_14_loss: 0.9492 - val_out_15_loss: 0.9337 - val_out_16_loss: 0.9296 - val_out_17_loss: 0.9357 - val_out_18_loss: 0.9301 - val_out_19_loss: 0.9361 - val_out_20_loss: 0.9387 - val_out_21_loss: 0.9321 - val_out_22_loss: 0.9323 - val_out_23_loss: 0.9382 - val_out_24_loss: 0.9381 - val_out_25_loss: 0.9371 - val_out_26_loss: 0.9355 - val_out_27_loss: 0.9385 - val_out_28_loss: 0.9351 - val_out_29_loss: 0.9270 - val_out_30_loss: 0.9341 - val_out_31_loss: 0.9360 - val_out_32_loss: 0.9290 - val_out_acc: 0.6659 - val_out_0_acc: 0.7397 - val_out_1_acc: 0.6616 - val_out_2_acc: 0.6616 - val_out_3_acc: 0.6594 - val_out_4_acc: 0.6681 - val_out_5_acc: 0.6659 - val_out_6_acc: 0.6594 - val_out_7_acc: 0.6703 - val_out_8_acc: 0.6638 - val_out_9_acc: 0.6616 - val_out_10_acc: 0.6638 - val_out_11_acc: 0.6681 - val_out_12_acc: 0.6616 - val_out_13_acc: 0.6594 - val_out_14_acc: 0.6594 - val_out_15_acc: 0.6746 - val_out_16_acc: 0.6616 - val_out_17_acc: 0.6638 - val_out_18_acc: 0.6703 - val_out_19_acc: 0.6616 - val_out_20_acc: 0.6551 - val_out_21_acc: 0.6638 - val_out_22_acc: 0.6725 - val_out_23_acc: 0.6529 - val_out_24_acc: 0.6594 - val_out_25_acc: 0.6529 - val_out_26_acc: 0.6659 - val_out_27_acc: 0.6638 - val_out_28_acc: 0.6638 - val_out_29_acc: 0.6638 - val_out_30_acc: 0.6638 - val_out_31_acc: 0.6638 - val_out_32_acc: 0.6638\n",
      "Epoch 59/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 34.9500 - out_loss: 0.9267 - out_0_loss: 0.7241 - out_1_loss: 1.0406 - out_2_loss: 1.0363 - out_3_loss: 1.0440 - out_4_loss: 1.0258 - out_5_loss: 1.0412 - out_6_loss: 1.0358 - out_7_loss: 1.0573 - out_8_loss: 1.0434 - out_9_loss: 1.0481 - out_10_loss: 1.0279 - out_11_loss: 1.0366 - out_12_loss: 1.0327 - out_13_loss: 1.0398 - out_14_loss: 1.0472 - out_15_loss: 1.0398 - out_16_loss: 1.0551 - out_17_loss: 1.0418 - out_18_loss: 1.0443 - out_19_loss: 1.0319 - out_20_loss: 1.0380 - out_21_loss: 1.0296 - out_22_loss: 1.0494 - out_23_loss: 1.0341 - out_24_loss: 1.0501 - out_25_loss: 1.0378 - out_26_loss: 1.0505 - out_27_loss: 1.0445 - out_28_loss: 1.0484 - out_29_loss: 1.0407 - out_30_loss: 1.0428 - out_31_loss: 1.0301 - out_32_loss: 1.0340 - out_acc: 0.6776 - out_0_acc: 0.7458 - out_1_acc: 0.6324 - out_2_acc: 0.6314 - out_3_acc: 0.6308 - out_4_acc: 0.6333 - out_5_acc: 0.6296 - out_6_acc: 0.6314 - out_7_acc: 0.6302 - out_8_acc: 0.6311 - out_9_acc: 0.6259 - out_10_acc: 0.6218 - out_11_acc: 0.6345 - out_12_acc: 0.6336 - out_13_acc: 0.6283 - out_14_acc: 0.6296 - out_15_acc: 0.6252 - out_16_acc: 0.6327 - out_17_acc: 0.6379 - out_18_acc: 0.6246 - out_19_acc: 0.6420 - out_20_acc: 0.6333 - out_21_acc: 0.6336 - out_22_acc: 0.6327 - out_23_acc: 0.6414 - out_24_acc: 0.6317 - out_25_acc: 0.6305 - out_26_acc: 0.6283 - out_27_acc: 0.6252 - out_28_acc: 0.6345 - out_29_acc: 0.6286 - out_30_acc: 0.6265 - out_31_acc: 0.6299 - out_32_acc: 0.6336 - val_loss: 29.2204 - val_out_loss: 0.8102 - val_out_0_loss: 0.6309 - val_out_1_loss: 0.8351 - val_out_2_loss: 0.8285 - val_out_3_loss: 0.8353 - val_out_4_loss: 0.8322 - val_out_5_loss: 0.8270 - val_out_6_loss: 0.8326 - val_out_7_loss: 0.8295 - val_out_8_loss: 0.8273 - val_out_9_loss: 0.8251 - val_out_10_loss: 0.8354 - val_out_11_loss: 0.8338 - val_out_12_loss: 0.8311 - val_out_13_loss: 0.8301 - val_out_14_loss: 0.8339 - val_out_15_loss: 0.8290 - val_out_16_loss: 0.8292 - val_out_17_loss: 0.8292 - val_out_18_loss: 0.8315 - val_out_19_loss: 0.8406 - val_out_20_loss: 0.8343 - val_out_21_loss: 0.8319 - val_out_22_loss: 0.8285 - val_out_23_loss: 0.8313 - val_out_24_loss: 0.8316 - val_out_25_loss: 0.8400 - val_out_26_loss: 0.8365 - val_out_27_loss: 0.8280 - val_out_28_loss: 0.8344 - val_out_29_loss: 0.8353 - val_out_30_loss: 0.8270 - val_out_31_loss: 0.8323 - val_out_32_loss: 0.8354 - val_out_acc: 0.7093 - val_out_0_acc: 0.7766 - val_out_1_acc: 0.7115 - val_out_2_acc: 0.7050 - val_out_3_acc: 0.7072 - val_out_4_acc: 0.7072 - val_out_5_acc: 0.7007 - val_out_6_acc: 0.7050 - val_out_7_acc: 0.7072 - val_out_8_acc: 0.7007 - val_out_9_acc: 0.7072 - val_out_10_acc: 0.7028 - val_out_11_acc: 0.7028 - val_out_12_acc: 0.7072 - val_out_13_acc: 0.7028 - val_out_14_acc: 0.6985 - val_out_15_acc: 0.7072 - val_out_16_acc: 0.7072 - val_out_17_acc: 0.6898 - val_out_18_acc: 0.7007 - val_out_19_acc: 0.6963 - val_out_20_acc: 0.7115 - val_out_21_acc: 0.7028 - val_out_22_acc: 0.6985 - val_out_23_acc: 0.7072 - val_out_24_acc: 0.6985 - val_out_25_acc: 0.7007 - val_out_26_acc: 0.7007 - val_out_27_acc: 0.7072 - val_out_28_acc: 0.7050 - val_out_29_acc: 0.7050 - val_out_30_acc: 0.7007 - val_out_31_acc: 0.7115 - val_out_32_acc: 0.7028\n",
      "Epoch 60/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 33.4035 - out_loss: 0.8838 - out_0_loss: 0.7055 - out_1_loss: 0.9834 - out_2_loss: 0.9958 - out_3_loss: 0.9891 - out_4_loss: 0.9906 - out_5_loss: 1.0095 - out_6_loss: 0.9764 - out_7_loss: 0.9975 - out_8_loss: 1.0024 - out_9_loss: 0.9855 - out_10_loss: 1.0022 - out_11_loss: 0.9889 - out_12_loss: 0.9952 - out_13_loss: 1.0003 - out_14_loss: 1.0058 - out_15_loss: 0.9834 - out_16_loss: 1.0046 - out_17_loss: 0.9926 - out_18_loss: 0.9866 - out_19_loss: 1.0064 - out_20_loss: 1.0045 - out_21_loss: 1.0005 - out_22_loss: 0.9919 - out_23_loss: 1.0025 - out_24_loss: 0.9980 - out_25_loss: 0.9979 - out_26_loss: 0.9876 - out_27_loss: 0.9872 - out_28_loss: 0.9954 - out_29_loss: 0.9831 - out_30_loss: 0.9841 - out_31_loss: 0.9874 - out_32_loss: 0.9978 - out_acc: 0.6882 - out_0_acc: 0.7508 - out_1_acc: 0.6479 - out_2_acc: 0.6410 - out_3_acc: 0.6401 - out_4_acc: 0.6457 - out_5_acc: 0.6491 - out_6_acc: 0.6556 - out_7_acc: 0.6414 - out_8_acc: 0.6414 - out_9_acc: 0.6466 - out_10_acc: 0.6401 - out_11_acc: 0.6550 - out_12_acc: 0.6435 - out_13_acc: 0.6404 - out_14_acc: 0.6386 - out_15_acc: 0.6476 - out_16_acc: 0.6339 - out_17_acc: 0.6432 - out_18_acc: 0.6494 - out_19_acc: 0.6420 - out_20_acc: 0.6414 - out_21_acc: 0.6429 - out_22_acc: 0.6401 - out_23_acc: 0.6460 - out_24_acc: 0.6466 - out_25_acc: 0.6445 - out_26_acc: 0.6466 - out_27_acc: 0.6525 - out_28_acc: 0.6429 - out_29_acc: 0.6513 - out_30_acc: 0.6491 - out_31_acc: 0.6482 - out_32_acc: 0.6389 - val_loss: 32.9640 - val_out_loss: 0.9132 - val_out_0_loss: 0.7046 - val_out_1_loss: 0.9340 - val_out_2_loss: 0.9382 - val_out_3_loss: 0.9406 - val_out_4_loss: 0.9362 - val_out_5_loss: 0.9401 - val_out_6_loss: 0.9381 - val_out_7_loss: 0.9383 - val_out_8_loss: 0.9412 - val_out_9_loss: 0.9356 - val_out_10_loss: 0.9424 - val_out_11_loss: 0.9457 - val_out_12_loss: 0.9365 - val_out_13_loss: 0.9412 - val_out_14_loss: 0.9379 - val_out_15_loss: 0.9398 - val_out_16_loss: 0.9375 - val_out_17_loss: 0.9399 - val_out_18_loss: 0.9399 - val_out_19_loss: 0.9372 - val_out_20_loss: 0.9401 - val_out_21_loss: 0.9312 - val_out_22_loss: 0.9377 - val_out_23_loss: 0.9343 - val_out_24_loss: 0.9363 - val_out_25_loss: 0.9464 - val_out_26_loss: 0.9403 - val_out_27_loss: 0.9394 - val_out_28_loss: 0.9430 - val_out_29_loss: 0.9392 - val_out_30_loss: 0.9387 - val_out_31_loss: 0.9362 - val_out_32_loss: 0.9381 - val_out_acc: 0.6659 - val_out_0_acc: 0.7419 - val_out_1_acc: 0.6681 - val_out_2_acc: 0.6508 - val_out_3_acc: 0.6659 - val_out_4_acc: 0.6616 - val_out_5_acc: 0.6529 - val_out_6_acc: 0.6659 - val_out_7_acc: 0.6573 - val_out_8_acc: 0.6573 - val_out_9_acc: 0.6573 - val_out_10_acc: 0.6508 - val_out_11_acc: 0.6551 - val_out_12_acc: 0.6659 - val_out_13_acc: 0.6508 - val_out_14_acc: 0.6616 - val_out_15_acc: 0.6529 - val_out_16_acc: 0.6594 - val_out_17_acc: 0.6573 - val_out_18_acc: 0.6659 - val_out_19_acc: 0.6594 - val_out_20_acc: 0.6616 - val_out_21_acc: 0.6638 - val_out_22_acc: 0.6508 - val_out_23_acc: 0.6638 - val_out_24_acc: 0.6486 - val_out_25_acc: 0.6573 - val_out_26_acc: 0.6616 - val_out_27_acc: 0.6638 - val_out_28_acc: 0.6659 - val_out_29_acc: 0.6508 - val_out_30_acc: 0.6443 - val_out_31_acc: 0.6573 - val_out_32_acc: 0.6594\n",
      "Epoch 61/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 32.8099 - out_loss: 0.8657 - out_0_loss: 0.6894 - out_1_loss: 0.9653 - out_2_loss: 0.9629 - out_3_loss: 0.9732 - out_4_loss: 0.9722 - out_5_loss: 0.9787 - out_6_loss: 0.9831 - out_7_loss: 0.9841 - out_8_loss: 0.9730 - out_9_loss: 0.9908 - out_10_loss: 0.9736 - out_11_loss: 0.9833 - out_12_loss: 0.9848 - out_13_loss: 0.9830 - out_14_loss: 0.9830 - out_15_loss: 0.9803 - out_16_loss: 0.9909 - out_17_loss: 0.9811 - out_18_loss: 0.9749 - out_19_loss: 0.9820 - out_20_loss: 0.9862 - out_21_loss: 0.9664 - out_22_loss: 0.9592 - out_23_loss: 0.9691 - out_24_loss: 0.9831 - out_25_loss: 0.9734 - out_26_loss: 0.9645 - out_27_loss: 0.9848 - out_28_loss: 0.9726 - out_29_loss: 0.9742 - out_30_loss: 0.9728 - out_31_loss: 0.9720 - out_32_loss: 0.9768 - out_acc: 0.6950 - out_0_acc: 0.7564 - out_1_acc: 0.6590 - out_2_acc: 0.6618 - out_3_acc: 0.6547 - out_4_acc: 0.6472 - out_5_acc: 0.6547 - out_6_acc: 0.6562 - out_7_acc: 0.6522 - out_8_acc: 0.6578 - out_9_acc: 0.6420 - out_10_acc: 0.6569 - out_11_acc: 0.6482 - out_12_acc: 0.6525 - out_13_acc: 0.6494 - out_14_acc: 0.6435 - out_15_acc: 0.6538 - out_16_acc: 0.6531 - out_17_acc: 0.6497 - out_18_acc: 0.6553 - out_19_acc: 0.6516 - out_20_acc: 0.6485 - out_21_acc: 0.6606 - out_22_acc: 0.6525 - out_23_acc: 0.6522 - out_24_acc: 0.6472 - out_25_acc: 0.6510 - out_26_acc: 0.6587 - out_27_acc: 0.6494 - out_28_acc: 0.6581 - out_29_acc: 0.6476 - out_30_acc: 0.6503 - out_31_acc: 0.6565 - out_32_acc: 0.6435 - val_loss: 34.4189 - val_out_loss: 0.9496 - val_out_0_loss: 0.6852 - val_out_1_loss: 0.9787 - val_out_2_loss: 0.9797 - val_out_3_loss: 0.9745 - val_out_4_loss: 0.9827 - val_out_5_loss: 0.9811 - val_out_6_loss: 0.9811 - val_out_7_loss: 0.9855 - val_out_8_loss: 0.9839 - val_out_9_loss: 0.9813 - val_out_10_loss: 0.9827 - val_out_11_loss: 0.9830 - val_out_12_loss: 0.9743 - val_out_13_loss: 0.9795 - val_out_14_loss: 0.9798 - val_out_15_loss: 0.9828 - val_out_16_loss: 0.9839 - val_out_17_loss: 0.9857 - val_out_18_loss: 0.9784 - val_out_19_loss: 0.9881 - val_out_20_loss: 0.9905 - val_out_21_loss: 0.9795 - val_out_22_loss: 0.9816 - val_out_23_loss: 0.9845 - val_out_24_loss: 0.9844 - val_out_25_loss: 0.9835 - val_out_26_loss: 0.9748 - val_out_27_loss: 0.9812 - val_out_28_loss: 0.9828 - val_out_29_loss: 0.9784 - val_out_30_loss: 0.9942 - val_out_31_loss: 0.9827 - val_out_32_loss: 0.9769 - val_out_acc: 0.6551 - val_out_0_acc: 0.7722 - val_out_1_acc: 0.6464 - val_out_2_acc: 0.6486 - val_out_3_acc: 0.6334 - val_out_4_acc: 0.6399 - val_out_5_acc: 0.6464 - val_out_6_acc: 0.6464 - val_out_7_acc: 0.6334 - val_out_8_acc: 0.6508 - val_out_9_acc: 0.6508 - val_out_10_acc: 0.6443 - val_out_11_acc: 0.6443 - val_out_12_acc: 0.6486 - val_out_13_acc: 0.6399 - val_out_14_acc: 0.6464 - val_out_15_acc: 0.6443 - val_out_16_acc: 0.6421 - val_out_17_acc: 0.6443 - val_out_18_acc: 0.6508 - val_out_19_acc: 0.6334 - val_out_20_acc: 0.6377 - val_out_21_acc: 0.6443 - val_out_22_acc: 0.6399 - val_out_23_acc: 0.6421 - val_out_24_acc: 0.6334 - val_out_25_acc: 0.6421 - val_out_26_acc: 0.6399 - val_out_27_acc: 0.6443 - val_out_28_acc: 0.6421 - val_out_29_acc: 0.6464 - val_out_30_acc: 0.6399 - val_out_31_acc: 0.6421 - val_out_32_acc: 0.6486\n",
      "Epoch 62/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 33.4546 - out_loss: 0.8837 - out_0_loss: 0.7053 - out_1_loss: 0.9870 - out_2_loss: 1.0041 - out_3_loss: 0.9953 - out_4_loss: 1.0089 - out_5_loss: 0.9966 - out_6_loss: 0.9885 - out_7_loss: 0.9967 - out_8_loss: 0.9856 - out_9_loss: 0.9930 - out_10_loss: 0.9874 - out_11_loss: 1.0002 - out_12_loss: 0.9930 - out_13_loss: 1.0046 - out_14_loss: 0.9919 - out_15_loss: 0.9980 - out_16_loss: 0.9992 - out_17_loss: 1.0023 - out_18_loss: 0.9862 - out_19_loss: 0.9969 - out_20_loss: 0.9892 - out_21_loss: 0.9967 - out_22_loss: 0.9971 - out_23_loss: 1.0115 - out_24_loss: 0.9979 - out_25_loss: 0.9980 - out_26_loss: 0.9828 - out_27_loss: 0.9899 - out_28_loss: 0.9869 - out_29_loss: 1.0001 - out_30_loss: 1.0130 - out_31_loss: 0.9982 - out_32_loss: 0.9889 - out_acc: 0.6878 - out_0_acc: 0.7486 - out_1_acc: 0.6500 - out_2_acc: 0.6367 - out_3_acc: 0.6420 - out_4_acc: 0.6441 - out_5_acc: 0.6476 - out_6_acc: 0.6488 - out_7_acc: 0.6438 - out_8_acc: 0.6507 - out_9_acc: 0.6507 - out_10_acc: 0.6445 - out_11_acc: 0.6500 - out_12_acc: 0.6482 - out_13_acc: 0.6373 - out_14_acc: 0.6538 - out_15_acc: 0.6392 - out_16_acc: 0.6466 - out_17_acc: 0.6438 - out_18_acc: 0.6488 - out_19_acc: 0.6451 - out_20_acc: 0.6466 - out_21_acc: 0.6466 - out_22_acc: 0.6420 - out_23_acc: 0.6370 - out_24_acc: 0.6364 - out_25_acc: 0.6497 - out_26_acc: 0.6476 - out_27_acc: 0.6364 - out_28_acc: 0.6513 - out_29_acc: 0.6460 - out_30_acc: 0.6488 - out_31_acc: 0.6445 - out_32_acc: 0.6414 - val_loss: 33.0254 - val_out_loss: 0.9164 - val_out_0_loss: 0.7485 - val_out_1_loss: 0.9411 - val_out_2_loss: 0.9374 - val_out_3_loss: 0.9390 - val_out_4_loss: 0.9429 - val_out_5_loss: 0.9349 - val_out_6_loss: 0.9335 - val_out_7_loss: 0.9350 - val_out_8_loss: 0.9403 - val_out_9_loss: 0.9405 - val_out_10_loss: 0.9393 - val_out_11_loss: 0.9422 - val_out_12_loss: 0.9406 - val_out_13_loss: 0.9422 - val_out_14_loss: 0.9468 - val_out_15_loss: 0.9330 - val_out_16_loss: 0.9310 - val_out_17_loss: 0.9400 - val_out_18_loss: 0.9370 - val_out_19_loss: 0.9387 - val_out_20_loss: 0.9382 - val_out_21_loss: 0.9401 - val_out_22_loss: 0.9359 - val_out_23_loss: 0.9462 - val_out_24_loss: 0.9321 - val_out_25_loss: 0.9397 - val_out_26_loss: 0.9418 - val_out_27_loss: 0.9387 - val_out_28_loss: 0.9441 - val_out_29_loss: 0.9456 - val_out_30_loss: 0.9430 - val_out_31_loss: 0.9377 - val_out_32_loss: 0.9348 - val_out_acc: 0.6659 - val_out_0_acc: 0.7332 - val_out_1_acc: 0.6573 - val_out_2_acc: 0.6551 - val_out_3_acc: 0.6594 - val_out_4_acc: 0.6573 - val_out_5_acc: 0.6573 - val_out_6_acc: 0.6594 - val_out_7_acc: 0.6594 - val_out_8_acc: 0.6616 - val_out_9_acc: 0.6573 - val_out_10_acc: 0.6616 - val_out_11_acc: 0.6616 - val_out_12_acc: 0.6594 - val_out_13_acc: 0.6616 - val_out_14_acc: 0.6594 - val_out_15_acc: 0.6594 - val_out_16_acc: 0.6616 - val_out_17_acc: 0.6529 - val_out_18_acc: 0.6616 - val_out_19_acc: 0.6573 - val_out_20_acc: 0.6616 - val_out_21_acc: 0.6551 - val_out_22_acc: 0.6594 - val_out_23_acc: 0.6616 - val_out_24_acc: 0.6573 - val_out_25_acc: 0.6594 - val_out_26_acc: 0.6638 - val_out_27_acc: 0.6551 - val_out_28_acc: 0.6551 - val_out_29_acc: 0.6594 - val_out_30_acc: 0.6551 - val_out_31_acc: 0.6594 - val_out_32_acc: 0.6616\n",
      "Epoch 63/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 33.4485 - out_loss: 0.8860 - out_0_loss: 0.7102 - out_1_loss: 0.9963 - out_2_loss: 0.9925 - out_3_loss: 1.0098 - out_4_loss: 0.9921 - out_5_loss: 0.9990 - out_6_loss: 0.9932 - out_7_loss: 0.9902 - out_8_loss: 0.9990 - out_9_loss: 0.9805 - out_10_loss: 0.9915 - out_11_loss: 0.9953 - out_12_loss: 0.9995 - out_13_loss: 0.9915 - out_14_loss: 0.9988 - out_15_loss: 1.0042 - out_16_loss: 1.0131 - out_17_loss: 1.0044 - out_18_loss: 0.9873 - out_19_loss: 0.9907 - out_20_loss: 0.9738 - out_21_loss: 0.9902 - out_22_loss: 0.9886 - out_23_loss: 0.9991 - out_24_loss: 0.9962 - out_25_loss: 0.9912 - out_26_loss: 0.9965 - out_27_loss: 1.0003 - out_28_loss: 0.9933 - out_29_loss: 0.9910 - out_30_loss: 0.9924 - out_31_loss: 1.0051 - out_32_loss: 1.0055 - out_acc: 0.6987 - out_0_acc: 0.7474 - out_1_acc: 0.6522 - out_2_acc: 0.6488 - out_3_acc: 0.6457 - out_4_acc: 0.6463 - out_5_acc: 0.6479 - out_6_acc: 0.6438 - out_7_acc: 0.6451 - out_8_acc: 0.6435 - out_9_acc: 0.6544 - out_10_acc: 0.6479 - out_11_acc: 0.6438 - out_12_acc: 0.6410 - out_13_acc: 0.6500 - out_14_acc: 0.6395 - out_15_acc: 0.6469 - out_16_acc: 0.6451 - out_17_acc: 0.6463 - out_18_acc: 0.6575 - out_19_acc: 0.6507 - out_20_acc: 0.6448 - out_21_acc: 0.6469 - out_22_acc: 0.6538 - out_23_acc: 0.6454 - out_24_acc: 0.6522 - out_25_acc: 0.6572 - out_26_acc: 0.6441 - out_27_acc: 0.6513 - out_28_acc: 0.6494 - out_29_acc: 0.6494 - out_30_acc: 0.6479 - out_31_acc: 0.6469 - out_32_acc: 0.6435 - val_loss: 31.9730 - val_out_loss: 0.8824 - val_out_0_loss: 0.6159 - val_out_1_loss: 0.9063 - val_out_2_loss: 0.9056 - val_out_3_loss: 0.9064 - val_out_4_loss: 0.9239 - val_out_5_loss: 0.9126 - val_out_6_loss: 0.9083 - val_out_7_loss: 0.9197 - val_out_8_loss: 0.9236 - val_out_9_loss: 0.9043 - val_out_10_loss: 0.9115 - val_out_11_loss: 0.9161 - val_out_12_loss: 0.9082 - val_out_13_loss: 0.9081 - val_out_14_loss: 0.9140 - val_out_15_loss: 0.9100 - val_out_16_loss: 0.9113 - val_out_17_loss: 0.9131 - val_out_18_loss: 0.9036 - val_out_19_loss: 0.9223 - val_out_20_loss: 0.9219 - val_out_21_loss: 0.9117 - val_out_22_loss: 0.9128 - val_out_23_loss: 0.9213 - val_out_24_loss: 0.9156 - val_out_25_loss: 0.9156 - val_out_26_loss: 0.9147 - val_out_27_loss: 0.9058 - val_out_28_loss: 0.9096 - val_out_29_loss: 0.9215 - val_out_30_loss: 0.9120 - val_out_31_loss: 0.9162 - val_out_32_loss: 0.9016 - val_out_acc: 0.6876 - val_out_0_acc: 0.7701 - val_out_1_acc: 0.6811 - val_out_2_acc: 0.6746 - val_out_3_acc: 0.6855 - val_out_4_acc: 0.6681 - val_out_5_acc: 0.6681 - val_out_6_acc: 0.6768 - val_out_7_acc: 0.6616 - val_out_8_acc: 0.6594 - val_out_9_acc: 0.6746 - val_out_10_acc: 0.6681 - val_out_11_acc: 0.6768 - val_out_12_acc: 0.6725 - val_out_13_acc: 0.6898 - val_out_14_acc: 0.6790 - val_out_15_acc: 0.6659 - val_out_16_acc: 0.6638 - val_out_17_acc: 0.6746 - val_out_18_acc: 0.6725 - val_out_19_acc: 0.6768 - val_out_20_acc: 0.6746 - val_out_21_acc: 0.6746 - val_out_22_acc: 0.6725 - val_out_23_acc: 0.6811 - val_out_24_acc: 0.6725 - val_out_25_acc: 0.6659 - val_out_26_acc: 0.6681 - val_out_27_acc: 0.6746 - val_out_28_acc: 0.6746 - val_out_29_acc: 0.6746 - val_out_30_acc: 0.6659 - val_out_31_acc: 0.6681 - val_out_32_acc: 0.6833\n",
      "Epoch 64/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 32.6962 - out_loss: 0.8629 - out_0_loss: 0.7003 - out_1_loss: 0.9681 - out_2_loss: 0.9670 - out_3_loss: 0.9595 - out_4_loss: 0.9706 - out_5_loss: 0.9761 - out_6_loss: 0.9766 - out_7_loss: 0.9779 - out_8_loss: 0.9820 - out_9_loss: 0.9854 - out_10_loss: 0.9720 - out_11_loss: 0.9737 - out_12_loss: 0.9721 - out_13_loss: 0.9680 - out_14_loss: 0.9671 - out_15_loss: 0.9655 - out_16_loss: 0.9539 - out_17_loss: 0.9765 - out_18_loss: 0.9622 - out_19_loss: 0.9764 - out_20_loss: 0.9836 - out_21_loss: 0.9666 - out_22_loss: 0.9655 - out_23_loss: 0.9671 - out_24_loss: 0.9807 - out_25_loss: 0.9678 - out_26_loss: 0.9697 - out_27_loss: 0.9730 - out_28_loss: 0.9812 - out_29_loss: 0.9766 - out_30_loss: 1.0054 - out_31_loss: 0.9785 - out_32_loss: 0.9668 - out_acc: 0.6925 - out_0_acc: 0.7536 - out_1_acc: 0.6516 - out_2_acc: 0.6500 - out_3_acc: 0.6513 - out_4_acc: 0.6525 - out_5_acc: 0.6426 - out_6_acc: 0.6491 - out_7_acc: 0.6482 - out_8_acc: 0.6485 - out_9_acc: 0.6466 - out_10_acc: 0.6445 - out_11_acc: 0.6451 - out_12_acc: 0.6376 - out_13_acc: 0.6528 - out_14_acc: 0.6534 - out_15_acc: 0.6562 - out_16_acc: 0.6640 - out_17_acc: 0.6466 - out_18_acc: 0.6516 - out_19_acc: 0.6454 - out_20_acc: 0.6488 - out_21_acc: 0.6609 - out_22_acc: 0.6463 - out_23_acc: 0.6559 - out_24_acc: 0.6479 - out_25_acc: 0.6562 - out_26_acc: 0.6565 - out_27_acc: 0.6423 - out_28_acc: 0.6423 - out_29_acc: 0.6432 - out_30_acc: 0.6379 - out_31_acc: 0.6441 - out_32_acc: 0.6541 - val_loss: 33.6980 - val_out_loss: 0.9272 - val_out_0_loss: 0.6490 - val_out_1_loss: 0.9654 - val_out_2_loss: 0.9577 - val_out_3_loss: 0.9663 - val_out_4_loss: 0.9592 - val_out_5_loss: 0.9619 - val_out_6_loss: 0.9611 - val_out_7_loss: 0.9610 - val_out_8_loss: 0.9644 - val_out_9_loss: 0.9643 - val_out_10_loss: 0.9599 - val_out_11_loss: 0.9646 - val_out_12_loss: 0.9596 - val_out_13_loss: 0.9653 - val_out_14_loss: 0.9609 - val_out_15_loss: 0.9681 - val_out_16_loss: 0.9615 - val_out_17_loss: 0.9649 - val_out_18_loss: 0.9552 - val_out_19_loss: 0.9628 - val_out_20_loss: 0.9638 - val_out_21_loss: 0.9645 - val_out_22_loss: 0.9594 - val_out_23_loss: 0.9657 - val_out_24_loss: 0.9584 - val_out_25_loss: 0.9553 - val_out_26_loss: 0.9619 - val_out_27_loss: 0.9600 - val_out_28_loss: 0.9669 - val_out_29_loss: 0.9623 - val_out_30_loss: 0.9624 - val_out_31_loss: 0.9612 - val_out_32_loss: 0.9620 - val_out_acc: 0.6638 - val_out_0_acc: 0.7375 - val_out_1_acc: 0.6486 - val_out_2_acc: 0.6508 - val_out_3_acc: 0.6551 - val_out_4_acc: 0.6464 - val_out_5_acc: 0.6551 - val_out_6_acc: 0.6573 - val_out_7_acc: 0.6486 - val_out_8_acc: 0.6551 - val_out_9_acc: 0.6508 - val_out_10_acc: 0.6594 - val_out_11_acc: 0.6377 - val_out_12_acc: 0.6551 - val_out_13_acc: 0.6443 - val_out_14_acc: 0.6616 - val_out_15_acc: 0.6529 - val_out_16_acc: 0.6464 - val_out_17_acc: 0.6377 - val_out_18_acc: 0.6573 - val_out_19_acc: 0.6486 - val_out_20_acc: 0.6464 - val_out_21_acc: 0.6551 - val_out_22_acc: 0.6573 - val_out_23_acc: 0.6551 - val_out_24_acc: 0.6486 - val_out_25_acc: 0.6464 - val_out_26_acc: 0.6486 - val_out_27_acc: 0.6573 - val_out_28_acc: 0.6551 - val_out_29_acc: 0.6529 - val_out_30_acc: 0.6529 - val_out_31_acc: 0.6529 - val_out_32_acc: 0.6551\n",
      "Epoch 65/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 32.6360 - out_loss: 0.8623 - out_0_loss: 0.6655 - out_1_loss: 0.9713 - out_2_loss: 0.9734 - out_3_loss: 0.9723 - out_4_loss: 0.9767 - out_5_loss: 0.9870 - out_6_loss: 0.9664 - out_7_loss: 0.9572 - out_8_loss: 0.9708 - out_9_loss: 0.9654 - out_10_loss: 0.9738 - out_11_loss: 0.9700 - out_12_loss: 0.9854 - out_13_loss: 0.9793 - out_14_loss: 0.9645 - out_15_loss: 0.9745 - out_16_loss: 0.9658 - out_17_loss: 0.9861 - out_18_loss: 0.9739 - out_19_loss: 0.9840 - out_20_loss: 0.9639 - out_21_loss: 0.9646 - out_22_loss: 0.9825 - out_23_loss: 0.9567 - out_24_loss: 0.9734 - out_25_loss: 0.9764 - out_26_loss: 0.9745 - out_27_loss: 0.9773 - out_28_loss: 0.9562 - out_29_loss: 0.9752 - out_30_loss: 0.9674 - out_31_loss: 0.9707 - out_32_loss: 0.9715 - out_acc: 0.7030 - out_0_acc: 0.7533 - out_1_acc: 0.6631 - out_2_acc: 0.6541 - out_3_acc: 0.6544 - out_4_acc: 0.6596 - out_5_acc: 0.6463 - out_6_acc: 0.6603 - out_7_acc: 0.6575 - out_8_acc: 0.6631 - out_9_acc: 0.6559 - out_10_acc: 0.6519 - out_11_acc: 0.6612 - out_12_acc: 0.6531 - out_13_acc: 0.6500 - out_14_acc: 0.6572 - out_15_acc: 0.6466 - out_16_acc: 0.6550 - out_17_acc: 0.6525 - out_18_acc: 0.6600 - out_19_acc: 0.6550 - out_20_acc: 0.6631 - out_21_acc: 0.6578 - out_22_acc: 0.6547 - out_23_acc: 0.6609 - out_24_acc: 0.6503 - out_25_acc: 0.6528 - out_26_acc: 0.6414 - out_27_acc: 0.6665 - out_28_acc: 0.6596 - out_29_acc: 0.6596 - out_30_acc: 0.6624 - out_31_acc: 0.6640 - out_32_acc: 0.6522 - val_loss: 30.7627 - val_out_loss: 0.8538 - val_out_0_loss: 0.6244 - val_out_1_loss: 0.8758 - val_out_2_loss: 0.8818 - val_out_3_loss: 0.8732 - val_out_4_loss: 0.8808 - val_out_5_loss: 0.8735 - val_out_6_loss: 0.8771 - val_out_7_loss: 0.8785 - val_out_8_loss: 0.8746 - val_out_9_loss: 0.8768 - val_out_10_loss: 0.8818 - val_out_11_loss: 0.8784 - val_out_12_loss: 0.8745 - val_out_13_loss: 0.8753 - val_out_14_loss: 0.8771 - val_out_15_loss: 0.8761 - val_out_16_loss: 0.8732 - val_out_17_loss: 0.8736 - val_out_18_loss: 0.8746 - val_out_19_loss: 0.8845 - val_out_20_loss: 0.8775 - val_out_21_loss: 0.8703 - val_out_22_loss: 0.8754 - val_out_23_loss: 0.8744 - val_out_24_loss: 0.8809 - val_out_25_loss: 0.8830 - val_out_26_loss: 0.8838 - val_out_27_loss: 0.8727 - val_out_28_loss: 0.8767 - val_out_29_loss: 0.8796 - val_out_30_loss: 0.8816 - val_out_31_loss: 0.8778 - val_out_32_loss: 0.8717 - val_out_acc: 0.6876 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.6833 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6833 - val_out_4_acc: 0.6790 - val_out_5_acc: 0.6833 - val_out_6_acc: 0.6833 - val_out_7_acc: 0.6898 - val_out_8_acc: 0.6855 - val_out_9_acc: 0.6768 - val_out_10_acc: 0.6768 - val_out_11_acc: 0.6746 - val_out_12_acc: 0.6746 - val_out_13_acc: 0.6833 - val_out_14_acc: 0.6855 - val_out_15_acc: 0.6833 - val_out_16_acc: 0.6876 - val_out_17_acc: 0.6811 - val_out_18_acc: 0.6790 - val_out_19_acc: 0.6768 - val_out_20_acc: 0.6790 - val_out_21_acc: 0.6790 - val_out_22_acc: 0.6833 - val_out_23_acc: 0.6790 - val_out_24_acc: 0.6811 - val_out_25_acc: 0.6790 - val_out_26_acc: 0.6746 - val_out_27_acc: 0.6855 - val_out_28_acc: 0.6855 - val_out_29_acc: 0.6768 - val_out_30_acc: 0.6941 - val_out_31_acc: 0.6833 - val_out_32_acc: 0.6833\n",
      "Epoch 66/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 33.0426 - out_loss: 0.8700 - out_0_loss: 0.6681 - out_1_loss: 0.9900 - out_2_loss: 0.9820 - out_3_loss: 0.9751 - out_4_loss: 0.9803 - out_5_loss: 0.9821 - out_6_loss: 0.9663 - out_7_loss: 0.9944 - out_8_loss: 0.9738 - out_9_loss: 0.9864 - out_10_loss: 0.9764 - out_11_loss: 0.9756 - out_12_loss: 0.9834 - out_13_loss: 0.9891 - out_14_loss: 0.9856 - out_15_loss: 0.9902 - out_16_loss: 0.9911 - out_17_loss: 0.9685 - out_18_loss: 0.9944 - out_19_loss: 0.9752 - out_20_loss: 0.9675 - out_21_loss: 0.9825 - out_22_loss: 0.9883 - out_23_loss: 0.9959 - out_24_loss: 0.9839 - out_25_loss: 0.9958 - out_26_loss: 1.0055 - out_27_loss: 0.9993 - out_28_loss: 0.9990 - out_29_loss: 0.9849 - out_30_loss: 0.9935 - out_31_loss: 0.9798 - out_32_loss: 0.9686 - out_acc: 0.6999 - out_0_acc: 0.7672 - out_1_acc: 0.6463 - out_2_acc: 0.6658 - out_3_acc: 0.6485 - out_4_acc: 0.6556 - out_5_acc: 0.6603 - out_6_acc: 0.6550 - out_7_acc: 0.6565 - out_8_acc: 0.6606 - out_9_acc: 0.6565 - out_10_acc: 0.6538 - out_11_acc: 0.6513 - out_12_acc: 0.6584 - out_13_acc: 0.6528 - out_14_acc: 0.6476 - out_15_acc: 0.6445 - out_16_acc: 0.6547 - out_17_acc: 0.6578 - out_18_acc: 0.6565 - out_19_acc: 0.6572 - out_20_acc: 0.6618 - out_21_acc: 0.6528 - out_22_acc: 0.6516 - out_23_acc: 0.6451 - out_24_acc: 0.6581 - out_25_acc: 0.6445 - out_26_acc: 0.6472 - out_27_acc: 0.6497 - out_28_acc: 0.6451 - out_29_acc: 0.6531 - out_30_acc: 0.6463 - out_31_acc: 0.6491 - out_32_acc: 0.6553 - val_loss: 31.1746 - val_out_loss: 0.8533 - val_out_0_loss: 0.6702 - val_out_1_loss: 0.8885 - val_out_2_loss: 0.8945 - val_out_3_loss: 0.8846 - val_out_4_loss: 0.8894 - val_out_5_loss: 0.8821 - val_out_6_loss: 0.8920 - val_out_7_loss: 0.8919 - val_out_8_loss: 0.8866 - val_out_9_loss: 0.8849 - val_out_10_loss: 0.8922 - val_out_11_loss: 0.8882 - val_out_12_loss: 0.8862 - val_out_13_loss: 0.8886 - val_out_14_loss: 0.8925 - val_out_15_loss: 0.8870 - val_out_16_loss: 0.8875 - val_out_17_loss: 0.8852 - val_out_18_loss: 0.8892 - val_out_19_loss: 0.8923 - val_out_20_loss: 0.8901 - val_out_21_loss: 0.8844 - val_out_22_loss: 0.8850 - val_out_23_loss: 0.8833 - val_out_24_loss: 0.8902 - val_out_25_loss: 0.8821 - val_out_26_loss: 0.8894 - val_out_27_loss: 0.8854 - val_out_28_loss: 0.8945 - val_out_29_loss: 0.8907 - val_out_30_loss: 0.8892 - val_out_31_loss: 0.8806 - val_out_32_loss: 0.8889 - val_out_acc: 0.7007 - val_out_0_acc: 0.7505 - val_out_1_acc: 0.6811 - val_out_2_acc: 0.6833 - val_out_3_acc: 0.6941 - val_out_4_acc: 0.6876 - val_out_5_acc: 0.6876 - val_out_6_acc: 0.6898 - val_out_7_acc: 0.6746 - val_out_8_acc: 0.6833 - val_out_9_acc: 0.6833 - val_out_10_acc: 0.6790 - val_out_11_acc: 0.6855 - val_out_12_acc: 0.6898 - val_out_13_acc: 0.6833 - val_out_14_acc: 0.6833 - val_out_15_acc: 0.6876 - val_out_16_acc: 0.6811 - val_out_17_acc: 0.6833 - val_out_18_acc: 0.6811 - val_out_19_acc: 0.6811 - val_out_20_acc: 0.6855 - val_out_21_acc: 0.6833 - val_out_22_acc: 0.6941 - val_out_23_acc: 0.6898 - val_out_24_acc: 0.6790 - val_out_25_acc: 0.6898 - val_out_26_acc: 0.6790 - val_out_27_acc: 0.6855 - val_out_28_acc: 0.6876 - val_out_29_acc: 0.6790 - val_out_30_acc: 0.6768 - val_out_31_acc: 0.6833 - val_out_32_acc: 0.6811\n",
      "Epoch 67/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 32.6303 - out_loss: 0.8643 - out_0_loss: 0.7171 - out_1_loss: 0.9703 - out_2_loss: 0.9638 - out_3_loss: 0.9646 - out_4_loss: 0.9798 - out_5_loss: 0.9655 - out_6_loss: 0.9753 - out_7_loss: 0.9672 - out_8_loss: 0.9699 - out_9_loss: 0.9629 - out_10_loss: 0.9700 - out_11_loss: 0.9731 - out_12_loss: 0.9624 - out_13_loss: 0.9536 - out_14_loss: 0.9796 - out_15_loss: 0.9608 - out_16_loss: 0.9668 - out_17_loss: 0.9732 - out_18_loss: 0.9721 - out_19_loss: 0.9788 - out_20_loss: 0.9664 - out_21_loss: 0.9801 - out_22_loss: 0.9885 - out_23_loss: 0.9690 - out_24_loss: 0.9598 - out_25_loss: 0.9673 - out_26_loss: 0.9730 - out_27_loss: 0.9724 - out_28_loss: 0.9692 - out_29_loss: 0.9781 - out_30_loss: 0.9755 - out_31_loss: 0.9744 - out_32_loss: 0.9656 - out_acc: 0.6978 - out_0_acc: 0.7579 - out_1_acc: 0.6503 - out_2_acc: 0.6575 - out_3_acc: 0.6584 - out_4_acc: 0.6559 - out_5_acc: 0.6516 - out_6_acc: 0.6538 - out_7_acc: 0.6491 - out_8_acc: 0.6525 - out_9_acc: 0.6553 - out_10_acc: 0.6559 - out_11_acc: 0.6556 - out_12_acc: 0.6572 - out_13_acc: 0.6522 - out_14_acc: 0.6522 - out_15_acc: 0.6646 - out_16_acc: 0.6522 - out_17_acc: 0.6609 - out_18_acc: 0.6544 - out_19_acc: 0.6562 - out_20_acc: 0.6491 - out_21_acc: 0.6466 - out_22_acc: 0.6534 - out_23_acc: 0.6476 - out_24_acc: 0.6621 - out_25_acc: 0.6538 - out_26_acc: 0.6538 - out_27_acc: 0.6575 - out_28_acc: 0.6565 - out_29_acc: 0.6488 - out_30_acc: 0.6522 - out_31_acc: 0.6429 - out_32_acc: 0.6612 - val_loss: 29.2400 - val_out_loss: 0.8123 - val_out_0_loss: 0.7149 - val_out_1_loss: 0.8286 - val_out_2_loss: 0.8279 - val_out_3_loss: 0.8299 - val_out_4_loss: 0.8284 - val_out_5_loss: 0.8271 - val_out_6_loss: 0.8272 - val_out_7_loss: 0.8287 - val_out_8_loss: 0.8294 - val_out_9_loss: 0.8305 - val_out_10_loss: 0.8300 - val_out_11_loss: 0.8290 - val_out_12_loss: 0.8274 - val_out_13_loss: 0.8328 - val_out_14_loss: 0.8352 - val_out_15_loss: 0.8289 - val_out_16_loss: 0.8273 - val_out_17_loss: 0.8294 - val_out_18_loss: 0.8258 - val_out_19_loss: 0.8389 - val_out_20_loss: 0.8360 - val_out_21_loss: 0.8294 - val_out_22_loss: 0.8312 - val_out_23_loss: 0.8300 - val_out_24_loss: 0.8315 - val_out_25_loss: 0.8327 - val_out_26_loss: 0.8300 - val_out_27_loss: 0.8272 - val_out_28_loss: 0.8291 - val_out_29_loss: 0.8284 - val_out_30_loss: 0.8306 - val_out_31_loss: 0.8274 - val_out_32_loss: 0.8293 - val_out_acc: 0.7115 - val_out_0_acc: 0.7397 - val_out_1_acc: 0.7093 - val_out_2_acc: 0.6985 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.6941 - val_out_5_acc: 0.6985 - val_out_6_acc: 0.7050 - val_out_7_acc: 0.7028 - val_out_8_acc: 0.7137 - val_out_9_acc: 0.7028 - val_out_10_acc: 0.7137 - val_out_11_acc: 0.6941 - val_out_12_acc: 0.7072 - val_out_13_acc: 0.7180 - val_out_14_acc: 0.7028 - val_out_15_acc: 0.7072 - val_out_16_acc: 0.7050 - val_out_17_acc: 0.7115 - val_out_18_acc: 0.7137 - val_out_19_acc: 0.7050 - val_out_20_acc: 0.7028 - val_out_21_acc: 0.7050 - val_out_22_acc: 0.7028 - val_out_23_acc: 0.7093 - val_out_24_acc: 0.6985 - val_out_25_acc: 0.7050 - val_out_26_acc: 0.7093 - val_out_27_acc: 0.6985 - val_out_28_acc: 0.7050 - val_out_29_acc: 0.7050 - val_out_30_acc: 0.7007 - val_out_31_acc: 0.7115 - val_out_32_acc: 0.7137\n",
      "Epoch 68/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 31.6881 - out_loss: 0.8359 - out_0_loss: 0.6777 - out_1_loss: 0.9394 - out_2_loss: 0.9390 - out_3_loss: 0.9526 - out_4_loss: 0.9430 - out_5_loss: 0.9405 - out_6_loss: 0.9377 - out_7_loss: 0.9432 - out_8_loss: 0.9318 - out_9_loss: 0.9352 - out_10_loss: 0.9369 - out_11_loss: 0.9387 - out_12_loss: 0.9265 - out_13_loss: 0.9285 - out_14_loss: 0.9498 - out_15_loss: 0.9483 - out_16_loss: 0.9413 - out_17_loss: 0.9436 - out_18_loss: 0.9509 - out_19_loss: 0.9488 - out_20_loss: 0.9532 - out_21_loss: 0.9448 - out_22_loss: 0.9421 - out_23_loss: 0.9309 - out_24_loss: 0.9480 - out_25_loss: 0.9399 - out_26_loss: 0.9415 - out_27_loss: 0.9578 - out_28_loss: 0.9457 - out_29_loss: 0.9537 - out_30_loss: 0.9515 - out_31_loss: 0.9562 - out_32_loss: 0.9336 - out_acc: 0.6990 - out_0_acc: 0.7626 - out_1_acc: 0.6677 - out_2_acc: 0.6600 - out_3_acc: 0.6569 - out_4_acc: 0.6658 - out_5_acc: 0.6658 - out_6_acc: 0.6655 - out_7_acc: 0.6631 - out_8_acc: 0.6655 - out_9_acc: 0.6730 - out_10_acc: 0.6686 - out_11_acc: 0.6600 - out_12_acc: 0.6711 - out_13_acc: 0.6733 - out_14_acc: 0.6658 - out_15_acc: 0.6618 - out_16_acc: 0.6643 - out_17_acc: 0.6615 - out_18_acc: 0.6581 - out_19_acc: 0.6689 - out_20_acc: 0.6652 - out_21_acc: 0.6692 - out_22_acc: 0.6662 - out_23_acc: 0.6668 - out_24_acc: 0.6652 - out_25_acc: 0.6646 - out_26_acc: 0.6634 - out_27_acc: 0.6621 - out_28_acc: 0.6615 - out_29_acc: 0.6652 - out_30_acc: 0.6575 - out_31_acc: 0.6696 - out_32_acc: 0.6637 - val_loss: 29.7456 - val_out_loss: 0.8195 - val_out_0_loss: 0.5603 - val_out_1_loss: 0.8494 - val_out_2_loss: 0.8503 - val_out_3_loss: 0.8430 - val_out_4_loss: 0.8534 - val_out_5_loss: 0.8459 - val_out_6_loss: 0.8460 - val_out_7_loss: 0.8529 - val_out_8_loss: 0.8467 - val_out_9_loss: 0.8482 - val_out_10_loss: 0.8528 - val_out_11_loss: 0.8550 - val_out_12_loss: 0.8487 - val_out_13_loss: 0.8507 - val_out_14_loss: 0.8539 - val_out_15_loss: 0.8449 - val_out_16_loss: 0.8442 - val_out_17_loss: 0.8532 - val_out_18_loss: 0.8465 - val_out_19_loss: 0.8546 - val_out_20_loss: 0.8533 - val_out_21_loss: 0.8429 - val_out_22_loss: 0.8504 - val_out_23_loss: 0.8435 - val_out_24_loss: 0.8535 - val_out_25_loss: 0.8528 - val_out_26_loss: 0.8472 - val_out_27_loss: 0.8471 - val_out_28_loss: 0.8505 - val_out_29_loss: 0.8528 - val_out_30_loss: 0.8550 - val_out_31_loss: 0.8477 - val_out_32_loss: 0.8515 - val_out_acc: 0.7245 - val_out_0_acc: 0.7896 - val_out_1_acc: 0.7223 - val_out_2_acc: 0.7245 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.7223 - val_out_5_acc: 0.7245 - val_out_6_acc: 0.7158 - val_out_7_acc: 0.7202 - val_out_8_acc: 0.7245 - val_out_9_acc: 0.7267 - val_out_10_acc: 0.7267 - val_out_11_acc: 0.7267 - val_out_12_acc: 0.7158 - val_out_13_acc: 0.7245 - val_out_14_acc: 0.7180 - val_out_15_acc: 0.7137 - val_out_16_acc: 0.7137 - val_out_17_acc: 0.7354 - val_out_18_acc: 0.7289 - val_out_19_acc: 0.7223 - val_out_20_acc: 0.7158 - val_out_21_acc: 0.7267 - val_out_22_acc: 0.7245 - val_out_23_acc: 0.7245 - val_out_24_acc: 0.7245 - val_out_25_acc: 0.7289 - val_out_26_acc: 0.7223 - val_out_27_acc: 0.7137 - val_out_28_acc: 0.7202 - val_out_29_acc: 0.7158 - val_out_30_acc: 0.7202 - val_out_31_acc: 0.7245 - val_out_32_acc: 0.7223\n",
      "Epoch 69/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 32.5045 - out_loss: 0.8561 - out_0_loss: 0.6627 - out_1_loss: 0.9778 - out_2_loss: 0.9755 - out_3_loss: 0.9711 - out_4_loss: 0.9719 - out_5_loss: 0.9673 - out_6_loss: 0.9736 - out_7_loss: 0.9807 - out_8_loss: 0.9641 - out_9_loss: 0.9562 - out_10_loss: 0.9583 - out_11_loss: 0.9724 - out_12_loss: 0.9710 - out_13_loss: 0.9694 - out_14_loss: 0.9834 - out_15_loss: 0.9740 - out_16_loss: 0.9609 - out_17_loss: 0.9624 - out_18_loss: 0.9720 - out_19_loss: 0.9676 - out_20_loss: 0.9573 - out_21_loss: 0.9704 - out_22_loss: 0.9747 - out_23_loss: 0.9688 - out_24_loss: 0.9548 - out_25_loss: 0.9607 - out_26_loss: 0.9799 - out_27_loss: 0.9661 - out_28_loss: 0.9671 - out_29_loss: 0.9685 - out_30_loss: 0.9738 - out_31_loss: 0.9522 - out_32_loss: 0.9619 - out_acc: 0.7027 - out_0_acc: 0.7610 - out_1_acc: 0.6565 - out_2_acc: 0.6550 - out_3_acc: 0.6522 - out_4_acc: 0.6575 - out_5_acc: 0.6541 - out_6_acc: 0.6503 - out_7_acc: 0.6559 - out_8_acc: 0.6584 - out_9_acc: 0.6556 - out_10_acc: 0.6565 - out_11_acc: 0.6466 - out_12_acc: 0.6528 - out_13_acc: 0.6525 - out_14_acc: 0.6472 - out_15_acc: 0.6472 - out_16_acc: 0.6600 - out_17_acc: 0.6603 - out_18_acc: 0.6510 - out_19_acc: 0.6538 - out_20_acc: 0.6634 - out_21_acc: 0.6593 - out_22_acc: 0.6451 - out_23_acc: 0.6572 - out_24_acc: 0.6569 - out_25_acc: 0.6615 - out_26_acc: 0.6528 - out_27_acc: 0.6547 - out_28_acc: 0.6525 - out_29_acc: 0.6575 - out_30_acc: 0.6590 - out_31_acc: 0.6572 - out_32_acc: 0.6609 - val_loss: 32.0902 - val_out_loss: 0.8911 - val_out_0_loss: 0.7045 - val_out_1_loss: 0.9139 - val_out_2_loss: 0.9169 - val_out_3_loss: 0.9062 - val_out_4_loss: 0.9162 - val_out_5_loss: 0.9083 - val_out_6_loss: 0.9108 - val_out_7_loss: 0.9118 - val_out_8_loss: 0.9080 - val_out_9_loss: 0.9111 - val_out_10_loss: 0.9187 - val_out_11_loss: 0.9174 - val_out_12_loss: 0.9157 - val_out_13_loss: 0.9131 - val_out_14_loss: 0.9144 - val_out_15_loss: 0.9080 - val_out_16_loss: 0.9098 - val_out_17_loss: 0.9104 - val_out_18_loss: 0.9106 - val_out_19_loss: 0.9212 - val_out_20_loss: 0.9144 - val_out_21_loss: 0.9147 - val_out_22_loss: 0.9135 - val_out_23_loss: 0.9124 - val_out_24_loss: 0.9131 - val_out_25_loss: 0.9215 - val_out_26_loss: 0.9184 - val_out_27_loss: 0.9157 - val_out_28_loss: 0.9092 - val_out_29_loss: 0.9124 - val_out_30_loss: 0.9186 - val_out_31_loss: 0.9096 - val_out_32_loss: 0.9084 - val_out_acc: 0.6811 - val_out_0_acc: 0.7484 - val_out_1_acc: 0.6768 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6768 - val_out_4_acc: 0.6790 - val_out_5_acc: 0.6790 - val_out_6_acc: 0.6790 - val_out_7_acc: 0.6790 - val_out_8_acc: 0.6790 - val_out_9_acc: 0.6768 - val_out_10_acc: 0.6725 - val_out_11_acc: 0.6855 - val_out_12_acc: 0.6725 - val_out_13_acc: 0.6833 - val_out_14_acc: 0.6790 - val_out_15_acc: 0.6790 - val_out_16_acc: 0.6855 - val_out_17_acc: 0.6790 - val_out_18_acc: 0.6746 - val_out_19_acc: 0.6746 - val_out_20_acc: 0.6855 - val_out_21_acc: 0.6703 - val_out_22_acc: 0.6833 - val_out_23_acc: 0.6746 - val_out_24_acc: 0.6790 - val_out_25_acc: 0.6746 - val_out_26_acc: 0.6833 - val_out_27_acc: 0.6811 - val_out_28_acc: 0.6811 - val_out_29_acc: 0.6855 - val_out_30_acc: 0.6725 - val_out_31_acc: 0.6768 - val_out_32_acc: 0.6833\n",
      "Epoch 70/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 31.5989 - out_loss: 0.8352 - out_0_loss: 0.6461 - out_1_loss: 0.9295 - out_2_loss: 0.9392 - out_3_loss: 0.9423 - out_4_loss: 0.9382 - out_5_loss: 0.9540 - out_6_loss: 0.9486 - out_7_loss: 0.9500 - out_8_loss: 0.9333 - out_9_loss: 0.9384 - out_10_loss: 0.9328 - out_11_loss: 0.9473 - out_12_loss: 0.9433 - out_13_loss: 0.9402 - out_14_loss: 0.9396 - out_15_loss: 0.9463 - out_16_loss: 0.9415 - out_17_loss: 0.9482 - out_18_loss: 0.9500 - out_19_loss: 0.9407 - out_20_loss: 0.9399 - out_21_loss: 0.9298 - out_22_loss: 0.9299 - out_23_loss: 0.9265 - out_24_loss: 0.9346 - out_25_loss: 0.9392 - out_26_loss: 0.9494 - out_27_loss: 0.9360 - out_28_loss: 0.9440 - out_29_loss: 0.9409 - out_30_loss: 0.9482 - out_31_loss: 0.9459 - out_32_loss: 0.9504 - out_acc: 0.7086 - out_0_acc: 0.7638 - out_1_acc: 0.6593 - out_2_acc: 0.6637 - out_3_acc: 0.6643 - out_4_acc: 0.6699 - out_5_acc: 0.6565 - out_6_acc: 0.6584 - out_7_acc: 0.6578 - out_8_acc: 0.6631 - out_9_acc: 0.6618 - out_10_acc: 0.6680 - out_11_acc: 0.6562 - out_12_acc: 0.6658 - out_13_acc: 0.6646 - out_14_acc: 0.6652 - out_15_acc: 0.6575 - out_16_acc: 0.6600 - out_17_acc: 0.6606 - out_18_acc: 0.6556 - out_19_acc: 0.6575 - out_20_acc: 0.6575 - out_21_acc: 0.6705 - out_22_acc: 0.6689 - out_23_acc: 0.6764 - out_24_acc: 0.6584 - out_25_acc: 0.6643 - out_26_acc: 0.6569 - out_27_acc: 0.6655 - out_28_acc: 0.6562 - out_29_acc: 0.6652 - out_30_acc: 0.6584 - out_31_acc: 0.6606 - out_32_acc: 0.6516 - val_loss: 31.6443 - val_out_loss: 0.8743 - val_out_0_loss: 0.6496 - val_out_1_loss: 0.9009 - val_out_2_loss: 0.8997 - val_out_3_loss: 0.9050 - val_out_4_loss: 0.9030 - val_out_5_loss: 0.9061 - val_out_6_loss: 0.9091 - val_out_7_loss: 0.9078 - val_out_8_loss: 0.8955 - val_out_9_loss: 0.8967 - val_out_10_loss: 0.9118 - val_out_11_loss: 0.9055 - val_out_12_loss: 0.9000 - val_out_13_loss: 0.8992 - val_out_14_loss: 0.9034 - val_out_15_loss: 0.9027 - val_out_16_loss: 0.8909 - val_out_17_loss: 0.9045 - val_out_18_loss: 0.8975 - val_out_19_loss: 0.9056 - val_out_20_loss: 0.9020 - val_out_21_loss: 0.8956 - val_out_22_loss: 0.9006 - val_out_23_loss: 0.8958 - val_out_24_loss: 0.9068 - val_out_25_loss: 0.9061 - val_out_26_loss: 0.9038 - val_out_27_loss: 0.8983 - val_out_28_loss: 0.9082 - val_out_29_loss: 0.9073 - val_out_30_loss: 0.9060 - val_out_31_loss: 0.8958 - val_out_32_loss: 0.8966 - val_out_acc: 0.6833 - val_out_0_acc: 0.7614 - val_out_1_acc: 0.6811 - val_out_2_acc: 0.6790 - val_out_3_acc: 0.6746 - val_out_4_acc: 0.6790 - val_out_5_acc: 0.6746 - val_out_6_acc: 0.6811 - val_out_7_acc: 0.6790 - val_out_8_acc: 0.6746 - val_out_9_acc: 0.6833 - val_out_10_acc: 0.6833 - val_out_11_acc: 0.6790 - val_out_12_acc: 0.6790 - val_out_13_acc: 0.6768 - val_out_14_acc: 0.6746 - val_out_15_acc: 0.6811 - val_out_16_acc: 0.6833 - val_out_17_acc: 0.6746 - val_out_18_acc: 0.6855 - val_out_19_acc: 0.6746 - val_out_20_acc: 0.6768 - val_out_21_acc: 0.6855 - val_out_22_acc: 0.6725 - val_out_23_acc: 0.6768 - val_out_24_acc: 0.6746 - val_out_25_acc: 0.6790 - val_out_26_acc: 0.6725 - val_out_27_acc: 0.6681 - val_out_28_acc: 0.6725 - val_out_29_acc: 0.6746 - val_out_30_acc: 0.6768 - val_out_31_acc: 0.6768 - val_out_32_acc: 0.6790\n",
      "Epoch 71/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 31.7477 - out_loss: 0.8401 - out_0_loss: 0.6592 - out_1_loss: 0.9439 - out_2_loss: 0.9475 - out_3_loss: 0.9471 - out_4_loss: 0.9447 - out_5_loss: 0.9490 - out_6_loss: 0.9543 - out_7_loss: 0.9549 - out_8_loss: 0.9362 - out_9_loss: 0.9451 - out_10_loss: 0.9400 - out_11_loss: 0.9360 - out_12_loss: 0.9393 - out_13_loss: 0.9461 - out_14_loss: 0.9621 - out_15_loss: 0.9437 - out_16_loss: 0.9237 - out_17_loss: 0.9400 - out_18_loss: 0.9522 - out_19_loss: 0.9557 - out_20_loss: 0.9439 - out_21_loss: 0.9493 - out_22_loss: 0.9544 - out_23_loss: 0.9339 - out_24_loss: 0.9455 - out_25_loss: 0.9502 - out_26_loss: 0.9387 - out_27_loss: 0.9356 - out_28_loss: 0.9445 - out_29_loss: 0.9531 - out_30_loss: 0.9442 - out_31_loss: 0.9402 - out_32_loss: 0.9532 - out_acc: 0.7033 - out_0_acc: 0.7604 - out_1_acc: 0.6640 - out_2_acc: 0.6569 - out_3_acc: 0.6683 - out_4_acc: 0.6634 - out_5_acc: 0.6627 - out_6_acc: 0.6553 - out_7_acc: 0.6569 - out_8_acc: 0.6711 - out_9_acc: 0.6522 - out_10_acc: 0.6569 - out_11_acc: 0.6643 - out_12_acc: 0.6668 - out_13_acc: 0.6609 - out_14_acc: 0.6590 - out_15_acc: 0.6631 - out_16_acc: 0.6600 - out_17_acc: 0.6584 - out_18_acc: 0.6538 - out_19_acc: 0.6572 - out_20_acc: 0.6562 - out_21_acc: 0.6556 - out_22_acc: 0.6500 - out_23_acc: 0.6596 - out_24_acc: 0.6587 - out_25_acc: 0.6658 - out_26_acc: 0.6674 - out_27_acc: 0.6631 - out_28_acc: 0.6714 - out_29_acc: 0.6544 - out_30_acc: 0.6587 - out_31_acc: 0.6581 - out_32_acc: 0.6590 - val_loss: 31.3895 - val_out_loss: 0.8689 - val_out_0_loss: 0.6295 - val_out_1_loss: 0.8931 - val_out_2_loss: 0.8879 - val_out_3_loss: 0.8865 - val_out_4_loss: 0.9017 - val_out_5_loss: 0.8916 - val_out_6_loss: 0.8955 - val_out_7_loss: 0.9018 - val_out_8_loss: 0.8917 - val_out_9_loss: 0.8914 - val_out_10_loss: 0.8962 - val_out_11_loss: 0.8994 - val_out_12_loss: 0.8900 - val_out_13_loss: 0.8935 - val_out_14_loss: 0.9080 - val_out_15_loss: 0.8960 - val_out_16_loss: 0.8947 - val_out_17_loss: 0.9014 - val_out_18_loss: 0.8945 - val_out_19_loss: 0.9007 - val_out_20_loss: 0.9005 - val_out_21_loss: 0.8951 - val_out_22_loss: 0.8923 - val_out_23_loss: 0.8956 - val_out_24_loss: 0.9037 - val_out_25_loss: 0.8938 - val_out_26_loss: 0.8953 - val_out_27_loss: 0.8926 - val_out_28_loss: 0.8946 - val_out_29_loss: 0.8898 - val_out_30_loss: 0.8943 - val_out_31_loss: 0.8944 - val_out_32_loss: 0.8910 - val_out_acc: 0.6746 - val_out_0_acc: 0.7787 - val_out_1_acc: 0.6746 - val_out_2_acc: 0.6811 - val_out_3_acc: 0.6746 - val_out_4_acc: 0.6790 - val_out_5_acc: 0.6725 - val_out_6_acc: 0.6703 - val_out_7_acc: 0.6703 - val_out_8_acc: 0.6811 - val_out_9_acc: 0.6746 - val_out_10_acc: 0.6855 - val_out_11_acc: 0.6768 - val_out_12_acc: 0.6790 - val_out_13_acc: 0.6725 - val_out_14_acc: 0.6790 - val_out_15_acc: 0.6790 - val_out_16_acc: 0.6768 - val_out_17_acc: 0.6768 - val_out_18_acc: 0.6768 - val_out_19_acc: 0.6811 - val_out_20_acc: 0.6790 - val_out_21_acc: 0.6811 - val_out_22_acc: 0.6746 - val_out_23_acc: 0.6811 - val_out_24_acc: 0.6768 - val_out_25_acc: 0.6746 - val_out_26_acc: 0.6768 - val_out_27_acc: 0.6833 - val_out_28_acc: 0.6790 - val_out_29_acc: 0.6768 - val_out_30_acc: 0.6746 - val_out_31_acc: 0.6725 - val_out_32_acc: 0.6768\n",
      "Epoch 72/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 31.5063 - out_loss: 0.8320 - out_0_loss: 0.6598 - out_1_loss: 0.9530 - out_2_loss: 0.9365 - out_3_loss: 0.9367 - out_4_loss: 0.9426 - out_5_loss: 0.9232 - out_6_loss: 0.9356 - out_7_loss: 0.9414 - out_8_loss: 0.9366 - out_9_loss: 0.9328 - out_10_loss: 0.9318 - out_11_loss: 0.9412 - out_12_loss: 0.9339 - out_13_loss: 0.9227 - out_14_loss: 0.9506 - out_15_loss: 0.9473 - out_16_loss: 0.9368 - out_17_loss: 0.9442 - out_18_loss: 0.9495 - out_19_loss: 0.9278 - out_20_loss: 0.9386 - out_21_loss: 0.9419 - out_22_loss: 0.9451 - out_23_loss: 0.9363 - out_24_loss: 0.9411 - out_25_loss: 0.9457 - out_26_loss: 0.9458 - out_27_loss: 0.9478 - out_28_loss: 0.9395 - out_29_loss: 0.9394 - out_30_loss: 0.9182 - out_31_loss: 0.9235 - out_32_loss: 0.9276 - out_acc: 0.7068 - out_0_acc: 0.7657 - out_1_acc: 0.6615 - out_2_acc: 0.6618 - out_3_acc: 0.6643 - out_4_acc: 0.6621 - out_5_acc: 0.6723 - out_6_acc: 0.6655 - out_7_acc: 0.6606 - out_8_acc: 0.6714 - out_9_acc: 0.6677 - out_10_acc: 0.6662 - out_11_acc: 0.6708 - out_12_acc: 0.6680 - out_13_acc: 0.6730 - out_14_acc: 0.6668 - out_15_acc: 0.6612 - out_16_acc: 0.6665 - out_17_acc: 0.6575 - out_18_acc: 0.6717 - out_19_acc: 0.6745 - out_20_acc: 0.6705 - out_21_acc: 0.6714 - out_22_acc: 0.6547 - out_23_acc: 0.6686 - out_24_acc: 0.6618 - out_25_acc: 0.6680 - out_26_acc: 0.6674 - out_27_acc: 0.6550 - out_28_acc: 0.6649 - out_29_acc: 0.6658 - out_30_acc: 0.6720 - out_31_acc: 0.6674 - out_32_acc: 0.6699 - val_loss: 28.5900 - val_out_loss: 0.7816 - val_out_0_loss: 0.5664 - val_out_1_loss: 0.8178 - val_out_2_loss: 0.8128 - val_out_3_loss: 0.8132 - val_out_4_loss: 0.8146 - val_out_5_loss: 0.8159 - val_out_6_loss: 0.8207 - val_out_7_loss: 0.8181 - val_out_8_loss: 0.8176 - val_out_9_loss: 0.8154 - val_out_10_loss: 0.8208 - val_out_11_loss: 0.8185 - val_out_12_loss: 0.8203 - val_out_13_loss: 0.8202 - val_out_14_loss: 0.8169 - val_out_15_loss: 0.8139 - val_out_16_loss: 0.8131 - val_out_17_loss: 0.8166 - val_out_18_loss: 0.8199 - val_out_19_loss: 0.8186 - val_out_20_loss: 0.8163 - val_out_21_loss: 0.8111 - val_out_22_loss: 0.8124 - val_out_23_loss: 0.8154 - val_out_24_loss: 0.8155 - val_out_25_loss: 0.8153 - val_out_26_loss: 0.8123 - val_out_27_loss: 0.8140 - val_out_28_loss: 0.8181 - val_out_29_loss: 0.8177 - val_out_30_loss: 0.8170 - val_out_31_loss: 0.8065 - val_out_32_loss: 0.8138 - val_out_acc: 0.7223 - val_out_0_acc: 0.7939 - val_out_1_acc: 0.7180 - val_out_2_acc: 0.7137 - val_out_3_acc: 0.7245 - val_out_4_acc: 0.7158 - val_out_5_acc: 0.7202 - val_out_6_acc: 0.7180 - val_out_7_acc: 0.7223 - val_out_8_acc: 0.7223 - val_out_9_acc: 0.7180 - val_out_10_acc: 0.7202 - val_out_11_acc: 0.7202 - val_out_12_acc: 0.7115 - val_out_13_acc: 0.7093 - val_out_14_acc: 0.7115 - val_out_15_acc: 0.7202 - val_out_16_acc: 0.7223 - val_out_17_acc: 0.7158 - val_out_18_acc: 0.7158 - val_out_19_acc: 0.7115 - val_out_20_acc: 0.7267 - val_out_21_acc: 0.7267 - val_out_22_acc: 0.7158 - val_out_23_acc: 0.7223 - val_out_24_acc: 0.7137 - val_out_25_acc: 0.7267 - val_out_26_acc: 0.7245 - val_out_27_acc: 0.7180 - val_out_28_acc: 0.7180 - val_out_29_acc: 0.7267 - val_out_30_acc: 0.7137 - val_out_31_acc: 0.7245 - val_out_32_acc: 0.7202\n",
      "Epoch 73/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 31.3821 - out_loss: 0.8262 - out_0_loss: 0.6491 - out_1_loss: 0.9329 - out_2_loss: 0.9348 - out_3_loss: 0.9262 - out_4_loss: 0.9260 - out_5_loss: 0.9317 - out_6_loss: 0.9339 - out_7_loss: 0.9549 - out_8_loss: 0.9433 - out_9_loss: 0.9381 - out_10_loss: 0.9339 - out_11_loss: 0.9389 - out_12_loss: 0.9311 - out_13_loss: 0.9309 - out_14_loss: 0.9370 - out_15_loss: 0.9329 - out_16_loss: 0.9324 - out_17_loss: 0.9266 - out_18_loss: 0.9281 - out_19_loss: 0.9176 - out_20_loss: 0.9467 - out_21_loss: 0.9289 - out_22_loss: 0.9418 - out_23_loss: 0.9171 - out_24_loss: 0.9415 - out_25_loss: 0.9317 - out_26_loss: 0.9371 - out_27_loss: 0.9335 - out_28_loss: 0.9255 - out_29_loss: 0.9381 - out_30_loss: 0.9382 - out_31_loss: 0.9478 - out_32_loss: 0.9478 - out_acc: 0.7058 - out_0_acc: 0.7743 - out_1_acc: 0.6584 - out_2_acc: 0.6587 - out_3_acc: 0.6668 - out_4_acc: 0.6655 - out_5_acc: 0.6637 - out_6_acc: 0.6600 - out_7_acc: 0.6562 - out_8_acc: 0.6550 - out_9_acc: 0.6627 - out_10_acc: 0.6562 - out_11_acc: 0.6665 - out_12_acc: 0.6662 - out_13_acc: 0.6649 - out_14_acc: 0.6646 - out_15_acc: 0.6627 - out_16_acc: 0.6631 - out_17_acc: 0.6634 - out_18_acc: 0.6637 - out_19_acc: 0.6689 - out_20_acc: 0.6569 - out_21_acc: 0.6621 - out_22_acc: 0.6612 - out_23_acc: 0.6643 - out_24_acc: 0.6658 - out_25_acc: 0.6655 - out_26_acc: 0.6692 - out_27_acc: 0.6593 - out_28_acc: 0.6624 - out_29_acc: 0.6618 - out_30_acc: 0.6649 - out_31_acc: 0.6618 - out_32_acc: 0.6652 - val_loss: 33.0398 - val_out_loss: 0.9122 - val_out_0_loss: 0.6076 - val_out_1_loss: 0.9494 - val_out_2_loss: 0.9385 - val_out_3_loss: 0.9451 - val_out_4_loss: 0.9427 - val_out_5_loss: 0.9385 - val_out_6_loss: 0.9445 - val_out_7_loss: 0.9459 - val_out_8_loss: 0.9366 - val_out_9_loss: 0.9375 - val_out_10_loss: 0.9436 - val_out_11_loss: 0.9483 - val_out_12_loss: 0.9435 - val_out_13_loss: 0.9465 - val_out_14_loss: 0.9464 - val_out_15_loss: 0.9502 - val_out_16_loss: 0.9449 - val_out_17_loss: 0.9442 - val_out_18_loss: 0.9467 - val_out_19_loss: 0.9467 - val_out_20_loss: 0.9411 - val_out_21_loss: 0.9428 - val_out_22_loss: 0.9366 - val_out_23_loss: 0.9489 - val_out_24_loss: 0.9453 - val_out_25_loss: 0.9463 - val_out_26_loss: 0.9427 - val_out_27_loss: 0.9479 - val_out_28_loss: 0.9459 - val_out_29_loss: 0.9437 - val_out_30_loss: 0.9429 - val_out_31_loss: 0.9427 - val_out_32_loss: 0.9457 - val_out_acc: 0.6594 - val_out_0_acc: 0.7874 - val_out_1_acc: 0.6551 - val_out_2_acc: 0.6638 - val_out_3_acc: 0.6573 - val_out_4_acc: 0.6551 - val_out_5_acc: 0.6638 - val_out_6_acc: 0.6573 - val_out_7_acc: 0.6594 - val_out_8_acc: 0.6638 - val_out_9_acc: 0.6616 - val_out_10_acc: 0.6659 - val_out_11_acc: 0.6594 - val_out_12_acc: 0.6551 - val_out_13_acc: 0.6573 - val_out_14_acc: 0.6573 - val_out_15_acc: 0.6508 - val_out_16_acc: 0.6616 - val_out_17_acc: 0.6594 - val_out_18_acc: 0.6616 - val_out_19_acc: 0.6486 - val_out_20_acc: 0.6616 - val_out_21_acc: 0.6638 - val_out_22_acc: 0.6616 - val_out_23_acc: 0.6464 - val_out_24_acc: 0.6659 - val_out_25_acc: 0.6638 - val_out_26_acc: 0.6616 - val_out_27_acc: 0.6551 - val_out_28_acc: 0.6594 - val_out_29_acc: 0.6594 - val_out_30_acc: 0.6616 - val_out_31_acc: 0.6616 - val_out_32_acc: 0.6594\n",
      "Epoch 74/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 31.2445 - out_loss: 0.8240 - out_0_loss: 0.6643 - out_1_loss: 0.9235 - out_2_loss: 0.9296 - out_3_loss: 0.9207 - out_4_loss: 0.9340 - out_5_loss: 0.9372 - out_6_loss: 0.9245 - out_7_loss: 0.9299 - out_8_loss: 0.9320 - out_9_loss: 0.9211 - out_10_loss: 0.9227 - out_11_loss: 0.9317 - out_12_loss: 0.9264 - out_13_loss: 0.9066 - out_14_loss: 0.9401 - out_15_loss: 0.9283 - out_16_loss: 0.9377 - out_17_loss: 0.9300 - out_18_loss: 0.9152 - out_19_loss: 0.9369 - out_20_loss: 0.9331 - out_21_loss: 0.9405 - out_22_loss: 0.9157 - out_23_loss: 0.9341 - out_24_loss: 0.9277 - out_25_loss: 0.9378 - out_26_loss: 0.9188 - out_27_loss: 0.9352 - out_28_loss: 0.9391 - out_29_loss: 0.9316 - out_30_loss: 0.9376 - out_31_loss: 0.9448 - out_32_loss: 0.9322 - out_acc: 0.7126 - out_0_acc: 0.7632 - out_1_acc: 0.6782 - out_2_acc: 0.6655 - out_3_acc: 0.6736 - out_4_acc: 0.6705 - out_5_acc: 0.6692 - out_6_acc: 0.6705 - out_7_acc: 0.6742 - out_8_acc: 0.6727 - out_9_acc: 0.6717 - out_10_acc: 0.6723 - out_11_acc: 0.6745 - out_12_acc: 0.6637 - out_13_acc: 0.6742 - out_14_acc: 0.6767 - out_15_acc: 0.6789 - out_16_acc: 0.6596 - out_17_acc: 0.6677 - out_18_acc: 0.6773 - out_19_acc: 0.6677 - out_20_acc: 0.6705 - out_21_acc: 0.6634 - out_22_acc: 0.6730 - out_23_acc: 0.6680 - out_24_acc: 0.6723 - out_25_acc: 0.6627 - out_26_acc: 0.6792 - out_27_acc: 0.6692 - out_28_acc: 0.6547 - out_29_acc: 0.6668 - out_30_acc: 0.6631 - out_31_acc: 0.6668 - out_32_acc: 0.6683 - val_loss: 28.9672 - val_out_loss: 0.7993 - val_out_0_loss: 0.6302 - val_out_1_loss: 0.8269 - val_out_2_loss: 0.8252 - val_out_3_loss: 0.8142 - val_out_4_loss: 0.8264 - val_out_5_loss: 0.8216 - val_out_6_loss: 0.8241 - val_out_7_loss: 0.8260 - val_out_8_loss: 0.8258 - val_out_9_loss: 0.8208 - val_out_10_loss: 0.8228 - val_out_11_loss: 0.8287 - val_out_12_loss: 0.8223 - val_out_13_loss: 0.8219 - val_out_14_loss: 0.8308 - val_out_15_loss: 0.8240 - val_out_16_loss: 0.8258 - val_out_17_loss: 0.8223 - val_out_18_loss: 0.8255 - val_out_19_loss: 0.8325 - val_out_20_loss: 0.8266 - val_out_21_loss: 0.8217 - val_out_22_loss: 0.8252 - val_out_23_loss: 0.8251 - val_out_24_loss: 0.8248 - val_out_25_loss: 0.8315 - val_out_26_loss: 0.8233 - val_out_27_loss: 0.8284 - val_out_28_loss: 0.8280 - val_out_29_loss: 0.8222 - val_out_30_loss: 0.8242 - val_out_31_loss: 0.8226 - val_out_32_loss: 0.8196 - val_out_acc: 0.7007 - val_out_0_acc: 0.7679 - val_out_1_acc: 0.6920 - val_out_2_acc: 0.6855 - val_out_3_acc: 0.6941 - val_out_4_acc: 0.6855 - val_out_5_acc: 0.6920 - val_out_6_acc: 0.6855 - val_out_7_acc: 0.6985 - val_out_8_acc: 0.6855 - val_out_9_acc: 0.6898 - val_out_10_acc: 0.6833 - val_out_11_acc: 0.6920 - val_out_12_acc: 0.6898 - val_out_13_acc: 0.6898 - val_out_14_acc: 0.6941 - val_out_15_acc: 0.6855 - val_out_16_acc: 0.6833 - val_out_17_acc: 0.6941 - val_out_18_acc: 0.6941 - val_out_19_acc: 0.6811 - val_out_20_acc: 0.6811 - val_out_21_acc: 0.6898 - val_out_22_acc: 0.6898 - val_out_23_acc: 0.6833 - val_out_24_acc: 0.6876 - val_out_25_acc: 0.6941 - val_out_26_acc: 0.6768 - val_out_27_acc: 0.6811 - val_out_28_acc: 0.6920 - val_out_29_acc: 0.6898 - val_out_30_acc: 0.6898 - val_out_31_acc: 0.6811 - val_out_32_acc: 0.6941\n",
      "Epoch 75/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 30.5870 - out_loss: 0.8070 - out_0_loss: 0.6316 - out_1_loss: 0.9049 - out_2_loss: 0.9039 - out_3_loss: 0.9067 - out_4_loss: 0.9103 - out_5_loss: 0.8978 - out_6_loss: 0.9005 - out_7_loss: 0.9227 - out_8_loss: 0.9122 - out_9_loss: 0.9088 - out_10_loss: 0.9126 - out_11_loss: 0.9181 - out_12_loss: 0.9120 - out_13_loss: 0.8971 - out_14_loss: 0.9214 - out_15_loss: 0.8974 - out_16_loss: 0.9161 - out_17_loss: 0.9108 - out_18_loss: 0.9041 - out_19_loss: 0.9142 - out_20_loss: 0.8964 - out_21_loss: 0.9138 - out_22_loss: 0.9164 - out_23_loss: 0.9122 - out_24_loss: 0.9209 - out_25_loss: 0.9122 - out_26_loss: 0.9174 - out_27_loss: 0.9167 - out_28_loss: 0.9240 - out_29_loss: 0.9024 - out_30_loss: 0.9097 - out_31_loss: 0.9192 - out_32_loss: 0.9153 - out_acc: 0.7216 - out_0_acc: 0.7725 - out_1_acc: 0.6823 - out_2_acc: 0.6767 - out_3_acc: 0.6838 - out_4_acc: 0.6832 - out_5_acc: 0.6863 - out_6_acc: 0.6761 - out_7_acc: 0.6748 - out_8_acc: 0.6838 - out_9_acc: 0.6770 - out_10_acc: 0.6798 - out_11_acc: 0.6795 - out_12_acc: 0.6813 - out_13_acc: 0.6851 - out_14_acc: 0.6764 - out_15_acc: 0.6897 - out_16_acc: 0.6810 - out_17_acc: 0.6782 - out_18_acc: 0.6813 - out_19_acc: 0.6758 - out_20_acc: 0.6829 - out_21_acc: 0.6742 - out_22_acc: 0.6789 - out_23_acc: 0.6835 - out_24_acc: 0.6807 - out_25_acc: 0.6699 - out_26_acc: 0.6761 - out_27_acc: 0.6714 - out_28_acc: 0.6674 - out_29_acc: 0.6745 - out_30_acc: 0.6779 - out_31_acc: 0.6665 - out_32_acc: 0.6640 - val_loss: 31.5745 - val_out_loss: 0.8720 - val_out_0_loss: 0.6087 - val_out_1_loss: 0.9026 - val_out_2_loss: 0.8977 - val_out_3_loss: 0.9026 - val_out_4_loss: 0.8998 - val_out_5_loss: 0.9000 - val_out_6_loss: 0.9053 - val_out_7_loss: 0.9098 - val_out_8_loss: 0.9046 - val_out_9_loss: 0.9008 - val_out_10_loss: 0.9014 - val_out_11_loss: 0.9002 - val_out_12_loss: 0.8990 - val_out_13_loss: 0.8970 - val_out_14_loss: 0.8991 - val_out_15_loss: 0.9021 - val_out_16_loss: 0.8972 - val_out_17_loss: 0.9039 - val_out_18_loss: 0.9003 - val_out_19_loss: 0.9026 - val_out_20_loss: 0.8990 - val_out_21_loss: 0.8940 - val_out_22_loss: 0.8967 - val_out_23_loss: 0.9016 - val_out_24_loss: 0.9078 - val_out_25_loss: 0.8996 - val_out_26_loss: 0.9035 - val_out_27_loss: 0.9048 - val_out_28_loss: 0.9040 - val_out_29_loss: 0.9026 - val_out_30_loss: 0.9040 - val_out_31_loss: 0.9030 - val_out_32_loss: 0.8977 - val_out_acc: 0.6616 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.6529 - val_out_2_acc: 0.6529 - val_out_3_acc: 0.6508 - val_out_4_acc: 0.6421 - val_out_5_acc: 0.6443 - val_out_6_acc: 0.6508 - val_out_7_acc: 0.6508 - val_out_8_acc: 0.6464 - val_out_9_acc: 0.6508 - val_out_10_acc: 0.6594 - val_out_11_acc: 0.6551 - val_out_12_acc: 0.6573 - val_out_13_acc: 0.6573 - val_out_14_acc: 0.6508 - val_out_15_acc: 0.6573 - val_out_16_acc: 0.6508 - val_out_17_acc: 0.6464 - val_out_18_acc: 0.6529 - val_out_19_acc: 0.6464 - val_out_20_acc: 0.6464 - val_out_21_acc: 0.6616 - val_out_22_acc: 0.6529 - val_out_23_acc: 0.6529 - val_out_24_acc: 0.6573 - val_out_25_acc: 0.6508 - val_out_26_acc: 0.6551 - val_out_27_acc: 0.6464 - val_out_28_acc: 0.6529 - val_out_29_acc: 0.6573 - val_out_30_acc: 0.6464 - val_out_31_acc: 0.6551 - val_out_32_acc: 0.6508\n",
      "Epoch 76/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 31.0648 - out_loss: 0.8167 - out_0_loss: 0.6202 - out_1_loss: 0.9337 - out_2_loss: 0.9145 - out_3_loss: 0.9226 - out_4_loss: 0.9108 - out_5_loss: 0.9213 - out_6_loss: 0.9250 - out_7_loss: 0.9263 - out_8_loss: 0.9329 - out_9_loss: 0.9230 - out_10_loss: 0.9326 - out_11_loss: 0.9261 - out_12_loss: 0.9298 - out_13_loss: 0.9305 - out_14_loss: 0.9233 - out_15_loss: 0.9217 - out_16_loss: 0.9316 - out_17_loss: 0.9268 - out_18_loss: 0.9163 - out_19_loss: 0.9426 - out_20_loss: 0.9349 - out_21_loss: 0.9383 - out_22_loss: 0.9319 - out_23_loss: 0.9195 - out_24_loss: 0.9244 - out_25_loss: 0.9202 - out_26_loss: 0.9335 - out_27_loss: 0.9205 - out_28_loss: 0.9306 - out_29_loss: 0.9159 - out_30_loss: 0.9172 - out_31_loss: 0.9302 - out_32_loss: 0.9195 - out_acc: 0.7099 - out_0_acc: 0.7821 - out_1_acc: 0.6714 - out_2_acc: 0.6804 - out_3_acc: 0.6612 - out_4_acc: 0.6702 - out_5_acc: 0.6723 - out_6_acc: 0.6671 - out_7_acc: 0.6603 - out_8_acc: 0.6569 - out_9_acc: 0.6736 - out_10_acc: 0.6683 - out_11_acc: 0.6668 - out_12_acc: 0.6615 - out_13_acc: 0.6600 - out_14_acc: 0.6677 - out_15_acc: 0.6646 - out_16_acc: 0.6590 - out_17_acc: 0.6717 - out_18_acc: 0.6739 - out_19_acc: 0.6640 - out_20_acc: 0.6578 - out_21_acc: 0.6680 - out_22_acc: 0.6612 - out_23_acc: 0.6646 - out_24_acc: 0.6627 - out_25_acc: 0.6658 - out_26_acc: 0.6621 - out_27_acc: 0.6680 - out_28_acc: 0.6590 - out_29_acc: 0.6699 - out_30_acc: 0.6652 - out_31_acc: 0.6674 - out_32_acc: 0.6662 - val_loss: 30.7900 - val_out_loss: 0.8484 - val_out_0_loss: 0.6297 - val_out_1_loss: 0.8793 - val_out_2_loss: 0.8758 - val_out_3_loss: 0.8673 - val_out_4_loss: 0.8843 - val_out_5_loss: 0.8779 - val_out_6_loss: 0.8752 - val_out_7_loss: 0.8858 - val_out_8_loss: 0.8751 - val_out_9_loss: 0.8775 - val_out_10_loss: 0.8782 - val_out_11_loss: 0.8783 - val_out_12_loss: 0.8764 - val_out_13_loss: 0.8791 - val_out_14_loss: 0.8828 - val_out_15_loss: 0.8802 - val_out_16_loss: 0.8734 - val_out_17_loss: 0.8821 - val_out_18_loss: 0.8805 - val_out_19_loss: 0.8874 - val_out_20_loss: 0.8826 - val_out_21_loss: 0.8738 - val_out_22_loss: 0.8757 - val_out_23_loss: 0.8774 - val_out_24_loss: 0.8756 - val_out_25_loss: 0.8817 - val_out_26_loss: 0.8790 - val_out_27_loss: 0.8738 - val_out_28_loss: 0.8816 - val_out_29_loss: 0.8734 - val_out_30_loss: 0.8727 - val_out_31_loss: 0.8760 - val_out_32_loss: 0.8733 - val_out_acc: 0.7050 - val_out_0_acc: 0.7722 - val_out_1_acc: 0.7007 - val_out_2_acc: 0.6941 - val_out_3_acc: 0.6963 - val_out_4_acc: 0.7050 - val_out_5_acc: 0.7028 - val_out_6_acc: 0.7007 - val_out_7_acc: 0.7028 - val_out_8_acc: 0.7050 - val_out_9_acc: 0.7050 - val_out_10_acc: 0.7007 - val_out_11_acc: 0.7028 - val_out_12_acc: 0.7050 - val_out_13_acc: 0.7028 - val_out_14_acc: 0.6920 - val_out_15_acc: 0.7028 - val_out_16_acc: 0.6985 - val_out_17_acc: 0.7007 - val_out_18_acc: 0.7028 - val_out_19_acc: 0.6963 - val_out_20_acc: 0.6920 - val_out_21_acc: 0.7028 - val_out_22_acc: 0.7050 - val_out_23_acc: 0.6920 - val_out_24_acc: 0.7115 - val_out_25_acc: 0.7050 - val_out_26_acc: 0.6985 - val_out_27_acc: 0.7050 - val_out_28_acc: 0.6941 - val_out_29_acc: 0.7050 - val_out_30_acc: 0.7007 - val_out_31_acc: 0.7007 - val_out_32_acc: 0.7007\n",
      "Epoch 77/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 30.5824 - out_loss: 0.8062 - out_0_loss: 0.6214 - out_1_loss: 0.9067 - out_2_loss: 0.9087 - out_3_loss: 0.9161 - out_4_loss: 0.9164 - out_5_loss: 0.9043 - out_6_loss: 0.9077 - out_7_loss: 0.9126 - out_8_loss: 0.8968 - out_9_loss: 0.9018 - out_10_loss: 0.9058 - out_11_loss: 0.9295 - out_12_loss: 0.9064 - out_13_loss: 0.9084 - out_14_loss: 0.9255 - out_15_loss: 0.9309 - out_16_loss: 0.9029 - out_17_loss: 0.8954 - out_18_loss: 0.9110 - out_19_loss: 0.9160 - out_20_loss: 0.9015 - out_21_loss: 0.9082 - out_22_loss: 0.9096 - out_23_loss: 0.9146 - out_24_loss: 0.9114 - out_25_loss: 0.9081 - out_26_loss: 0.9191 - out_27_loss: 0.9158 - out_28_loss: 0.9083 - out_29_loss: 0.9218 - out_30_loss: 0.9114 - out_31_loss: 0.9109 - out_32_loss: 0.9113 - out_acc: 0.7226 - out_0_acc: 0.7864 - out_1_acc: 0.6807 - out_2_acc: 0.6761 - out_3_acc: 0.6748 - out_4_acc: 0.6764 - out_5_acc: 0.6773 - out_6_acc: 0.6795 - out_7_acc: 0.6699 - out_8_acc: 0.6742 - out_9_acc: 0.6813 - out_10_acc: 0.6798 - out_11_acc: 0.6785 - out_12_acc: 0.6745 - out_13_acc: 0.6689 - out_14_acc: 0.6723 - out_15_acc: 0.6702 - out_16_acc: 0.6807 - out_17_acc: 0.6779 - out_18_acc: 0.6683 - out_19_acc: 0.6658 - out_20_acc: 0.6758 - out_21_acc: 0.6720 - out_22_acc: 0.6795 - out_23_acc: 0.6692 - out_24_acc: 0.6792 - out_25_acc: 0.6730 - out_26_acc: 0.6801 - out_27_acc: 0.6649 - out_28_acc: 0.6779 - out_29_acc: 0.6717 - out_30_acc: 0.6692 - out_31_acc: 0.6770 - out_32_acc: 0.6733 - val_loss: 32.0569 - val_out_loss: 0.8771 - val_out_0_loss: 0.5777 - val_out_1_loss: 0.9156 - val_out_2_loss: 0.9200 - val_out_3_loss: 0.9214 - val_out_4_loss: 0.9193 - val_out_5_loss: 0.9131 - val_out_6_loss: 0.9170 - val_out_7_loss: 0.9175 - val_out_8_loss: 0.9130 - val_out_9_loss: 0.9149 - val_out_10_loss: 0.9157 - val_out_11_loss: 0.9199 - val_out_12_loss: 0.9160 - val_out_13_loss: 0.9112 - val_out_14_loss: 0.9195 - val_out_15_loss: 0.9193 - val_out_16_loss: 0.9189 - val_out_17_loss: 0.9134 - val_out_18_loss: 0.9140 - val_out_19_loss: 0.9227 - val_out_20_loss: 0.9094 - val_out_21_loss: 0.9108 - val_out_22_loss: 0.9157 - val_out_23_loss: 0.9190 - val_out_24_loss: 0.9192 - val_out_25_loss: 0.9205 - val_out_26_loss: 0.9166 - val_out_27_loss: 0.9201 - val_out_28_loss: 0.9140 - val_out_29_loss: 0.9190 - val_out_30_loss: 0.9127 - val_out_31_loss: 0.9182 - val_out_32_loss: 0.9155 - val_out_acc: 0.6725 - val_out_0_acc: 0.7939 - val_out_1_acc: 0.6659 - val_out_2_acc: 0.6616 - val_out_3_acc: 0.6659 - val_out_4_acc: 0.6725 - val_out_5_acc: 0.6681 - val_out_6_acc: 0.6638 - val_out_7_acc: 0.6616 - val_out_8_acc: 0.6681 - val_out_9_acc: 0.6659 - val_out_10_acc: 0.6594 - val_out_11_acc: 0.6659 - val_out_12_acc: 0.6681 - val_out_13_acc: 0.6681 - val_out_14_acc: 0.6594 - val_out_15_acc: 0.6616 - val_out_16_acc: 0.6638 - val_out_17_acc: 0.6703 - val_out_18_acc: 0.6638 - val_out_19_acc: 0.6659 - val_out_20_acc: 0.6594 - val_out_21_acc: 0.6638 - val_out_22_acc: 0.6703 - val_out_23_acc: 0.6681 - val_out_24_acc: 0.6659 - val_out_25_acc: 0.6638 - val_out_26_acc: 0.6638 - val_out_27_acc: 0.6594 - val_out_28_acc: 0.6681 - val_out_29_acc: 0.6659 - val_out_30_acc: 0.6681 - val_out_31_acc: 0.6638 - val_out_32_acc: 0.6616\n",
      "Epoch 78/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 31.0128 - out_loss: 0.8171 - out_0_loss: 0.6348 - out_1_loss: 0.9147 - out_2_loss: 0.9213 - out_3_loss: 0.9182 - out_4_loss: 0.9191 - out_5_loss: 0.9112 - out_6_loss: 0.9172 - out_7_loss: 0.9385 - out_8_loss: 0.9188 - out_9_loss: 0.9198 - out_10_loss: 0.9186 - out_11_loss: 0.9213 - out_12_loss: 0.9174 - out_13_loss: 0.9214 - out_14_loss: 0.9341 - out_15_loss: 0.9261 - out_16_loss: 0.9123 - out_17_loss: 0.9342 - out_18_loss: 0.9334 - out_19_loss: 0.9227 - out_20_loss: 0.9360 - out_21_loss: 0.9357 - out_22_loss: 0.9289 - out_23_loss: 0.9329 - out_24_loss: 0.9161 - out_25_loss: 0.9301 - out_26_loss: 0.9298 - out_27_loss: 0.9324 - out_28_loss: 0.9219 - out_29_loss: 0.9229 - out_30_loss: 0.9157 - out_31_loss: 0.9247 - out_32_loss: 0.9136 - out_acc: 0.7105 - out_0_acc: 0.7855 - out_1_acc: 0.6813 - out_2_acc: 0.6689 - out_3_acc: 0.6658 - out_4_acc: 0.6761 - out_5_acc: 0.6723 - out_6_acc: 0.6767 - out_7_acc: 0.6723 - out_8_acc: 0.6742 - out_9_acc: 0.6702 - out_10_acc: 0.6829 - out_11_acc: 0.6798 - out_12_acc: 0.6739 - out_13_acc: 0.6761 - out_14_acc: 0.6754 - out_15_acc: 0.6748 - out_16_acc: 0.6717 - out_17_acc: 0.6764 - out_18_acc: 0.6668 - out_19_acc: 0.6739 - out_20_acc: 0.6665 - out_21_acc: 0.6612 - out_22_acc: 0.6717 - out_23_acc: 0.6668 - out_24_acc: 0.6717 - out_25_acc: 0.6733 - out_26_acc: 0.6686 - out_27_acc: 0.6723 - out_28_acc: 0.6789 - out_29_acc: 0.6779 - out_30_acc: 0.6773 - out_31_acc: 0.6677 - out_32_acc: 0.6727 - val_loss: 28.3644 - val_out_loss: 0.7813 - val_out_0_loss: 0.6190 - val_out_1_loss: 0.8083 - val_out_2_loss: 0.8085 - val_out_3_loss: 0.8011 - val_out_4_loss: 0.8117 - val_out_5_loss: 0.8060 - val_out_6_loss: 0.8018 - val_out_7_loss: 0.8088 - val_out_8_loss: 0.8061 - val_out_9_loss: 0.8057 - val_out_10_loss: 0.8050 - val_out_11_loss: 0.8120 - val_out_12_loss: 0.8020 - val_out_13_loss: 0.8063 - val_out_14_loss: 0.8245 - val_out_15_loss: 0.8097 - val_out_16_loss: 0.8072 - val_out_17_loss: 0.8057 - val_out_18_loss: 0.8078 - val_out_19_loss: 0.8125 - val_out_20_loss: 0.8068 - val_out_21_loss: 0.8022 - val_out_22_loss: 0.8072 - val_out_23_loss: 0.8075 - val_out_24_loss: 0.8165 - val_out_25_loss: 0.8084 - val_out_26_loss: 0.8088 - val_out_27_loss: 0.8041 - val_out_28_loss: 0.8050 - val_out_29_loss: 0.8065 - val_out_30_loss: 0.8122 - val_out_31_loss: 0.8029 - val_out_32_loss: 0.8025 - val_out_acc: 0.6941 - val_out_0_acc: 0.7874 - val_out_1_acc: 0.6855 - val_out_2_acc: 0.7007 - val_out_3_acc: 0.6833 - val_out_4_acc: 0.7050 - val_out_5_acc: 0.6855 - val_out_6_acc: 0.6920 - val_out_7_acc: 0.6941 - val_out_8_acc: 0.6963 - val_out_9_acc: 0.6941 - val_out_10_acc: 0.6920 - val_out_11_acc: 0.6876 - val_out_12_acc: 0.6833 - val_out_13_acc: 0.7050 - val_out_14_acc: 0.6920 - val_out_15_acc: 0.7007 - val_out_16_acc: 0.6963 - val_out_17_acc: 0.6920 - val_out_18_acc: 0.6898 - val_out_19_acc: 0.6855 - val_out_20_acc: 0.6920 - val_out_21_acc: 0.6876 - val_out_22_acc: 0.6920 - val_out_23_acc: 0.6876 - val_out_24_acc: 0.6898 - val_out_25_acc: 0.6920 - val_out_26_acc: 0.6941 - val_out_27_acc: 0.6876 - val_out_28_acc: 0.7007 - val_out_29_acc: 0.6963 - val_out_30_acc: 0.6811 - val_out_31_acc: 0.6941 - val_out_32_acc: 0.6985\n",
      "Epoch 79/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 30.3166 - out_loss: 0.7974 - out_0_loss: 0.6092 - out_1_loss: 0.9061 - out_2_loss: 0.8936 - out_3_loss: 0.9009 - out_4_loss: 0.9025 - out_5_loss: 0.9134 - out_6_loss: 0.9044 - out_7_loss: 0.9074 - out_8_loss: 0.8974 - out_9_loss: 0.9021 - out_10_loss: 0.9154 - out_11_loss: 0.9030 - out_12_loss: 0.8848 - out_13_loss: 0.9052 - out_14_loss: 0.9099 - out_15_loss: 0.8962 - out_16_loss: 0.9013 - out_17_loss: 0.9089 - out_18_loss: 0.9022 - out_19_loss: 0.9004 - out_20_loss: 0.9067 - out_21_loss: 0.9257 - out_22_loss: 0.8947 - out_23_loss: 0.9048 - out_24_loss: 0.8974 - out_25_loss: 0.8872 - out_26_loss: 0.9149 - out_27_loss: 0.8894 - out_28_loss: 0.9037 - out_29_loss: 0.9091 - out_30_loss: 0.9114 - out_31_loss: 0.8910 - out_32_loss: 0.9187 - out_acc: 0.7229 - out_0_acc: 0.7821 - out_1_acc: 0.6764 - out_2_acc: 0.6847 - out_3_acc: 0.6736 - out_4_acc: 0.6810 - out_5_acc: 0.6708 - out_6_acc: 0.6804 - out_7_acc: 0.6826 - out_8_acc: 0.6872 - out_9_acc: 0.6804 - out_10_acc: 0.6720 - out_11_acc: 0.6751 - out_12_acc: 0.6816 - out_13_acc: 0.6826 - out_14_acc: 0.6826 - out_15_acc: 0.6782 - out_16_acc: 0.6761 - out_17_acc: 0.6742 - out_18_acc: 0.6810 - out_19_acc: 0.6872 - out_20_acc: 0.6801 - out_21_acc: 0.6813 - out_22_acc: 0.6885 - out_23_acc: 0.6888 - out_24_acc: 0.6866 - out_25_acc: 0.6826 - out_26_acc: 0.6810 - out_27_acc: 0.6841 - out_28_acc: 0.6820 - out_29_acc: 0.6854 - out_30_acc: 0.6838 - out_31_acc: 0.6835 - out_32_acc: 0.6733 - val_loss: 28.8338 - val_out_loss: 0.7988 - val_out_0_loss: 0.6255 - val_out_1_loss: 0.8196 - val_out_2_loss: 0.8172 - val_out_3_loss: 0.8157 - val_out_4_loss: 0.8244 - val_out_5_loss: 0.8183 - val_out_6_loss: 0.8231 - val_out_7_loss: 0.8167 - val_out_8_loss: 0.8243 - val_out_9_loss: 0.8244 - val_out_10_loss: 0.8163 - val_out_11_loss: 0.8227 - val_out_12_loss: 0.8206 - val_out_13_loss: 0.8192 - val_out_14_loss: 0.8301 - val_out_15_loss: 0.8182 - val_out_16_loss: 0.8215 - val_out_17_loss: 0.8197 - val_out_18_loss: 0.8216 - val_out_19_loss: 0.8272 - val_out_20_loss: 0.8274 - val_out_21_loss: 0.8169 - val_out_22_loss: 0.8186 - val_out_23_loss: 0.8222 - val_out_24_loss: 0.8230 - val_out_25_loss: 0.8224 - val_out_26_loss: 0.8204 - val_out_27_loss: 0.8233 - val_out_28_loss: 0.8201 - val_out_29_loss: 0.8193 - val_out_30_loss: 0.8218 - val_out_31_loss: 0.8208 - val_out_32_loss: 0.8115 - val_out_acc: 0.7267 - val_out_0_acc: 0.7766 - val_out_1_acc: 0.7158 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7202 - val_out_4_acc: 0.7137 - val_out_5_acc: 0.7245 - val_out_6_acc: 0.7180 - val_out_7_acc: 0.7267 - val_out_8_acc: 0.7289 - val_out_9_acc: 0.7158 - val_out_10_acc: 0.7267 - val_out_11_acc: 0.7202 - val_out_12_acc: 0.7115 - val_out_13_acc: 0.7245 - val_out_14_acc: 0.7115 - val_out_15_acc: 0.7180 - val_out_16_acc: 0.7223 - val_out_17_acc: 0.7223 - val_out_18_acc: 0.7245 - val_out_19_acc: 0.7180 - val_out_20_acc: 0.7158 - val_out_21_acc: 0.7245 - val_out_22_acc: 0.7267 - val_out_23_acc: 0.7289 - val_out_24_acc: 0.7267 - val_out_25_acc: 0.7202 - val_out_26_acc: 0.7245 - val_out_27_acc: 0.7158 - val_out_28_acc: 0.7158 - val_out_29_acc: 0.7202 - val_out_30_acc: 0.7202 - val_out_31_acc: 0.7267 - val_out_32_acc: 0.7158\n",
      "Epoch 80/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 30.3259 - out_loss: 0.8008 - out_0_loss: 0.6045 - out_1_loss: 0.8958 - out_2_loss: 0.9022 - out_3_loss: 0.9142 - out_4_loss: 0.9088 - out_5_loss: 0.9099 - out_6_loss: 0.9118 - out_7_loss: 0.9037 - out_8_loss: 0.9037 - out_9_loss: 0.9141 - out_10_loss: 0.8997 - out_11_loss: 0.9173 - out_12_loss: 0.9033 - out_13_loss: 0.8940 - out_14_loss: 0.8985 - out_15_loss: 0.8926 - out_16_loss: 0.9107 - out_17_loss: 0.9151 - out_18_loss: 0.9072 - out_19_loss: 0.9038 - out_20_loss: 0.9092 - out_21_loss: 0.8977 - out_22_loss: 0.8953 - out_23_loss: 0.9048 - out_24_loss: 0.9015 - out_25_loss: 0.9096 - out_26_loss: 0.9049 - out_27_loss: 0.8880 - out_28_loss: 0.8980 - out_29_loss: 0.8967 - out_30_loss: 0.8976 - out_31_loss: 0.9076 - out_32_loss: 0.9034 - out_acc: 0.7092 - out_0_acc: 0.7877 - out_1_acc: 0.6810 - out_2_acc: 0.6637 - out_3_acc: 0.6720 - out_4_acc: 0.6733 - out_5_acc: 0.6696 - out_6_acc: 0.6699 - out_7_acc: 0.6754 - out_8_acc: 0.6730 - out_9_acc: 0.6643 - out_10_acc: 0.6748 - out_11_acc: 0.6677 - out_12_acc: 0.6702 - out_13_acc: 0.6723 - out_14_acc: 0.6609 - out_15_acc: 0.6748 - out_16_acc: 0.6823 - out_17_acc: 0.6683 - out_18_acc: 0.6699 - out_19_acc: 0.6720 - out_20_acc: 0.6683 - out_21_acc: 0.6767 - out_22_acc: 0.6677 - out_23_acc: 0.6662 - out_24_acc: 0.6689 - out_25_acc: 0.6686 - out_26_acc: 0.6702 - out_27_acc: 0.6795 - out_28_acc: 0.6798 - out_29_acc: 0.6789 - out_30_acc: 0.6779 - out_31_acc: 0.6655 - out_32_acc: 0.6745 - val_loss: 29.9245 - val_out_loss: 0.8271 - val_out_0_loss: 0.6078 - val_out_1_loss: 0.8554 - val_out_2_loss: 0.8547 - val_out_3_loss: 0.8500 - val_out_4_loss: 0.8504 - val_out_5_loss: 0.8555 - val_out_6_loss: 0.8504 - val_out_7_loss: 0.8539 - val_out_8_loss: 0.8520 - val_out_9_loss: 0.8544 - val_out_10_loss: 0.8562 - val_out_11_loss: 0.8547 - val_out_12_loss: 0.8516 - val_out_13_loss: 0.8501 - val_out_14_loss: 0.8535 - val_out_15_loss: 0.8508 - val_out_16_loss: 0.8524 - val_out_17_loss: 0.8513 - val_out_18_loss: 0.8501 - val_out_19_loss: 0.8605 - val_out_20_loss: 0.8569 - val_out_21_loss: 0.8511 - val_out_22_loss: 0.8511 - val_out_23_loss: 0.8542 - val_out_24_loss: 0.8629 - val_out_25_loss: 0.8512 - val_out_26_loss: 0.8551 - val_out_27_loss: 0.8492 - val_out_28_loss: 0.8533 - val_out_29_loss: 0.8550 - val_out_30_loss: 0.8594 - val_out_31_loss: 0.8468 - val_out_32_loss: 0.8515 - val_out_acc: 0.6985 - val_out_0_acc: 0.8134 - val_out_1_acc: 0.6985 - val_out_2_acc: 0.6790 - val_out_3_acc: 0.6876 - val_out_4_acc: 0.6941 - val_out_5_acc: 0.6941 - val_out_6_acc: 0.6898 - val_out_7_acc: 0.6876 - val_out_8_acc: 0.6941 - val_out_9_acc: 0.6941 - val_out_10_acc: 0.6920 - val_out_11_acc: 0.6963 - val_out_12_acc: 0.6920 - val_out_13_acc: 0.6876 - val_out_14_acc: 0.6920 - val_out_15_acc: 0.6898 - val_out_16_acc: 0.6790 - val_out_17_acc: 0.6963 - val_out_18_acc: 0.6941 - val_out_19_acc: 0.6941 - val_out_20_acc: 0.6920 - val_out_21_acc: 0.6963 - val_out_22_acc: 0.6855 - val_out_23_acc: 0.6898 - val_out_24_acc: 0.6811 - val_out_25_acc: 0.6985 - val_out_26_acc: 0.6920 - val_out_27_acc: 0.6941 - val_out_28_acc: 0.6920 - val_out_29_acc: 0.6898 - val_out_30_acc: 0.6855 - val_out_31_acc: 0.6985 - val_out_32_acc: 0.6941\n",
      "Epoch 81/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 30.5639 - out_loss: 0.8067 - out_0_loss: 0.6298 - out_1_loss: 0.9059 - out_2_loss: 0.9055 - out_3_loss: 0.9135 - out_4_loss: 0.9076 - out_5_loss: 0.9103 - out_6_loss: 0.8957 - out_7_loss: 0.9230 - out_8_loss: 0.9041 - out_9_loss: 0.9010 - out_10_loss: 0.9052 - out_11_loss: 0.9028 - out_12_loss: 0.9152 - out_13_loss: 0.9148 - out_14_loss: 0.9084 - out_15_loss: 0.9039 - out_16_loss: 0.9223 - out_17_loss: 0.9088 - out_18_loss: 0.9046 - out_19_loss: 0.9092 - out_20_loss: 0.9161 - out_21_loss: 0.9198 - out_22_loss: 0.9090 - out_23_loss: 0.9317 - out_24_loss: 0.9118 - out_25_loss: 0.9140 - out_26_loss: 0.9194 - out_27_loss: 0.9040 - out_28_loss: 0.9075 - out_29_loss: 0.8966 - out_30_loss: 0.9067 - out_31_loss: 0.9105 - out_32_loss: 0.9183 - out_acc: 0.7195 - out_0_acc: 0.7799 - out_1_acc: 0.6758 - out_2_acc: 0.6835 - out_3_acc: 0.6699 - out_4_acc: 0.6767 - out_5_acc: 0.6785 - out_6_acc: 0.6801 - out_7_acc: 0.6708 - out_8_acc: 0.6770 - out_9_acc: 0.6758 - out_10_acc: 0.6835 - out_11_acc: 0.6782 - out_12_acc: 0.6720 - out_13_acc: 0.6832 - out_14_acc: 0.6730 - out_15_acc: 0.6860 - out_16_acc: 0.6606 - out_17_acc: 0.6785 - out_18_acc: 0.6782 - out_19_acc: 0.6748 - out_20_acc: 0.6702 - out_21_acc: 0.6711 - out_22_acc: 0.6810 - out_23_acc: 0.6683 - out_24_acc: 0.6742 - out_25_acc: 0.6829 - out_26_acc: 0.6782 - out_27_acc: 0.6733 - out_28_acc: 0.6882 - out_29_acc: 0.6816 - out_30_acc: 0.6723 - out_31_acc: 0.6795 - out_32_acc: 0.6739 - val_loss: 29.6039 - val_out_loss: 0.8215 - val_out_0_loss: 0.6574 - val_out_1_loss: 0.8414 - val_out_2_loss: 0.8434 - val_out_3_loss: 0.8401 - val_out_4_loss: 0.8489 - val_out_5_loss: 0.8406 - val_out_6_loss: 0.8413 - val_out_7_loss: 0.8409 - val_out_8_loss: 0.8397 - val_out_9_loss: 0.8428 - val_out_10_loss: 0.8433 - val_out_11_loss: 0.8383 - val_out_12_loss: 0.8404 - val_out_13_loss: 0.8379 - val_out_14_loss: 0.8474 - val_out_15_loss: 0.8398 - val_out_16_loss: 0.8392 - val_out_17_loss: 0.8377 - val_out_18_loss: 0.8385 - val_out_19_loss: 0.8434 - val_out_20_loss: 0.8431 - val_out_21_loss: 0.8436 - val_out_22_loss: 0.8448 - val_out_23_loss: 0.8436 - val_out_24_loss: 0.8478 - val_out_25_loss: 0.8490 - val_out_26_loss: 0.8443 - val_out_27_loss: 0.8420 - val_out_28_loss: 0.8415 - val_out_29_loss: 0.8412 - val_out_30_loss: 0.8496 - val_out_31_loss: 0.8392 - val_out_32_loss: 0.8385 - val_out_acc: 0.7007 - val_out_0_acc: 0.7614 - val_out_1_acc: 0.6790 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.6833 - val_out_4_acc: 0.6855 - val_out_5_acc: 0.6811 - val_out_6_acc: 0.6941 - val_out_7_acc: 0.6876 - val_out_8_acc: 0.6941 - val_out_9_acc: 0.6941 - val_out_10_acc: 0.6920 - val_out_11_acc: 0.6876 - val_out_12_acc: 0.6920 - val_out_13_acc: 0.6985 - val_out_14_acc: 0.6876 - val_out_15_acc: 0.6898 - val_out_16_acc: 0.6920 - val_out_17_acc: 0.6920 - val_out_18_acc: 0.6855 - val_out_19_acc: 0.6811 - val_out_20_acc: 0.6898 - val_out_21_acc: 0.6963 - val_out_22_acc: 0.6876 - val_out_23_acc: 0.6941 - val_out_24_acc: 0.6876 - val_out_25_acc: 0.6920 - val_out_26_acc: 0.6855 - val_out_27_acc: 0.6790 - val_out_28_acc: 0.6963 - val_out_29_acc: 0.6963 - val_out_30_acc: 0.6790 - val_out_31_acc: 0.6941 - val_out_32_acc: 0.6855\n",
      "Epoch 82/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 31.5593 - out_loss: 0.8319 - out_0_loss: 0.6123 - out_1_loss: 0.9445 - out_2_loss: 0.9466 - out_3_loss: 0.9408 - out_4_loss: 0.9480 - out_5_loss: 0.9274 - out_6_loss: 0.9408 - out_7_loss: 0.9475 - out_8_loss: 0.9437 - out_9_loss: 0.9403 - out_10_loss: 0.9437 - out_11_loss: 0.9325 - out_12_loss: 0.9357 - out_13_loss: 0.9370 - out_14_loss: 0.9273 - out_15_loss: 0.9471 - out_16_loss: 0.9343 - out_17_loss: 0.9311 - out_18_loss: 0.9543 - out_19_loss: 0.9475 - out_20_loss: 0.9469 - out_21_loss: 0.9346 - out_22_loss: 0.9494 - out_23_loss: 0.9443 - out_24_loss: 0.9522 - out_25_loss: 0.9427 - out_26_loss: 0.9402 - out_27_loss: 0.9355 - out_28_loss: 0.9354 - out_29_loss: 0.9330 - out_30_loss: 0.9547 - out_31_loss: 0.9358 - out_32_loss: 0.9402 - out_acc: 0.7074 - out_0_acc: 0.7781 - out_1_acc: 0.6618 - out_2_acc: 0.6612 - out_3_acc: 0.6593 - out_4_acc: 0.6686 - out_5_acc: 0.6717 - out_6_acc: 0.6634 - out_7_acc: 0.6637 - out_8_acc: 0.6600 - out_9_acc: 0.6640 - out_10_acc: 0.6711 - out_11_acc: 0.6699 - out_12_acc: 0.6624 - out_13_acc: 0.6680 - out_14_acc: 0.6727 - out_15_acc: 0.6674 - out_16_acc: 0.6711 - out_17_acc: 0.6643 - out_18_acc: 0.6652 - out_19_acc: 0.6559 - out_20_acc: 0.6668 - out_21_acc: 0.6556 - out_22_acc: 0.6584 - out_23_acc: 0.6655 - out_24_acc: 0.6671 - out_25_acc: 0.6525 - out_26_acc: 0.6631 - out_27_acc: 0.6689 - out_28_acc: 0.6748 - out_29_acc: 0.6686 - out_30_acc: 0.6652 - out_31_acc: 0.6631 - out_32_acc: 0.6652 - val_loss: 29.3430 - val_out_loss: 0.8149 - val_out_0_loss: 0.6558 - val_out_1_loss: 0.8345 - val_out_2_loss: 0.8327 - val_out_3_loss: 0.8305 - val_out_4_loss: 0.8390 - val_out_5_loss: 0.8313 - val_out_6_loss: 0.8349 - val_out_7_loss: 0.8369 - val_out_8_loss: 0.8370 - val_out_9_loss: 0.8319 - val_out_10_loss: 0.8345 - val_out_11_loss: 0.8347 - val_out_12_loss: 0.8288 - val_out_13_loss: 0.8317 - val_out_14_loss: 0.8367 - val_out_15_loss: 0.8357 - val_out_16_loss: 0.8292 - val_out_17_loss: 0.8380 - val_out_18_loss: 0.8349 - val_out_19_loss: 0.8404 - val_out_20_loss: 0.8377 - val_out_21_loss: 0.8279 - val_out_22_loss: 0.8353 - val_out_23_loss: 0.8298 - val_out_24_loss: 0.8416 - val_out_25_loss: 0.8350 - val_out_26_loss: 0.8314 - val_out_27_loss: 0.8393 - val_out_28_loss: 0.8443 - val_out_29_loss: 0.8328 - val_out_30_loss: 0.8409 - val_out_31_loss: 0.8316 - val_out_32_loss: 0.8301 - val_out_acc: 0.6941 - val_out_0_acc: 0.7570 - val_out_1_acc: 0.6898 - val_out_2_acc: 0.6920 - val_out_3_acc: 0.6963 - val_out_4_acc: 0.6963 - val_out_5_acc: 0.6941 - val_out_6_acc: 0.6963 - val_out_7_acc: 0.6941 - val_out_8_acc: 0.6920 - val_out_9_acc: 0.6920 - val_out_10_acc: 0.6985 - val_out_11_acc: 0.6963 - val_out_12_acc: 0.6920 - val_out_13_acc: 0.6963 - val_out_14_acc: 0.6898 - val_out_15_acc: 0.6941 - val_out_16_acc: 0.6920 - val_out_17_acc: 0.6920 - val_out_18_acc: 0.6920 - val_out_19_acc: 0.6898 - val_out_20_acc: 0.6941 - val_out_21_acc: 0.6920 - val_out_22_acc: 0.6876 - val_out_23_acc: 0.6963 - val_out_24_acc: 0.6855 - val_out_25_acc: 0.6876 - val_out_26_acc: 0.6985 - val_out_27_acc: 0.6963 - val_out_28_acc: 0.6898 - val_out_29_acc: 0.6920 - val_out_30_acc: 0.6876 - val_out_31_acc: 0.6920 - val_out_32_acc: 0.6963\n",
      "Epoch 83/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 30.2333 - out_loss: 0.7959 - out_0_loss: 0.6116 - out_1_loss: 0.9121 - out_2_loss: 0.8993 - out_3_loss: 0.9071 - out_4_loss: 0.8918 - out_5_loss: 0.9088 - out_6_loss: 0.9071 - out_7_loss: 0.9012 - out_8_loss: 0.9040 - out_9_loss: 0.8996 - out_10_loss: 0.8924 - out_11_loss: 0.8967 - out_12_loss: 0.9096 - out_13_loss: 0.8909 - out_14_loss: 0.8924 - out_15_loss: 0.9169 - out_16_loss: 0.9059 - out_17_loss: 0.9026 - out_18_loss: 0.8928 - out_19_loss: 0.8982 - out_20_loss: 0.8838 - out_21_loss: 0.9033 - out_22_loss: 0.8995 - out_23_loss: 0.8945 - out_24_loss: 0.9069 - out_25_loss: 0.9078 - out_26_loss: 0.8875 - out_27_loss: 0.8974 - out_28_loss: 0.9025 - out_29_loss: 0.9078 - out_30_loss: 0.9202 - out_31_loss: 0.8809 - out_32_loss: 0.9042 - out_acc: 0.7269 - out_0_acc: 0.7870 - out_1_acc: 0.6835 - out_2_acc: 0.6916 - out_3_acc: 0.6810 - out_4_acc: 0.6807 - out_5_acc: 0.6832 - out_6_acc: 0.6810 - out_7_acc: 0.6820 - out_8_acc: 0.6779 - out_9_acc: 0.6823 - out_10_acc: 0.6860 - out_11_acc: 0.6838 - out_12_acc: 0.6820 - out_13_acc: 0.6816 - out_14_acc: 0.6854 - out_15_acc: 0.6798 - out_16_acc: 0.6838 - out_17_acc: 0.6789 - out_18_acc: 0.6835 - out_19_acc: 0.6913 - out_20_acc: 0.6869 - out_21_acc: 0.6863 - out_22_acc: 0.6931 - out_23_acc: 0.6903 - out_24_acc: 0.6854 - out_25_acc: 0.6863 - out_26_acc: 0.6888 - out_27_acc: 0.6832 - out_28_acc: 0.6804 - out_29_acc: 0.6866 - out_30_acc: 0.6754 - out_31_acc: 0.6925 - out_32_acc: 0.6906 - val_loss: 28.6146 - val_out_loss: 0.7915 - val_out_0_loss: 0.6075 - val_out_1_loss: 0.8125 - val_out_2_loss: 0.8114 - val_out_3_loss: 0.8122 - val_out_4_loss: 0.8176 - val_out_5_loss: 0.8144 - val_out_6_loss: 0.8133 - val_out_7_loss: 0.8152 - val_out_8_loss: 0.8168 - val_out_9_loss: 0.8145 - val_out_10_loss: 0.8187 - val_out_11_loss: 0.8185 - val_out_12_loss: 0.8101 - val_out_13_loss: 0.8138 - val_out_14_loss: 0.8127 - val_out_15_loss: 0.8149 - val_out_16_loss: 0.8170 - val_out_17_loss: 0.8179 - val_out_18_loss: 0.8172 - val_out_19_loss: 0.8193 - val_out_20_loss: 0.8204 - val_out_21_loss: 0.8097 - val_out_22_loss: 0.8140 - val_out_23_loss: 0.8140 - val_out_24_loss: 0.8128 - val_out_25_loss: 0.8182 - val_out_26_loss: 0.8150 - val_out_27_loss: 0.8214 - val_out_28_loss: 0.8168 - val_out_29_loss: 0.8123 - val_out_30_loss: 0.8145 - val_out_31_loss: 0.8138 - val_out_32_loss: 0.8119 - val_out_acc: 0.7072 - val_out_0_acc: 0.7939 - val_out_1_acc: 0.7007 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.7007 - val_out_4_acc: 0.6941 - val_out_5_acc: 0.6855 - val_out_6_acc: 0.7007 - val_out_7_acc: 0.6920 - val_out_8_acc: 0.7007 - val_out_9_acc: 0.6941 - val_out_10_acc: 0.6985 - val_out_11_acc: 0.7050 - val_out_12_acc: 0.7007 - val_out_13_acc: 0.7028 - val_out_14_acc: 0.7050 - val_out_15_acc: 0.6941 - val_out_16_acc: 0.6941 - val_out_17_acc: 0.6985 - val_out_18_acc: 0.7007 - val_out_19_acc: 0.7072 - val_out_20_acc: 0.6963 - val_out_21_acc: 0.7028 - val_out_22_acc: 0.6920 - val_out_23_acc: 0.6941 - val_out_24_acc: 0.6920 - val_out_25_acc: 0.6963 - val_out_26_acc: 0.6985 - val_out_27_acc: 0.6920 - val_out_28_acc: 0.7028 - val_out_29_acc: 0.7050 - val_out_30_acc: 0.6963 - val_out_31_acc: 0.6985 - val_out_32_acc: 0.7072\n",
      "Epoch 84/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 29.7988 - out_loss: 0.7820 - out_0_loss: 0.6077 - out_1_loss: 0.8667 - out_2_loss: 0.8792 - out_3_loss: 0.8774 - out_4_loss: 0.8902 - out_5_loss: 0.8942 - out_6_loss: 0.8960 - out_7_loss: 0.8881 - out_8_loss: 0.8993 - out_9_loss: 0.8882 - out_10_loss: 0.8810 - out_11_loss: 0.8936 - out_12_loss: 0.8922 - out_13_loss: 0.8811 - out_14_loss: 0.8948 - out_15_loss: 0.8855 - out_16_loss: 0.8748 - out_17_loss: 0.8801 - out_18_loss: 0.8827 - out_19_loss: 0.8925 - out_20_loss: 0.8897 - out_21_loss: 0.8875 - out_22_loss: 0.8878 - out_23_loss: 0.8886 - out_24_loss: 0.8917 - out_25_loss: 0.8895 - out_26_loss: 0.8773 - out_27_loss: 0.8837 - out_28_loss: 0.8858 - out_29_loss: 0.8985 - out_30_loss: 0.8953 - out_31_loss: 0.9067 - out_32_loss: 0.8901 - out_acc: 0.7291 - out_0_acc: 0.7908 - out_1_acc: 0.6993 - out_2_acc: 0.6888 - out_3_acc: 0.6885 - out_4_acc: 0.6875 - out_5_acc: 0.6810 - out_6_acc: 0.6807 - out_7_acc: 0.6838 - out_8_acc: 0.6823 - out_9_acc: 0.6888 - out_10_acc: 0.6810 - out_11_acc: 0.6816 - out_12_acc: 0.6847 - out_13_acc: 0.6956 - out_14_acc: 0.6925 - out_15_acc: 0.6900 - out_16_acc: 0.6885 - out_17_acc: 0.6841 - out_18_acc: 0.6835 - out_19_acc: 0.6838 - out_20_acc: 0.6835 - out_21_acc: 0.6875 - out_22_acc: 0.6863 - out_23_acc: 0.6866 - out_24_acc: 0.6888 - out_25_acc: 0.6875 - out_26_acc: 0.6919 - out_27_acc: 0.6906 - out_28_acc: 0.6866 - out_29_acc: 0.6854 - out_30_acc: 0.6872 - out_31_acc: 0.6795 - out_32_acc: 0.6857 - val_loss: 28.7349 - val_out_loss: 0.7912 - val_out_0_loss: 0.5621 - val_out_1_loss: 0.8184 - val_out_2_loss: 0.8218 - val_out_3_loss: 0.8170 - val_out_4_loss: 0.8258 - val_out_5_loss: 0.8197 - val_out_6_loss: 0.8209 - val_out_7_loss: 0.8205 - val_out_8_loss: 0.8214 - val_out_9_loss: 0.8177 - val_out_10_loss: 0.8200 - val_out_11_loss: 0.8237 - val_out_12_loss: 0.8154 - val_out_13_loss: 0.8181 - val_out_14_loss: 0.8246 - val_out_15_loss: 0.8187 - val_out_16_loss: 0.8183 - val_out_17_loss: 0.8189 - val_out_18_loss: 0.8156 - val_out_19_loss: 0.8207 - val_out_20_loss: 0.8194 - val_out_21_loss: 0.8156 - val_out_22_loss: 0.8197 - val_out_23_loss: 0.8261 - val_out_24_loss: 0.8219 - val_out_25_loss: 0.8252 - val_out_26_loss: 0.8203 - val_out_27_loss: 0.8199 - val_out_28_loss: 0.8236 - val_out_29_loss: 0.8214 - val_out_30_loss: 0.8189 - val_out_31_loss: 0.8191 - val_out_32_loss: 0.8159 - val_out_acc: 0.7137 - val_out_0_acc: 0.7831 - val_out_1_acc: 0.7115 - val_out_2_acc: 0.7007 - val_out_3_acc: 0.7050 - val_out_4_acc: 0.7072 - val_out_5_acc: 0.7093 - val_out_6_acc: 0.7072 - val_out_7_acc: 0.7093 - val_out_8_acc: 0.7093 - val_out_9_acc: 0.7050 - val_out_10_acc: 0.7072 - val_out_11_acc: 0.7072 - val_out_12_acc: 0.7137 - val_out_13_acc: 0.7158 - val_out_14_acc: 0.7115 - val_out_15_acc: 0.7158 - val_out_16_acc: 0.7093 - val_out_17_acc: 0.7137 - val_out_18_acc: 0.7072 - val_out_19_acc: 0.6941 - val_out_20_acc: 0.7158 - val_out_21_acc: 0.7137 - val_out_22_acc: 0.7007 - val_out_23_acc: 0.7093 - val_out_24_acc: 0.7093 - val_out_25_acc: 0.7007 - val_out_26_acc: 0.7050 - val_out_27_acc: 0.7007 - val_out_28_acc: 0.7007 - val_out_29_acc: 0.7093 - val_out_30_acc: 0.7093 - val_out_31_acc: 0.7028 - val_out_32_acc: 0.7093\n",
      "Epoch 85/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 30.2261 - out_loss: 0.7933 - out_0_loss: 0.5874 - out_1_loss: 0.9049 - out_2_loss: 0.9067 - out_3_loss: 0.8863 - out_4_loss: 0.9041 - out_5_loss: 0.8933 - out_6_loss: 0.9096 - out_7_loss: 0.9136 - out_8_loss: 0.8948 - out_9_loss: 0.9058 - out_10_loss: 0.8951 - out_11_loss: 0.9017 - out_12_loss: 0.9125 - out_13_loss: 0.9095 - out_14_loss: 0.9104 - out_15_loss: 0.9028 - out_16_loss: 0.9045 - out_17_loss: 0.9035 - out_18_loss: 0.8940 - out_19_loss: 0.8808 - out_20_loss: 0.9096 - out_21_loss: 0.8860 - out_22_loss: 0.9075 - out_23_loss: 0.9045 - out_24_loss: 0.9043 - out_25_loss: 0.8896 - out_26_loss: 0.9133 - out_27_loss: 0.9100 - out_28_loss: 0.8962 - out_29_loss: 0.8964 - out_30_loss: 0.9081 - out_31_loss: 0.8957 - out_32_loss: 0.8902 - out_acc: 0.7201 - out_0_acc: 0.7830 - out_1_acc: 0.6789 - out_2_acc: 0.6776 - out_3_acc: 0.6829 - out_4_acc: 0.6785 - out_5_acc: 0.6804 - out_6_acc: 0.6748 - out_7_acc: 0.6751 - out_8_acc: 0.6851 - out_9_acc: 0.6727 - out_10_acc: 0.6813 - out_11_acc: 0.6804 - out_12_acc: 0.6714 - out_13_acc: 0.6677 - out_14_acc: 0.6813 - out_15_acc: 0.6739 - out_16_acc: 0.6758 - out_17_acc: 0.6863 - out_18_acc: 0.6869 - out_19_acc: 0.6832 - out_20_acc: 0.6764 - out_21_acc: 0.6798 - out_22_acc: 0.6792 - out_23_acc: 0.6795 - out_24_acc: 0.6813 - out_25_acc: 0.6919 - out_26_acc: 0.6723 - out_27_acc: 0.6773 - out_28_acc: 0.6736 - out_29_acc: 0.6847 - out_30_acc: 0.6727 - out_31_acc: 0.6789 - out_32_acc: 0.6761 - val_loss: 29.4102 - val_out_loss: 0.8053 - val_out_0_loss: 0.6157 - val_out_1_loss: 0.8390 - val_out_2_loss: 0.8403 - val_out_3_loss: 0.8345 - val_out_4_loss: 0.8414 - val_out_5_loss: 0.8373 - val_out_6_loss: 0.8409 - val_out_7_loss: 0.8412 - val_out_8_loss: 0.8368 - val_out_9_loss: 0.8394 - val_out_10_loss: 0.8334 - val_out_11_loss: 0.8383 - val_out_12_loss: 0.8376 - val_out_13_loss: 0.8433 - val_out_14_loss: 0.8404 - val_out_15_loss: 0.8358 - val_out_16_loss: 0.8367 - val_out_17_loss: 0.8345 - val_out_18_loss: 0.8379 - val_out_19_loss: 0.8395 - val_out_20_loss: 0.8453 - val_out_21_loss: 0.8279 - val_out_22_loss: 0.8363 - val_out_23_loss: 0.8386 - val_out_24_loss: 0.8387 - val_out_25_loss: 0.8402 - val_out_26_loss: 0.8436 - val_out_27_loss: 0.8411 - val_out_28_loss: 0.8351 - val_out_29_loss: 0.8333 - val_out_30_loss: 0.8407 - val_out_31_loss: 0.8428 - val_out_32_loss: 0.8335 - val_out_acc: 0.7050 - val_out_0_acc: 0.7918 - val_out_1_acc: 0.6941 - val_out_2_acc: 0.7050 - val_out_3_acc: 0.7093 - val_out_4_acc: 0.7115 - val_out_5_acc: 0.7028 - val_out_6_acc: 0.7050 - val_out_7_acc: 0.7007 - val_out_8_acc: 0.7072 - val_out_9_acc: 0.7050 - val_out_10_acc: 0.7050 - val_out_11_acc: 0.7093 - val_out_12_acc: 0.7028 - val_out_13_acc: 0.6985 - val_out_14_acc: 0.7072 - val_out_15_acc: 0.7028 - val_out_16_acc: 0.7072 - val_out_17_acc: 0.7028 - val_out_18_acc: 0.7050 - val_out_19_acc: 0.6963 - val_out_20_acc: 0.7007 - val_out_21_acc: 0.7050 - val_out_22_acc: 0.6963 - val_out_23_acc: 0.7072 - val_out_24_acc: 0.7115 - val_out_25_acc: 0.7050 - val_out_26_acc: 0.6985 - val_out_27_acc: 0.7028 - val_out_28_acc: 0.7137 - val_out_29_acc: 0.7115 - val_out_30_acc: 0.7007 - val_out_31_acc: 0.6985 - val_out_32_acc: 0.7072\n",
      "Epoch 86/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 29.3481 - out_loss: 0.7704 - out_0_loss: 0.6015 - out_1_loss: 0.8704 - out_2_loss: 0.8720 - out_3_loss: 0.8782 - out_4_loss: 0.8800 - out_5_loss: 0.8788 - out_6_loss: 0.8848 - out_7_loss: 0.8844 - out_8_loss: 0.8656 - out_9_loss: 0.8659 - out_10_loss: 0.8662 - out_11_loss: 0.8909 - out_12_loss: 0.8739 - out_13_loss: 0.8716 - out_14_loss: 0.8791 - out_15_loss: 0.8735 - out_16_loss: 0.8744 - out_17_loss: 0.8770 - out_18_loss: 0.8738 - out_19_loss: 0.8626 - out_20_loss: 0.8715 - out_21_loss: 0.8680 - out_22_loss: 0.8649 - out_23_loss: 0.8730 - out_24_loss: 0.8709 - out_25_loss: 0.8770 - out_26_loss: 0.8849 - out_27_loss: 0.8675 - out_28_loss: 0.8708 - out_29_loss: 0.8665 - out_30_loss: 0.8796 - out_31_loss: 0.8957 - out_32_loss: 0.8628 - out_acc: 0.7272 - out_0_acc: 0.7892 - out_1_acc: 0.6940 - out_2_acc: 0.6903 - out_3_acc: 0.6897 - out_4_acc: 0.6869 - out_5_acc: 0.6903 - out_6_acc: 0.6882 - out_7_acc: 0.6866 - out_8_acc: 0.6953 - out_9_acc: 0.6888 - out_10_acc: 0.6903 - out_11_acc: 0.6838 - out_12_acc: 0.6857 - out_13_acc: 0.6816 - out_14_acc: 0.6857 - out_15_acc: 0.6832 - out_16_acc: 0.6900 - out_17_acc: 0.6866 - out_18_acc: 0.6900 - out_19_acc: 0.6962 - out_20_acc: 0.6906 - out_21_acc: 0.6894 - out_22_acc: 0.6872 - out_23_acc: 0.6919 - out_24_acc: 0.6909 - out_25_acc: 0.6916 - out_26_acc: 0.6854 - out_27_acc: 0.6919 - out_28_acc: 0.6863 - out_29_acc: 0.6947 - out_30_acc: 0.6869 - out_31_acc: 0.6789 - out_32_acc: 0.6968 - val_loss: 36.8176 - val_out_loss: 1.0019 - val_out_0_loss: 0.8000 - val_out_1_loss: 1.0518 - val_out_2_loss: 1.0510 - val_out_3_loss: 1.0432 - val_out_4_loss: 1.0542 - val_out_5_loss: 1.0472 - val_out_6_loss: 1.0502 - val_out_7_loss: 1.0408 - val_out_8_loss: 1.0442 - val_out_9_loss: 1.0430 - val_out_10_loss: 1.0484 - val_out_11_loss: 1.0531 - val_out_12_loss: 1.0492 - val_out_13_loss: 1.0544 - val_out_14_loss: 1.0526 - val_out_15_loss: 1.0513 - val_out_16_loss: 1.0474 - val_out_17_loss: 1.0598 - val_out_18_loss: 1.0438 - val_out_19_loss: 1.0527 - val_out_20_loss: 1.0465 - val_out_21_loss: 1.0453 - val_out_22_loss: 1.0403 - val_out_23_loss: 1.0542 - val_out_24_loss: 1.0477 - val_out_25_loss: 1.0503 - val_out_26_loss: 1.0413 - val_out_27_loss: 1.0493 - val_out_28_loss: 1.0533 - val_out_29_loss: 1.0515 - val_out_30_loss: 1.0463 - val_out_31_loss: 1.0470 - val_out_32_loss: 1.0470 - val_out_acc: 0.6269 - val_out_0_acc: 0.7158 - val_out_1_acc: 0.6247 - val_out_2_acc: 0.6226 - val_out_3_acc: 0.6226 - val_out_4_acc: 0.6161 - val_out_5_acc: 0.6182 - val_out_6_acc: 0.6291 - val_out_7_acc: 0.6269 - val_out_8_acc: 0.6226 - val_out_9_acc: 0.6226 - val_out_10_acc: 0.6269 - val_out_11_acc: 0.6182 - val_out_12_acc: 0.6226 - val_out_13_acc: 0.6204 - val_out_14_acc: 0.6247 - val_out_15_acc: 0.6226 - val_out_16_acc: 0.6247 - val_out_17_acc: 0.6182 - val_out_18_acc: 0.6182 - val_out_19_acc: 0.6182 - val_out_20_acc: 0.6334 - val_out_21_acc: 0.6247 - val_out_22_acc: 0.6247 - val_out_23_acc: 0.6182 - val_out_24_acc: 0.6269 - val_out_25_acc: 0.6204 - val_out_26_acc: 0.6247 - val_out_27_acc: 0.6247 - val_out_28_acc: 0.6204 - val_out_29_acc: 0.6204 - val_out_30_acc: 0.6182 - val_out_31_acc: 0.6269 - val_out_32_acc: 0.6247\n",
      "Epoch 87/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 29.7712 - out_loss: 0.7861 - out_0_loss: 0.6076 - out_1_loss: 0.8837 - out_2_loss: 0.9066 - out_3_loss: 0.8918 - out_4_loss: 0.8827 - out_5_loss: 0.8774 - out_6_loss: 0.8815 - out_7_loss: 0.8772 - out_8_loss: 0.8891 - out_9_loss: 0.8706 - out_10_loss: 0.8775 - out_11_loss: 0.8820 - out_12_loss: 0.8796 - out_13_loss: 0.8888 - out_14_loss: 0.8975 - out_15_loss: 0.8939 - out_16_loss: 0.8873 - out_17_loss: 0.8954 - out_18_loss: 0.8986 - out_19_loss: 0.8830 - out_20_loss: 0.8918 - out_21_loss: 0.8802 - out_22_loss: 0.8830 - out_23_loss: 0.8868 - out_24_loss: 0.8847 - out_25_loss: 0.8832 - out_26_loss: 0.8826 - out_27_loss: 0.8891 - out_28_loss: 0.8806 - out_29_loss: 0.8793 - out_30_loss: 0.8998 - out_31_loss: 0.9059 - out_32_loss: 0.8862 - out_acc: 0.7300 - out_0_acc: 0.7867 - out_1_acc: 0.6804 - out_2_acc: 0.6785 - out_3_acc: 0.6816 - out_4_acc: 0.6919 - out_5_acc: 0.6835 - out_6_acc: 0.6863 - out_7_acc: 0.6847 - out_8_acc: 0.6885 - out_9_acc: 0.6978 - out_10_acc: 0.6928 - out_11_acc: 0.6909 - out_12_acc: 0.6931 - out_13_acc: 0.6925 - out_14_acc: 0.6863 - out_15_acc: 0.6934 - out_16_acc: 0.6832 - out_17_acc: 0.6820 - out_18_acc: 0.6903 - out_19_acc: 0.6875 - out_20_acc: 0.6841 - out_21_acc: 0.6866 - out_22_acc: 0.6965 - out_23_acc: 0.6925 - out_24_acc: 0.6869 - out_25_acc: 0.6956 - out_26_acc: 0.6906 - out_27_acc: 0.6925 - out_28_acc: 0.6968 - out_29_acc: 0.6894 - out_30_acc: 0.6773 - out_31_acc: 0.6832 - out_32_acc: 0.6882 - val_loss: 27.2238 - val_out_loss: 0.7486 - val_out_0_loss: 0.5711 - val_out_1_loss: 0.7735 - val_out_2_loss: 0.7733 - val_out_3_loss: 0.7741 - val_out_4_loss: 0.7738 - val_out_5_loss: 0.7772 - val_out_6_loss: 0.7744 - val_out_7_loss: 0.7768 - val_out_8_loss: 0.7785 - val_out_9_loss: 0.7750 - val_out_10_loss: 0.7740 - val_out_11_loss: 0.7787 - val_out_12_loss: 0.7695 - val_out_13_loss: 0.7768 - val_out_14_loss: 0.7776 - val_out_15_loss: 0.7784 - val_out_16_loss: 0.7753 - val_out_17_loss: 0.7749 - val_out_18_loss: 0.7749 - val_out_19_loss: 0.7787 - val_out_20_loss: 0.7781 - val_out_21_loss: 0.7739 - val_out_22_loss: 0.7768 - val_out_23_loss: 0.7777 - val_out_24_loss: 0.7767 - val_out_25_loss: 0.7765 - val_out_26_loss: 0.7754 - val_out_27_loss: 0.7776 - val_out_28_loss: 0.7779 - val_out_29_loss: 0.7741 - val_out_30_loss: 0.7784 - val_out_31_loss: 0.7742 - val_out_32_loss: 0.7739 - val_out_acc: 0.7223 - val_out_0_acc: 0.8091 - val_out_1_acc: 0.7289 - val_out_2_acc: 0.7180 - val_out_3_acc: 0.7245 - val_out_4_acc: 0.7223 - val_out_5_acc: 0.7180 - val_out_6_acc: 0.7245 - val_out_7_acc: 0.7137 - val_out_8_acc: 0.7158 - val_out_9_acc: 0.7267 - val_out_10_acc: 0.7223 - val_out_11_acc: 0.7180 - val_out_12_acc: 0.7289 - val_out_13_acc: 0.7137 - val_out_14_acc: 0.7202 - val_out_15_acc: 0.7180 - val_out_16_acc: 0.7202 - val_out_17_acc: 0.7202 - val_out_18_acc: 0.7202 - val_out_19_acc: 0.7245 - val_out_20_acc: 0.7158 - val_out_21_acc: 0.7223 - val_out_22_acc: 0.7223 - val_out_23_acc: 0.7180 - val_out_24_acc: 0.7158 - val_out_25_acc: 0.7158 - val_out_26_acc: 0.7202 - val_out_27_acc: 0.7223 - val_out_28_acc: 0.7180 - val_out_29_acc: 0.7115 - val_out_30_acc: 0.7137 - val_out_31_acc: 0.7137 - val_out_32_acc: 0.7289\n",
      "Epoch 88/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 29.8663 - out_loss: 0.7831 - out_0_loss: 0.5922 - out_1_loss: 0.8979 - out_2_loss: 0.8936 - out_3_loss: 0.8990 - out_4_loss: 0.8973 - out_5_loss: 0.8861 - out_6_loss: 0.8844 - out_7_loss: 0.8907 - out_8_loss: 0.8896 - out_9_loss: 0.8876 - out_10_loss: 0.8919 - out_11_loss: 0.8879 - out_12_loss: 0.8805 - out_13_loss: 0.8833 - out_14_loss: 0.8908 - out_15_loss: 0.8864 - out_16_loss: 0.8841 - out_17_loss: 0.8975 - out_18_loss: 0.8865 - out_19_loss: 0.8854 - out_20_loss: 0.8984 - out_21_loss: 0.8872 - out_22_loss: 0.8887 - out_23_loss: 0.8987 - out_24_loss: 0.8802 - out_25_loss: 0.8963 - out_26_loss: 0.8937 - out_27_loss: 0.8851 - out_28_loss: 0.8944 - out_29_loss: 0.8856 - out_30_loss: 0.8992 - out_31_loss: 0.8892 - out_32_loss: 0.8936 - out_acc: 0.7387 - out_0_acc: 0.7898 - out_1_acc: 0.6944 - out_2_acc: 0.6829 - out_3_acc: 0.6854 - out_4_acc: 0.6928 - out_5_acc: 0.6987 - out_6_acc: 0.6956 - out_7_acc: 0.6888 - out_8_acc: 0.6860 - out_9_acc: 0.6925 - out_10_acc: 0.6906 - out_11_acc: 0.6953 - out_12_acc: 0.6928 - out_13_acc: 0.6928 - out_14_acc: 0.6925 - out_15_acc: 0.6937 - out_16_acc: 0.6922 - out_17_acc: 0.6913 - out_18_acc: 0.6900 - out_19_acc: 0.6947 - out_20_acc: 0.6878 - out_21_acc: 0.6978 - out_22_acc: 0.6922 - out_23_acc: 0.6900 - out_24_acc: 0.6851 - out_25_acc: 0.6894 - out_26_acc: 0.6832 - out_27_acc: 0.6934 - out_28_acc: 0.6909 - out_29_acc: 0.6971 - out_30_acc: 0.6869 - out_31_acc: 0.6962 - out_32_acc: 0.6916 - val_loss: 29.3017 - val_out_loss: 0.8047 - val_out_0_loss: 0.6175 - val_out_1_loss: 0.8354 - val_out_2_loss: 0.8312 - val_out_3_loss: 0.8346 - val_out_4_loss: 0.8357 - val_out_5_loss: 0.8350 - val_out_6_loss: 0.8367 - val_out_7_loss: 0.8424 - val_out_8_loss: 0.8327 - val_out_9_loss: 0.8326 - val_out_10_loss: 0.8363 - val_out_11_loss: 0.8368 - val_out_12_loss: 0.8323 - val_out_13_loss: 0.8355 - val_out_14_loss: 0.8368 - val_out_15_loss: 0.8326 - val_out_16_loss: 0.8329 - val_out_17_loss: 0.8344 - val_out_18_loss: 0.8380 - val_out_19_loss: 0.8336 - val_out_20_loss: 0.8454 - val_out_21_loss: 0.8343 - val_out_22_loss: 0.8333 - val_out_23_loss: 0.8317 - val_out_24_loss: 0.8309 - val_out_25_loss: 0.8368 - val_out_26_loss: 0.8385 - val_out_27_loss: 0.8378 - val_out_28_loss: 0.8319 - val_out_29_loss: 0.8343 - val_out_30_loss: 0.8356 - val_out_31_loss: 0.8329 - val_out_32_loss: 0.8308 - val_out_acc: 0.7093 - val_out_0_acc: 0.7570 - val_out_1_acc: 0.7050 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.6941 - val_out_4_acc: 0.7050 - val_out_5_acc: 0.6963 - val_out_6_acc: 0.6963 - val_out_7_acc: 0.6898 - val_out_8_acc: 0.7050 - val_out_9_acc: 0.7007 - val_out_10_acc: 0.7028 - val_out_11_acc: 0.6920 - val_out_12_acc: 0.7050 - val_out_13_acc: 0.7007 - val_out_14_acc: 0.7093 - val_out_15_acc: 0.7093 - val_out_16_acc: 0.7072 - val_out_17_acc: 0.7028 - val_out_18_acc: 0.7072 - val_out_19_acc: 0.7093 - val_out_20_acc: 0.6985 - val_out_21_acc: 0.7093 - val_out_22_acc: 0.7050 - val_out_23_acc: 0.6985 - val_out_24_acc: 0.6941 - val_out_25_acc: 0.6985 - val_out_26_acc: 0.7093 - val_out_27_acc: 0.6941 - val_out_28_acc: 0.7072 - val_out_29_acc: 0.6920 - val_out_30_acc: 0.6985 - val_out_31_acc: 0.7050 - val_out_32_acc: 0.7007\n",
      "Epoch 89/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 29.0760 - out_loss: 0.7660 - out_0_loss: 0.5876 - out_1_loss: 0.8681 - out_2_loss: 0.8607 - out_3_loss: 0.8739 - out_4_loss: 0.8804 - out_5_loss: 0.8715 - out_6_loss: 0.8743 - out_7_loss: 0.8645 - out_8_loss: 0.8589 - out_9_loss: 0.8541 - out_10_loss: 0.8762 - out_11_loss: 0.8656 - out_12_loss: 0.8709 - out_13_loss: 0.8648 - out_14_loss: 0.8519 - out_15_loss: 0.8599 - out_16_loss: 0.8673 - out_17_loss: 0.8574 - out_18_loss: 0.8597 - out_19_loss: 0.8602 - out_20_loss: 0.8712 - out_21_loss: 0.8704 - out_22_loss: 0.8709 - out_23_loss: 0.8534 - out_24_loss: 0.8704 - out_25_loss: 0.8733 - out_26_loss: 0.8695 - out_27_loss: 0.8769 - out_28_loss: 0.8617 - out_29_loss: 0.8760 - out_30_loss: 0.8702 - out_31_loss: 0.8632 - out_32_loss: 0.8550 - out_acc: 0.7272 - out_0_acc: 0.7942 - out_1_acc: 0.6869 - out_2_acc: 0.6882 - out_3_acc: 0.6894 - out_4_acc: 0.6847 - out_5_acc: 0.6804 - out_6_acc: 0.6841 - out_7_acc: 0.6885 - out_8_acc: 0.6906 - out_9_acc: 0.6937 - out_10_acc: 0.6792 - out_11_acc: 0.6816 - out_12_acc: 0.6813 - out_13_acc: 0.6919 - out_14_acc: 0.6950 - out_15_acc: 0.6962 - out_16_acc: 0.6782 - out_17_acc: 0.6916 - out_18_acc: 0.6928 - out_19_acc: 0.6878 - out_20_acc: 0.6866 - out_21_acc: 0.6882 - out_22_acc: 0.6847 - out_23_acc: 0.6919 - out_24_acc: 0.6841 - out_25_acc: 0.6894 - out_26_acc: 0.6844 - out_27_acc: 0.6869 - out_28_acc: 0.6959 - out_29_acc: 0.6875 - out_30_acc: 0.6909 - out_31_acc: 0.6863 - out_32_acc: 0.6900 - val_loss: 29.8468 - val_out_loss: 0.8190 - val_out_0_loss: 0.5864 - val_out_1_loss: 0.8522 - val_out_2_loss: 0.8532 - val_out_3_loss: 0.8474 - val_out_4_loss: 0.8600 - val_out_5_loss: 0.8481 - val_out_6_loss: 0.8503 - val_out_7_loss: 0.8538 - val_out_8_loss: 0.8488 - val_out_9_loss: 0.8496 - val_out_10_loss: 0.8523 - val_out_11_loss: 0.8536 - val_out_12_loss: 0.8467 - val_out_13_loss: 0.8493 - val_out_14_loss: 0.8538 - val_out_15_loss: 0.8532 - val_out_16_loss: 0.8506 - val_out_17_loss: 0.8527 - val_out_18_loss: 0.8524 - val_out_19_loss: 0.8543 - val_out_20_loss: 0.8519 - val_out_21_loss: 0.8504 - val_out_22_loss: 0.8535 - val_out_23_loss: 0.8561 - val_out_24_loss: 0.8493 - val_out_25_loss: 0.8552 - val_out_26_loss: 0.8527 - val_out_27_loss: 0.8526 - val_out_28_loss: 0.8545 - val_out_29_loss: 0.8483 - val_out_30_loss: 0.8525 - val_out_31_loss: 0.8494 - val_out_32_loss: 0.8512 - val_out_acc: 0.6876 - val_out_0_acc: 0.7939 - val_out_1_acc: 0.6790 - val_out_2_acc: 0.6768 - val_out_3_acc: 0.6811 - val_out_4_acc: 0.6790 - val_out_5_acc: 0.6833 - val_out_6_acc: 0.6855 - val_out_7_acc: 0.6876 - val_out_8_acc: 0.6876 - val_out_9_acc: 0.6941 - val_out_10_acc: 0.6855 - val_out_11_acc: 0.6790 - val_out_12_acc: 0.6833 - val_out_13_acc: 0.6811 - val_out_14_acc: 0.6746 - val_out_15_acc: 0.6811 - val_out_16_acc: 0.6833 - val_out_17_acc: 0.6790 - val_out_18_acc: 0.6855 - val_out_19_acc: 0.6768 - val_out_20_acc: 0.6833 - val_out_21_acc: 0.6833 - val_out_22_acc: 0.6811 - val_out_23_acc: 0.6746 - val_out_24_acc: 0.6876 - val_out_25_acc: 0.6768 - val_out_26_acc: 0.6790 - val_out_27_acc: 0.6725 - val_out_28_acc: 0.6746 - val_out_29_acc: 0.6790 - val_out_30_acc: 0.6855 - val_out_31_acc: 0.6790 - val_out_32_acc: 0.6790\n",
      "Epoch 90/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 28.8537 - out_loss: 0.7585 - out_0_loss: 0.5668 - out_1_loss: 0.8633 - out_2_loss: 0.8481 - out_3_loss: 0.8400 - out_4_loss: 0.8703 - out_5_loss: 0.8553 - out_6_loss: 0.8702 - out_7_loss: 0.8537 - out_8_loss: 0.8429 - out_9_loss: 0.8697 - out_10_loss: 0.8545 - out_11_loss: 0.8647 - out_12_loss: 0.8540 - out_13_loss: 0.8608 - out_14_loss: 0.8607 - out_15_loss: 0.8562 - out_16_loss: 0.8731 - out_17_loss: 0.8578 - out_18_loss: 0.8748 - out_19_loss: 0.8672 - out_20_loss: 0.8559 - out_21_loss: 0.8679 - out_22_loss: 0.8772 - out_23_loss: 0.8582 - out_24_loss: 0.8650 - out_25_loss: 0.8608 - out_26_loss: 0.8664 - out_27_loss: 0.8589 - out_28_loss: 0.8548 - out_29_loss: 0.8662 - out_30_loss: 0.8530 - out_31_loss: 0.8536 - out_32_loss: 0.8531 - out_acc: 0.7343 - out_0_acc: 0.8013 - out_1_acc: 0.6962 - out_2_acc: 0.7068 - out_3_acc: 0.6981 - out_4_acc: 0.6909 - out_5_acc: 0.6950 - out_6_acc: 0.6872 - out_7_acc: 0.6947 - out_8_acc: 0.6968 - out_9_acc: 0.6854 - out_10_acc: 0.6944 - out_11_acc: 0.6944 - out_12_acc: 0.6959 - out_13_acc: 0.6984 - out_14_acc: 0.6987 - out_15_acc: 0.6916 - out_16_acc: 0.6909 - out_17_acc: 0.6944 - out_18_acc: 0.6922 - out_19_acc: 0.6944 - out_20_acc: 0.7012 - out_21_acc: 0.6891 - out_22_acc: 0.6863 - out_23_acc: 0.6978 - out_24_acc: 0.6947 - out_25_acc: 0.6975 - out_26_acc: 0.6894 - out_27_acc: 0.6934 - out_28_acc: 0.6900 - out_29_acc: 0.6851 - out_30_acc: 0.6882 - out_31_acc: 0.6823 - out_32_acc: 0.6990 - val_loss: 27.7970 - val_out_loss: 0.7650 - val_out_0_loss: 0.5308 - val_out_1_loss: 0.7962 - val_out_2_loss: 0.7933 - val_out_3_loss: 0.7893 - val_out_4_loss: 0.7946 - val_out_5_loss: 0.7932 - val_out_6_loss: 0.7932 - val_out_7_loss: 0.7926 - val_out_8_loss: 0.7947 - val_out_9_loss: 0.7905 - val_out_10_loss: 0.7885 - val_out_11_loss: 0.7930 - val_out_12_loss: 0.7859 - val_out_13_loss: 0.7942 - val_out_14_loss: 0.7955 - val_out_15_loss: 0.7886 - val_out_16_loss: 0.7958 - val_out_17_loss: 0.7903 - val_out_18_loss: 0.7933 - val_out_19_loss: 0.8042 - val_out_20_loss: 0.7931 - val_out_21_loss: 0.7923 - val_out_22_loss: 0.7959 - val_out_23_loss: 0.7928 - val_out_24_loss: 0.8004 - val_out_25_loss: 0.8001 - val_out_26_loss: 0.7957 - val_out_27_loss: 0.7901 - val_out_28_loss: 0.7954 - val_out_29_loss: 0.7947 - val_out_30_loss: 0.7979 - val_out_31_loss: 0.7917 - val_out_32_loss: 0.7939 - val_out_acc: 0.7028 - val_out_0_acc: 0.7852 - val_out_1_acc: 0.6963 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.6985 - val_out_4_acc: 0.6985 - val_out_5_acc: 0.6963 - val_out_6_acc: 0.6898 - val_out_7_acc: 0.6963 - val_out_8_acc: 0.6963 - val_out_9_acc: 0.6941 - val_out_10_acc: 0.6963 - val_out_11_acc: 0.6941 - val_out_12_acc: 0.7028 - val_out_13_acc: 0.7007 - val_out_14_acc: 0.7050 - val_out_15_acc: 0.7050 - val_out_16_acc: 0.6898 - val_out_17_acc: 0.6963 - val_out_18_acc: 0.6985 - val_out_19_acc: 0.6985 - val_out_20_acc: 0.7007 - val_out_21_acc: 0.7028 - val_out_22_acc: 0.7028 - val_out_23_acc: 0.6985 - val_out_24_acc: 0.6941 - val_out_25_acc: 0.6920 - val_out_26_acc: 0.6941 - val_out_27_acc: 0.6941 - val_out_28_acc: 0.6920 - val_out_29_acc: 0.7007 - val_out_30_acc: 0.6941 - val_out_31_acc: 0.7028 - val_out_32_acc: 0.7007\n",
      "Epoch 91/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 28.9402 - out_loss: 0.7598 - out_0_loss: 0.5702 - out_1_loss: 0.8472 - out_2_loss: 0.8642 - out_3_loss: 0.8661 - out_4_loss: 0.8623 - out_5_loss: 0.8680 - out_6_loss: 0.8623 - out_7_loss: 0.8714 - out_8_loss: 0.8658 - out_9_loss: 0.8544 - out_10_loss: 0.8750 - out_11_loss: 0.8576 - out_12_loss: 0.8579 - out_13_loss: 0.8608 - out_14_loss: 0.8689 - out_15_loss: 0.8652 - out_16_loss: 0.8669 - out_17_loss: 0.8620 - out_18_loss: 0.8609 - out_19_loss: 0.8685 - out_20_loss: 0.8647 - out_21_loss: 0.8654 - out_22_loss: 0.8705 - out_23_loss: 0.8592 - out_24_loss: 0.8527 - out_25_loss: 0.8692 - out_26_loss: 0.8709 - out_27_loss: 0.8544 - out_28_loss: 0.8656 - out_29_loss: 0.8522 - out_30_loss: 0.8543 - out_31_loss: 0.8701 - out_32_loss: 0.8554 - out_acc: 0.7309 - out_0_acc: 0.7932 - out_1_acc: 0.6928 - out_2_acc: 0.6897 - out_3_acc: 0.6934 - out_4_acc: 0.6875 - out_5_acc: 0.6888 - out_6_acc: 0.6810 - out_7_acc: 0.6919 - out_8_acc: 0.6922 - out_9_acc: 0.6975 - out_10_acc: 0.6888 - out_11_acc: 0.6900 - out_12_acc: 0.6940 - out_13_acc: 0.6844 - out_14_acc: 0.6916 - out_15_acc: 0.6894 - out_16_acc: 0.6937 - out_17_acc: 0.6869 - out_18_acc: 0.6885 - out_19_acc: 0.6832 - out_20_acc: 0.6981 - out_21_acc: 0.6965 - out_22_acc: 0.6878 - out_23_acc: 0.6909 - out_24_acc: 0.6916 - out_25_acc: 0.6816 - out_26_acc: 0.6990 - out_27_acc: 0.6950 - out_28_acc: 0.6909 - out_29_acc: 0.6937 - out_30_acc: 0.6931 - out_31_acc: 0.6820 - out_32_acc: 0.6940 - val_loss: 31.1454 - val_out_loss: 0.8471 - val_out_0_loss: 0.5619 - val_out_1_loss: 0.8897 - val_out_2_loss: 0.8856 - val_out_3_loss: 0.8890 - val_out_4_loss: 0.8942 - val_out_5_loss: 0.8857 - val_out_6_loss: 0.8956 - val_out_7_loss: 0.8897 - val_out_8_loss: 0.8959 - val_out_9_loss: 0.8881 - val_out_10_loss: 0.8872 - val_out_11_loss: 0.8945 - val_out_12_loss: 0.8851 - val_out_13_loss: 0.8899 - val_out_14_loss: 0.8936 - val_out_15_loss: 0.8878 - val_out_16_loss: 0.8952 - val_out_17_loss: 0.8900 - val_out_18_loss: 0.8890 - val_out_19_loss: 0.8918 - val_out_20_loss: 0.8939 - val_out_21_loss: 0.8891 - val_out_22_loss: 0.8975 - val_out_23_loss: 0.8931 - val_out_24_loss: 0.8919 - val_out_25_loss: 0.8899 - val_out_26_loss: 0.8962 - val_out_27_loss: 0.8959 - val_out_28_loss: 0.8897 - val_out_29_loss: 0.8882 - val_out_30_loss: 0.8910 - val_out_31_loss: 0.8845 - val_out_32_loss: 0.8849 - val_out_acc: 0.6985 - val_out_0_acc: 0.7983 - val_out_1_acc: 0.6941 - val_out_2_acc: 0.6941 - val_out_3_acc: 0.6876 - val_out_4_acc: 0.6985 - val_out_5_acc: 0.6985 - val_out_6_acc: 0.6963 - val_out_7_acc: 0.7028 - val_out_8_acc: 0.6920 - val_out_9_acc: 0.6985 - val_out_10_acc: 0.6941 - val_out_11_acc: 0.6920 - val_out_12_acc: 0.6963 - val_out_13_acc: 0.6963 - val_out_14_acc: 0.6941 - val_out_15_acc: 0.6920 - val_out_16_acc: 0.6941 - val_out_17_acc: 0.7028 - val_out_18_acc: 0.7028 - val_out_19_acc: 0.6941 - val_out_20_acc: 0.6920 - val_out_21_acc: 0.6941 - val_out_22_acc: 0.6920 - val_out_23_acc: 0.6985 - val_out_24_acc: 0.6920 - val_out_25_acc: 0.6941 - val_out_26_acc: 0.6941 - val_out_27_acc: 0.6898 - val_out_28_acc: 0.7007 - val_out_29_acc: 0.6941 - val_out_30_acc: 0.6898 - val_out_31_acc: 0.6898 - val_out_32_acc: 0.6920\n",
      "Epoch 92/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 28.9110 - out_loss: 0.7600 - out_0_loss: 0.5786 - out_1_loss: 0.8595 - out_2_loss: 0.8510 - out_3_loss: 0.8630 - out_4_loss: 0.8549 - out_5_loss: 0.8491 - out_6_loss: 0.8665 - out_7_loss: 0.8658 - out_8_loss: 0.8618 - out_9_loss: 0.8709 - out_10_loss: 0.8709 - out_11_loss: 0.8850 - out_12_loss: 0.8546 - out_13_loss: 0.8563 - out_14_loss: 0.8682 - out_15_loss: 0.8626 - out_16_loss: 0.8618 - out_17_loss: 0.8606 - out_18_loss: 0.8674 - out_19_loss: 0.8635 - out_20_loss: 0.8502 - out_21_loss: 0.8642 - out_22_loss: 0.8504 - out_23_loss: 0.8603 - out_24_loss: 0.8637 - out_25_loss: 0.8718 - out_26_loss: 0.8617 - out_27_loss: 0.8555 - out_28_loss: 0.8596 - out_29_loss: 0.8544 - out_30_loss: 0.8719 - out_31_loss: 0.8522 - out_32_loss: 0.8633 - out_acc: 0.7350 - out_0_acc: 0.8038 - out_1_acc: 0.6999 - out_2_acc: 0.6978 - out_3_acc: 0.6925 - out_4_acc: 0.7037 - out_5_acc: 0.7046 - out_6_acc: 0.6944 - out_7_acc: 0.6944 - out_8_acc: 0.6897 - out_9_acc: 0.6922 - out_10_acc: 0.6913 - out_11_acc: 0.6916 - out_12_acc: 0.6981 - out_13_acc: 0.7018 - out_14_acc: 0.6956 - out_15_acc: 0.6940 - out_16_acc: 0.6987 - out_17_acc: 0.6931 - out_18_acc: 0.6968 - out_19_acc: 0.6947 - out_20_acc: 0.7015 - out_21_acc: 0.6975 - out_22_acc: 0.6944 - out_23_acc: 0.6996 - out_24_acc: 0.7006 - out_25_acc: 0.6909 - out_26_acc: 0.6922 - out_27_acc: 0.7033 - out_28_acc: 0.6968 - out_29_acc: 0.6962 - out_30_acc: 0.6940 - out_31_acc: 0.7030 - out_32_acc: 0.6947 - val_loss: 27.8786 - val_out_loss: 0.7637 - val_out_0_loss: 0.5448 - val_out_1_loss: 0.7958 - val_out_2_loss: 0.7923 - val_out_3_loss: 0.7914 - val_out_4_loss: 0.7977 - val_out_5_loss: 0.7919 - val_out_6_loss: 0.7959 - val_out_7_loss: 0.7908 - val_out_8_loss: 0.7948 - val_out_9_loss: 0.7979 - val_out_10_loss: 0.8010 - val_out_11_loss: 0.7974 - val_out_12_loss: 0.7936 - val_out_13_loss: 0.7979 - val_out_14_loss: 0.8064 - val_out_15_loss: 0.7950 - val_out_16_loss: 0.7895 - val_out_17_loss: 0.8010 - val_out_18_loss: 0.7940 - val_out_19_loss: 0.8020 - val_out_20_loss: 0.7959 - val_out_21_loss: 0.7950 - val_out_22_loss: 0.7948 - val_out_23_loss: 0.7949 - val_out_24_loss: 0.7913 - val_out_25_loss: 0.8012 - val_out_26_loss: 0.7985 - val_out_27_loss: 0.7928 - val_out_28_loss: 0.7997 - val_out_29_loss: 0.7923 - val_out_30_loss: 0.8022 - val_out_31_loss: 0.7899 - val_out_32_loss: 0.7918 - val_out_acc: 0.6985 - val_out_0_acc: 0.7939 - val_out_1_acc: 0.6963 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.6963 - val_out_4_acc: 0.7028 - val_out_5_acc: 0.6963 - val_out_6_acc: 0.6941 - val_out_7_acc: 0.7050 - val_out_8_acc: 0.6963 - val_out_9_acc: 0.6963 - val_out_10_acc: 0.6941 - val_out_11_acc: 0.6920 - val_out_12_acc: 0.6985 - val_out_13_acc: 0.6898 - val_out_14_acc: 0.6985 - val_out_15_acc: 0.6963 - val_out_16_acc: 0.6941 - val_out_17_acc: 0.6963 - val_out_18_acc: 0.6963 - val_out_19_acc: 0.6963 - val_out_20_acc: 0.6920 - val_out_21_acc: 0.6963 - val_out_22_acc: 0.7007 - val_out_23_acc: 0.6963 - val_out_24_acc: 0.6985 - val_out_25_acc: 0.6941 - val_out_26_acc: 0.6985 - val_out_27_acc: 0.6985 - val_out_28_acc: 0.6941 - val_out_29_acc: 0.7072 - val_out_30_acc: 0.6963 - val_out_31_acc: 0.7050 - val_out_32_acc: 0.6963\n",
      "Epoch 93/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 28.6072 - out_loss: 0.7502 - out_0_loss: 0.5392 - out_1_loss: 0.8561 - out_2_loss: 0.8590 - out_3_loss: 0.8615 - out_4_loss: 0.8466 - out_5_loss: 0.8427 - out_6_loss: 0.8329 - out_7_loss: 0.8516 - out_8_loss: 0.8595 - out_9_loss: 0.8555 - out_10_loss: 0.8510 - out_11_loss: 0.8631 - out_12_loss: 0.8716 - out_13_loss: 0.8383 - out_14_loss: 0.8443 - out_15_loss: 0.8543 - out_16_loss: 0.8430 - out_17_loss: 0.8481 - out_18_loss: 0.8526 - out_19_loss: 0.8586 - out_20_loss: 0.8435 - out_21_loss: 0.8486 - out_22_loss: 0.8523 - out_23_loss: 0.8586 - out_24_loss: 0.8468 - out_25_loss: 0.8680 - out_26_loss: 0.8535 - out_27_loss: 0.8609 - out_28_loss: 0.8586 - out_29_loss: 0.8625 - out_30_loss: 0.8557 - out_31_loss: 0.8538 - out_32_loss: 0.8647 - out_acc: 0.7353 - out_0_acc: 0.8149 - out_1_acc: 0.6940 - out_2_acc: 0.6897 - out_3_acc: 0.6919 - out_4_acc: 0.7071 - out_5_acc: 0.6996 - out_6_acc: 0.7006 - out_7_acc: 0.6987 - out_8_acc: 0.6953 - out_9_acc: 0.6999 - out_10_acc: 0.7012 - out_11_acc: 0.6897 - out_12_acc: 0.6847 - out_13_acc: 0.7064 - out_14_acc: 0.7015 - out_15_acc: 0.6940 - out_16_acc: 0.6916 - out_17_acc: 0.7009 - out_18_acc: 0.6928 - out_19_acc: 0.6869 - out_20_acc: 0.6996 - out_21_acc: 0.7058 - out_22_acc: 0.6981 - out_23_acc: 0.6947 - out_24_acc: 0.7009 - out_25_acc: 0.6962 - out_26_acc: 0.6999 - out_27_acc: 0.6953 - out_28_acc: 0.6971 - out_29_acc: 0.6913 - out_30_acc: 0.6968 - out_31_acc: 0.6950 - out_32_acc: 0.7015 - val_loss: 28.3054 - val_out_loss: 0.7757 - val_out_0_loss: 0.5939 - val_out_1_loss: 0.8098 - val_out_2_loss: 0.8048 - val_out_3_loss: 0.8090 - val_out_4_loss: 0.8073 - val_out_5_loss: 0.8046 - val_out_6_loss: 0.8098 - val_out_7_loss: 0.8080 - val_out_8_loss: 0.8043 - val_out_9_loss: 0.8084 - val_out_10_loss: 0.8047 - val_out_11_loss: 0.8059 - val_out_12_loss: 0.8035 - val_out_13_loss: 0.8047 - val_out_14_loss: 0.8090 - val_out_15_loss: 0.8073 - val_out_16_loss: 0.8062 - val_out_17_loss: 0.8109 - val_out_18_loss: 0.8064 - val_out_19_loss: 0.8098 - val_out_20_loss: 0.8042 - val_out_21_loss: 0.8004 - val_out_22_loss: 0.8010 - val_out_23_loss: 0.8039 - val_out_24_loss: 0.8093 - val_out_25_loss: 0.8087 - val_out_26_loss: 0.8095 - val_out_27_loss: 0.8118 - val_out_28_loss: 0.8058 - val_out_29_loss: 0.8082 - val_out_30_loss: 0.8064 - val_out_31_loss: 0.8051 - val_out_32_loss: 0.8066 - val_out_acc: 0.7007 - val_out_0_acc: 0.7679 - val_out_1_acc: 0.6855 - val_out_2_acc: 0.6985 - val_out_3_acc: 0.6941 - val_out_4_acc: 0.6898 - val_out_5_acc: 0.6920 - val_out_6_acc: 0.6963 - val_out_7_acc: 0.6920 - val_out_8_acc: 0.6985 - val_out_9_acc: 0.6876 - val_out_10_acc: 0.6855 - val_out_11_acc: 0.7007 - val_out_12_acc: 0.6941 - val_out_13_acc: 0.6920 - val_out_14_acc: 0.6985 - val_out_15_acc: 0.6920 - val_out_16_acc: 0.6941 - val_out_17_acc: 0.6985 - val_out_18_acc: 0.6876 - val_out_19_acc: 0.6898 - val_out_20_acc: 0.6985 - val_out_21_acc: 0.7007 - val_out_22_acc: 0.6920 - val_out_23_acc: 0.6985 - val_out_24_acc: 0.6963 - val_out_25_acc: 0.6920 - val_out_26_acc: 0.6941 - val_out_27_acc: 0.6985 - val_out_28_acc: 0.6898 - val_out_29_acc: 0.7028 - val_out_30_acc: 0.6941 - val_out_31_acc: 0.6985 - val_out_32_acc: 0.6876\n",
      "Epoch 94/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 29.0222 - out_loss: 0.7645 - out_0_loss: 0.5604 - out_1_loss: 0.8563 - out_2_loss: 0.8661 - out_3_loss: 0.8675 - out_4_loss: 0.8557 - out_5_loss: 0.8658 - out_6_loss: 0.8829 - out_7_loss: 0.8666 - out_8_loss: 0.8716 - out_9_loss: 0.8527 - out_10_loss: 0.8645 - out_11_loss: 0.8631 - out_12_loss: 0.8770 - out_13_loss: 0.8640 - out_14_loss: 0.8747 - out_15_loss: 0.8637 - out_16_loss: 0.8779 - out_17_loss: 0.8646 - out_18_loss: 0.8562 - out_19_loss: 0.8733 - out_20_loss: 0.8647 - out_21_loss: 0.8607 - out_22_loss: 0.8633 - out_23_loss: 0.8771 - out_24_loss: 0.8607 - out_25_loss: 0.8537 - out_26_loss: 0.8717 - out_27_loss: 0.8559 - out_28_loss: 0.8652 - out_29_loss: 0.8631 - out_30_loss: 0.8729 - out_31_loss: 0.8582 - out_32_loss: 0.8658 - out_acc: 0.7343 - out_0_acc: 0.8087 - out_1_acc: 0.6891 - out_2_acc: 0.6978 - out_3_acc: 0.6872 - out_4_acc: 0.6882 - out_5_acc: 0.6844 - out_6_acc: 0.6888 - out_7_acc: 0.6950 - out_8_acc: 0.6891 - out_9_acc: 0.6971 - out_10_acc: 0.6909 - out_11_acc: 0.6959 - out_12_acc: 0.6795 - out_13_acc: 0.6835 - out_14_acc: 0.6841 - out_15_acc: 0.6916 - out_16_acc: 0.6792 - out_17_acc: 0.6944 - out_18_acc: 0.7009 - out_19_acc: 0.6813 - out_20_acc: 0.6878 - out_21_acc: 0.6937 - out_22_acc: 0.6931 - out_23_acc: 0.6829 - out_24_acc: 0.6968 - out_25_acc: 0.6894 - out_26_acc: 0.6866 - out_27_acc: 0.6940 - out_28_acc: 0.6906 - out_29_acc: 0.6922 - out_30_acc: 0.6940 - out_31_acc: 0.6875 - out_32_acc: 0.6925 - val_loss: 25.5218 - val_out_loss: 0.7013 - val_out_0_loss: 0.5343 - val_out_1_loss: 0.7253 - val_out_2_loss: 0.7233 - val_out_3_loss: 0.7285 - val_out_4_loss: 0.7262 - val_out_5_loss: 0.7265 - val_out_6_loss: 0.7295 - val_out_7_loss: 0.7276 - val_out_8_loss: 0.7232 - val_out_9_loss: 0.7262 - val_out_10_loss: 0.7287 - val_out_11_loss: 0.7304 - val_out_12_loss: 0.7261 - val_out_13_loss: 0.7298 - val_out_14_loss: 0.7284 - val_out_15_loss: 0.7286 - val_out_16_loss: 0.7270 - val_out_17_loss: 0.7273 - val_out_18_loss: 0.7255 - val_out_19_loss: 0.7299 - val_out_20_loss: 0.7317 - val_out_21_loss: 0.7227 - val_out_22_loss: 0.7261 - val_out_23_loss: 0.7258 - val_out_24_loss: 0.7217 - val_out_25_loss: 0.7326 - val_out_26_loss: 0.7295 - val_out_27_loss: 0.7326 - val_out_28_loss: 0.7257 - val_out_29_loss: 0.7243 - val_out_30_loss: 0.7303 - val_out_31_loss: 0.7315 - val_out_32_loss: 0.7238 - val_out_acc: 0.7267 - val_out_0_acc: 0.8069 - val_out_1_acc: 0.7223 - val_out_2_acc: 0.7245 - val_out_3_acc: 0.7223 - val_out_4_acc: 0.7267 - val_out_5_acc: 0.7245 - val_out_6_acc: 0.7223 - val_out_7_acc: 0.7180 - val_out_8_acc: 0.7310 - val_out_9_acc: 0.7223 - val_out_10_acc: 0.7289 - val_out_11_acc: 0.7245 - val_out_12_acc: 0.7289 - val_out_13_acc: 0.7245 - val_out_14_acc: 0.7245 - val_out_15_acc: 0.7180 - val_out_16_acc: 0.7267 - val_out_17_acc: 0.7115 - val_out_18_acc: 0.7289 - val_out_19_acc: 0.7267 - val_out_20_acc: 0.7223 - val_out_21_acc: 0.7289 - val_out_22_acc: 0.7202 - val_out_23_acc: 0.7245 - val_out_24_acc: 0.7245 - val_out_25_acc: 0.7202 - val_out_26_acc: 0.7180 - val_out_27_acc: 0.7267 - val_out_28_acc: 0.7245 - val_out_29_acc: 0.7202 - val_out_30_acc: 0.7202 - val_out_31_acc: 0.7202 - val_out_32_acc: 0.7267\n",
      "Epoch 95/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 28.7924 - out_loss: 0.7539 - out_0_loss: 0.5475 - out_1_loss: 0.8675 - out_2_loss: 0.8588 - out_3_loss: 0.8595 - out_4_loss: 0.8545 - out_5_loss: 0.8458 - out_6_loss: 0.8523 - out_7_loss: 0.8581 - out_8_loss: 0.8578 - out_9_loss: 0.8639 - out_10_loss: 0.8622 - out_11_loss: 0.8463 - out_12_loss: 0.8612 - out_13_loss: 0.8555 - out_14_loss: 0.8444 - out_15_loss: 0.8595 - out_16_loss: 0.8532 - out_17_loss: 0.8689 - out_18_loss: 0.8641 - out_19_loss: 0.8820 - out_20_loss: 0.8541 - out_21_loss: 0.8563 - out_22_loss: 0.8799 - out_23_loss: 0.8565 - out_24_loss: 0.8592 - out_25_loss: 0.8507 - out_26_loss: 0.8582 - out_27_loss: 0.8574 - out_28_loss: 0.8522 - out_29_loss: 0.8660 - out_30_loss: 0.8652 - out_31_loss: 0.8547 - out_32_loss: 0.8648 - out_acc: 0.7337 - out_0_acc: 0.8001 - out_1_acc: 0.6944 - out_2_acc: 0.6897 - out_3_acc: 0.6962 - out_4_acc: 0.6962 - out_5_acc: 0.6962 - out_6_acc: 0.6931 - out_7_acc: 0.6984 - out_8_acc: 0.6950 - out_9_acc: 0.6981 - out_10_acc: 0.6931 - out_11_acc: 0.7006 - out_12_acc: 0.7002 - out_13_acc: 0.6971 - out_14_acc: 0.6944 - out_15_acc: 0.6925 - out_16_acc: 0.7012 - out_17_acc: 0.6885 - out_18_acc: 0.6944 - out_19_acc: 0.6894 - out_20_acc: 0.7043 - out_21_acc: 0.7049 - out_22_acc: 0.6823 - out_23_acc: 0.7002 - out_24_acc: 0.6975 - out_25_acc: 0.6934 - out_26_acc: 0.6987 - out_27_acc: 0.6975 - out_28_acc: 0.6962 - out_29_acc: 0.6844 - out_30_acc: 0.6919 - out_31_acc: 0.6888 - out_32_acc: 0.6906 - val_loss: 28.7546 - val_out_loss: 0.7820 - val_out_0_loss: 0.4732 - val_out_1_loss: 0.8270 - val_out_2_loss: 0.8231 - val_out_3_loss: 0.8249 - val_out_4_loss: 0.8259 - val_out_5_loss: 0.8248 - val_out_6_loss: 0.8222 - val_out_7_loss: 0.8209 - val_out_8_loss: 0.8259 - val_out_9_loss: 0.8228 - val_out_10_loss: 0.8215 - val_out_11_loss: 0.8226 - val_out_12_loss: 0.8237 - val_out_13_loss: 0.8180 - val_out_14_loss: 0.8269 - val_out_15_loss: 0.8213 - val_out_16_loss: 0.8187 - val_out_17_loss: 0.8218 - val_out_18_loss: 0.8255 - val_out_19_loss: 0.8277 - val_out_20_loss: 0.8225 - val_out_21_loss: 0.8182 - val_out_22_loss: 0.8247 - val_out_23_loss: 0.8228 - val_out_24_loss: 0.8260 - val_out_25_loss: 0.8307 - val_out_26_loss: 0.8252 - val_out_27_loss: 0.8256 - val_out_28_loss: 0.8251 - val_out_29_loss: 0.8272 - val_out_30_loss: 0.8238 - val_out_31_loss: 0.8224 - val_out_32_loss: 0.8221 - val_out_acc: 0.7202 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7158 - val_out_2_acc: 0.7158 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.7158 - val_out_5_acc: 0.7223 - val_out_6_acc: 0.7137 - val_out_7_acc: 0.7202 - val_out_8_acc: 0.7158 - val_out_9_acc: 0.7158 - val_out_10_acc: 0.7202 - val_out_11_acc: 0.7115 - val_out_12_acc: 0.7245 - val_out_13_acc: 0.7137 - val_out_14_acc: 0.7158 - val_out_15_acc: 0.7202 - val_out_16_acc: 0.7223 - val_out_17_acc: 0.7223 - val_out_18_acc: 0.7115 - val_out_19_acc: 0.7115 - val_out_20_acc: 0.7093 - val_out_21_acc: 0.7093 - val_out_22_acc: 0.7158 - val_out_23_acc: 0.7202 - val_out_24_acc: 0.7050 - val_out_25_acc: 0.7137 - val_out_26_acc: 0.7158 - val_out_27_acc: 0.7137 - val_out_28_acc: 0.7115 - val_out_29_acc: 0.7115 - val_out_30_acc: 0.7180 - val_out_31_acc: 0.7115 - val_out_32_acc: 0.7180\n",
      "Epoch 96/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.8782 - out_loss: 0.7324 - out_0_loss: 0.5455 - out_1_loss: 0.8264 - out_2_loss: 0.8202 - out_3_loss: 0.8273 - out_4_loss: 0.8178 - out_5_loss: 0.8248 - out_6_loss: 0.8411 - out_7_loss: 0.8393 - out_8_loss: 0.8233 - out_9_loss: 0.8286 - out_10_loss: 0.8291 - out_11_loss: 0.8236 - out_12_loss: 0.8329 - out_13_loss: 0.8385 - out_14_loss: 0.8402 - out_15_loss: 0.8242 - out_16_loss: 0.8400 - out_17_loss: 0.8374 - out_18_loss: 0.8302 - out_19_loss: 0.8341 - out_20_loss: 0.8274 - out_21_loss: 0.8292 - out_22_loss: 0.8256 - out_23_loss: 0.8296 - out_24_loss: 0.8474 - out_25_loss: 0.8242 - out_26_loss: 0.8354 - out_27_loss: 0.8360 - out_28_loss: 0.8399 - out_29_loss: 0.8296 - out_30_loss: 0.8477 - out_31_loss: 0.8161 - out_32_loss: 0.8332 - out_acc: 0.7415 - out_0_acc: 0.8044 - out_1_acc: 0.7139 - out_2_acc: 0.7077 - out_3_acc: 0.6965 - out_4_acc: 0.7151 - out_5_acc: 0.6968 - out_6_acc: 0.7043 - out_7_acc: 0.7083 - out_8_acc: 0.7154 - out_9_acc: 0.7095 - out_10_acc: 0.7040 - out_11_acc: 0.7080 - out_12_acc: 0.7037 - out_13_acc: 0.7055 - out_14_acc: 0.7071 - out_15_acc: 0.7080 - out_16_acc: 0.7049 - out_17_acc: 0.6981 - out_18_acc: 0.7018 - out_19_acc: 0.7009 - out_20_acc: 0.7052 - out_21_acc: 0.7021 - out_22_acc: 0.7148 - out_23_acc: 0.7009 - out_24_acc: 0.6990 - out_25_acc: 0.7080 - out_26_acc: 0.7015 - out_27_acc: 0.7043 - out_28_acc: 0.6984 - out_29_acc: 0.7027 - out_30_acc: 0.6968 - out_31_acc: 0.7068 - out_32_acc: 0.7068 - val_loss: 29.5018 - val_out_loss: 0.8016 - val_out_0_loss: 0.5294 - val_out_1_loss: 0.8428 - val_out_2_loss: 0.8419 - val_out_3_loss: 0.8404 - val_out_4_loss: 0.8458 - val_out_5_loss: 0.8353 - val_out_6_loss: 0.8465 - val_out_7_loss: 0.8449 - val_out_8_loss: 0.8430 - val_out_9_loss: 0.8467 - val_out_10_loss: 0.8423 - val_out_11_loss: 0.8494 - val_out_12_loss: 0.8394 - val_out_13_loss: 0.8405 - val_out_14_loss: 0.8450 - val_out_15_loss: 0.8455 - val_out_16_loss: 0.8382 - val_out_17_loss: 0.8412 - val_out_18_loss: 0.8450 - val_out_19_loss: 0.8444 - val_out_20_loss: 0.8505 - val_out_21_loss: 0.8406 - val_out_22_loss: 0.8438 - val_out_23_loss: 0.8459 - val_out_24_loss: 0.8451 - val_out_25_loss: 0.8435 - val_out_26_loss: 0.8434 - val_out_27_loss: 0.8516 - val_out_28_loss: 0.8432 - val_out_29_loss: 0.8413 - val_out_30_loss: 0.8484 - val_out_31_loss: 0.8470 - val_out_32_loss: 0.8406 - val_out_acc: 0.7115 - val_out_0_acc: 0.8178 - val_out_1_acc: 0.7007 - val_out_2_acc: 0.7050 - val_out_3_acc: 0.7072 - val_out_4_acc: 0.7028 - val_out_5_acc: 0.7007 - val_out_6_acc: 0.7028 - val_out_7_acc: 0.7072 - val_out_8_acc: 0.7007 - val_out_9_acc: 0.6985 - val_out_10_acc: 0.7115 - val_out_11_acc: 0.7050 - val_out_12_acc: 0.7115 - val_out_13_acc: 0.7072 - val_out_14_acc: 0.7050 - val_out_15_acc: 0.7028 - val_out_16_acc: 0.7028 - val_out_17_acc: 0.7115 - val_out_18_acc: 0.6985 - val_out_19_acc: 0.7028 - val_out_20_acc: 0.7050 - val_out_21_acc: 0.7072 - val_out_22_acc: 0.7050 - val_out_23_acc: 0.7050 - val_out_24_acc: 0.7028 - val_out_25_acc: 0.7093 - val_out_26_acc: 0.7028 - val_out_27_acc: 0.7007 - val_out_28_acc: 0.6985 - val_out_29_acc: 0.7007 - val_out_30_acc: 0.7028 - val_out_31_acc: 0.7007 - val_out_32_acc: 0.7093\n",
      "Epoch 97/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 28.1391 - out_loss: 0.7362 - out_0_loss: 0.5356 - out_1_loss: 0.8409 - out_2_loss: 0.8437 - out_3_loss: 0.8491 - out_4_loss: 0.8249 - out_5_loss: 0.8316 - out_6_loss: 0.8481 - out_7_loss: 0.8390 - out_8_loss: 0.8564 - out_9_loss: 0.8439 - out_10_loss: 0.8422 - out_11_loss: 0.8412 - out_12_loss: 0.8345 - out_13_loss: 0.8283 - out_14_loss: 0.8436 - out_15_loss: 0.8467 - out_16_loss: 0.8441 - out_17_loss: 0.8256 - out_18_loss: 0.8377 - out_19_loss: 0.8333 - out_20_loss: 0.8264 - out_21_loss: 0.8427 - out_22_loss: 0.8425 - out_23_loss: 0.8216 - out_24_loss: 0.8490 - out_25_loss: 0.8417 - out_26_loss: 0.8392 - out_27_loss: 0.8456 - out_28_loss: 0.8528 - out_29_loss: 0.8351 - out_30_loss: 0.8457 - out_31_loss: 0.8273 - out_32_loss: 0.8429 - out_acc: 0.7325 - out_0_acc: 0.8143 - out_1_acc: 0.6937 - out_2_acc: 0.6925 - out_3_acc: 0.6925 - out_4_acc: 0.7006 - out_5_acc: 0.6981 - out_6_acc: 0.6885 - out_7_acc: 0.6978 - out_8_acc: 0.6990 - out_9_acc: 0.6965 - out_10_acc: 0.6869 - out_11_acc: 0.7030 - out_12_acc: 0.6987 - out_13_acc: 0.6993 - out_14_acc: 0.6934 - out_15_acc: 0.6953 - out_16_acc: 0.6947 - out_17_acc: 0.7015 - out_18_acc: 0.6999 - out_19_acc: 0.7058 - out_20_acc: 0.6981 - out_21_acc: 0.6950 - out_22_acc: 0.7018 - out_23_acc: 0.7030 - out_24_acc: 0.6925 - out_25_acc: 0.6981 - out_26_acc: 0.6971 - out_27_acc: 0.6913 - out_28_acc: 0.6965 - out_29_acc: 0.6953 - out_30_acc: 0.6962 - out_31_acc: 0.7046 - out_32_acc: 0.6959 - val_loss: 29.6013 - val_out_loss: 0.8167 - val_out_0_loss: 0.6615 - val_out_1_loss: 0.8435 - val_out_2_loss: 0.8408 - val_out_3_loss: 0.8361 - val_out_4_loss: 0.8424 - val_out_5_loss: 0.8389 - val_out_6_loss: 0.8450 - val_out_7_loss: 0.8390 - val_out_8_loss: 0.8422 - val_out_9_loss: 0.8420 - val_out_10_loss: 0.8474 - val_out_11_loss: 0.8408 - val_out_12_loss: 0.8395 - val_out_13_loss: 0.8408 - val_out_14_loss: 0.8461 - val_out_15_loss: 0.8447 - val_out_16_loss: 0.8392 - val_out_17_loss: 0.8468 - val_out_18_loss: 0.8431 - val_out_19_loss: 0.8426 - val_out_20_loss: 0.8463 - val_out_21_loss: 0.8385 - val_out_22_loss: 0.8430 - val_out_23_loss: 0.8480 - val_out_24_loss: 0.8407 - val_out_25_loss: 0.8437 - val_out_26_loss: 0.8442 - val_out_27_loss: 0.8443 - val_out_28_loss: 0.8396 - val_out_29_loss: 0.8379 - val_out_30_loss: 0.8439 - val_out_31_loss: 0.8449 - val_out_32_loss: 0.8355 - val_out_acc: 0.7223 - val_out_0_acc: 0.7744 - val_out_1_acc: 0.7007 - val_out_2_acc: 0.7180 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.7180 - val_out_5_acc: 0.7158 - val_out_6_acc: 0.7137 - val_out_7_acc: 0.7158 - val_out_8_acc: 0.7137 - val_out_9_acc: 0.7093 - val_out_10_acc: 0.7180 - val_out_11_acc: 0.7158 - val_out_12_acc: 0.7223 - val_out_13_acc: 0.7223 - val_out_14_acc: 0.7202 - val_out_15_acc: 0.7158 - val_out_16_acc: 0.7202 - val_out_17_acc: 0.7202 - val_out_18_acc: 0.7158 - val_out_19_acc: 0.7137 - val_out_20_acc: 0.7245 - val_out_21_acc: 0.7223 - val_out_22_acc: 0.7202 - val_out_23_acc: 0.7180 - val_out_24_acc: 0.7093 - val_out_25_acc: 0.7115 - val_out_26_acc: 0.7180 - val_out_27_acc: 0.7115 - val_out_28_acc: 0.7050 - val_out_29_acc: 0.7202 - val_out_30_acc: 0.7115 - val_out_31_acc: 0.7158 - val_out_32_acc: 0.7158\n",
      "Epoch 98/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 28.1407 - out_loss: 0.7369 - out_0_loss: 0.5432 - out_1_loss: 0.8210 - out_2_loss: 0.8342 - out_3_loss: 0.8370 - out_4_loss: 0.8369 - out_5_loss: 0.8419 - out_6_loss: 0.8460 - out_7_loss: 0.8407 - out_8_loss: 0.8451 - out_9_loss: 0.8504 - out_10_loss: 0.8342 - out_11_loss: 0.8372 - out_12_loss: 0.8320 - out_13_loss: 0.8460 - out_14_loss: 0.8301 - out_15_loss: 0.8444 - out_16_loss: 0.8409 - out_17_loss: 0.8474 - out_18_loss: 0.8472 - out_19_loss: 0.8451 - out_20_loss: 0.8495 - out_21_loss: 0.8336 - out_22_loss: 0.8248 - out_23_loss: 0.8335 - out_24_loss: 0.8449 - out_25_loss: 0.8411 - out_26_loss: 0.8350 - out_27_loss: 0.8532 - out_28_loss: 0.8331 - out_29_loss: 0.8526 - out_30_loss: 0.8288 - out_31_loss: 0.8344 - out_32_loss: 0.8382 - out_acc: 0.7347 - out_0_acc: 0.8010 - out_1_acc: 0.7015 - out_2_acc: 0.7006 - out_3_acc: 0.6981 - out_4_acc: 0.7102 - out_5_acc: 0.7033 - out_6_acc: 0.6996 - out_7_acc: 0.7002 - out_8_acc: 0.6925 - out_9_acc: 0.6968 - out_10_acc: 0.7092 - out_11_acc: 0.6971 - out_12_acc: 0.7006 - out_13_acc: 0.7002 - out_14_acc: 0.7012 - out_15_acc: 0.6931 - out_16_acc: 0.6996 - out_17_acc: 0.6931 - out_18_acc: 0.6993 - out_19_acc: 0.6869 - out_20_acc: 0.6981 - out_21_acc: 0.6990 - out_22_acc: 0.7086 - out_23_acc: 0.6996 - out_24_acc: 0.6931 - out_25_acc: 0.6993 - out_26_acc: 0.6971 - out_27_acc: 0.6897 - out_28_acc: 0.6962 - out_29_acc: 0.6913 - out_30_acc: 0.7120 - out_31_acc: 0.7015 - out_32_acc: 0.7052 - val_loss: 27.1476 - val_out_loss: 0.7453 - val_out_0_loss: 0.6290 - val_out_1_loss: 0.7753 - val_out_2_loss: 0.7710 - val_out_3_loss: 0.7706 - val_out_4_loss: 0.7745 - val_out_5_loss: 0.7704 - val_out_6_loss: 0.7734 - val_out_7_loss: 0.7688 - val_out_8_loss: 0.7729 - val_out_9_loss: 0.7738 - val_out_10_loss: 0.7746 - val_out_11_loss: 0.7720 - val_out_12_loss: 0.7686 - val_out_13_loss: 0.7686 - val_out_14_loss: 0.7724 - val_out_15_loss: 0.7735 - val_out_16_loss: 0.7661 - val_out_17_loss: 0.7668 - val_out_18_loss: 0.7718 - val_out_19_loss: 0.7771 - val_out_20_loss: 0.7775 - val_out_21_loss: 0.7656 - val_out_22_loss: 0.7702 - val_out_23_loss: 0.7734 - val_out_24_loss: 0.7726 - val_out_25_loss: 0.7741 - val_out_26_loss: 0.7738 - val_out_27_loss: 0.7715 - val_out_28_loss: 0.7777 - val_out_29_loss: 0.7691 - val_out_30_loss: 0.7744 - val_out_31_loss: 0.7664 - val_out_32_loss: 0.7703 - val_out_acc: 0.7072 - val_out_0_acc: 0.7896 - val_out_1_acc: 0.7028 - val_out_2_acc: 0.7093 - val_out_3_acc: 0.7072 - val_out_4_acc: 0.7093 - val_out_5_acc: 0.7028 - val_out_6_acc: 0.7007 - val_out_7_acc: 0.7180 - val_out_8_acc: 0.7028 - val_out_9_acc: 0.7028 - val_out_10_acc: 0.7158 - val_out_11_acc: 0.7028 - val_out_12_acc: 0.7093 - val_out_13_acc: 0.7028 - val_out_14_acc: 0.7050 - val_out_15_acc: 0.7072 - val_out_16_acc: 0.7093 - val_out_17_acc: 0.7050 - val_out_18_acc: 0.7072 - val_out_19_acc: 0.7028 - val_out_20_acc: 0.7007 - val_out_21_acc: 0.7115 - val_out_22_acc: 0.7007 - val_out_23_acc: 0.7050 - val_out_24_acc: 0.7115 - val_out_25_acc: 0.7093 - val_out_26_acc: 0.7050 - val_out_27_acc: 0.7072 - val_out_28_acc: 0.7028 - val_out_29_acc: 0.7007 - val_out_30_acc: 0.6985 - val_out_31_acc: 0.7007 - val_out_32_acc: 0.7093\n",
      "Epoch 99/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 28.2504 - out_loss: 0.7371 - out_0_loss: 0.5461 - out_1_loss: 0.8384 - out_2_loss: 0.8419 - out_3_loss: 0.8257 - out_4_loss: 0.8288 - out_5_loss: 0.8395 - out_6_loss: 0.8402 - out_7_loss: 0.8378 - out_8_loss: 0.8484 - out_9_loss: 0.8371 - out_10_loss: 0.8444 - out_11_loss: 0.8541 - out_12_loss: 0.8443 - out_13_loss: 0.8465 - out_14_loss: 0.8553 - out_15_loss: 0.8541 - out_16_loss: 0.8441 - out_17_loss: 0.8472 - out_18_loss: 0.8460 - out_19_loss: 0.8625 - out_20_loss: 0.8331 - out_21_loss: 0.8452 - out_22_loss: 0.8341 - out_23_loss: 0.8323 - out_24_loss: 0.8565 - out_25_loss: 0.8423 - out_26_loss: 0.8467 - out_27_loss: 0.8401 - out_28_loss: 0.8365 - out_29_loss: 0.8408 - out_30_loss: 0.8419 - out_31_loss: 0.8499 - out_32_loss: 0.8316 - out_acc: 0.7436 - out_0_acc: 0.8146 - out_1_acc: 0.7037 - out_2_acc: 0.7117 - out_3_acc: 0.7086 - out_4_acc: 0.7009 - out_5_acc: 0.7009 - out_6_acc: 0.7061 - out_7_acc: 0.7052 - out_8_acc: 0.7037 - out_9_acc: 0.7024 - out_10_acc: 0.6971 - out_11_acc: 0.6993 - out_12_acc: 0.6999 - out_13_acc: 0.6996 - out_14_acc: 0.7064 - out_15_acc: 0.6940 - out_16_acc: 0.7024 - out_17_acc: 0.7046 - out_18_acc: 0.7089 - out_19_acc: 0.7002 - out_20_acc: 0.7089 - out_21_acc: 0.7009 - out_22_acc: 0.7077 - out_23_acc: 0.7049 - out_24_acc: 0.7002 - out_25_acc: 0.7061 - out_26_acc: 0.7074 - out_27_acc: 0.7015 - out_28_acc: 0.7089 - out_29_acc: 0.7040 - out_30_acc: 0.7083 - out_31_acc: 0.7024 - out_32_acc: 0.7102 - val_loss: 25.0185 - val_out_loss: 0.6890 - val_out_0_loss: 0.5501 - val_out_1_loss: 0.7118 - val_out_2_loss: 0.7098 - val_out_3_loss: 0.7142 - val_out_4_loss: 0.7142 - val_out_5_loss: 0.7070 - val_out_6_loss: 0.7163 - val_out_7_loss: 0.7107 - val_out_8_loss: 0.7109 - val_out_9_loss: 0.7163 - val_out_10_loss: 0.7112 - val_out_11_loss: 0.7150 - val_out_12_loss: 0.7071 - val_out_13_loss: 0.7134 - val_out_14_loss: 0.7133 - val_out_15_loss: 0.7133 - val_out_16_loss: 0.7113 - val_out_17_loss: 0.7139 - val_out_18_loss: 0.7131 - val_out_19_loss: 0.7153 - val_out_20_loss: 0.7161 - val_out_21_loss: 0.7087 - val_out_22_loss: 0.7075 - val_out_23_loss: 0.7115 - val_out_24_loss: 0.7117 - val_out_25_loss: 0.7156 - val_out_26_loss: 0.7101 - val_out_27_loss: 0.7154 - val_out_28_loss: 0.7123 - val_out_29_loss: 0.7122 - val_out_30_loss: 0.7143 - val_out_31_loss: 0.7060 - val_out_32_loss: 0.7097 - val_out_acc: 0.7354 - val_out_0_acc: 0.8048 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7289 - val_out_3_acc: 0.7332 - val_out_4_acc: 0.7245 - val_out_5_acc: 0.7354 - val_out_6_acc: 0.7289 - val_out_7_acc: 0.7223 - val_out_8_acc: 0.7158 - val_out_9_acc: 0.7267 - val_out_10_acc: 0.7267 - val_out_11_acc: 0.7267 - val_out_12_acc: 0.7267 - val_out_13_acc: 0.7267 - val_out_14_acc: 0.7289 - val_out_15_acc: 0.7245 - val_out_16_acc: 0.7158 - val_out_17_acc: 0.7245 - val_out_18_acc: 0.7354 - val_out_19_acc: 0.7202 - val_out_20_acc: 0.7310 - val_out_21_acc: 0.7289 - val_out_22_acc: 0.7223 - val_out_23_acc: 0.7093 - val_out_24_acc: 0.7223 - val_out_25_acc: 0.7354 - val_out_26_acc: 0.7267 - val_out_27_acc: 0.7202 - val_out_28_acc: 0.7223 - val_out_29_acc: 0.7245 - val_out_30_acc: 0.7245 - val_out_31_acc: 0.7289 - val_out_32_acc: 0.7310\n",
      "Epoch 100/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.9457 - out_loss: 0.7345 - out_0_loss: 0.5502 - out_1_loss: 0.8222 - out_2_loss: 0.8294 - out_3_loss: 0.8369 - out_4_loss: 0.8206 - out_5_loss: 0.8364 - out_6_loss: 0.8395 - out_7_loss: 0.8308 - out_8_loss: 0.8286 - out_9_loss: 0.8320 - out_10_loss: 0.8234 - out_11_loss: 0.8370 - out_12_loss: 0.8295 - out_13_loss: 0.8317 - out_14_loss: 0.8357 - out_15_loss: 0.8347 - out_16_loss: 0.8358 - out_17_loss: 0.8384 - out_18_loss: 0.8398 - out_19_loss: 0.8168 - out_20_loss: 0.8323 - out_21_loss: 0.8460 - out_22_loss: 0.8311 - out_23_loss: 0.8393 - out_24_loss: 0.8290 - out_25_loss: 0.8431 - out_26_loss: 0.8289 - out_27_loss: 0.8259 - out_28_loss: 0.8394 - out_29_loss: 0.8336 - out_30_loss: 0.8401 - out_31_loss: 0.8278 - out_32_loss: 0.8455 - out_acc: 0.7393 - out_0_acc: 0.8041 - out_1_acc: 0.7055 - out_2_acc: 0.7030 - out_3_acc: 0.6962 - out_4_acc: 0.7055 - out_5_acc: 0.7009 - out_6_acc: 0.6962 - out_7_acc: 0.7030 - out_8_acc: 0.7080 - out_9_acc: 0.7033 - out_10_acc: 0.7033 - out_11_acc: 0.7015 - out_12_acc: 0.7043 - out_13_acc: 0.6990 - out_14_acc: 0.6968 - out_15_acc: 0.7018 - out_16_acc: 0.7002 - out_17_acc: 0.6968 - out_18_acc: 0.6993 - out_19_acc: 0.7064 - out_20_acc: 0.7015 - out_21_acc: 0.6953 - out_22_acc: 0.7012 - out_23_acc: 0.6953 - out_24_acc: 0.6975 - out_25_acc: 0.7018 - out_26_acc: 0.6944 - out_27_acc: 0.6984 - out_28_acc: 0.7030 - out_29_acc: 0.7012 - out_30_acc: 0.6962 - out_31_acc: 0.7055 - out_32_acc: 0.6931 - val_loss: 27.3158 - val_out_loss: 0.7469 - val_out_0_loss: 0.5444 - val_out_1_loss: 0.7803 - val_out_2_loss: 0.7770 - val_out_3_loss: 0.7754 - val_out_4_loss: 0.7797 - val_out_5_loss: 0.7744 - val_out_6_loss: 0.7843 - val_out_7_loss: 0.7801 - val_out_8_loss: 0.7773 - val_out_9_loss: 0.7801 - val_out_10_loss: 0.7808 - val_out_11_loss: 0.7804 - val_out_12_loss: 0.7739 - val_out_13_loss: 0.7856 - val_out_14_loss: 0.7812 - val_out_15_loss: 0.7819 - val_out_16_loss: 0.7808 - val_out_17_loss: 0.7841 - val_out_18_loss: 0.7878 - val_out_19_loss: 0.7823 - val_out_20_loss: 0.7798 - val_out_21_loss: 0.7752 - val_out_22_loss: 0.7788 - val_out_23_loss: 0.7771 - val_out_24_loss: 0.7743 - val_out_25_loss: 0.7836 - val_out_26_loss: 0.7798 - val_out_27_loss: 0.7793 - val_out_28_loss: 0.7790 - val_out_29_loss: 0.7755 - val_out_30_loss: 0.7757 - val_out_31_loss: 0.7779 - val_out_32_loss: 0.7799 - val_out_acc: 0.7354 - val_out_0_acc: 0.8069 - val_out_1_acc: 0.7245 - val_out_2_acc: 0.7289 - val_out_3_acc: 0.7245 - val_out_4_acc: 0.7245 - val_out_5_acc: 0.7245 - val_out_6_acc: 0.7223 - val_out_7_acc: 0.7289 - val_out_8_acc: 0.7310 - val_out_9_acc: 0.7202 - val_out_10_acc: 0.7310 - val_out_11_acc: 0.7267 - val_out_12_acc: 0.7289 - val_out_13_acc: 0.7223 - val_out_14_acc: 0.7267 - val_out_15_acc: 0.7180 - val_out_16_acc: 0.7245 - val_out_17_acc: 0.7158 - val_out_18_acc: 0.7267 - val_out_19_acc: 0.7223 - val_out_20_acc: 0.7267 - val_out_21_acc: 0.7289 - val_out_22_acc: 0.7267 - val_out_23_acc: 0.7223 - val_out_24_acc: 0.7267 - val_out_25_acc: 0.7202 - val_out_26_acc: 0.7267 - val_out_27_acc: 0.7310 - val_out_28_acc: 0.7289 - val_out_29_acc: 0.7267 - val_out_30_acc: 0.7180 - val_out_31_acc: 0.7245 - val_out_32_acc: 0.7289\n",
      "Epoch 101/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.1130 - out_loss: 0.7096 - out_0_loss: 0.5369 - out_1_loss: 0.8166 - out_2_loss: 0.8044 - out_3_loss: 0.8078 - out_4_loss: 0.8243 - out_5_loss: 0.8190 - out_6_loss: 0.7969 - out_7_loss: 0.8037 - out_8_loss: 0.7965 - out_9_loss: 0.8080 - out_10_loss: 0.8019 - out_11_loss: 0.8130 - out_12_loss: 0.8051 - out_13_loss: 0.8012 - out_14_loss: 0.8032 - out_15_loss: 0.7976 - out_16_loss: 0.8134 - out_17_loss: 0.8173 - out_18_loss: 0.8241 - out_19_loss: 0.8092 - out_20_loss: 0.8163 - out_21_loss: 0.7944 - out_22_loss: 0.8016 - out_23_loss: 0.8060 - out_24_loss: 0.8077 - out_25_loss: 0.8189 - out_26_loss: 0.8054 - out_27_loss: 0.8063 - out_28_loss: 0.8007 - out_29_loss: 0.8325 - out_30_loss: 0.7962 - out_31_loss: 0.8035 - out_32_loss: 0.8139 - out_acc: 0.7498 - out_0_acc: 0.8134 - out_1_acc: 0.7117 - out_2_acc: 0.7083 - out_3_acc: 0.7083 - out_4_acc: 0.7015 - out_5_acc: 0.7099 - out_6_acc: 0.7188 - out_7_acc: 0.7002 - out_8_acc: 0.7142 - out_9_acc: 0.7139 - out_10_acc: 0.7123 - out_11_acc: 0.7102 - out_12_acc: 0.7111 - out_13_acc: 0.7126 - out_14_acc: 0.7092 - out_15_acc: 0.7120 - out_16_acc: 0.7123 - out_17_acc: 0.7015 - out_18_acc: 0.6987 - out_19_acc: 0.7061 - out_20_acc: 0.7126 - out_21_acc: 0.7154 - out_22_acc: 0.7130 - out_23_acc: 0.7151 - out_24_acc: 0.7055 - out_25_acc: 0.7049 - out_26_acc: 0.7052 - out_27_acc: 0.7126 - out_28_acc: 0.7173 - out_29_acc: 0.6891 - out_30_acc: 0.7204 - out_31_acc: 0.7092 - out_32_acc: 0.7114 - val_loss: 26.6232 - val_out_loss: 0.7325 - val_out_0_loss: 0.5537 - val_out_1_loss: 0.7625 - val_out_2_loss: 0.7577 - val_out_3_loss: 0.7585 - val_out_4_loss: 0.7599 - val_out_5_loss: 0.7588 - val_out_6_loss: 0.7569 - val_out_7_loss: 0.7577 - val_out_8_loss: 0.7608 - val_out_9_loss: 0.7575 - val_out_10_loss: 0.7559 - val_out_11_loss: 0.7572 - val_out_12_loss: 0.7571 - val_out_13_loss: 0.7538 - val_out_14_loss: 0.7663 - val_out_15_loss: 0.7613 - val_out_16_loss: 0.7617 - val_out_17_loss: 0.7593 - val_out_18_loss: 0.7596 - val_out_19_loss: 0.7576 - val_out_20_loss: 0.7603 - val_out_21_loss: 0.7592 - val_out_22_loss: 0.7610 - val_out_23_loss: 0.7588 - val_out_24_loss: 0.7605 - val_out_25_loss: 0.7560 - val_out_26_loss: 0.7613 - val_out_27_loss: 0.7565 - val_out_28_loss: 0.7583 - val_out_29_loss: 0.7588 - val_out_30_loss: 0.7591 - val_out_31_loss: 0.7565 - val_out_32_loss: 0.7569 - val_out_acc: 0.7137 - val_out_0_acc: 0.8113 - val_out_1_acc: 0.7072 - val_out_2_acc: 0.7072 - val_out_3_acc: 0.7093 - val_out_4_acc: 0.7050 - val_out_5_acc: 0.7137 - val_out_6_acc: 0.7072 - val_out_7_acc: 0.7072 - val_out_8_acc: 0.7093 - val_out_9_acc: 0.7028 - val_out_10_acc: 0.7115 - val_out_11_acc: 0.7050 - val_out_12_acc: 0.7072 - val_out_13_acc: 0.7093 - val_out_14_acc: 0.7050 - val_out_15_acc: 0.7007 - val_out_16_acc: 0.7028 - val_out_17_acc: 0.7028 - val_out_18_acc: 0.7093 - val_out_19_acc: 0.7072 - val_out_20_acc: 0.6963 - val_out_21_acc: 0.7093 - val_out_22_acc: 0.7093 - val_out_23_acc: 0.7137 - val_out_24_acc: 0.7158 - val_out_25_acc: 0.7050 - val_out_26_acc: 0.7050 - val_out_27_acc: 0.7093 - val_out_28_acc: 0.7028 - val_out_29_acc: 0.7007 - val_out_30_acc: 0.7093 - val_out_31_acc: 0.7072 - val_out_32_acc: 0.7115\n",
      "Epoch 102/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 28.0844 - out_loss: 0.7342 - out_0_loss: 0.5437 - out_1_loss: 0.8357 - out_2_loss: 0.8396 - out_3_loss: 0.8438 - out_4_loss: 0.8441 - out_5_loss: 0.8448 - out_6_loss: 0.8285 - out_7_loss: 0.8457 - out_8_loss: 0.8395 - out_9_loss: 0.8409 - out_10_loss: 0.8309 - out_11_loss: 0.8403 - out_12_loss: 0.8377 - out_13_loss: 0.8355 - out_14_loss: 0.8365 - out_15_loss: 0.8386 - out_16_loss: 0.8486 - out_17_loss: 0.8409 - out_18_loss: 0.8285 - out_19_loss: 0.8265 - out_20_loss: 0.8358 - out_21_loss: 0.8305 - out_22_loss: 0.8401 - out_23_loss: 0.8344 - out_24_loss: 0.8396 - out_25_loss: 0.8299 - out_26_loss: 0.8401 - out_27_loss: 0.8378 - out_28_loss: 0.8422 - out_29_loss: 0.8421 - out_30_loss: 0.8357 - out_31_loss: 0.8334 - out_32_loss: 0.8384 - out_acc: 0.7381 - out_0_acc: 0.8087 - out_1_acc: 0.7009 - out_2_acc: 0.7006 - out_3_acc: 0.6953 - out_4_acc: 0.6956 - out_5_acc: 0.6937 - out_6_acc: 0.6965 - out_7_acc: 0.6996 - out_8_acc: 0.6888 - out_9_acc: 0.6968 - out_10_acc: 0.7099 - out_11_acc: 0.6891 - out_12_acc: 0.6971 - out_13_acc: 0.6937 - out_14_acc: 0.6999 - out_15_acc: 0.7030 - out_16_acc: 0.6866 - out_17_acc: 0.7002 - out_18_acc: 0.7037 - out_19_acc: 0.7068 - out_20_acc: 0.7055 - out_21_acc: 0.7009 - out_22_acc: 0.7015 - out_23_acc: 0.7021 - out_24_acc: 0.6987 - out_25_acc: 0.6984 - out_26_acc: 0.6990 - out_27_acc: 0.6956 - out_28_acc: 0.6909 - out_29_acc: 0.7009 - out_30_acc: 0.6956 - out_31_acc: 0.6971 - out_32_acc: 0.7064 - val_loss: 25.8766 - val_out_loss: 0.7115 - val_out_0_loss: 0.4612 - val_out_1_loss: 0.7371 - val_out_2_loss: 0.7379 - val_out_3_loss: 0.7343 - val_out_4_loss: 0.7421 - val_out_5_loss: 0.7361 - val_out_6_loss: 0.7382 - val_out_7_loss: 0.7379 - val_out_8_loss: 0.7363 - val_out_9_loss: 0.7406 - val_out_10_loss: 0.7436 - val_out_11_loss: 0.7456 - val_out_12_loss: 0.7356 - val_out_13_loss: 0.7408 - val_out_14_loss: 0.7457 - val_out_15_loss: 0.7414 - val_out_16_loss: 0.7399 - val_out_17_loss: 0.7401 - val_out_18_loss: 0.7380 - val_out_19_loss: 0.7438 - val_out_20_loss: 0.7460 - val_out_21_loss: 0.7329 - val_out_22_loss: 0.7370 - val_out_23_loss: 0.7413 - val_out_24_loss: 0.7391 - val_out_25_loss: 0.7411 - val_out_26_loss: 0.7403 - val_out_27_loss: 0.7411 - val_out_28_loss: 0.7408 - val_out_29_loss: 0.7398 - val_out_30_loss: 0.7468 - val_out_31_loss: 0.7429 - val_out_32_loss: 0.7359 - val_out_acc: 0.7375 - val_out_0_acc: 0.8286 - val_out_1_acc: 0.7180 - val_out_2_acc: 0.7354 - val_out_3_acc: 0.7223 - val_out_4_acc: 0.7267 - val_out_5_acc: 0.7289 - val_out_6_acc: 0.7310 - val_out_7_acc: 0.7267 - val_out_8_acc: 0.7332 - val_out_9_acc: 0.7223 - val_out_10_acc: 0.7289 - val_out_11_acc: 0.7289 - val_out_12_acc: 0.7310 - val_out_13_acc: 0.7332 - val_out_14_acc: 0.7245 - val_out_15_acc: 0.7267 - val_out_16_acc: 0.7267 - val_out_17_acc: 0.7245 - val_out_18_acc: 0.7245 - val_out_19_acc: 0.7202 - val_out_20_acc: 0.7332 - val_out_21_acc: 0.7245 - val_out_22_acc: 0.7289 - val_out_23_acc: 0.7289 - val_out_24_acc: 0.7310 - val_out_25_acc: 0.7289 - val_out_26_acc: 0.7354 - val_out_27_acc: 0.7180 - val_out_28_acc: 0.7310 - val_out_29_acc: 0.7289 - val_out_30_acc: 0.7332 - val_out_31_acc: 0.7310 - val_out_32_acc: 0.7332\n",
      "Epoch 103/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.6359 - out_loss: 0.7259 - out_0_loss: 0.5403 - out_1_loss: 0.8198 - out_2_loss: 0.8278 - out_3_loss: 0.8100 - out_4_loss: 0.8312 - out_5_loss: 0.8233 - out_6_loss: 0.8425 - out_7_loss: 0.8348 - out_8_loss: 0.8209 - out_9_loss: 0.8342 - out_10_loss: 0.8219 - out_11_loss: 0.8224 - out_12_loss: 0.8212 - out_13_loss: 0.8163 - out_14_loss: 0.8276 - out_15_loss: 0.8276 - out_16_loss: 0.8232 - out_17_loss: 0.8170 - out_18_loss: 0.8162 - out_19_loss: 0.8176 - out_20_loss: 0.8257 - out_21_loss: 0.8328 - out_22_loss: 0.8283 - out_23_loss: 0.8311 - out_24_loss: 0.8178 - out_25_loss: 0.8310 - out_26_loss: 0.8143 - out_27_loss: 0.8216 - out_28_loss: 0.8136 - out_29_loss: 0.8226 - out_30_loss: 0.8244 - out_31_loss: 0.8318 - out_32_loss: 0.8191 - out_acc: 0.7331 - out_0_acc: 0.8177 - out_1_acc: 0.6928 - out_2_acc: 0.7046 - out_3_acc: 0.6968 - out_4_acc: 0.6894 - out_5_acc: 0.6971 - out_6_acc: 0.6878 - out_7_acc: 0.6947 - out_8_acc: 0.7105 - out_9_acc: 0.6882 - out_10_acc: 0.6996 - out_11_acc: 0.7030 - out_12_acc: 0.6981 - out_13_acc: 0.6981 - out_14_acc: 0.6875 - out_15_acc: 0.6975 - out_16_acc: 0.6959 - out_17_acc: 0.6965 - out_18_acc: 0.7071 - out_19_acc: 0.6990 - out_20_acc: 0.6925 - out_21_acc: 0.6965 - out_22_acc: 0.6996 - out_23_acc: 0.7018 - out_24_acc: 0.7046 - out_25_acc: 0.7018 - out_26_acc: 0.6990 - out_27_acc: 0.6978 - out_28_acc: 0.6965 - out_29_acc: 0.7068 - out_30_acc: 0.6934 - out_31_acc: 0.6944 - out_32_acc: 0.6928 - val_loss: 28.5141 - val_out_loss: 0.7858 - val_out_0_loss: 0.6011 - val_out_1_loss: 0.8094 - val_out_2_loss: 0.8106 - val_out_3_loss: 0.8112 - val_out_4_loss: 0.8114 - val_out_5_loss: 0.8122 - val_out_6_loss: 0.8099 - val_out_7_loss: 0.8100 - val_out_8_loss: 0.8108 - val_out_9_loss: 0.8099 - val_out_10_loss: 0.8137 - val_out_11_loss: 0.8163 - val_out_12_loss: 0.8095 - val_out_13_loss: 0.8096 - val_out_14_loss: 0.8171 - val_out_15_loss: 0.8161 - val_out_16_loss: 0.8144 - val_out_17_loss: 0.8152 - val_out_18_loss: 0.8100 - val_out_19_loss: 0.8181 - val_out_20_loss: 0.8140 - val_out_21_loss: 0.8123 - val_out_22_loss: 0.8134 - val_out_23_loss: 0.8119 - val_out_24_loss: 0.8093 - val_out_25_loss: 0.8117 - val_out_26_loss: 0.8163 - val_out_27_loss: 0.8151 - val_out_28_loss: 0.8128 - val_out_29_loss: 0.8129 - val_out_30_loss: 0.8116 - val_out_31_loss: 0.8113 - val_out_32_loss: 0.8106 - val_out_acc: 0.6985 - val_out_0_acc: 0.7809 - val_out_1_acc: 0.6920 - val_out_2_acc: 0.6963 - val_out_3_acc: 0.6985 - val_out_4_acc: 0.6963 - val_out_5_acc: 0.6985 - val_out_6_acc: 0.6963 - val_out_7_acc: 0.6963 - val_out_8_acc: 0.6985 - val_out_9_acc: 0.6855 - val_out_10_acc: 0.6920 - val_out_11_acc: 0.6963 - val_out_12_acc: 0.6985 - val_out_13_acc: 0.6963 - val_out_14_acc: 0.6920 - val_out_15_acc: 0.6920 - val_out_16_acc: 0.6963 - val_out_17_acc: 0.7007 - val_out_18_acc: 0.6985 - val_out_19_acc: 0.6898 - val_out_20_acc: 0.6963 - val_out_21_acc: 0.6963 - val_out_22_acc: 0.7007 - val_out_23_acc: 0.6898 - val_out_24_acc: 0.6985 - val_out_25_acc: 0.6920 - val_out_26_acc: 0.6963 - val_out_27_acc: 0.6963 - val_out_28_acc: 0.6985 - val_out_29_acc: 0.6963 - val_out_30_acc: 0.6941 - val_out_31_acc: 0.6941 - val_out_32_acc: 0.6920\n",
      "Epoch 104/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 27.5872 - out_loss: 0.7243 - out_0_loss: 0.5210 - out_1_loss: 0.8281 - out_2_loss: 0.8199 - out_3_loss: 0.8251 - out_4_loss: 0.8299 - out_5_loss: 0.8451 - out_6_loss: 0.8078 - out_7_loss: 0.8250 - out_8_loss: 0.8313 - out_9_loss: 0.8199 - out_10_loss: 0.8365 - out_11_loss: 0.8351 - out_12_loss: 0.8203 - out_13_loss: 0.8262 - out_14_loss: 0.8200 - out_15_loss: 0.8160 - out_16_loss: 0.8285 - out_17_loss: 0.8238 - out_18_loss: 0.8128 - out_19_loss: 0.8276 - out_20_loss: 0.8113 - out_21_loss: 0.8295 - out_22_loss: 0.8230 - out_23_loss: 0.8105 - out_24_loss: 0.8202 - out_25_loss: 0.8216 - out_26_loss: 0.8109 - out_27_loss: 0.8264 - out_28_loss: 0.8220 - out_29_loss: 0.8242 - out_30_loss: 0.8232 - out_31_loss: 0.8291 - out_32_loss: 0.8111 - out_acc: 0.7387 - out_0_acc: 0.8143 - out_1_acc: 0.6984 - out_2_acc: 0.7046 - out_3_acc: 0.7061 - out_4_acc: 0.6981 - out_5_acc: 0.6965 - out_6_acc: 0.7092 - out_7_acc: 0.7002 - out_8_acc: 0.7018 - out_9_acc: 0.7071 - out_10_acc: 0.7043 - out_11_acc: 0.6996 - out_12_acc: 0.7046 - out_13_acc: 0.7046 - out_14_acc: 0.7061 - out_15_acc: 0.7052 - out_16_acc: 0.7077 - out_17_acc: 0.7055 - out_18_acc: 0.7055 - out_19_acc: 0.7080 - out_20_acc: 0.7024 - out_21_acc: 0.7033 - out_22_acc: 0.7030 - out_23_acc: 0.7105 - out_24_acc: 0.7015 - out_25_acc: 0.7015 - out_26_acc: 0.7142 - out_27_acc: 0.7033 - out_28_acc: 0.7058 - out_29_acc: 0.7161 - out_30_acc: 0.7015 - out_31_acc: 0.7027 - out_32_acc: 0.7049 - val_loss: 25.1727 - val_out_loss: 0.6970 - val_out_0_loss: 0.4530 - val_out_1_loss: 0.7166 - val_out_2_loss: 0.7194 - val_out_3_loss: 0.7208 - val_out_4_loss: 0.7219 - val_out_5_loss: 0.7187 - val_out_6_loss: 0.7217 - val_out_7_loss: 0.7141 - val_out_8_loss: 0.7182 - val_out_9_loss: 0.7205 - val_out_10_loss: 0.7203 - val_out_11_loss: 0.7161 - val_out_12_loss: 0.7168 - val_out_13_loss: 0.7186 - val_out_14_loss: 0.7217 - val_out_15_loss: 0.7205 - val_out_16_loss: 0.7191 - val_out_17_loss: 0.7185 - val_out_18_loss: 0.7223 - val_out_19_loss: 0.7248 - val_out_20_loss: 0.7158 - val_out_21_loss: 0.7169 - val_out_22_loss: 0.7223 - val_out_23_loss: 0.7219 - val_out_24_loss: 0.7219 - val_out_25_loss: 0.7182 - val_out_26_loss: 0.7183 - val_out_27_loss: 0.7212 - val_out_28_loss: 0.7222 - val_out_29_loss: 0.7202 - val_out_30_loss: 0.7229 - val_out_31_loss: 0.7167 - val_out_32_loss: 0.7172 - val_out_acc: 0.7462 - val_out_0_acc: 0.8351 - val_out_1_acc: 0.7332 - val_out_2_acc: 0.7332 - val_out_3_acc: 0.7397 - val_out_4_acc: 0.7332 - val_out_5_acc: 0.7462 - val_out_6_acc: 0.7332 - val_out_7_acc: 0.7375 - val_out_8_acc: 0.7375 - val_out_9_acc: 0.7419 - val_out_10_acc: 0.7397 - val_out_11_acc: 0.7375 - val_out_12_acc: 0.7375 - val_out_13_acc: 0.7440 - val_out_14_acc: 0.7354 - val_out_15_acc: 0.7419 - val_out_16_acc: 0.7354 - val_out_17_acc: 0.7332 - val_out_18_acc: 0.7375 - val_out_19_acc: 0.7332 - val_out_20_acc: 0.7375 - val_out_21_acc: 0.7354 - val_out_22_acc: 0.7332 - val_out_23_acc: 0.7419 - val_out_24_acc: 0.7375 - val_out_25_acc: 0.7332 - val_out_26_acc: 0.7354 - val_out_27_acc: 0.7375 - val_out_28_acc: 0.7419 - val_out_29_acc: 0.7397 - val_out_30_acc: 0.7310 - val_out_31_acc: 0.7332 - val_out_32_acc: 0.7440\n",
      "Epoch 105/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 28.4210 - out_loss: 0.7460 - out_0_loss: 0.5139 - out_1_loss: 0.8435 - out_2_loss: 0.8498 - out_3_loss: 0.8416 - out_4_loss: 0.8588 - out_5_loss: 0.8525 - out_6_loss: 0.8330 - out_7_loss: 0.8569 - out_8_loss: 0.8464 - out_9_loss: 0.8446 - out_10_loss: 0.8472 - out_11_loss: 0.8506 - out_12_loss: 0.8556 - out_13_loss: 0.8480 - out_14_loss: 0.8528 - out_15_loss: 0.8506 - out_16_loss: 0.8462 - out_17_loss: 0.8640 - out_18_loss: 0.8527 - out_19_loss: 0.8463 - out_20_loss: 0.8474 - out_21_loss: 0.8490 - out_22_loss: 0.8508 - out_23_loss: 0.8338 - out_24_loss: 0.8623 - out_25_loss: 0.8572 - out_26_loss: 0.8469 - out_27_loss: 0.8359 - out_28_loss: 0.8506 - out_29_loss: 0.8407 - out_30_loss: 0.8552 - out_31_loss: 0.8510 - out_32_loss: 0.8392 - out_acc: 0.7340 - out_0_acc: 0.8205 - out_1_acc: 0.7040 - out_2_acc: 0.6968 - out_3_acc: 0.7018 - out_4_acc: 0.6962 - out_5_acc: 0.7009 - out_6_acc: 0.7089 - out_7_acc: 0.6897 - out_8_acc: 0.6990 - out_9_acc: 0.6981 - out_10_acc: 0.7024 - out_11_acc: 0.7012 - out_12_acc: 0.7046 - out_13_acc: 0.7064 - out_14_acc: 0.6959 - out_15_acc: 0.6931 - out_16_acc: 0.6996 - out_17_acc: 0.6993 - out_18_acc: 0.6999 - out_19_acc: 0.6965 - out_20_acc: 0.6987 - out_21_acc: 0.6925 - out_22_acc: 0.6913 - out_23_acc: 0.7077 - out_24_acc: 0.6937 - out_25_acc: 0.7021 - out_26_acc: 0.6953 - out_27_acc: 0.7055 - out_28_acc: 0.7021 - out_29_acc: 0.7033 - out_30_acc: 0.6959 - out_31_acc: 0.6971 - out_32_acc: 0.7002 - val_loss: 26.1255 - val_out_loss: 0.7193 - val_out_0_loss: 0.5485 - val_out_1_loss: 0.7420 - val_out_2_loss: 0.7396 - val_out_3_loss: 0.7396 - val_out_4_loss: 0.7473 - val_out_5_loss: 0.7429 - val_out_6_loss: 0.7444 - val_out_7_loss: 0.7439 - val_out_8_loss: 0.7491 - val_out_9_loss: 0.7420 - val_out_10_loss: 0.7442 - val_out_11_loss: 0.7464 - val_out_12_loss: 0.7403 - val_out_13_loss: 0.7386 - val_out_14_loss: 0.7547 - val_out_15_loss: 0.7487 - val_out_16_loss: 0.7420 - val_out_17_loss: 0.7490 - val_out_18_loss: 0.7454 - val_out_19_loss: 0.7477 - val_out_20_loss: 0.7481 - val_out_21_loss: 0.7392 - val_out_22_loss: 0.7444 - val_out_23_loss: 0.7456 - val_out_24_loss: 0.7442 - val_out_25_loss: 0.7467 - val_out_26_loss: 0.7455 - val_out_27_loss: 0.7439 - val_out_28_loss: 0.7427 - val_out_29_loss: 0.7435 - val_out_30_loss: 0.7461 - val_out_31_loss: 0.7455 - val_out_32_loss: 0.7404 - val_out_acc: 0.7310 - val_out_0_acc: 0.7983 - val_out_1_acc: 0.7310 - val_out_2_acc: 0.7310 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.7354 - val_out_5_acc: 0.7267 - val_out_6_acc: 0.7289 - val_out_7_acc: 0.7267 - val_out_8_acc: 0.7245 - val_out_9_acc: 0.7267 - val_out_10_acc: 0.7354 - val_out_11_acc: 0.7267 - val_out_12_acc: 0.7375 - val_out_13_acc: 0.7375 - val_out_14_acc: 0.7332 - val_out_15_acc: 0.7310 - val_out_16_acc: 0.7310 - val_out_17_acc: 0.7267 - val_out_18_acc: 0.7223 - val_out_19_acc: 0.7245 - val_out_20_acc: 0.7289 - val_out_21_acc: 0.7267 - val_out_22_acc: 0.7267 - val_out_23_acc: 0.7289 - val_out_24_acc: 0.7289 - val_out_25_acc: 0.7202 - val_out_26_acc: 0.7245 - val_out_27_acc: 0.7289 - val_out_28_acc: 0.7267 - val_out_29_acc: 0.7267 - val_out_30_acc: 0.7289 - val_out_31_acc: 0.7310 - val_out_32_acc: 0.7223\n",
      "Epoch 106/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.7378 - out_loss: 0.7233 - out_0_loss: 0.5258 - out_1_loss: 0.8186 - out_2_loss: 0.8245 - out_3_loss: 0.8220 - out_4_loss: 0.8146 - out_5_loss: 0.8292 - out_6_loss: 0.8242 - out_7_loss: 0.8212 - out_8_loss: 0.8235 - out_9_loss: 0.8175 - out_10_loss: 0.8364 - out_11_loss: 0.8340 - out_12_loss: 0.8197 - out_13_loss: 0.8356 - out_14_loss: 0.8215 - out_15_loss: 0.8338 - out_16_loss: 0.8409 - out_17_loss: 0.8263 - out_18_loss: 0.8152 - out_19_loss: 0.8451 - out_20_loss: 0.8266 - out_21_loss: 0.8320 - out_22_loss: 0.8318 - out_23_loss: 0.8235 - out_24_loss: 0.8365 - out_25_loss: 0.8290 - out_26_loss: 0.8203 - out_27_loss: 0.8329 - out_28_loss: 0.8132 - out_29_loss: 0.8298 - out_30_loss: 0.8356 - out_31_loss: 0.8450 - out_32_loss: 0.8289 - out_acc: 0.7458 - out_0_acc: 0.8153 - out_1_acc: 0.7052 - out_2_acc: 0.7086 - out_3_acc: 0.7148 - out_4_acc: 0.7117 - out_5_acc: 0.7068 - out_6_acc: 0.7148 - out_7_acc: 0.7055 - out_8_acc: 0.7089 - out_9_acc: 0.7157 - out_10_acc: 0.7037 - out_11_acc: 0.7046 - out_12_acc: 0.7170 - out_13_acc: 0.7102 - out_14_acc: 0.6968 - out_15_acc: 0.7012 - out_16_acc: 0.7015 - out_17_acc: 0.7086 - out_18_acc: 0.7064 - out_19_acc: 0.6916 - out_20_acc: 0.7099 - out_21_acc: 0.7068 - out_22_acc: 0.7009 - out_23_acc: 0.7108 - out_24_acc: 0.7123 - out_25_acc: 0.7058 - out_26_acc: 0.7049 - out_27_acc: 0.7055 - out_28_acc: 0.7095 - out_29_acc: 0.7052 - out_30_acc: 0.7037 - out_31_acc: 0.7086 - out_32_acc: 0.7117 - val_loss: 24.6701 - val_out_loss: 0.6829 - val_out_0_loss: 0.5097 - val_out_1_loss: 0.6992 - val_out_2_loss: 0.6999 - val_out_3_loss: 0.7044 - val_out_4_loss: 0.7068 - val_out_5_loss: 0.7042 - val_out_6_loss: 0.7019 - val_out_7_loss: 0.7047 - val_out_8_loss: 0.6998 - val_out_9_loss: 0.7026 - val_out_10_loss: 0.7048 - val_out_11_loss: 0.6992 - val_out_12_loss: 0.7000 - val_out_13_loss: 0.7039 - val_out_14_loss: 0.7033 - val_out_15_loss: 0.7081 - val_out_16_loss: 0.7008 - val_out_17_loss: 0.7027 - val_out_18_loss: 0.7006 - val_out_19_loss: 0.7046 - val_out_20_loss: 0.7019 - val_out_21_loss: 0.7000 - val_out_22_loss: 0.7066 - val_out_23_loss: 0.7011 - val_out_24_loss: 0.7042 - val_out_25_loss: 0.7064 - val_out_26_loss: 0.7039 - val_out_27_loss: 0.7038 - val_out_28_loss: 0.7071 - val_out_29_loss: 0.7047 - val_out_30_loss: 0.7068 - val_out_31_loss: 0.7004 - val_out_32_loss: 0.7024 - val_out_acc: 0.7375 - val_out_0_acc: 0.8200 - val_out_1_acc: 0.7310 - val_out_2_acc: 0.7397 - val_out_3_acc: 0.7223 - val_out_4_acc: 0.7289 - val_out_5_acc: 0.7245 - val_out_6_acc: 0.7332 - val_out_7_acc: 0.7332 - val_out_8_acc: 0.7310 - val_out_9_acc: 0.7332 - val_out_10_acc: 0.7202 - val_out_11_acc: 0.7310 - val_out_12_acc: 0.7397 - val_out_13_acc: 0.7354 - val_out_14_acc: 0.7289 - val_out_15_acc: 0.7267 - val_out_16_acc: 0.7332 - val_out_17_acc: 0.7332 - val_out_18_acc: 0.7354 - val_out_19_acc: 0.7375 - val_out_20_acc: 0.7289 - val_out_21_acc: 0.7354 - val_out_22_acc: 0.7289 - val_out_23_acc: 0.7245 - val_out_24_acc: 0.7354 - val_out_25_acc: 0.7267 - val_out_26_acc: 0.7332 - val_out_27_acc: 0.7202 - val_out_28_acc: 0.7332 - val_out_29_acc: 0.7289 - val_out_30_acc: 0.7332 - val_out_31_acc: 0.7310 - val_out_32_acc: 0.7310\n",
      "Epoch 107/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.3318 - out_loss: 0.7162 - out_0_loss: 0.5159 - out_1_loss: 0.8162 - out_2_loss: 0.8079 - out_3_loss: 0.8051 - out_4_loss: 0.8171 - out_5_loss: 0.8247 - out_6_loss: 0.8343 - out_7_loss: 0.8238 - out_8_loss: 0.8224 - out_9_loss: 0.8116 - out_10_loss: 0.8214 - out_11_loss: 0.8141 - out_12_loss: 0.8249 - out_13_loss: 0.8056 - out_14_loss: 0.8019 - out_15_loss: 0.8166 - out_16_loss: 0.8083 - out_17_loss: 0.8245 - out_18_loss: 0.8148 - out_19_loss: 0.8140 - out_20_loss: 0.8193 - out_21_loss: 0.8381 - out_22_loss: 0.8116 - out_23_loss: 0.8024 - out_24_loss: 0.8186 - out_25_loss: 0.8110 - out_26_loss: 0.8054 - out_27_loss: 0.8096 - out_28_loss: 0.8178 - out_29_loss: 0.8058 - out_30_loss: 0.8091 - out_31_loss: 0.8281 - out_32_loss: 0.8137 - out_acc: 0.7399 - out_0_acc: 0.8221 - out_1_acc: 0.7099 - out_2_acc: 0.7080 - out_3_acc: 0.7105 - out_4_acc: 0.7123 - out_5_acc: 0.7006 - out_6_acc: 0.6993 - out_7_acc: 0.7024 - out_8_acc: 0.7049 - out_9_acc: 0.7049 - out_10_acc: 0.7046 - out_11_acc: 0.7161 - out_12_acc: 0.7043 - out_13_acc: 0.7148 - out_14_acc: 0.7086 - out_15_acc: 0.6971 - out_16_acc: 0.7092 - out_17_acc: 0.7145 - out_18_acc: 0.7117 - out_19_acc: 0.7037 - out_20_acc: 0.7068 - out_21_acc: 0.6891 - out_22_acc: 0.7061 - out_23_acc: 0.7117 - out_24_acc: 0.7105 - out_25_acc: 0.7111 - out_26_acc: 0.7095 - out_27_acc: 0.7046 - out_28_acc: 0.7133 - out_29_acc: 0.7195 - out_30_acc: 0.7092 - out_31_acc: 0.7074 - out_32_acc: 0.7123 - val_loss: 26.5695 - val_out_loss: 0.7304 - val_out_0_loss: 0.5150 - val_out_1_loss: 0.7565 - val_out_2_loss: 0.7581 - val_out_3_loss: 0.7531 - val_out_4_loss: 0.7594 - val_out_5_loss: 0.7535 - val_out_6_loss: 0.7581 - val_out_7_loss: 0.7573 - val_out_8_loss: 0.7560 - val_out_9_loss: 0.7569 - val_out_10_loss: 0.7593 - val_out_11_loss: 0.7577 - val_out_12_loss: 0.7595 - val_out_13_loss: 0.7559 - val_out_14_loss: 0.7644 - val_out_15_loss: 0.7590 - val_out_16_loss: 0.7562 - val_out_17_loss: 0.7598 - val_out_18_loss: 0.7573 - val_out_19_loss: 0.7611 - val_out_20_loss: 0.7625 - val_out_21_loss: 0.7585 - val_out_22_loss: 0.7573 - val_out_23_loss: 0.7575 - val_out_24_loss: 0.7538 - val_out_25_loss: 0.7622 - val_out_26_loss: 0.7644 - val_out_27_loss: 0.7594 - val_out_28_loss: 0.7603 - val_out_29_loss: 0.7582 - val_out_30_loss: 0.7654 - val_out_31_loss: 0.7579 - val_out_32_loss: 0.7556 - val_out_acc: 0.7289 - val_out_0_acc: 0.7983 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7332 - val_out_4_acc: 0.7202 - val_out_5_acc: 0.7310 - val_out_6_acc: 0.7202 - val_out_7_acc: 0.7289 - val_out_8_acc: 0.7202 - val_out_9_acc: 0.7245 - val_out_10_acc: 0.7267 - val_out_11_acc: 0.7180 - val_out_12_acc: 0.7202 - val_out_13_acc: 0.7245 - val_out_14_acc: 0.7223 - val_out_15_acc: 0.7267 - val_out_16_acc: 0.7202 - val_out_17_acc: 0.7223 - val_out_18_acc: 0.7332 - val_out_19_acc: 0.7245 - val_out_20_acc: 0.7158 - val_out_21_acc: 0.7223 - val_out_22_acc: 0.7245 - val_out_23_acc: 0.7289 - val_out_24_acc: 0.7223 - val_out_25_acc: 0.7223 - val_out_26_acc: 0.7202 - val_out_27_acc: 0.7202 - val_out_28_acc: 0.7289 - val_out_29_acc: 0.7267 - val_out_30_acc: 0.7223 - val_out_31_acc: 0.7223 - val_out_32_acc: 0.7267\n",
      "Epoch 108/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.8170 - out_loss: 0.7253 - out_0_loss: 0.5050 - out_1_loss: 0.8361 - out_2_loss: 0.8333 - out_3_loss: 0.8329 - out_4_loss: 0.8313 - out_5_loss: 0.8256 - out_6_loss: 0.8250 - out_7_loss: 0.8293 - out_8_loss: 0.8360 - out_9_loss: 0.8227 - out_10_loss: 0.8329 - out_11_loss: 0.8192 - out_12_loss: 0.8359 - out_13_loss: 0.8143 - out_14_loss: 0.8221 - out_15_loss: 0.8301 - out_16_loss: 0.8285 - out_17_loss: 0.8341 - out_18_loss: 0.8235 - out_19_loss: 0.8392 - out_20_loss: 0.8295 - out_21_loss: 0.8324 - out_22_loss: 0.8281 - out_23_loss: 0.8330 - out_24_loss: 0.8223 - out_25_loss: 0.8325 - out_26_loss: 0.8371 - out_27_loss: 0.8269 - out_28_loss: 0.8388 - out_29_loss: 0.8445 - out_30_loss: 0.8296 - out_31_loss: 0.8384 - out_32_loss: 0.8413 - out_acc: 0.7418 - out_0_acc: 0.8301 - out_1_acc: 0.7037 - out_2_acc: 0.7015 - out_3_acc: 0.7058 - out_4_acc: 0.6990 - out_5_acc: 0.7083 - out_6_acc: 0.7006 - out_7_acc: 0.7030 - out_8_acc: 0.7061 - out_9_acc: 0.6978 - out_10_acc: 0.7105 - out_11_acc: 0.7151 - out_12_acc: 0.6956 - out_13_acc: 0.7086 - out_14_acc: 0.7086 - out_15_acc: 0.7002 - out_16_acc: 0.7102 - out_17_acc: 0.6996 - out_18_acc: 0.7120 - out_19_acc: 0.6990 - out_20_acc: 0.7086 - out_21_acc: 0.6953 - out_22_acc: 0.7071 - out_23_acc: 0.7015 - out_24_acc: 0.7074 - out_25_acc: 0.7099 - out_26_acc: 0.6965 - out_27_acc: 0.7055 - out_28_acc: 0.7006 - out_29_acc: 0.6959 - out_30_acc: 0.7052 - out_31_acc: 0.7030 - out_32_acc: 0.6971 - val_loss: 27.3356 - val_out_loss: 0.7523 - val_out_0_loss: 0.5674 - val_out_1_loss: 0.7772 - val_out_2_loss: 0.7763 - val_out_3_loss: 0.7842 - val_out_4_loss: 0.7743 - val_out_5_loss: 0.7812 - val_out_6_loss: 0.7804 - val_out_7_loss: 0.7784 - val_out_8_loss: 0.7728 - val_out_9_loss: 0.7779 - val_out_10_loss: 0.7865 - val_out_11_loss: 0.7782 - val_out_12_loss: 0.7716 - val_out_13_loss: 0.7767 - val_out_14_loss: 0.7791 - val_out_15_loss: 0.7820 - val_out_16_loss: 0.7809 - val_out_17_loss: 0.7827 - val_out_18_loss: 0.7785 - val_out_19_loss: 0.7795 - val_out_20_loss: 0.7781 - val_out_21_loss: 0.7835 - val_out_22_loss: 0.7780 - val_out_23_loss: 0.7805 - val_out_24_loss: 0.7775 - val_out_25_loss: 0.7765 - val_out_26_loss: 0.7855 - val_out_27_loss: 0.7849 - val_out_28_loss: 0.7805 - val_out_29_loss: 0.7743 - val_out_30_loss: 0.7791 - val_out_31_loss: 0.7775 - val_out_32_loss: 0.7796 - val_out_acc: 0.7267 - val_out_0_acc: 0.7961 - val_out_1_acc: 0.7289 - val_out_2_acc: 0.7245 - val_out_3_acc: 0.7332 - val_out_4_acc: 0.7180 - val_out_5_acc: 0.7267 - val_out_6_acc: 0.7223 - val_out_7_acc: 0.7310 - val_out_8_acc: 0.7267 - val_out_9_acc: 0.7310 - val_out_10_acc: 0.7202 - val_out_11_acc: 0.7202 - val_out_12_acc: 0.7332 - val_out_13_acc: 0.7289 - val_out_14_acc: 0.7245 - val_out_15_acc: 0.7245 - val_out_16_acc: 0.7202 - val_out_17_acc: 0.7267 - val_out_18_acc: 0.7223 - val_out_19_acc: 0.7310 - val_out_20_acc: 0.7223 - val_out_21_acc: 0.7245 - val_out_22_acc: 0.7245 - val_out_23_acc: 0.7267 - val_out_24_acc: 0.7267 - val_out_25_acc: 0.7310 - val_out_26_acc: 0.7245 - val_out_27_acc: 0.7223 - val_out_28_acc: 0.7245 - val_out_29_acc: 0.7289 - val_out_30_acc: 0.7289 - val_out_31_acc: 0.7223 - val_out_32_acc: 0.7310\n",
      "Epoch 109/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.5328 - out_loss: 0.7240 - out_0_loss: 0.5139 - out_1_loss: 0.8154 - out_2_loss: 0.8166 - out_3_loss: 0.8222 - out_4_loss: 0.8237 - out_5_loss: 0.8294 - out_6_loss: 0.8212 - out_7_loss: 0.8285 - out_8_loss: 0.8190 - out_9_loss: 0.8262 - out_10_loss: 0.8158 - out_11_loss: 0.8255 - out_12_loss: 0.8195 - out_13_loss: 0.8143 - out_14_loss: 0.8333 - out_15_loss: 0.8159 - out_16_loss: 0.8115 - out_17_loss: 0.8249 - out_18_loss: 0.8178 - out_19_loss: 0.8101 - out_20_loss: 0.8148 - out_21_loss: 0.8207 - out_22_loss: 0.8214 - out_23_loss: 0.8210 - out_24_loss: 0.8084 - out_25_loss: 0.8403 - out_26_loss: 0.8193 - out_27_loss: 0.8169 - out_28_loss: 0.8330 - out_29_loss: 0.8248 - out_30_loss: 0.8242 - out_31_loss: 0.8312 - out_32_loss: 0.8275 - out_acc: 0.7436 - out_0_acc: 0.8233 - out_1_acc: 0.6996 - out_2_acc: 0.7083 - out_3_acc: 0.7027 - out_4_acc: 0.7018 - out_5_acc: 0.7002 - out_6_acc: 0.7037 - out_7_acc: 0.6993 - out_8_acc: 0.7089 - out_9_acc: 0.7009 - out_10_acc: 0.7086 - out_11_acc: 0.7002 - out_12_acc: 0.7040 - out_13_acc: 0.7077 - out_14_acc: 0.6956 - out_15_acc: 0.7058 - out_16_acc: 0.7058 - out_17_acc: 0.7058 - out_18_acc: 0.7064 - out_19_acc: 0.7077 - out_20_acc: 0.7037 - out_21_acc: 0.7015 - out_22_acc: 0.7018 - out_23_acc: 0.6996 - out_24_acc: 0.7071 - out_25_acc: 0.6984 - out_26_acc: 0.7037 - out_27_acc: 0.7002 - out_28_acc: 0.6947 - out_29_acc: 0.7080 - out_30_acc: 0.7061 - out_31_acc: 0.6978 - out_32_acc: 0.6996 - val_loss: 26.2475 - val_out_loss: 0.7229 - val_out_0_loss: 0.4694 - val_out_1_loss: 0.7498 - val_out_2_loss: 0.7557 - val_out_3_loss: 0.7504 - val_out_4_loss: 0.7505 - val_out_5_loss: 0.7499 - val_out_6_loss: 0.7483 - val_out_7_loss: 0.7530 - val_out_8_loss: 0.7442 - val_out_9_loss: 0.7470 - val_out_10_loss: 0.7490 - val_out_11_loss: 0.7500 - val_out_12_loss: 0.7501 - val_out_13_loss: 0.7518 - val_out_14_loss: 0.7569 - val_out_15_loss: 0.7495 - val_out_16_loss: 0.7519 - val_out_17_loss: 0.7513 - val_out_18_loss: 0.7437 - val_out_19_loss: 0.7536 - val_out_20_loss: 0.7504 - val_out_21_loss: 0.7473 - val_out_22_loss: 0.7506 - val_out_23_loss: 0.7489 - val_out_24_loss: 0.7569 - val_out_25_loss: 0.7535 - val_out_26_loss: 0.7477 - val_out_27_loss: 0.7524 - val_out_28_loss: 0.7469 - val_out_29_loss: 0.7506 - val_out_30_loss: 0.7510 - val_out_31_loss: 0.7492 - val_out_32_loss: 0.7542 - val_out_acc: 0.7223 - val_out_0_acc: 0.8330 - val_out_1_acc: 0.7137 - val_out_2_acc: 0.7137 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.7115 - val_out_5_acc: 0.7202 - val_out_6_acc: 0.7202 - val_out_7_acc: 0.7093 - val_out_8_acc: 0.7158 - val_out_9_acc: 0.7202 - val_out_10_acc: 0.7072 - val_out_11_acc: 0.7158 - val_out_12_acc: 0.7180 - val_out_13_acc: 0.7093 - val_out_14_acc: 0.7050 - val_out_15_acc: 0.7202 - val_out_16_acc: 0.7137 - val_out_17_acc: 0.7158 - val_out_18_acc: 0.7180 - val_out_19_acc: 0.7137 - val_out_20_acc: 0.7137 - val_out_21_acc: 0.7093 - val_out_22_acc: 0.7202 - val_out_23_acc: 0.7180 - val_out_24_acc: 0.7115 - val_out_25_acc: 0.7050 - val_out_26_acc: 0.7093 - val_out_27_acc: 0.7158 - val_out_28_acc: 0.7158 - val_out_29_acc: 0.7202 - val_out_30_acc: 0.7202 - val_out_31_acc: 0.7137 - val_out_32_acc: 0.7050\n",
      "Epoch 110/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 26.8671 - out_loss: 0.7044 - out_0_loss: 0.5133 - out_1_loss: 0.7875 - out_2_loss: 0.8099 - out_3_loss: 0.7974 - out_4_loss: 0.7995 - out_5_loss: 0.8035 - out_6_loss: 0.8131 - out_7_loss: 0.7999 - out_8_loss: 0.7948 - out_9_loss: 0.7914 - out_10_loss: 0.8024 - out_11_loss: 0.7995 - out_12_loss: 0.7916 - out_13_loss: 0.8017 - out_14_loss: 0.8211 - out_15_loss: 0.8064 - out_16_loss: 0.8068 - out_17_loss: 0.8079 - out_18_loss: 0.8017 - out_19_loss: 0.8049 - out_20_loss: 0.8034 - out_21_loss: 0.7972 - out_22_loss: 0.7955 - out_23_loss: 0.8060 - out_24_loss: 0.8027 - out_25_loss: 0.7931 - out_26_loss: 0.7875 - out_27_loss: 0.8085 - out_28_loss: 0.7973 - out_29_loss: 0.7972 - out_30_loss: 0.8069 - out_31_loss: 0.8105 - out_32_loss: 0.8024 - out_acc: 0.7455 - out_0_acc: 0.8227 - out_1_acc: 0.7055 - out_2_acc: 0.7105 - out_3_acc: 0.7145 - out_4_acc: 0.7133 - out_5_acc: 0.7068 - out_6_acc: 0.6996 - out_7_acc: 0.7015 - out_8_acc: 0.7105 - out_9_acc: 0.7111 - out_10_acc: 0.7086 - out_11_acc: 0.7089 - out_12_acc: 0.7136 - out_13_acc: 0.7099 - out_14_acc: 0.7040 - out_15_acc: 0.7095 - out_16_acc: 0.7095 - out_17_acc: 0.7120 - out_18_acc: 0.7095 - out_19_acc: 0.7117 - out_20_acc: 0.7046 - out_21_acc: 0.7092 - out_22_acc: 0.7157 - out_23_acc: 0.7055 - out_24_acc: 0.7095 - out_25_acc: 0.7126 - out_26_acc: 0.7207 - out_27_acc: 0.7111 - out_28_acc: 0.7126 - out_29_acc: 0.7074 - out_30_acc: 0.6999 - out_31_acc: 0.7108 - out_32_acc: 0.7074 - val_loss: 26.0330 - val_out_loss: 0.7171 - val_out_0_loss: 0.4971 - val_out_1_loss: 0.7431 - val_out_2_loss: 0.7426 - val_out_3_loss: 0.7390 - val_out_4_loss: 0.7478 - val_out_5_loss: 0.7425 - val_out_6_loss: 0.7415 - val_out_7_loss: 0.7428 - val_out_8_loss: 0.7483 - val_out_9_loss: 0.7429 - val_out_10_loss: 0.7419 - val_out_11_loss: 0.7469 - val_out_12_loss: 0.7426 - val_out_13_loss: 0.7402 - val_out_14_loss: 0.7491 - val_out_15_loss: 0.7456 - val_out_16_loss: 0.7429 - val_out_17_loss: 0.7438 - val_out_18_loss: 0.7458 - val_out_19_loss: 0.7449 - val_out_20_loss: 0.7443 - val_out_21_loss: 0.7410 - val_out_22_loss: 0.7406 - val_out_23_loss: 0.7410 - val_out_24_loss: 0.7417 - val_out_25_loss: 0.7436 - val_out_26_loss: 0.7383 - val_out_27_loss: 0.7433 - val_out_28_loss: 0.7481 - val_out_29_loss: 0.7458 - val_out_30_loss: 0.7457 - val_out_31_loss: 0.7393 - val_out_32_loss: 0.7413 - val_out_acc: 0.7289 - val_out_0_acc: 0.8308 - val_out_1_acc: 0.7158 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7180 - val_out_4_acc: 0.7137 - val_out_5_acc: 0.7267 - val_out_6_acc: 0.7158 - val_out_7_acc: 0.7137 - val_out_8_acc: 0.7158 - val_out_9_acc: 0.7137 - val_out_10_acc: 0.7202 - val_out_11_acc: 0.7115 - val_out_12_acc: 0.7245 - val_out_13_acc: 0.7245 - val_out_14_acc: 0.7158 - val_out_15_acc: 0.7093 - val_out_16_acc: 0.7137 - val_out_17_acc: 0.7202 - val_out_18_acc: 0.7202 - val_out_19_acc: 0.7115 - val_out_20_acc: 0.7115 - val_out_21_acc: 0.7267 - val_out_22_acc: 0.7223 - val_out_23_acc: 0.7180 - val_out_24_acc: 0.7158 - val_out_25_acc: 0.7289 - val_out_26_acc: 0.7158 - val_out_27_acc: 0.7180 - val_out_28_acc: 0.7223 - val_out_29_acc: 0.7180 - val_out_30_acc: 0.7115 - val_out_31_acc: 0.7202 - val_out_32_acc: 0.7137\n",
      "Epoch 111/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.8060 - out_loss: 0.7289 - out_0_loss: 0.5115 - out_1_loss: 0.8277 - out_2_loss: 0.8249 - out_3_loss: 0.8319 - out_4_loss: 0.8228 - out_5_loss: 0.8348 - out_6_loss: 0.8291 - out_7_loss: 0.8254 - out_8_loss: 0.8395 - out_9_loss: 0.8226 - out_10_loss: 0.8504 - out_11_loss: 0.8305 - out_12_loss: 0.8321 - out_13_loss: 0.8429 - out_14_loss: 0.8383 - out_15_loss: 0.8327 - out_16_loss: 0.8286 - out_17_loss: 0.8310 - out_18_loss: 0.8306 - out_19_loss: 0.8234 - out_20_loss: 0.8216 - out_21_loss: 0.8245 - out_22_loss: 0.8260 - out_23_loss: 0.8183 - out_24_loss: 0.8411 - out_25_loss: 0.8292 - out_26_loss: 0.8277 - out_27_loss: 0.8371 - out_28_loss: 0.8306 - out_29_loss: 0.8296 - out_30_loss: 0.8374 - out_31_loss: 0.8334 - out_32_loss: 0.8100 - out_acc: 0.7421 - out_0_acc: 0.8190 - out_1_acc: 0.7030 - out_2_acc: 0.7071 - out_3_acc: 0.7033 - out_4_acc: 0.7071 - out_5_acc: 0.7080 - out_6_acc: 0.7120 - out_7_acc: 0.7080 - out_8_acc: 0.7015 - out_9_acc: 0.7074 - out_10_acc: 0.6875 - out_11_acc: 0.7092 - out_12_acc: 0.7015 - out_13_acc: 0.7002 - out_14_acc: 0.7108 - out_15_acc: 0.7024 - out_16_acc: 0.7002 - out_17_acc: 0.7099 - out_18_acc: 0.7167 - out_19_acc: 0.7105 - out_20_acc: 0.7086 - out_21_acc: 0.7012 - out_22_acc: 0.7123 - out_23_acc: 0.7018 - out_24_acc: 0.7080 - out_25_acc: 0.7080 - out_26_acc: 0.7157 - out_27_acc: 0.7006 - out_28_acc: 0.7086 - out_29_acc: 0.7030 - out_30_acc: 0.7046 - out_31_acc: 0.7033 - out_32_acc: 0.7148 - val_loss: 27.6889 - val_out_loss: 0.7507 - val_out_0_loss: 0.4836 - val_out_1_loss: 0.7920 - val_out_2_loss: 0.7939 - val_out_3_loss: 0.7884 - val_out_4_loss: 0.7908 - val_out_5_loss: 0.7903 - val_out_6_loss: 0.7924 - val_out_7_loss: 0.7889 - val_out_8_loss: 0.7903 - val_out_9_loss: 0.7920 - val_out_10_loss: 0.7983 - val_out_11_loss: 0.7944 - val_out_12_loss: 0.7892 - val_out_13_loss: 0.7919 - val_out_14_loss: 0.7941 - val_out_15_loss: 0.7967 - val_out_16_loss: 0.7942 - val_out_17_loss: 0.7952 - val_out_18_loss: 0.7919 - val_out_19_loss: 0.7907 - val_out_20_loss: 0.8026 - val_out_21_loss: 0.7913 - val_out_22_loss: 0.7955 - val_out_23_loss: 0.7887 - val_out_24_loss: 0.7937 - val_out_25_loss: 0.7929 - val_out_26_loss: 0.7911 - val_out_27_loss: 0.7874 - val_out_28_loss: 0.7971 - val_out_29_loss: 0.7899 - val_out_30_loss: 0.7916 - val_out_31_loss: 0.7909 - val_out_32_loss: 0.7901 - val_out_acc: 0.7332 - val_out_0_acc: 0.8373 - val_out_1_acc: 0.7289 - val_out_2_acc: 0.7267 - val_out_3_acc: 0.7267 - val_out_4_acc: 0.7310 - val_out_5_acc: 0.7267 - val_out_6_acc: 0.7267 - val_out_7_acc: 0.7289 - val_out_8_acc: 0.7289 - val_out_9_acc: 0.7223 - val_out_10_acc: 0.7289 - val_out_11_acc: 0.7332 - val_out_12_acc: 0.7310 - val_out_13_acc: 0.7289 - val_out_14_acc: 0.7354 - val_out_15_acc: 0.7310 - val_out_16_acc: 0.7267 - val_out_17_acc: 0.7245 - val_out_18_acc: 0.7267 - val_out_19_acc: 0.7310 - val_out_20_acc: 0.7289 - val_out_21_acc: 0.7310 - val_out_22_acc: 0.7267 - val_out_23_acc: 0.7310 - val_out_24_acc: 0.7354 - val_out_25_acc: 0.7289 - val_out_26_acc: 0.7375 - val_out_27_acc: 0.7267 - val_out_28_acc: 0.7267 - val_out_29_acc: 0.7267 - val_out_30_acc: 0.7245 - val_out_31_acc: 0.7289 - val_out_32_acc: 0.7332\n",
      "Epoch 112/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 27.3475 - out_loss: 0.7164 - out_0_loss: 0.4969 - out_1_loss: 0.8146 - out_2_loss: 0.8211 - out_3_loss: 0.8055 - out_4_loss: 0.8220 - out_5_loss: 0.8146 - out_6_loss: 0.8245 - out_7_loss: 0.8239 - out_8_loss: 0.8220 - out_9_loss: 0.8228 - out_10_loss: 0.8282 - out_11_loss: 0.8209 - out_12_loss: 0.7994 - out_13_loss: 0.8209 - out_14_loss: 0.8229 - out_15_loss: 0.8159 - out_16_loss: 0.8366 - out_17_loss: 0.8170 - out_18_loss: 0.8118 - out_19_loss: 0.8174 - out_20_loss: 0.8062 - out_21_loss: 0.8119 - out_22_loss: 0.8120 - out_23_loss: 0.8261 - out_24_loss: 0.8205 - out_25_loss: 0.8107 - out_26_loss: 0.8131 - out_27_loss: 0.8183 - out_28_loss: 0.8180 - out_29_loss: 0.7993 - out_30_loss: 0.8065 - out_31_loss: 0.8192 - out_32_loss: 0.8106 - out_acc: 0.7396 - out_0_acc: 0.8267 - out_1_acc: 0.7089 - out_2_acc: 0.7043 - out_3_acc: 0.7092 - out_4_acc: 0.7030 - out_5_acc: 0.6978 - out_6_acc: 0.7030 - out_7_acc: 0.6925 - out_8_acc: 0.7071 - out_9_acc: 0.7030 - out_10_acc: 0.6975 - out_11_acc: 0.7052 - out_12_acc: 0.7043 - out_13_acc: 0.6999 - out_14_acc: 0.6953 - out_15_acc: 0.7055 - out_16_acc: 0.6940 - out_17_acc: 0.7018 - out_18_acc: 0.7049 - out_19_acc: 0.6984 - out_20_acc: 0.7099 - out_21_acc: 0.7018 - out_22_acc: 0.6925 - out_23_acc: 0.6953 - out_24_acc: 0.7102 - out_25_acc: 0.7018 - out_26_acc: 0.7095 - out_27_acc: 0.7021 - out_28_acc: 0.7021 - out_29_acc: 0.7030 - out_30_acc: 0.7095 - out_31_acc: 0.6978 - out_32_acc: 0.7033 - val_loss: 24.8362 - val_out_loss: 0.6772 - val_out_0_loss: 0.4651 - val_out_1_loss: 0.7128 - val_out_2_loss: 0.7111 - val_out_3_loss: 0.7118 - val_out_4_loss: 0.7078 - val_out_5_loss: 0.7067 - val_out_6_loss: 0.7133 - val_out_7_loss: 0.7127 - val_out_8_loss: 0.7080 - val_out_9_loss: 0.7102 - val_out_10_loss: 0.7108 - val_out_11_loss: 0.7120 - val_out_12_loss: 0.7022 - val_out_13_loss: 0.7098 - val_out_14_loss: 0.7148 - val_out_15_loss: 0.7130 - val_out_16_loss: 0.7076 - val_out_17_loss: 0.7125 - val_out_18_loss: 0.7058 - val_out_19_loss: 0.7106 - val_out_20_loss: 0.7078 - val_out_21_loss: 0.7064 - val_out_22_loss: 0.7116 - val_out_23_loss: 0.7059 - val_out_24_loss: 0.7091 - val_out_25_loss: 0.7178 - val_out_26_loss: 0.7083 - val_out_27_loss: 0.7088 - val_out_28_loss: 0.7103 - val_out_29_loss: 0.7072 - val_out_30_loss: 0.7120 - val_out_31_loss: 0.7072 - val_out_32_loss: 0.7050 - val_out_acc: 0.7354 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7332 - val_out_2_acc: 0.7289 - val_out_3_acc: 0.7354 - val_out_4_acc: 0.7289 - val_out_5_acc: 0.7354 - val_out_6_acc: 0.7245 - val_out_7_acc: 0.7310 - val_out_8_acc: 0.7310 - val_out_9_acc: 0.7332 - val_out_10_acc: 0.7289 - val_out_11_acc: 0.7310 - val_out_12_acc: 0.7332 - val_out_13_acc: 0.7310 - val_out_14_acc: 0.7310 - val_out_15_acc: 0.7267 - val_out_16_acc: 0.7289 - val_out_17_acc: 0.7289 - val_out_18_acc: 0.7332 - val_out_19_acc: 0.7332 - val_out_20_acc: 0.7245 - val_out_21_acc: 0.7332 - val_out_22_acc: 0.7310 - val_out_23_acc: 0.7310 - val_out_24_acc: 0.7332 - val_out_25_acc: 0.7267 - val_out_26_acc: 0.7289 - val_out_27_acc: 0.7310 - val_out_28_acc: 0.7354 - val_out_29_acc: 0.7310 - val_out_30_acc: 0.7332 - val_out_31_acc: 0.7310 - val_out_32_acc: 0.7354\n",
      "Epoch 113/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 26.8143 - out_loss: 0.7028 - out_0_loss: 0.4955 - out_1_loss: 0.7933 - out_2_loss: 0.7952 - out_3_loss: 0.7892 - out_4_loss: 0.7983 - out_5_loss: 0.7983 - out_6_loss: 0.8097 - out_7_loss: 0.8036 - out_8_loss: 0.7963 - out_9_loss: 0.7936 - out_10_loss: 0.7928 - out_11_loss: 0.7932 - out_12_loss: 0.8071 - out_13_loss: 0.7955 - out_14_loss: 0.8014 - out_15_loss: 0.8058 - out_16_loss: 0.7980 - out_17_loss: 0.8004 - out_18_loss: 0.7985 - out_19_loss: 0.7980 - out_20_loss: 0.8032 - out_21_loss: 0.8024 - out_22_loss: 0.8082 - out_23_loss: 0.7972 - out_24_loss: 0.7948 - out_25_loss: 0.8072 - out_26_loss: 0.8138 - out_27_loss: 0.8019 - out_28_loss: 0.7895 - out_29_loss: 0.8091 - out_30_loss: 0.8014 - out_31_loss: 0.8152 - out_32_loss: 0.8039 - out_acc: 0.7474 - out_0_acc: 0.8177 - out_1_acc: 0.7064 - out_2_acc: 0.7046 - out_3_acc: 0.7077 - out_4_acc: 0.7040 - out_5_acc: 0.7064 - out_6_acc: 0.7037 - out_7_acc: 0.7167 - out_8_acc: 0.7120 - out_9_acc: 0.7049 - out_10_acc: 0.7182 - out_11_acc: 0.7033 - out_12_acc: 0.7061 - out_13_acc: 0.7154 - out_14_acc: 0.7074 - out_15_acc: 0.7061 - out_16_acc: 0.7142 - out_17_acc: 0.7083 - out_18_acc: 0.7006 - out_19_acc: 0.7086 - out_20_acc: 0.7083 - out_21_acc: 0.7092 - out_22_acc: 0.7117 - out_23_acc: 0.7126 - out_24_acc: 0.7080 - out_25_acc: 0.7089 - out_26_acc: 0.7061 - out_27_acc: 0.7092 - out_28_acc: 0.7179 - out_29_acc: 0.7049 - out_30_acc: 0.7111 - out_31_acc: 0.7061 - out_32_acc: 0.7030 - val_loss: 25.4465 - val_out_loss: 0.7045 - val_out_0_loss: 0.4955 - val_out_1_loss: 0.7237 - val_out_2_loss: 0.7264 - val_out_3_loss: 0.7222 - val_out_4_loss: 0.7263 - val_out_5_loss: 0.7282 - val_out_6_loss: 0.7270 - val_out_7_loss: 0.7260 - val_out_8_loss: 0.7285 - val_out_9_loss: 0.7275 - val_out_10_loss: 0.7242 - val_out_11_loss: 0.7283 - val_out_12_loss: 0.7222 - val_out_13_loss: 0.7204 - val_out_14_loss: 0.7302 - val_out_15_loss: 0.7284 - val_out_16_loss: 0.7285 - val_out_17_loss: 0.7238 - val_out_18_loss: 0.7255 - val_out_19_loss: 0.7331 - val_out_20_loss: 0.7313 - val_out_21_loss: 0.7203 - val_out_22_loss: 0.7281 - val_out_23_loss: 0.7295 - val_out_24_loss: 0.7255 - val_out_25_loss: 0.7278 - val_out_26_loss: 0.7237 - val_out_27_loss: 0.7283 - val_out_28_loss: 0.7261 - val_out_29_loss: 0.7241 - val_out_30_loss: 0.7267 - val_out_31_loss: 0.7232 - val_out_32_loss: 0.7242 - val_out_acc: 0.7419 - val_out_0_acc: 0.8113 - val_out_1_acc: 0.7332 - val_out_2_acc: 0.7289 - val_out_3_acc: 0.7375 - val_out_4_acc: 0.7267 - val_out_5_acc: 0.7354 - val_out_6_acc: 0.7354 - val_out_7_acc: 0.7375 - val_out_8_acc: 0.7354 - val_out_9_acc: 0.7354 - val_out_10_acc: 0.7354 - val_out_11_acc: 0.7310 - val_out_12_acc: 0.7375 - val_out_13_acc: 0.7375 - val_out_14_acc: 0.7332 - val_out_15_acc: 0.7332 - val_out_16_acc: 0.7332 - val_out_17_acc: 0.7332 - val_out_18_acc: 0.7397 - val_out_19_acc: 0.7332 - val_out_20_acc: 0.7310 - val_out_21_acc: 0.7310 - val_out_22_acc: 0.7397 - val_out_23_acc: 0.7375 - val_out_24_acc: 0.7332 - val_out_25_acc: 0.7332 - val_out_26_acc: 0.7375 - val_out_27_acc: 0.7310 - val_out_28_acc: 0.7245 - val_out_29_acc: 0.7397 - val_out_30_acc: 0.7354 - val_out_31_acc: 0.7354 - val_out_32_acc: 0.7419\n",
      "Epoch 114/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 26.4976 - out_loss: 0.6899 - out_0_loss: 0.4880 - out_1_loss: 0.7867 - out_2_loss: 0.7902 - out_3_loss: 0.7963 - out_4_loss: 0.7816 - out_5_loss: 0.7962 - out_6_loss: 0.7857 - out_7_loss: 0.7769 - out_8_loss: 0.7858 - out_9_loss: 0.7801 - out_10_loss: 0.7878 - out_11_loss: 0.7989 - out_12_loss: 0.8042 - out_13_loss: 0.7782 - out_14_loss: 0.7939 - out_15_loss: 0.7792 - out_16_loss: 0.8067 - out_17_loss: 0.7912 - out_18_loss: 0.7940 - out_19_loss: 0.7992 - out_20_loss: 0.7986 - out_21_loss: 0.7958 - out_22_loss: 0.7976 - out_23_loss: 0.7778 - out_24_loss: 0.7923 - out_25_loss: 0.7864 - out_26_loss: 0.7999 - out_27_loss: 0.7874 - out_28_loss: 0.7956 - out_29_loss: 0.7886 - out_30_loss: 0.8062 - out_31_loss: 0.7939 - out_32_loss: 0.7870 - out_acc: 0.7517 - out_0_acc: 0.8320 - out_1_acc: 0.7089 - out_2_acc: 0.7139 - out_3_acc: 0.7157 - out_4_acc: 0.7238 - out_5_acc: 0.7198 - out_6_acc: 0.7167 - out_7_acc: 0.7176 - out_8_acc: 0.7204 - out_9_acc: 0.7167 - out_10_acc: 0.7179 - out_11_acc: 0.7161 - out_12_acc: 0.7052 - out_13_acc: 0.7148 - out_14_acc: 0.7095 - out_15_acc: 0.7235 - out_16_acc: 0.7092 - out_17_acc: 0.7111 - out_18_acc: 0.7179 - out_19_acc: 0.7117 - out_20_acc: 0.7176 - out_21_acc: 0.7219 - out_22_acc: 0.7167 - out_23_acc: 0.7266 - out_24_acc: 0.7157 - out_25_acc: 0.7139 - out_26_acc: 0.7123 - out_27_acc: 0.7095 - out_28_acc: 0.7188 - out_29_acc: 0.7130 - out_30_acc: 0.7049 - out_31_acc: 0.7157 - out_32_acc: 0.7148 - val_loss: 25.7875 - val_out_loss: 0.7078 - val_out_0_loss: 0.5376 - val_out_1_loss: 0.7361 - val_out_2_loss: 0.7341 - val_out_3_loss: 0.7378 - val_out_4_loss: 0.7423 - val_out_5_loss: 0.7353 - val_out_6_loss: 0.7363 - val_out_7_loss: 0.7303 - val_out_8_loss: 0.7337 - val_out_9_loss: 0.7389 - val_out_10_loss: 0.7326 - val_out_11_loss: 0.7359 - val_out_12_loss: 0.7316 - val_out_13_loss: 0.7348 - val_out_14_loss: 0.7397 - val_out_15_loss: 0.7336 - val_out_16_loss: 0.7376 - val_out_17_loss: 0.7357 - val_out_18_loss: 0.7315 - val_out_19_loss: 0.7307 - val_out_20_loss: 0.7321 - val_out_21_loss: 0.7355 - val_out_22_loss: 0.7358 - val_out_23_loss: 0.7342 - val_out_24_loss: 0.7354 - val_out_25_loss: 0.7388 - val_out_26_loss: 0.7359 - val_out_27_loss: 0.7345 - val_out_28_loss: 0.7338 - val_out_29_loss: 0.7327 - val_out_30_loss: 0.7380 - val_out_31_loss: 0.7340 - val_out_32_loss: 0.7324 - val_out_acc: 0.7440 - val_out_0_acc: 0.8178 - val_out_1_acc: 0.7310 - val_out_2_acc: 0.7332 - val_out_3_acc: 0.7375 - val_out_4_acc: 0.7332 - val_out_5_acc: 0.7397 - val_out_6_acc: 0.7462 - val_out_7_acc: 0.7354 - val_out_8_acc: 0.7462 - val_out_9_acc: 0.7332 - val_out_10_acc: 0.7419 - val_out_11_acc: 0.7310 - val_out_12_acc: 0.7397 - val_out_13_acc: 0.7354 - val_out_14_acc: 0.7354 - val_out_15_acc: 0.7375 - val_out_16_acc: 0.7419 - val_out_17_acc: 0.7375 - val_out_18_acc: 0.7354 - val_out_19_acc: 0.7375 - val_out_20_acc: 0.7419 - val_out_21_acc: 0.7375 - val_out_22_acc: 0.7419 - val_out_23_acc: 0.7375 - val_out_24_acc: 0.7419 - val_out_25_acc: 0.7354 - val_out_26_acc: 0.7462 - val_out_27_acc: 0.7462 - val_out_28_acc: 0.7354 - val_out_29_acc: 0.7375 - val_out_30_acc: 0.7397 - val_out_31_acc: 0.7354 - val_out_32_acc: 0.7419\n",
      "Epoch 115/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 25.9142 - out_loss: 0.6777 - out_0_loss: 0.4839 - out_1_loss: 0.7682 - out_2_loss: 0.7658 - out_3_loss: 0.7718 - out_4_loss: 0.7869 - out_5_loss: 0.7787 - out_6_loss: 0.7765 - out_7_loss: 0.7770 - out_8_loss: 0.7742 - out_9_loss: 0.7815 - out_10_loss: 0.7649 - out_11_loss: 0.7724 - out_12_loss: 0.7604 - out_13_loss: 0.7676 - out_14_loss: 0.7768 - out_15_loss: 0.7634 - out_16_loss: 0.7965 - out_17_loss: 0.7723 - out_18_loss: 0.7859 - out_19_loss: 0.7761 - out_20_loss: 0.7641 - out_21_loss: 0.7686 - out_22_loss: 0.7781 - out_23_loss: 0.7754 - out_24_loss: 0.7713 - out_25_loss: 0.7764 - out_26_loss: 0.7720 - out_27_loss: 0.7621 - out_28_loss: 0.7728 - out_29_loss: 0.7725 - out_30_loss: 0.7752 - out_31_loss: 0.7731 - out_32_loss: 0.7738 - out_acc: 0.7564 - out_0_acc: 0.8239 - out_1_acc: 0.7257 - out_2_acc: 0.7179 - out_3_acc: 0.7167 - out_4_acc: 0.7086 - out_5_acc: 0.7179 - out_6_acc: 0.7142 - out_7_acc: 0.7157 - out_8_acc: 0.7136 - out_9_acc: 0.7108 - out_10_acc: 0.7198 - out_11_acc: 0.7216 - out_12_acc: 0.7148 - out_13_acc: 0.7195 - out_14_acc: 0.7226 - out_15_acc: 0.7173 - out_16_acc: 0.7123 - out_17_acc: 0.7192 - out_18_acc: 0.7064 - out_19_acc: 0.7213 - out_20_acc: 0.7207 - out_21_acc: 0.7260 - out_22_acc: 0.7117 - out_23_acc: 0.7161 - out_24_acc: 0.7244 - out_25_acc: 0.7247 - out_26_acc: 0.7157 - out_27_acc: 0.7154 - out_28_acc: 0.7111 - out_29_acc: 0.7157 - out_30_acc: 0.7173 - out_31_acc: 0.7154 - out_32_acc: 0.7136 - val_loss: 24.9305 - val_out_loss: 0.6854 - val_out_0_loss: 0.4891 - val_out_1_loss: 0.7122 - val_out_2_loss: 0.7135 - val_out_3_loss: 0.7096 - val_out_4_loss: 0.7134 - val_out_5_loss: 0.7142 - val_out_6_loss: 0.7134 - val_out_7_loss: 0.7137 - val_out_8_loss: 0.7100 - val_out_9_loss: 0.7092 - val_out_10_loss: 0.7105 - val_out_11_loss: 0.7180 - val_out_12_loss: 0.7066 - val_out_13_loss: 0.7123 - val_out_14_loss: 0.7155 - val_out_15_loss: 0.7110 - val_out_16_loss: 0.7093 - val_out_17_loss: 0.7111 - val_out_18_loss: 0.7104 - val_out_19_loss: 0.7129 - val_out_20_loss: 0.7192 - val_out_21_loss: 0.7084 - val_out_22_loss: 0.7108 - val_out_23_loss: 0.7138 - val_out_24_loss: 0.7137 - val_out_25_loss: 0.7137 - val_out_26_loss: 0.7101 - val_out_27_loss: 0.7108 - val_out_28_loss: 0.7114 - val_out_29_loss: 0.7060 - val_out_30_loss: 0.7121 - val_out_31_loss: 0.7059 - val_out_32_loss: 0.7066 - val_out_acc: 0.7397 - val_out_0_acc: 0.8134 - val_out_1_acc: 0.7354 - val_out_2_acc: 0.7354 - val_out_3_acc: 0.7332 - val_out_4_acc: 0.7397 - val_out_5_acc: 0.7397 - val_out_6_acc: 0.7354 - val_out_7_acc: 0.7332 - val_out_8_acc: 0.7440 - val_out_9_acc: 0.7462 - val_out_10_acc: 0.7397 - val_out_11_acc: 0.7310 - val_out_12_acc: 0.7440 - val_out_13_acc: 0.7375 - val_out_14_acc: 0.7354 - val_out_15_acc: 0.7419 - val_out_16_acc: 0.7440 - val_out_17_acc: 0.7354 - val_out_18_acc: 0.7397 - val_out_19_acc: 0.7419 - val_out_20_acc: 0.7419 - val_out_21_acc: 0.7419 - val_out_22_acc: 0.7397 - val_out_23_acc: 0.7440 - val_out_24_acc: 0.7332 - val_out_25_acc: 0.7310 - val_out_26_acc: 0.7310 - val_out_27_acc: 0.7354 - val_out_28_acc: 0.7484 - val_out_29_acc: 0.7332 - val_out_30_acc: 0.7354 - val_out_31_acc: 0.7505 - val_out_32_acc: 0.7375\n",
      "Epoch 116/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 26.5218 - out_loss: 0.6925 - out_0_loss: 0.5099 - out_1_loss: 0.7837 - out_2_loss: 0.7835 - out_3_loss: 0.7944 - out_4_loss: 0.7888 - out_5_loss: 0.8097 - out_6_loss: 0.8000 - out_7_loss: 0.7982 - out_8_loss: 0.8012 - out_9_loss: 0.7906 - out_10_loss: 0.7952 - out_11_loss: 0.7848 - out_12_loss: 0.7878 - out_13_loss: 0.7937 - out_14_loss: 0.7909 - out_15_loss: 0.7941 - out_16_loss: 0.7975 - out_17_loss: 0.7967 - out_18_loss: 0.7929 - out_19_loss: 0.7932 - out_20_loss: 0.7940 - out_21_loss: 0.7817 - out_22_loss: 0.7877 - out_23_loss: 0.7909 - out_24_loss: 0.7952 - out_25_loss: 0.7794 - out_26_loss: 0.7856 - out_27_loss: 0.7759 - out_28_loss: 0.7940 - out_29_loss: 0.7961 - out_30_loss: 0.7992 - out_31_loss: 0.7901 - out_32_loss: 0.7727 - out_acc: 0.7622 - out_0_acc: 0.8199 - out_1_acc: 0.7325 - out_2_acc: 0.7226 - out_3_acc: 0.7244 - out_4_acc: 0.7238 - out_5_acc: 0.7154 - out_6_acc: 0.7247 - out_7_acc: 0.7176 - out_8_acc: 0.7139 - out_9_acc: 0.7179 - out_10_acc: 0.7232 - out_11_acc: 0.7219 - out_12_acc: 0.7319 - out_13_acc: 0.7173 - out_14_acc: 0.7179 - out_15_acc: 0.7247 - out_16_acc: 0.7213 - out_17_acc: 0.7241 - out_18_acc: 0.7210 - out_19_acc: 0.7198 - out_20_acc: 0.7207 - out_21_acc: 0.7235 - out_22_acc: 0.7250 - out_23_acc: 0.7167 - out_24_acc: 0.7235 - out_25_acc: 0.7257 - out_26_acc: 0.7192 - out_27_acc: 0.7278 - out_28_acc: 0.7213 - out_29_acc: 0.7198 - out_30_acc: 0.7210 - out_31_acc: 0.7278 - out_32_acc: 0.7254 - val_loss: 25.4184 - val_out_loss: 0.6999 - val_out_0_loss: 0.4852 - val_out_1_loss: 0.7281 - val_out_2_loss: 0.7217 - val_out_3_loss: 0.7345 - val_out_4_loss: 0.7308 - val_out_5_loss: 0.7212 - val_out_6_loss: 0.7334 - val_out_7_loss: 0.7258 - val_out_8_loss: 0.7190 - val_out_9_loss: 0.7228 - val_out_10_loss: 0.7274 - val_out_11_loss: 0.7254 - val_out_12_loss: 0.7217 - val_out_13_loss: 0.7293 - val_out_14_loss: 0.7274 - val_out_15_loss: 0.7288 - val_out_16_loss: 0.7275 - val_out_17_loss: 0.7280 - val_out_18_loss: 0.7242 - val_out_19_loss: 0.7208 - val_out_20_loss: 0.7293 - val_out_21_loss: 0.7255 - val_out_22_loss: 0.7269 - val_out_23_loss: 0.7237 - val_out_24_loss: 0.7275 - val_out_25_loss: 0.7307 - val_out_26_loss: 0.7202 - val_out_27_loss: 0.7227 - val_out_28_loss: 0.7254 - val_out_29_loss: 0.7192 - val_out_30_loss: 0.7279 - val_out_31_loss: 0.7269 - val_out_32_loss: 0.7232 - val_out_acc: 0.7137 - val_out_0_acc: 0.8265 - val_out_1_acc: 0.7158 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7093 - val_out_4_acc: 0.7180 - val_out_5_acc: 0.7137 - val_out_6_acc: 0.7072 - val_out_7_acc: 0.7137 - val_out_8_acc: 0.7180 - val_out_9_acc: 0.7180 - val_out_10_acc: 0.7093 - val_out_11_acc: 0.7093 - val_out_12_acc: 0.7158 - val_out_13_acc: 0.7137 - val_out_14_acc: 0.7093 - val_out_15_acc: 0.7180 - val_out_16_acc: 0.7137 - val_out_17_acc: 0.7115 - val_out_18_acc: 0.7137 - val_out_19_acc: 0.7137 - val_out_20_acc: 0.7158 - val_out_21_acc: 0.7158 - val_out_22_acc: 0.7180 - val_out_23_acc: 0.7223 - val_out_24_acc: 0.7137 - val_out_25_acc: 0.7180 - val_out_26_acc: 0.7180 - val_out_27_acc: 0.7137 - val_out_28_acc: 0.7115 - val_out_29_acc: 0.7158 - val_out_30_acc: 0.7180 - val_out_31_acc: 0.7158 - val_out_32_acc: 0.7115\n",
      "Epoch 117/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 26.2345 - out_loss: 0.6832 - out_0_loss: 0.4896 - out_1_loss: 0.7928 - out_2_loss: 0.7870 - out_3_loss: 0.7748 - out_4_loss: 0.7921 - out_5_loss: 0.7738 - out_6_loss: 0.7637 - out_7_loss: 0.7874 - out_8_loss: 0.7773 - out_9_loss: 0.7936 - out_10_loss: 0.7843 - out_11_loss: 0.7881 - out_12_loss: 0.7877 - out_13_loss: 0.7744 - out_14_loss: 0.7784 - out_15_loss: 0.7759 - out_16_loss: 0.7955 - out_17_loss: 0.7792 - out_18_loss: 0.7842 - out_19_loss: 0.7904 - out_20_loss: 0.7839 - out_21_loss: 0.8026 - out_22_loss: 0.7766 - out_23_loss: 0.7833 - out_24_loss: 0.7741 - out_25_loss: 0.7880 - out_26_loss: 0.7772 - out_27_loss: 0.7774 - out_28_loss: 0.7830 - out_29_loss: 0.7874 - out_30_loss: 0.7858 - out_31_loss: 0.7760 - out_32_loss: 0.7860 - out_acc: 0.7626 - out_0_acc: 0.8267 - out_1_acc: 0.7278 - out_2_acc: 0.7210 - out_3_acc: 0.7272 - out_4_acc: 0.7257 - out_5_acc: 0.7285 - out_6_acc: 0.7285 - out_7_acc: 0.7235 - out_8_acc: 0.7250 - out_9_acc: 0.7114 - out_10_acc: 0.7257 - out_11_acc: 0.7241 - out_12_acc: 0.7226 - out_13_acc: 0.7303 - out_14_acc: 0.7281 - out_15_acc: 0.7223 - out_16_acc: 0.7223 - out_17_acc: 0.7275 - out_18_acc: 0.7272 - out_19_acc: 0.7272 - out_20_acc: 0.7210 - out_21_acc: 0.7139 - out_22_acc: 0.7213 - out_23_acc: 0.7303 - out_24_acc: 0.7319 - out_25_acc: 0.7229 - out_26_acc: 0.7272 - out_27_acc: 0.7260 - out_28_acc: 0.7247 - out_29_acc: 0.7244 - out_30_acc: 0.7226 - out_31_acc: 0.7266 - out_32_acc: 0.7235 - val_loss: 26.6534 - val_out_loss: 0.7278 - val_out_0_loss: 0.5278 - val_out_1_loss: 0.7541 - val_out_2_loss: 0.7602 - val_out_3_loss: 0.7584 - val_out_4_loss: 0.7624 - val_out_5_loss: 0.7602 - val_out_6_loss: 0.7629 - val_out_7_loss: 0.7595 - val_out_8_loss: 0.7629 - val_out_9_loss: 0.7615 - val_out_10_loss: 0.7630 - val_out_11_loss: 0.7538 - val_out_12_loss: 0.7579 - val_out_13_loss: 0.7623 - val_out_14_loss: 0.7654 - val_out_15_loss: 0.7630 - val_out_16_loss: 0.7674 - val_out_17_loss: 0.7614 - val_out_18_loss: 0.7578 - val_out_19_loss: 0.7611 - val_out_20_loss: 0.7653 - val_out_21_loss: 0.7614 - val_out_22_loss: 0.7630 - val_out_23_loss: 0.7620 - val_out_24_loss: 0.7612 - val_out_25_loss: 0.7701 - val_out_26_loss: 0.7575 - val_out_27_loss: 0.7670 - val_out_28_loss: 0.7597 - val_out_29_loss: 0.7524 - val_out_30_loss: 0.7591 - val_out_31_loss: 0.7544 - val_out_32_loss: 0.7545 - val_out_acc: 0.7180 - val_out_0_acc: 0.8286 - val_out_1_acc: 0.7180 - val_out_2_acc: 0.7093 - val_out_3_acc: 0.7245 - val_out_4_acc: 0.7137 - val_out_5_acc: 0.7180 - val_out_6_acc: 0.7223 - val_out_7_acc: 0.7137 - val_out_8_acc: 0.7202 - val_out_9_acc: 0.7245 - val_out_10_acc: 0.7202 - val_out_11_acc: 0.7180 - val_out_12_acc: 0.7180 - val_out_13_acc: 0.7180 - val_out_14_acc: 0.7158 - val_out_15_acc: 0.7158 - val_out_16_acc: 0.7093 - val_out_17_acc: 0.7180 - val_out_18_acc: 0.7202 - val_out_19_acc: 0.7158 - val_out_20_acc: 0.7158 - val_out_21_acc: 0.7180 - val_out_22_acc: 0.7158 - val_out_23_acc: 0.7115 - val_out_24_acc: 0.7223 - val_out_25_acc: 0.7202 - val_out_26_acc: 0.7137 - val_out_27_acc: 0.7245 - val_out_28_acc: 0.7115 - val_out_29_acc: 0.7202 - val_out_30_acc: 0.7158 - val_out_31_acc: 0.7180 - val_out_32_acc: 0.7223\n",
      "Epoch 118/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 26.6458 - out_loss: 0.6962 - out_0_loss: 0.5240 - out_1_loss: 0.7987 - out_2_loss: 0.7990 - out_3_loss: 0.8000 - out_4_loss: 0.7936 - out_5_loss: 0.7833 - out_6_loss: 0.7901 - out_7_loss: 0.7932 - out_8_loss: 0.7864 - out_9_loss: 0.7881 - out_10_loss: 0.7823 - out_11_loss: 0.8162 - out_12_loss: 0.7903 - out_13_loss: 0.7937 - out_14_loss: 0.7926 - out_15_loss: 0.7959 - out_16_loss: 0.7987 - out_17_loss: 0.8035 - out_18_loss: 0.7941 - out_19_loss: 0.7960 - out_20_loss: 0.7922 - out_21_loss: 0.7908 - out_22_loss: 0.7880 - out_23_loss: 0.7959 - out_24_loss: 0.7973 - out_25_loss: 0.7919 - out_26_loss: 0.8040 - out_27_loss: 0.8000 - out_28_loss: 0.7932 - out_29_loss: 0.7908 - out_30_loss: 0.7906 - out_31_loss: 0.7914 - out_32_loss: 0.8039 - out_acc: 0.7514 - out_0_acc: 0.8236 - out_1_acc: 0.7095 - out_2_acc: 0.7139 - out_3_acc: 0.7043 - out_4_acc: 0.7164 - out_5_acc: 0.7195 - out_6_acc: 0.7123 - out_7_acc: 0.7114 - out_8_acc: 0.7179 - out_9_acc: 0.7148 - out_10_acc: 0.7185 - out_11_acc: 0.7012 - out_12_acc: 0.7176 - out_13_acc: 0.7133 - out_14_acc: 0.7164 - out_15_acc: 0.7198 - out_16_acc: 0.7111 - out_17_acc: 0.7133 - out_18_acc: 0.7126 - out_19_acc: 0.7083 - out_20_acc: 0.7111 - out_21_acc: 0.7201 - out_22_acc: 0.7114 - out_23_acc: 0.7099 - out_24_acc: 0.7167 - out_25_acc: 0.7092 - out_26_acc: 0.7033 - out_27_acc: 0.7105 - out_28_acc: 0.7179 - out_29_acc: 0.7145 - out_30_acc: 0.7071 - out_31_acc: 0.7185 - out_32_acc: 0.7130 - val_loss: 25.4164 - val_out_loss: 0.6909 - val_out_0_loss: 0.5373 - val_out_1_loss: 0.7256 - val_out_2_loss: 0.7229 - val_out_3_loss: 0.7247 - val_out_4_loss: 0.7276 - val_out_5_loss: 0.7224 - val_out_6_loss: 0.7228 - val_out_7_loss: 0.7219 - val_out_8_loss: 0.7218 - val_out_9_loss: 0.7226 - val_out_10_loss: 0.7269 - val_out_11_loss: 0.7258 - val_out_12_loss: 0.7222 - val_out_13_loss: 0.7267 - val_out_14_loss: 0.7275 - val_out_15_loss: 0.7241 - val_out_16_loss: 0.7234 - val_out_17_loss: 0.7275 - val_out_18_loss: 0.7229 - val_out_19_loss: 0.7266 - val_out_20_loss: 0.7274 - val_out_21_loss: 0.7231 - val_out_22_loss: 0.7243 - val_out_23_loss: 0.7238 - val_out_24_loss: 0.7210 - val_out_25_loss: 0.7263 - val_out_26_loss: 0.7231 - val_out_27_loss: 0.7245 - val_out_28_loss: 0.7275 - val_out_29_loss: 0.7234 - val_out_30_loss: 0.7273 - val_out_31_loss: 0.7205 - val_out_32_loss: 0.7243 - val_out_acc: 0.7397 - val_out_0_acc: 0.7961 - val_out_1_acc: 0.7419 - val_out_2_acc: 0.7375 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.7310 - val_out_5_acc: 0.7354 - val_out_6_acc: 0.7332 - val_out_7_acc: 0.7440 - val_out_8_acc: 0.7419 - val_out_9_acc: 0.7419 - val_out_10_acc: 0.7332 - val_out_11_acc: 0.7354 - val_out_12_acc: 0.7289 - val_out_13_acc: 0.7332 - val_out_14_acc: 0.7397 - val_out_15_acc: 0.7354 - val_out_16_acc: 0.7375 - val_out_17_acc: 0.7440 - val_out_18_acc: 0.7354 - val_out_19_acc: 0.7332 - val_out_20_acc: 0.7354 - val_out_21_acc: 0.7332 - val_out_22_acc: 0.7419 - val_out_23_acc: 0.7310 - val_out_24_acc: 0.7310 - val_out_25_acc: 0.7310 - val_out_26_acc: 0.7310 - val_out_27_acc: 0.7375 - val_out_28_acc: 0.7267 - val_out_29_acc: 0.7332 - val_out_30_acc: 0.7310 - val_out_31_acc: 0.7202 - val_out_32_acc: 0.7397\n",
      "Epoch 119/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 26.1827 - out_loss: 0.6850 - out_0_loss: 0.4840 - out_1_loss: 0.7754 - out_2_loss: 0.7686 - out_3_loss: 0.7768 - out_4_loss: 0.7690 - out_5_loss: 0.7772 - out_6_loss: 0.7881 - out_7_loss: 0.7889 - out_8_loss: 0.7822 - out_9_loss: 0.7823 - out_10_loss: 0.7756 - out_11_loss: 0.7739 - out_12_loss: 0.7864 - out_13_loss: 0.7864 - out_14_loss: 0.7769 - out_15_loss: 0.7879 - out_16_loss: 0.7902 - out_17_loss: 0.7743 - out_18_loss: 0.7778 - out_19_loss: 0.7877 - out_20_loss: 0.7860 - out_21_loss: 0.7836 - out_22_loss: 0.7778 - out_23_loss: 0.7924 - out_24_loss: 0.7833 - out_25_loss: 0.7791 - out_26_loss: 0.7887 - out_27_loss: 0.7833 - out_28_loss: 0.7721 - out_29_loss: 0.7870 - out_30_loss: 0.7772 - out_31_loss: 0.7948 - out_32_loss: 0.7827 - out_acc: 0.7632 - out_0_acc: 0.8286 - out_1_acc: 0.7250 - out_2_acc: 0.7281 - out_3_acc: 0.7250 - out_4_acc: 0.7331 - out_5_acc: 0.7204 - out_6_acc: 0.7216 - out_7_acc: 0.7232 - out_8_acc: 0.7294 - out_9_acc: 0.7207 - out_10_acc: 0.7216 - out_11_acc: 0.7223 - out_12_acc: 0.7201 - out_13_acc: 0.7232 - out_14_acc: 0.7297 - out_15_acc: 0.7241 - out_16_acc: 0.7170 - out_17_acc: 0.7201 - out_18_acc: 0.7281 - out_19_acc: 0.7235 - out_20_acc: 0.7201 - out_21_acc: 0.7291 - out_22_acc: 0.7207 - out_23_acc: 0.7232 - out_24_acc: 0.7232 - out_25_acc: 0.7188 - out_26_acc: 0.7185 - out_27_acc: 0.7216 - out_28_acc: 0.7294 - out_29_acc: 0.7223 - out_30_acc: 0.7275 - out_31_acc: 0.7232 - out_32_acc: 0.7337 - val_loss: 26.7956 - val_out_loss: 0.7314 - val_out_0_loss: 0.5896 - val_out_1_loss: 0.7618 - val_out_2_loss: 0.7606 - val_out_3_loss: 0.7636 - val_out_4_loss: 0.7666 - val_out_5_loss: 0.7640 - val_out_6_loss: 0.7621 - val_out_7_loss: 0.7591 - val_out_8_loss: 0.7661 - val_out_9_loss: 0.7655 - val_out_10_loss: 0.7623 - val_out_11_loss: 0.7664 - val_out_12_loss: 0.7608 - val_out_13_loss: 0.7576 - val_out_14_loss: 0.7690 - val_out_15_loss: 0.7578 - val_out_16_loss: 0.7628 - val_out_17_loss: 0.7614 - val_out_18_loss: 0.7628 - val_out_19_loss: 0.7665 - val_out_20_loss: 0.7628 - val_out_21_loss: 0.7635 - val_out_22_loss: 0.7634 - val_out_23_loss: 0.7653 - val_out_24_loss: 0.7611 - val_out_25_loss: 0.7683 - val_out_26_loss: 0.7639 - val_out_27_loss: 0.7663 - val_out_28_loss: 0.7619 - val_out_29_loss: 0.7623 - val_out_30_loss: 0.7582 - val_out_31_loss: 0.7631 - val_out_32_loss: 0.7569 - val_out_acc: 0.7267 - val_out_0_acc: 0.8091 - val_out_1_acc: 0.7245 - val_out_2_acc: 0.7267 - val_out_3_acc: 0.7332 - val_out_4_acc: 0.7245 - val_out_5_acc: 0.7267 - val_out_6_acc: 0.7245 - val_out_7_acc: 0.7289 - val_out_8_acc: 0.7289 - val_out_9_acc: 0.7202 - val_out_10_acc: 0.7267 - val_out_11_acc: 0.7289 - val_out_12_acc: 0.7267 - val_out_13_acc: 0.7267 - val_out_14_acc: 0.7267 - val_out_15_acc: 0.7310 - val_out_16_acc: 0.7332 - val_out_17_acc: 0.7375 - val_out_18_acc: 0.7289 - val_out_19_acc: 0.7245 - val_out_20_acc: 0.7267 - val_out_21_acc: 0.7245 - val_out_22_acc: 0.7267 - val_out_23_acc: 0.7375 - val_out_24_acc: 0.7245 - val_out_25_acc: 0.7289 - val_out_26_acc: 0.7267 - val_out_27_acc: 0.7180 - val_out_28_acc: 0.7332 - val_out_29_acc: 0.7267 - val_out_30_acc: 0.7289 - val_out_31_acc: 0.7310 - val_out_32_acc: 0.7310\n",
      "Epoch 120/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 26.0648 - out_loss: 0.6826 - out_0_loss: 0.4975 - out_1_loss: 0.7854 - out_2_loss: 0.7814 - out_3_loss: 0.7740 - out_4_loss: 0.7726 - out_5_loss: 0.7725 - out_6_loss: 0.7707 - out_7_loss: 0.7807 - out_8_loss: 0.7701 - out_9_loss: 0.7695 - out_10_loss: 0.7809 - out_11_loss: 0.7731 - out_12_loss: 0.7807 - out_13_loss: 0.7767 - out_14_loss: 0.7746 - out_15_loss: 0.7854 - out_16_loss: 0.7717 - out_17_loss: 0.7887 - out_18_loss: 0.7876 - out_19_loss: 0.7839 - out_20_loss: 0.7768 - out_21_loss: 0.7677 - out_22_loss: 0.7852 - out_23_loss: 0.7733 - out_24_loss: 0.7831 - out_25_loss: 0.7788 - out_26_loss: 0.7789 - out_27_loss: 0.7808 - out_28_loss: 0.7793 - out_29_loss: 0.7752 - out_30_loss: 0.7745 - out_31_loss: 0.7798 - out_32_loss: 0.7710 - out_acc: 0.7570 - out_0_acc: 0.8246 - out_1_acc: 0.7195 - out_2_acc: 0.7148 - out_3_acc: 0.7123 - out_4_acc: 0.7232 - out_5_acc: 0.7297 - out_6_acc: 0.7226 - out_7_acc: 0.7232 - out_8_acc: 0.7192 - out_9_acc: 0.7192 - out_10_acc: 0.7188 - out_11_acc: 0.7173 - out_12_acc: 0.7182 - out_13_acc: 0.7195 - out_14_acc: 0.7254 - out_15_acc: 0.7244 - out_16_acc: 0.7260 - out_17_acc: 0.7126 - out_18_acc: 0.7173 - out_19_acc: 0.7108 - out_20_acc: 0.7223 - out_21_acc: 0.7278 - out_22_acc: 0.7157 - out_23_acc: 0.7154 - out_24_acc: 0.7151 - out_25_acc: 0.7210 - out_26_acc: 0.7167 - out_27_acc: 0.7145 - out_28_acc: 0.7182 - out_29_acc: 0.7185 - out_30_acc: 0.7226 - out_31_acc: 0.7226 - out_32_acc: 0.7266 - val_loss: 25.7201 - val_out_loss: 0.7017 - val_out_0_loss: 0.4207 - val_out_1_loss: 0.7369 - val_out_2_loss: 0.7371 - val_out_3_loss: 0.7409 - val_out_4_loss: 0.7321 - val_out_5_loss: 0.7325 - val_out_6_loss: 0.7387 - val_out_7_loss: 0.7339 - val_out_8_loss: 0.7376 - val_out_9_loss: 0.7345 - val_out_10_loss: 0.7394 - val_out_11_loss: 0.7339 - val_out_12_loss: 0.7349 - val_out_13_loss: 0.7332 - val_out_14_loss: 0.7449 - val_out_15_loss: 0.7404 - val_out_16_loss: 0.7354 - val_out_17_loss: 0.7411 - val_out_18_loss: 0.7332 - val_out_19_loss: 0.7359 - val_out_20_loss: 0.7383 - val_out_21_loss: 0.7361 - val_out_22_loss: 0.7356 - val_out_23_loss: 0.7434 - val_out_24_loss: 0.7418 - val_out_25_loss: 0.7449 - val_out_26_loss: 0.7353 - val_out_27_loss: 0.7354 - val_out_28_loss: 0.7306 - val_out_29_loss: 0.7311 - val_out_30_loss: 0.7387 - val_out_31_loss: 0.7374 - val_out_32_loss: 0.7343 - val_out_acc: 0.7570 - val_out_0_acc: 0.8460 - val_out_1_acc: 0.7484 - val_out_2_acc: 0.7527 - val_out_3_acc: 0.7527 - val_out_4_acc: 0.7527 - val_out_5_acc: 0.7505 - val_out_6_acc: 0.7505 - val_out_7_acc: 0.7397 - val_out_8_acc: 0.7657 - val_out_9_acc: 0.7527 - val_out_10_acc: 0.7484 - val_out_11_acc: 0.7527 - val_out_12_acc: 0.7527 - val_out_13_acc: 0.7549 - val_out_14_acc: 0.7419 - val_out_15_acc: 0.7527 - val_out_16_acc: 0.7527 - val_out_17_acc: 0.7462 - val_out_18_acc: 0.7505 - val_out_19_acc: 0.7505 - val_out_20_acc: 0.7592 - val_out_21_acc: 0.7462 - val_out_22_acc: 0.7549 - val_out_23_acc: 0.7549 - val_out_24_acc: 0.7570 - val_out_25_acc: 0.7397 - val_out_26_acc: 0.7484 - val_out_27_acc: 0.7462 - val_out_28_acc: 0.7484 - val_out_29_acc: 0.7484 - val_out_30_acc: 0.7527 - val_out_31_acc: 0.7484 - val_out_32_acc: 0.7505\n",
      "Epoch 121/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 25.7136 - out_loss: 0.6712 - out_0_loss: 0.4770 - out_1_loss: 0.7660 - out_2_loss: 0.7787 - out_3_loss: 0.7607 - out_4_loss: 0.7672 - out_5_loss: 0.7703 - out_6_loss: 0.7667 - out_7_loss: 0.7642 - out_8_loss: 0.7710 - out_9_loss: 0.7610 - out_10_loss: 0.7683 - out_11_loss: 0.7703 - out_12_loss: 0.7617 - out_13_loss: 0.7612 - out_14_loss: 0.7690 - out_15_loss: 0.7760 - out_16_loss: 0.7719 - out_17_loss: 0.7713 - out_18_loss: 0.7794 - out_19_loss: 0.7720 - out_20_loss: 0.7650 - out_21_loss: 0.7736 - out_22_loss: 0.7768 - out_23_loss: 0.7720 - out_24_loss: 0.7680 - out_25_loss: 0.7478 - out_26_loss: 0.7641 - out_27_loss: 0.7644 - out_28_loss: 0.7646 - out_29_loss: 0.7676 - out_30_loss: 0.7741 - out_31_loss: 0.7721 - out_32_loss: 0.7482 - out_acc: 0.7601 - out_0_acc: 0.8301 - out_1_acc: 0.7226 - out_2_acc: 0.7173 - out_3_acc: 0.7260 - out_4_acc: 0.7303 - out_5_acc: 0.7154 - out_6_acc: 0.7207 - out_7_acc: 0.7201 - out_8_acc: 0.7164 - out_9_acc: 0.7254 - out_10_acc: 0.7161 - out_11_acc: 0.7114 - out_12_acc: 0.7244 - out_13_acc: 0.7226 - out_14_acc: 0.7201 - out_15_acc: 0.7213 - out_16_acc: 0.7185 - out_17_acc: 0.7229 - out_18_acc: 0.7170 - out_19_acc: 0.7216 - out_20_acc: 0.7269 - out_21_acc: 0.7139 - out_22_acc: 0.7111 - out_23_acc: 0.7179 - out_24_acc: 0.7179 - out_25_acc: 0.7223 - out_26_acc: 0.7167 - out_27_acc: 0.7316 - out_28_acc: 0.7213 - out_29_acc: 0.7204 - out_30_acc: 0.7266 - out_31_acc: 0.7238 - out_32_acc: 0.7269 - val_loss: 25.2755 - val_out_loss: 0.6902 - val_out_0_loss: 0.4473 - val_out_1_loss: 0.7254 - val_out_2_loss: 0.7246 - val_out_3_loss: 0.7292 - val_out_4_loss: 0.7206 - val_out_5_loss: 0.7197 - val_out_6_loss: 0.7235 - val_out_7_loss: 0.7216 - val_out_8_loss: 0.7227 - val_out_9_loss: 0.7256 - val_out_10_loss: 0.7257 - val_out_11_loss: 0.7250 - val_out_12_loss: 0.7186 - val_out_13_loss: 0.7212 - val_out_14_loss: 0.7226 - val_out_15_loss: 0.7225 - val_out_16_loss: 0.7257 - val_out_17_loss: 0.7192 - val_out_18_loss: 0.7272 - val_out_19_loss: 0.7200 - val_out_20_loss: 0.7242 - val_out_21_loss: 0.7234 - val_out_22_loss: 0.7225 - val_out_23_loss: 0.7220 - val_out_24_loss: 0.7244 - val_out_25_loss: 0.7218 - val_out_26_loss: 0.7236 - val_out_27_loss: 0.7235 - val_out_28_loss: 0.7219 - val_out_29_loss: 0.7231 - val_out_30_loss: 0.7235 - val_out_31_loss: 0.7201 - val_out_32_loss: 0.7230 - val_out_acc: 0.7354 - val_out_0_acc: 0.8438 - val_out_1_acc: 0.7245 - val_out_2_acc: 0.7354 - val_out_3_acc: 0.7245 - val_out_4_acc: 0.7267 - val_out_5_acc: 0.7310 - val_out_6_acc: 0.7354 - val_out_7_acc: 0.7310 - val_out_8_acc: 0.7223 - val_out_9_acc: 0.7397 - val_out_10_acc: 0.7267 - val_out_11_acc: 0.7332 - val_out_12_acc: 0.7375 - val_out_13_acc: 0.7332 - val_out_14_acc: 0.7332 - val_out_15_acc: 0.7245 - val_out_16_acc: 0.7223 - val_out_17_acc: 0.7289 - val_out_18_acc: 0.7180 - val_out_19_acc: 0.7310 - val_out_20_acc: 0.7245 - val_out_21_acc: 0.7223 - val_out_22_acc: 0.7267 - val_out_23_acc: 0.7245 - val_out_24_acc: 0.7289 - val_out_25_acc: 0.7245 - val_out_26_acc: 0.7245 - val_out_27_acc: 0.7267 - val_out_28_acc: 0.7289 - val_out_29_acc: 0.7267 - val_out_30_acc: 0.7289 - val_out_31_acc: 0.7267 - val_out_32_acc: 0.7245\n",
      "Epoch 122/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 25.6640 - out_loss: 0.6682 - out_0_loss: 0.4576 - out_1_loss: 0.7672 - out_2_loss: 0.7448 - out_3_loss: 0.7803 - out_4_loss: 0.7734 - out_5_loss: 0.7558 - out_6_loss: 0.7572 - out_7_loss: 0.7611 - out_8_loss: 0.7718 - out_9_loss: 0.7797 - out_10_loss: 0.7822 - out_11_loss: 0.7573 - out_12_loss: 0.7637 - out_13_loss: 0.7697 - out_14_loss: 0.7745 - out_15_loss: 0.7544 - out_16_loss: 0.7613 - out_17_loss: 0.7754 - out_18_loss: 0.7637 - out_19_loss: 0.7559 - out_20_loss: 0.7693 - out_21_loss: 0.7727 - out_22_loss: 0.7645 - out_23_loss: 0.7769 - out_24_loss: 0.7693 - out_25_loss: 0.7701 - out_26_loss: 0.7581 - out_27_loss: 0.7663 - out_28_loss: 0.7606 - out_29_loss: 0.7725 - out_30_loss: 0.7731 - out_31_loss: 0.7597 - out_32_loss: 0.7753 - out_acc: 0.7619 - out_0_acc: 0.8382 - out_1_acc: 0.7291 - out_2_acc: 0.7294 - out_3_acc: 0.7179 - out_4_acc: 0.7226 - out_5_acc: 0.7260 - out_6_acc: 0.7247 - out_7_acc: 0.7244 - out_8_acc: 0.7157 - out_9_acc: 0.7198 - out_10_acc: 0.7195 - out_11_acc: 0.7316 - out_12_acc: 0.7250 - out_13_acc: 0.7167 - out_14_acc: 0.7179 - out_15_acc: 0.7275 - out_16_acc: 0.7223 - out_17_acc: 0.7229 - out_18_acc: 0.7263 - out_19_acc: 0.7192 - out_20_acc: 0.7229 - out_21_acc: 0.7195 - out_22_acc: 0.7300 - out_23_acc: 0.7250 - out_24_acc: 0.7185 - out_25_acc: 0.7257 - out_26_acc: 0.7272 - out_27_acc: 0.7257 - out_28_acc: 0.7241 - out_29_acc: 0.7247 - out_30_acc: 0.7229 - out_31_acc: 0.7120 - out_32_acc: 0.7216 - val_loss: 25.5434 - val_out_loss: 0.6999 - val_out_0_loss: 0.6198 - val_out_1_loss: 0.7216 - val_out_2_loss: 0.7260 - val_out_3_loss: 0.7240 - val_out_4_loss: 0.7234 - val_out_5_loss: 0.7213 - val_out_6_loss: 0.7271 - val_out_7_loss: 0.7258 - val_out_8_loss: 0.7256 - val_out_9_loss: 0.7238 - val_out_10_loss: 0.7290 - val_out_11_loss: 0.7211 - val_out_12_loss: 0.7247 - val_out_13_loss: 0.7253 - val_out_14_loss: 0.7249 - val_out_15_loss: 0.7277 - val_out_16_loss: 0.7248 - val_out_17_loss: 0.7242 - val_out_18_loss: 0.7252 - val_out_19_loss: 0.7280 - val_out_20_loss: 0.7306 - val_out_21_loss: 0.7232 - val_out_22_loss: 0.7283 - val_out_23_loss: 0.7282 - val_out_24_loss: 0.7285 - val_out_25_loss: 0.7280 - val_out_26_loss: 0.7225 - val_out_27_loss: 0.7253 - val_out_28_loss: 0.7244 - val_out_29_loss: 0.7228 - val_out_30_loss: 0.7275 - val_out_31_loss: 0.7257 - val_out_32_loss: 0.7243 - val_out_acc: 0.7549 - val_out_0_acc: 0.7918 - val_out_1_acc: 0.7440 - val_out_2_acc: 0.7440 - val_out_3_acc: 0.7419 - val_out_4_acc: 0.7375 - val_out_5_acc: 0.7505 - val_out_6_acc: 0.7397 - val_out_7_acc: 0.7419 - val_out_8_acc: 0.7354 - val_out_9_acc: 0.7397 - val_out_10_acc: 0.7419 - val_out_11_acc: 0.7419 - val_out_12_acc: 0.7354 - val_out_13_acc: 0.7397 - val_out_14_acc: 0.7505 - val_out_15_acc: 0.7419 - val_out_16_acc: 0.7332 - val_out_17_acc: 0.7462 - val_out_18_acc: 0.7354 - val_out_19_acc: 0.7419 - val_out_20_acc: 0.7375 - val_out_21_acc: 0.7462 - val_out_22_acc: 0.7397 - val_out_23_acc: 0.7397 - val_out_24_acc: 0.7419 - val_out_25_acc: 0.7354 - val_out_26_acc: 0.7484 - val_out_27_acc: 0.7332 - val_out_28_acc: 0.7397 - val_out_29_acc: 0.7440 - val_out_30_acc: 0.7354 - val_out_31_acc: 0.7419 - val_out_32_acc: 0.7527\n",
      "Epoch 123/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 26.2000 - out_loss: 0.6814 - out_0_loss: 0.4906 - out_1_loss: 0.7778 - out_2_loss: 0.7729 - out_3_loss: 0.7776 - out_4_loss: 0.7878 - out_5_loss: 0.7699 - out_6_loss: 0.7768 - out_7_loss: 0.7808 - out_8_loss: 0.7685 - out_9_loss: 0.7905 - out_10_loss: 0.7885 - out_11_loss: 0.7929 - out_12_loss: 0.7800 - out_13_loss: 0.7843 - out_14_loss: 0.7899 - out_15_loss: 0.7749 - out_16_loss: 0.7955 - out_17_loss: 0.7787 - out_18_loss: 0.7955 - out_19_loss: 0.7795 - out_20_loss: 0.7794 - out_21_loss: 0.7825 - out_22_loss: 0.7876 - out_23_loss: 0.7695 - out_24_loss: 0.7864 - out_25_loss: 0.7798 - out_26_loss: 0.7748 - out_27_loss: 0.7751 - out_28_loss: 0.7828 - out_29_loss: 0.7955 - out_30_loss: 0.7794 - out_31_loss: 0.7894 - out_32_loss: 0.7837 - out_acc: 0.7567 - out_0_acc: 0.8292 - out_1_acc: 0.7210 - out_2_acc: 0.7281 - out_3_acc: 0.7192 - out_4_acc: 0.7257 - out_5_acc: 0.7223 - out_6_acc: 0.7260 - out_7_acc: 0.7188 - out_8_acc: 0.7247 - out_9_acc: 0.7108 - out_10_acc: 0.7244 - out_11_acc: 0.7179 - out_12_acc: 0.7263 - out_13_acc: 0.7164 - out_14_acc: 0.7102 - out_15_acc: 0.7210 - out_16_acc: 0.7102 - out_17_acc: 0.7195 - out_18_acc: 0.7108 - out_19_acc: 0.7157 - out_20_acc: 0.7229 - out_21_acc: 0.7195 - out_22_acc: 0.7260 - out_23_acc: 0.7238 - out_24_acc: 0.7250 - out_25_acc: 0.7219 - out_26_acc: 0.7241 - out_27_acc: 0.7185 - out_28_acc: 0.7207 - out_29_acc: 0.7151 - out_30_acc: 0.7247 - out_31_acc: 0.7167 - out_32_acc: 0.7207 - val_loss: 24.2346 - val_out_loss: 0.6641 - val_out_0_loss: 0.4468 - val_out_1_loss: 0.6884 - val_out_2_loss: 0.6961 - val_out_3_loss: 0.6880 - val_out_4_loss: 0.7010 - val_out_5_loss: 0.6898 - val_out_6_loss: 0.6902 - val_out_7_loss: 0.6893 - val_out_8_loss: 0.6923 - val_out_9_loss: 0.6892 - val_out_10_loss: 0.6950 - val_out_11_loss: 0.6940 - val_out_12_loss: 0.6880 - val_out_13_loss: 0.6937 - val_out_14_loss: 0.6934 - val_out_15_loss: 0.6978 - val_out_16_loss: 0.6902 - val_out_17_loss: 0.6900 - val_out_18_loss: 0.6897 - val_out_19_loss: 0.6954 - val_out_20_loss: 0.6950 - val_out_21_loss: 0.6949 - val_out_22_loss: 0.6947 - val_out_23_loss: 0.6983 - val_out_24_loss: 0.6896 - val_out_25_loss: 0.6894 - val_out_26_loss: 0.6913 - val_out_27_loss: 0.6964 - val_out_28_loss: 0.6967 - val_out_29_loss: 0.6901 - val_out_30_loss: 0.6920 - val_out_31_loss: 0.6913 - val_out_32_loss: 0.6932 - val_out_acc: 0.7636 - val_out_0_acc: 0.8482 - val_out_1_acc: 0.7657 - val_out_2_acc: 0.7570 - val_out_3_acc: 0.7592 - val_out_4_acc: 0.7549 - val_out_5_acc: 0.7614 - val_out_6_acc: 0.7657 - val_out_7_acc: 0.7570 - val_out_8_acc: 0.7636 - val_out_9_acc: 0.7505 - val_out_10_acc: 0.7570 - val_out_11_acc: 0.7505 - val_out_12_acc: 0.7614 - val_out_13_acc: 0.7570 - val_out_14_acc: 0.7549 - val_out_15_acc: 0.7592 - val_out_16_acc: 0.7614 - val_out_17_acc: 0.7592 - val_out_18_acc: 0.7614 - val_out_19_acc: 0.7570 - val_out_20_acc: 0.7592 - val_out_21_acc: 0.7592 - val_out_22_acc: 0.7614 - val_out_23_acc: 0.7505 - val_out_24_acc: 0.7592 - val_out_25_acc: 0.7570 - val_out_26_acc: 0.7657 - val_out_27_acc: 0.7570 - val_out_28_acc: 0.7570 - val_out_29_acc: 0.7505 - val_out_30_acc: 0.7657 - val_out_31_acc: 0.7636 - val_out_32_acc: 0.7592\n",
      "Epoch 124/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 26.3691 - out_loss: 0.6876 - out_0_loss: 0.4695 - out_1_loss: 0.7841 - out_2_loss: 0.7826 - out_3_loss: 0.7837 - out_4_loss: 0.7881 - out_5_loss: 0.7721 - out_6_loss: 0.8068 - out_7_loss: 0.8036 - out_8_loss: 0.7910 - out_9_loss: 0.7803 - out_10_loss: 0.7821 - out_11_loss: 0.7782 - out_12_loss: 0.7779 - out_13_loss: 0.7839 - out_14_loss: 0.7908 - out_15_loss: 0.7907 - out_16_loss: 0.7977 - out_17_loss: 0.7919 - out_18_loss: 0.7936 - out_19_loss: 0.7976 - out_20_loss: 0.7950 - out_21_loss: 0.7944 - out_22_loss: 0.8054 - out_23_loss: 0.7857 - out_24_loss: 0.7840 - out_25_loss: 0.7752 - out_26_loss: 0.7889 - out_27_loss: 0.7807 - out_28_loss: 0.7870 - out_29_loss: 0.7706 - out_30_loss: 0.7957 - out_31_loss: 0.7919 - out_32_loss: 0.7808 - out_acc: 0.7542 - out_0_acc: 0.8422 - out_1_acc: 0.7238 - out_2_acc: 0.7250 - out_3_acc: 0.7092 - out_4_acc: 0.7151 - out_5_acc: 0.7185 - out_6_acc: 0.7126 - out_7_acc: 0.7086 - out_8_acc: 0.7179 - out_9_acc: 0.7207 - out_10_acc: 0.7164 - out_11_acc: 0.7260 - out_12_acc: 0.7195 - out_13_acc: 0.7167 - out_14_acc: 0.7157 - out_15_acc: 0.7151 - out_16_acc: 0.7074 - out_17_acc: 0.7167 - out_18_acc: 0.7123 - out_19_acc: 0.7130 - out_20_acc: 0.7139 - out_21_acc: 0.7130 - out_22_acc: 0.7151 - out_23_acc: 0.7068 - out_24_acc: 0.7185 - out_25_acc: 0.7139 - out_26_acc: 0.7151 - out_27_acc: 0.7170 - out_28_acc: 0.7105 - out_29_acc: 0.7176 - out_30_acc: 0.7083 - out_31_acc: 0.7164 - out_32_acc: 0.7126 - val_loss: 24.3325 - val_out_loss: 0.6652 - val_out_0_loss: 0.4519 - val_out_1_loss: 0.6942 - val_out_2_loss: 0.6972 - val_out_3_loss: 0.6949 - val_out_4_loss: 0.6988 - val_out_5_loss: 0.6970 - val_out_6_loss: 0.6975 - val_out_7_loss: 0.6941 - val_out_8_loss: 0.6905 - val_out_9_loss: 0.6937 - val_out_10_loss: 0.6971 - val_out_11_loss: 0.6960 - val_out_12_loss: 0.6952 - val_out_13_loss: 0.6947 - val_out_14_loss: 0.6999 - val_out_15_loss: 0.6952 - val_out_16_loss: 0.6944 - val_out_17_loss: 0.6925 - val_out_18_loss: 0.6982 - val_out_19_loss: 0.6948 - val_out_20_loss: 0.6976 - val_out_21_loss: 0.6919 - val_out_22_loss: 0.6936 - val_out_23_loss: 0.6946 - val_out_24_loss: 0.6946 - val_out_25_loss: 0.7003 - val_out_26_loss: 0.6953 - val_out_27_loss: 0.6941 - val_out_28_loss: 0.6948 - val_out_29_loss: 0.6933 - val_out_30_loss: 0.7000 - val_out_31_loss: 0.6945 - val_out_32_loss: 0.6917 - val_out_acc: 0.7766 - val_out_0_acc: 0.8460 - val_out_1_acc: 0.7679 - val_out_2_acc: 0.7679 - val_out_3_acc: 0.7614 - val_out_4_acc: 0.7701 - val_out_5_acc: 0.7722 - val_out_6_acc: 0.7657 - val_out_7_acc: 0.7722 - val_out_8_acc: 0.7679 - val_out_9_acc: 0.7744 - val_out_10_acc: 0.7636 - val_out_11_acc: 0.7701 - val_out_12_acc: 0.7657 - val_out_13_acc: 0.7766 - val_out_14_acc: 0.7722 - val_out_15_acc: 0.7701 - val_out_16_acc: 0.7722 - val_out_17_acc: 0.7722 - val_out_18_acc: 0.7657 - val_out_19_acc: 0.7657 - val_out_20_acc: 0.7701 - val_out_21_acc: 0.7722 - val_out_22_acc: 0.7701 - val_out_23_acc: 0.7657 - val_out_24_acc: 0.7722 - val_out_25_acc: 0.7657 - val_out_26_acc: 0.7679 - val_out_27_acc: 0.7701 - val_out_28_acc: 0.7701 - val_out_29_acc: 0.7722 - val_out_30_acc: 0.7722 - val_out_31_acc: 0.7679 - val_out_32_acc: 0.7744\n",
      "Epoch 125/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 25.4951 - out_loss: 0.6628 - out_0_loss: 0.4550 - out_1_loss: 0.7553 - out_2_loss: 0.7721 - out_3_loss: 0.7638 - out_4_loss: 0.7621 - out_5_loss: 0.7662 - out_6_loss: 0.7586 - out_7_loss: 0.7602 - out_8_loss: 0.7530 - out_9_loss: 0.7413 - out_10_loss: 0.7713 - out_11_loss: 0.7774 - out_12_loss: 0.7515 - out_13_loss: 0.7558 - out_14_loss: 0.7787 - out_15_loss: 0.7554 - out_16_loss: 0.7682 - out_17_loss: 0.7425 - out_18_loss: 0.7642 - out_19_loss: 0.7600 - out_20_loss: 0.7674 - out_21_loss: 0.7486 - out_22_loss: 0.7573 - out_23_loss: 0.7744 - out_24_loss: 0.7644 - out_25_loss: 0.7680 - out_26_loss: 0.7662 - out_27_loss: 0.7586 - out_28_loss: 0.7603 - out_29_loss: 0.7638 - out_30_loss: 0.7639 - out_31_loss: 0.7719 - out_32_loss: 0.7547 - out_acc: 0.7669 - out_0_acc: 0.8428 - out_1_acc: 0.7250 - out_2_acc: 0.7226 - out_3_acc: 0.7309 - out_4_acc: 0.7263 - out_5_acc: 0.7294 - out_6_acc: 0.7266 - out_7_acc: 0.7195 - out_8_acc: 0.7285 - out_9_acc: 0.7328 - out_10_acc: 0.7241 - out_11_acc: 0.7117 - out_12_acc: 0.7294 - out_13_acc: 0.7275 - out_14_acc: 0.7198 - out_15_acc: 0.7343 - out_16_acc: 0.7278 - out_17_acc: 0.7365 - out_18_acc: 0.7250 - out_19_acc: 0.7238 - out_20_acc: 0.7250 - out_21_acc: 0.7350 - out_22_acc: 0.7254 - out_23_acc: 0.7291 - out_24_acc: 0.7278 - out_25_acc: 0.7294 - out_26_acc: 0.7297 - out_27_acc: 0.7306 - out_28_acc: 0.7278 - out_29_acc: 0.7170 - out_30_acc: 0.7266 - out_31_acc: 0.7250 - out_32_acc: 0.7312 - val_loss: 24.5774 - val_out_loss: 0.6706 - val_out_0_loss: 0.4263 - val_out_1_loss: 0.7046 - val_out_2_loss: 0.7046 - val_out_3_loss: 0.6991 - val_out_4_loss: 0.7025 - val_out_5_loss: 0.7029 - val_out_6_loss: 0.7049 - val_out_7_loss: 0.7038 - val_out_8_loss: 0.7023 - val_out_9_loss: 0.7021 - val_out_10_loss: 0.7057 - val_out_11_loss: 0.7062 - val_out_12_loss: 0.7023 - val_out_13_loss: 0.7045 - val_out_14_loss: 0.7024 - val_out_15_loss: 0.7043 - val_out_16_loss: 0.7052 - val_out_17_loss: 0.7013 - val_out_18_loss: 0.7018 - val_out_19_loss: 0.7040 - val_out_20_loss: 0.7073 - val_out_21_loss: 0.7003 - val_out_22_loss: 0.7019 - val_out_23_loss: 0.7019 - val_out_24_loss: 0.7041 - val_out_25_loss: 0.7083 - val_out_26_loss: 0.7064 - val_out_27_loss: 0.7036 - val_out_28_loss: 0.7029 - val_out_29_loss: 0.7026 - val_out_30_loss: 0.7030 - val_out_31_loss: 0.7030 - val_out_32_loss: 0.6980 - val_out_acc: 0.7354 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7332 - val_out_2_acc: 0.7310 - val_out_3_acc: 0.7289 - val_out_4_acc: 0.7332 - val_out_5_acc: 0.7310 - val_out_6_acc: 0.7354 - val_out_7_acc: 0.7332 - val_out_8_acc: 0.7310 - val_out_9_acc: 0.7267 - val_out_10_acc: 0.7354 - val_out_11_acc: 0.7267 - val_out_12_acc: 0.7332 - val_out_13_acc: 0.7310 - val_out_14_acc: 0.7332 - val_out_15_acc: 0.7310 - val_out_16_acc: 0.7267 - val_out_17_acc: 0.7354 - val_out_18_acc: 0.7375 - val_out_19_acc: 0.7310 - val_out_20_acc: 0.7267 - val_out_21_acc: 0.7310 - val_out_22_acc: 0.7267 - val_out_23_acc: 0.7267 - val_out_24_acc: 0.7332 - val_out_25_acc: 0.7310 - val_out_26_acc: 0.7397 - val_out_27_acc: 0.7245 - val_out_28_acc: 0.7289 - val_out_29_acc: 0.7289 - val_out_30_acc: 0.7289 - val_out_31_acc: 0.7289 - val_out_32_acc: 0.7310\n",
      "Epoch 126/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 25.7563 - out_loss: 0.6699 - out_0_loss: 0.4981 - out_1_loss: 0.7711 - out_2_loss: 0.7850 - out_3_loss: 0.7716 - out_4_loss: 0.7861 - out_5_loss: 0.7602 - out_6_loss: 0.7729 - out_7_loss: 0.7599 - out_8_loss: 0.7810 - out_9_loss: 0.7645 - out_10_loss: 0.7725 - out_11_loss: 0.7598 - out_12_loss: 0.7684 - out_13_loss: 0.7673 - out_14_loss: 0.7528 - out_15_loss: 0.7468 - out_16_loss: 0.7717 - out_17_loss: 0.7662 - out_18_loss: 0.7724 - out_19_loss: 0.7635 - out_20_loss: 0.7722 - out_21_loss: 0.7572 - out_22_loss: 0.7561 - out_23_loss: 0.7644 - out_24_loss: 0.7667 - out_25_loss: 0.7870 - out_26_loss: 0.7665 - out_27_loss: 0.7770 - out_28_loss: 0.7673 - out_29_loss: 0.7858 - out_30_loss: 0.7675 - out_31_loss: 0.7624 - out_32_loss: 0.7647 - out_acc: 0.7666 - out_0_acc: 0.8292 - out_1_acc: 0.7322 - out_2_acc: 0.7192 - out_3_acc: 0.7216 - out_4_acc: 0.7244 - out_5_acc: 0.7300 - out_6_acc: 0.7257 - out_7_acc: 0.7309 - out_8_acc: 0.7250 - out_9_acc: 0.7337 - out_10_acc: 0.7260 - out_11_acc: 0.7288 - out_12_acc: 0.7291 - out_13_acc: 0.7247 - out_14_acc: 0.7250 - out_15_acc: 0.7353 - out_16_acc: 0.7179 - out_17_acc: 0.7238 - out_18_acc: 0.7223 - out_19_acc: 0.7229 - out_20_acc: 0.7201 - out_21_acc: 0.7350 - out_22_acc: 0.7331 - out_23_acc: 0.7272 - out_24_acc: 0.7340 - out_25_acc: 0.7250 - out_26_acc: 0.7272 - out_27_acc: 0.7241 - out_28_acc: 0.7241 - out_29_acc: 0.7219 - out_30_acc: 0.7219 - out_31_acc: 0.7266 - out_32_acc: 0.7281 - val_loss: 23.4120 - val_out_loss: 0.6429 - val_out_0_loss: 0.4105 - val_out_1_loss: 0.6679 - val_out_2_loss: 0.6678 - val_out_3_loss: 0.6675 - val_out_4_loss: 0.6718 - val_out_5_loss: 0.6683 - val_out_6_loss: 0.6713 - val_out_7_loss: 0.6702 - val_out_8_loss: 0.6696 - val_out_9_loss: 0.6705 - val_out_10_loss: 0.6697 - val_out_11_loss: 0.6737 - val_out_12_loss: 0.6717 - val_out_13_loss: 0.6700 - val_out_14_loss: 0.6706 - val_out_15_loss: 0.6711 - val_out_16_loss: 0.6674 - val_out_17_loss: 0.6673 - val_out_18_loss: 0.6666 - val_out_19_loss: 0.6707 - val_out_20_loss: 0.6709 - val_out_21_loss: 0.6704 - val_out_22_loss: 0.6679 - val_out_23_loss: 0.6682 - val_out_24_loss: 0.6682 - val_out_25_loss: 0.6733 - val_out_26_loss: 0.6687 - val_out_27_loss: 0.6726 - val_out_28_loss: 0.6670 - val_out_29_loss: 0.6722 - val_out_30_loss: 0.6691 - val_out_31_loss: 0.6715 - val_out_32_loss: 0.6679 - val_out_acc: 0.7570 - val_out_0_acc: 0.8547 - val_out_1_acc: 0.7462 - val_out_2_acc: 0.7505 - val_out_3_acc: 0.7440 - val_out_4_acc: 0.7484 - val_out_5_acc: 0.7505 - val_out_6_acc: 0.7419 - val_out_7_acc: 0.7505 - val_out_8_acc: 0.7505 - val_out_9_acc: 0.7440 - val_out_10_acc: 0.7484 - val_out_11_acc: 0.7505 - val_out_12_acc: 0.7484 - val_out_13_acc: 0.7505 - val_out_14_acc: 0.7527 - val_out_15_acc: 0.7527 - val_out_16_acc: 0.7440 - val_out_17_acc: 0.7505 - val_out_18_acc: 0.7505 - val_out_19_acc: 0.7462 - val_out_20_acc: 0.7397 - val_out_21_acc: 0.7462 - val_out_22_acc: 0.7440 - val_out_23_acc: 0.7484 - val_out_24_acc: 0.7462 - val_out_25_acc: 0.7375 - val_out_26_acc: 0.7462 - val_out_27_acc: 0.7440 - val_out_28_acc: 0.7440 - val_out_29_acc: 0.7462 - val_out_30_acc: 0.7440 - val_out_31_acc: 0.7419 - val_out_32_acc: 0.7527\n",
      "Epoch 127/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 25.9700 - out_loss: 0.6817 - out_0_loss: 0.4615 - out_1_loss: 0.7770 - out_2_loss: 0.7663 - out_3_loss: 0.7842 - out_4_loss: 0.7880 - out_5_loss: 0.7822 - out_6_loss: 0.7830 - out_7_loss: 0.7657 - out_8_loss: 0.7905 - out_9_loss: 0.7764 - out_10_loss: 0.7780 - out_11_loss: 0.7732 - out_12_loss: 0.7659 - out_13_loss: 0.7929 - out_14_loss: 0.7694 - out_15_loss: 0.7761 - out_16_loss: 0.7643 - out_17_loss: 0.7778 - out_18_loss: 0.7727 - out_19_loss: 0.7754 - out_20_loss: 0.7824 - out_21_loss: 0.7568 - out_22_loss: 0.7765 - out_23_loss: 0.7805 - out_24_loss: 0.7727 - out_25_loss: 0.7685 - out_26_loss: 0.7897 - out_27_loss: 0.7879 - out_28_loss: 0.7683 - out_29_loss: 0.7685 - out_30_loss: 0.7704 - out_31_loss: 0.7769 - out_32_loss: 0.7683 - out_acc: 0.7514 - out_0_acc: 0.8338 - out_1_acc: 0.7201 - out_2_acc: 0.7213 - out_3_acc: 0.7145 - out_4_acc: 0.7139 - out_5_acc: 0.7136 - out_6_acc: 0.7232 - out_7_acc: 0.7192 - out_8_acc: 0.7195 - out_9_acc: 0.7263 - out_10_acc: 0.7151 - out_11_acc: 0.7161 - out_12_acc: 0.7148 - out_13_acc: 0.7114 - out_14_acc: 0.7250 - out_15_acc: 0.7120 - out_16_acc: 0.7250 - out_17_acc: 0.7167 - out_18_acc: 0.7154 - out_19_acc: 0.7111 - out_20_acc: 0.7133 - out_21_acc: 0.7219 - out_22_acc: 0.7185 - out_23_acc: 0.7164 - out_24_acc: 0.7195 - out_25_acc: 0.7148 - out_26_acc: 0.7095 - out_27_acc: 0.7142 - out_28_acc: 0.7235 - out_29_acc: 0.7192 - out_30_acc: 0.7238 - out_31_acc: 0.7142 - out_32_acc: 0.7219 - val_loss: 23.6194 - val_out_loss: 0.6529 - val_out_0_loss: 0.4314 - val_out_1_loss: 0.6765 - val_out_2_loss: 0.6744 - val_out_3_loss: 0.6719 - val_out_4_loss: 0.6775 - val_out_5_loss: 0.6737 - val_out_6_loss: 0.6736 - val_out_7_loss: 0.6760 - val_out_8_loss: 0.6724 - val_out_9_loss: 0.6778 - val_out_10_loss: 0.6759 - val_out_11_loss: 0.6791 - val_out_12_loss: 0.6719 - val_out_13_loss: 0.6704 - val_out_14_loss: 0.6780 - val_out_15_loss: 0.6773 - val_out_16_loss: 0.6698 - val_out_17_loss: 0.6729 - val_out_18_loss: 0.6724 - val_out_19_loss: 0.6781 - val_out_20_loss: 0.6827 - val_out_21_loss: 0.6748 - val_out_22_loss: 0.6775 - val_out_23_loss: 0.6743 - val_out_24_loss: 0.6757 - val_out_25_loss: 0.6786 - val_out_26_loss: 0.6730 - val_out_27_loss: 0.6720 - val_out_28_loss: 0.6769 - val_out_29_loss: 0.6707 - val_out_30_loss: 0.6764 - val_out_31_loss: 0.6754 - val_out_32_loss: 0.6723 - val_out_acc: 0.7570 - val_out_0_acc: 0.8330 - val_out_1_acc: 0.7570 - val_out_2_acc: 0.7570 - val_out_3_acc: 0.7484 - val_out_4_acc: 0.7484 - val_out_5_acc: 0.7549 - val_out_6_acc: 0.7549 - val_out_7_acc: 0.7549 - val_out_8_acc: 0.7549 - val_out_9_acc: 0.7484 - val_out_10_acc: 0.7570 - val_out_11_acc: 0.7527 - val_out_12_acc: 0.7570 - val_out_13_acc: 0.7570 - val_out_14_acc: 0.7570 - val_out_15_acc: 0.7570 - val_out_16_acc: 0.7549 - val_out_17_acc: 0.7614 - val_out_18_acc: 0.7527 - val_out_19_acc: 0.7440 - val_out_20_acc: 0.7549 - val_out_21_acc: 0.7505 - val_out_22_acc: 0.7549 - val_out_23_acc: 0.7505 - val_out_24_acc: 0.7505 - val_out_25_acc: 0.7505 - val_out_26_acc: 0.7549 - val_out_27_acc: 0.7570 - val_out_28_acc: 0.7549 - val_out_29_acc: 0.7592 - val_out_30_acc: 0.7527 - val_out_31_acc: 0.7549 - val_out_32_acc: 0.7570\n",
      "Epoch 128/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 25.6082 - out_loss: 0.6666 - out_0_loss: 0.4756 - out_1_loss: 0.7733 - out_2_loss: 0.7508 - out_3_loss: 0.7724 - out_4_loss: 0.7766 - out_5_loss: 0.7590 - out_6_loss: 0.7707 - out_7_loss: 0.7687 - out_8_loss: 0.7647 - out_9_loss: 0.7634 - out_10_loss: 0.7714 - out_11_loss: 0.7699 - out_12_loss: 0.7610 - out_13_loss: 0.7720 - out_14_loss: 0.7565 - out_15_loss: 0.7711 - out_16_loss: 0.7639 - out_17_loss: 0.7499 - out_18_loss: 0.7626 - out_19_loss: 0.7505 - out_20_loss: 0.7656 - out_21_loss: 0.7667 - out_22_loss: 0.7609 - out_23_loss: 0.7733 - out_24_loss: 0.7582 - out_25_loss: 0.7630 - out_26_loss: 0.7674 - out_27_loss: 0.7636 - out_28_loss: 0.7675 - out_29_loss: 0.7710 - out_30_loss: 0.7654 - out_31_loss: 0.7529 - out_32_loss: 0.7624 - out_acc: 0.7635 - out_0_acc: 0.8345 - out_1_acc: 0.7266 - out_2_acc: 0.7275 - out_3_acc: 0.7192 - out_4_acc: 0.7241 - out_5_acc: 0.7188 - out_6_acc: 0.7216 - out_7_acc: 0.7247 - out_8_acc: 0.7294 - out_9_acc: 0.7244 - out_10_acc: 0.7337 - out_11_acc: 0.7281 - out_12_acc: 0.7325 - out_13_acc: 0.7257 - out_14_acc: 0.7337 - out_15_acc: 0.7216 - out_16_acc: 0.7250 - out_17_acc: 0.7266 - out_18_acc: 0.7297 - out_19_acc: 0.7275 - out_20_acc: 0.7238 - out_21_acc: 0.7278 - out_22_acc: 0.7281 - out_23_acc: 0.7241 - out_24_acc: 0.7362 - out_25_acc: 0.7281 - out_26_acc: 0.7244 - out_27_acc: 0.7257 - out_28_acc: 0.7263 - out_29_acc: 0.7272 - out_30_acc: 0.7322 - out_31_acc: 0.7368 - out_32_acc: 0.7294 - val_loss: 27.3539 - val_out_loss: 0.7442 - val_out_0_loss: 0.5017 - val_out_1_loss: 0.7800 - val_out_2_loss: 0.7800 - val_out_3_loss: 0.7847 - val_out_4_loss: 0.7859 - val_out_5_loss: 0.7844 - val_out_6_loss: 0.7856 - val_out_7_loss: 0.7841 - val_out_8_loss: 0.7801 - val_out_9_loss: 0.7843 - val_out_10_loss: 0.7785 - val_out_11_loss: 0.7807 - val_out_12_loss: 0.7861 - val_out_13_loss: 0.7781 - val_out_14_loss: 0.7890 - val_out_15_loss: 0.7810 - val_out_16_loss: 0.7793 - val_out_17_loss: 0.7783 - val_out_18_loss: 0.7811 - val_out_19_loss: 0.7875 - val_out_20_loss: 0.7809 - val_out_21_loss: 0.7841 - val_out_22_loss: 0.7793 - val_out_23_loss: 0.7865 - val_out_24_loss: 0.7754 - val_out_25_loss: 0.7881 - val_out_26_loss: 0.7723 - val_out_27_loss: 0.7878 - val_out_28_loss: 0.7839 - val_out_29_loss: 0.7790 - val_out_30_loss: 0.7846 - val_out_31_loss: 0.7817 - val_out_32_loss: 0.7730 - val_out_acc: 0.7202 - val_out_0_acc: 0.8178 - val_out_1_acc: 0.7223 - val_out_2_acc: 0.7115 - val_out_3_acc: 0.7223 - val_out_4_acc: 0.7180 - val_out_5_acc: 0.7158 - val_out_6_acc: 0.7223 - val_out_7_acc: 0.7072 - val_out_8_acc: 0.7158 - val_out_9_acc: 0.7115 - val_out_10_acc: 0.7158 - val_out_11_acc: 0.7180 - val_out_12_acc: 0.7115 - val_out_13_acc: 0.7180 - val_out_14_acc: 0.7115 - val_out_15_acc: 0.7137 - val_out_16_acc: 0.7093 - val_out_17_acc: 0.7158 - val_out_18_acc: 0.7158 - val_out_19_acc: 0.7115 - val_out_20_acc: 0.7137 - val_out_21_acc: 0.7158 - val_out_22_acc: 0.7202 - val_out_23_acc: 0.7180 - val_out_24_acc: 0.7115 - val_out_25_acc: 0.7093 - val_out_26_acc: 0.7137 - val_out_27_acc: 0.7180 - val_out_28_acc: 0.7137 - val_out_29_acc: 0.7158 - val_out_30_acc: 0.7137 - val_out_31_acc: 0.7137 - val_out_32_acc: 0.7180\n",
      "Epoch 129/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 25.5715 - out_loss: 0.6686 - out_0_loss: 0.4735 - out_1_loss: 0.7613 - out_2_loss: 0.7853 - out_3_loss: 0.7829 - out_4_loss: 0.7611 - out_5_loss: 0.7706 - out_6_loss: 0.7625 - out_7_loss: 0.7607 - out_8_loss: 0.7558 - out_9_loss: 0.7750 - out_10_loss: 0.7633 - out_11_loss: 0.7623 - out_12_loss: 0.7640 - out_13_loss: 0.7498 - out_14_loss: 0.7555 - out_15_loss: 0.7617 - out_16_loss: 0.7572 - out_17_loss: 0.7582 - out_18_loss: 0.7516 - out_19_loss: 0.7646 - out_20_loss: 0.7793 - out_21_loss: 0.7578 - out_22_loss: 0.7618 - out_23_loss: 0.7497 - out_24_loss: 0.7728 - out_25_loss: 0.7524 - out_26_loss: 0.7662 - out_27_loss: 0.7577 - out_28_loss: 0.7735 - out_29_loss: 0.7587 - out_30_loss: 0.7713 - out_31_loss: 0.7605 - out_32_loss: 0.7645 - out_acc: 0.7678 - out_0_acc: 0.8338 - out_1_acc: 0.7272 - out_2_acc: 0.7213 - out_3_acc: 0.7232 - out_4_acc: 0.7390 - out_5_acc: 0.7260 - out_6_acc: 0.7254 - out_7_acc: 0.7281 - out_8_acc: 0.7297 - out_9_acc: 0.7275 - out_10_acc: 0.7204 - out_11_acc: 0.7325 - out_12_acc: 0.7266 - out_13_acc: 0.7337 - out_14_acc: 0.7325 - out_15_acc: 0.7238 - out_16_acc: 0.7378 - out_17_acc: 0.7368 - out_18_acc: 0.7316 - out_19_acc: 0.7300 - out_20_acc: 0.7195 - out_21_acc: 0.7331 - out_22_acc: 0.7322 - out_23_acc: 0.7306 - out_24_acc: 0.7226 - out_25_acc: 0.7322 - out_26_acc: 0.7266 - out_27_acc: 0.7409 - out_28_acc: 0.7235 - out_29_acc: 0.7288 - out_30_acc: 0.7303 - out_31_acc: 0.7297 - out_32_acc: 0.7257 - val_loss: 22.7752 - val_out_loss: 0.6263 - val_out_0_loss: 0.4597 - val_out_1_loss: 0.6512 - val_out_2_loss: 0.6485 - val_out_3_loss: 0.6430 - val_out_4_loss: 0.6471 - val_out_5_loss: 0.6476 - val_out_6_loss: 0.6516 - val_out_7_loss: 0.6493 - val_out_8_loss: 0.6474 - val_out_9_loss: 0.6514 - val_out_10_loss: 0.6529 - val_out_11_loss: 0.6501 - val_out_12_loss: 0.6445 - val_out_13_loss: 0.6492 - val_out_14_loss: 0.6525 - val_out_15_loss: 0.6528 - val_out_16_loss: 0.6469 - val_out_17_loss: 0.6514 - val_out_18_loss: 0.6489 - val_out_19_loss: 0.6558 - val_out_20_loss: 0.6530 - val_out_21_loss: 0.6450 - val_out_22_loss: 0.6423 - val_out_23_loss: 0.6487 - val_out_24_loss: 0.6519 - val_out_25_loss: 0.6487 - val_out_26_loss: 0.6529 - val_out_27_loss: 0.6498 - val_out_28_loss: 0.6514 - val_out_29_loss: 0.6445 - val_out_30_loss: 0.6557 - val_out_31_loss: 0.6547 - val_out_32_loss: 0.6470 - val_out_acc: 0.7831 - val_out_0_acc: 0.8373 - val_out_1_acc: 0.7787 - val_out_2_acc: 0.7874 - val_out_3_acc: 0.7831 - val_out_4_acc: 0.7831 - val_out_5_acc: 0.7852 - val_out_6_acc: 0.7787 - val_out_7_acc: 0.7852 - val_out_8_acc: 0.7744 - val_out_9_acc: 0.7852 - val_out_10_acc: 0.7766 - val_out_11_acc: 0.7852 - val_out_12_acc: 0.7787 - val_out_13_acc: 0.7852 - val_out_14_acc: 0.7809 - val_out_15_acc: 0.7787 - val_out_16_acc: 0.7831 - val_out_17_acc: 0.7831 - val_out_18_acc: 0.7744 - val_out_19_acc: 0.7766 - val_out_20_acc: 0.7852 - val_out_21_acc: 0.7896 - val_out_22_acc: 0.7874 - val_out_23_acc: 0.7809 - val_out_24_acc: 0.7831 - val_out_25_acc: 0.7831 - val_out_26_acc: 0.7787 - val_out_27_acc: 0.7787 - val_out_28_acc: 0.7744 - val_out_29_acc: 0.7874 - val_out_30_acc: 0.7852 - val_out_31_acc: 0.7809 - val_out_32_acc: 0.7896\n",
      "Epoch 130/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 25.6666 - out_loss: 0.6699 - out_0_loss: 0.4458 - out_1_loss: 0.7712 - out_2_loss: 0.7604 - out_3_loss: 0.7705 - out_4_loss: 0.7600 - out_5_loss: 0.7699 - out_6_loss: 0.7702 - out_7_loss: 0.7558 - out_8_loss: 0.7700 - out_9_loss: 0.7617 - out_10_loss: 0.7746 - out_11_loss: 0.7791 - out_12_loss: 0.7729 - out_13_loss: 0.7627 - out_14_loss: 0.7749 - out_15_loss: 0.7553 - out_16_loss: 0.7674 - out_17_loss: 0.7794 - out_18_loss: 0.7773 - out_19_loss: 0.7627 - out_20_loss: 0.7583 - out_21_loss: 0.7815 - out_22_loss: 0.7562 - out_23_loss: 0.7522 - out_24_loss: 0.7681 - out_25_loss: 0.7588 - out_26_loss: 0.7780 - out_27_loss: 0.7711 - out_28_loss: 0.7583 - out_29_loss: 0.7690 - out_30_loss: 0.7774 - out_31_loss: 0.7650 - out_32_loss: 0.7610 - out_acc: 0.7570 - out_0_acc: 0.8385 - out_1_acc: 0.7195 - out_2_acc: 0.7257 - out_3_acc: 0.7198 - out_4_acc: 0.7319 - out_5_acc: 0.7170 - out_6_acc: 0.7219 - out_7_acc: 0.7219 - out_8_acc: 0.7238 - out_9_acc: 0.7219 - out_10_acc: 0.7241 - out_11_acc: 0.7195 - out_12_acc: 0.7185 - out_13_acc: 0.7297 - out_14_acc: 0.7238 - out_15_acc: 0.7272 - out_16_acc: 0.7201 - out_17_acc: 0.7254 - out_18_acc: 0.7161 - out_19_acc: 0.7229 - out_20_acc: 0.7244 - out_21_acc: 0.7185 - out_22_acc: 0.7207 - out_23_acc: 0.7309 - out_24_acc: 0.7238 - out_25_acc: 0.7232 - out_26_acc: 0.7254 - out_27_acc: 0.7204 - out_28_acc: 0.7350 - out_29_acc: 0.7201 - out_30_acc: 0.7198 - out_31_acc: 0.7198 - out_32_acc: 0.7238 - val_loss: 25.5254 - val_out_loss: 0.7046 - val_out_0_loss: 0.5044 - val_out_1_loss: 0.7252 - val_out_2_loss: 0.7287 - val_out_3_loss: 0.7285 - val_out_4_loss: 0.7328 - val_out_5_loss: 0.7275 - val_out_6_loss: 0.7291 - val_out_7_loss: 0.7278 - val_out_8_loss: 0.7251 - val_out_9_loss: 0.7276 - val_out_10_loss: 0.7294 - val_out_11_loss: 0.7299 - val_out_12_loss: 0.7232 - val_out_13_loss: 0.7251 - val_out_14_loss: 0.7335 - val_out_15_loss: 0.7270 - val_out_16_loss: 0.7268 - val_out_17_loss: 0.7281 - val_out_18_loss: 0.7279 - val_out_19_loss: 0.7300 - val_out_20_loss: 0.7318 - val_out_21_loss: 0.7266 - val_out_22_loss: 0.7320 - val_out_23_loss: 0.7269 - val_out_24_loss: 0.7308 - val_out_25_loss: 0.7305 - val_out_26_loss: 0.7282 - val_out_27_loss: 0.7312 - val_out_28_loss: 0.7258 - val_out_29_loss: 0.7281 - val_out_30_loss: 0.7285 - val_out_31_loss: 0.7268 - val_out_32_loss: 0.7256 - val_out_acc: 0.7484 - val_out_0_acc: 0.8026 - val_out_1_acc: 0.7332 - val_out_2_acc: 0.7375 - val_out_3_acc: 0.7462 - val_out_4_acc: 0.7375 - val_out_5_acc: 0.7332 - val_out_6_acc: 0.7310 - val_out_7_acc: 0.7397 - val_out_8_acc: 0.7419 - val_out_9_acc: 0.7354 - val_out_10_acc: 0.7419 - val_out_11_acc: 0.7310 - val_out_12_acc: 0.7332 - val_out_13_acc: 0.7397 - val_out_14_acc: 0.7354 - val_out_15_acc: 0.7419 - val_out_16_acc: 0.7397 - val_out_17_acc: 0.7484 - val_out_18_acc: 0.7354 - val_out_19_acc: 0.7397 - val_out_20_acc: 0.7397 - val_out_21_acc: 0.7419 - val_out_22_acc: 0.7354 - val_out_23_acc: 0.7375 - val_out_24_acc: 0.7354 - val_out_25_acc: 0.7397 - val_out_26_acc: 0.7332 - val_out_27_acc: 0.7310 - val_out_28_acc: 0.7419 - val_out_29_acc: 0.7354 - val_out_30_acc: 0.7419 - val_out_31_acc: 0.7440 - val_out_32_acc: 0.7354\n",
      "Epoch 131/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 104s - loss: 24.8097 - out_loss: 0.6469 - out_0_loss: 0.4388 - out_1_loss: 0.7319 - out_2_loss: 0.7417 - out_3_loss: 0.7470 - out_4_loss: 0.7516 - out_5_loss: 0.7319 - out_6_loss: 0.7331 - out_7_loss: 0.7525 - out_8_loss: 0.7284 - out_9_loss: 0.7423 - out_10_loss: 0.7414 - out_11_loss: 0.7427 - out_12_loss: 0.7475 - out_13_loss: 0.7372 - out_14_loss: 0.7493 - out_15_loss: 0.7458 - out_16_loss: 0.7463 - out_17_loss: 0.7338 - out_18_loss: 0.7450 - out_19_loss: 0.7444 - out_20_loss: 0.7369 - out_21_loss: 0.7358 - out_22_loss: 0.7408 - out_23_loss: 0.7380 - out_24_loss: 0.7490 - out_25_loss: 0.7369 - out_26_loss: 0.7534 - out_27_loss: 0.7516 - out_28_loss: 0.7468 - out_29_loss: 0.7282 - out_30_loss: 0.7324 - out_31_loss: 0.7444 - out_32_loss: 0.7359 - out_acc: 0.7691 - out_0_acc: 0.8366 - out_1_acc: 0.7337 - out_2_acc: 0.7316 - out_3_acc: 0.7353 - out_4_acc: 0.7297 - out_5_acc: 0.7334 - out_6_acc: 0.7331 - out_7_acc: 0.7306 - out_8_acc: 0.7368 - out_9_acc: 0.7328 - out_10_acc: 0.7291 - out_11_acc: 0.7378 - out_12_acc: 0.7238 - out_13_acc: 0.7381 - out_14_acc: 0.7337 - out_15_acc: 0.7297 - out_16_acc: 0.7303 - out_17_acc: 0.7415 - out_18_acc: 0.7331 - out_19_acc: 0.7347 - out_20_acc: 0.7312 - out_21_acc: 0.7331 - out_22_acc: 0.7362 - out_23_acc: 0.7328 - out_24_acc: 0.7350 - out_25_acc: 0.7365 - out_26_acc: 0.7328 - out_27_acc: 0.7303 - out_28_acc: 0.7306 - out_29_acc: 0.7362 - out_30_acc: 0.7412 - out_31_acc: 0.7275 - out_32_acc: 0.7281 - val_loss: 22.5931 - val_out_loss: 0.6189 - val_out_0_loss: 0.4417 - val_out_1_loss: 0.6486 - val_out_2_loss: 0.6439 - val_out_3_loss: 0.6427 - val_out_4_loss: 0.6488 - val_out_5_loss: 0.6429 - val_out_6_loss: 0.6433 - val_out_7_loss: 0.6459 - val_out_8_loss: 0.6458 - val_out_9_loss: 0.6458 - val_out_10_loss: 0.6457 - val_out_11_loss: 0.6420 - val_out_12_loss: 0.6440 - val_out_13_loss: 0.6483 - val_out_14_loss: 0.6476 - val_out_15_loss: 0.6460 - val_out_16_loss: 0.6465 - val_out_17_loss: 0.6437 - val_out_18_loss: 0.6428 - val_out_19_loss: 0.6482 - val_out_20_loss: 0.6461 - val_out_21_loss: 0.6455 - val_out_22_loss: 0.6455 - val_out_23_loss: 0.6446 - val_out_24_loss: 0.6438 - val_out_25_loss: 0.6459 - val_out_26_loss: 0.6422 - val_out_27_loss: 0.6446 - val_out_28_loss: 0.6449 - val_out_29_loss: 0.6435 - val_out_30_loss: 0.6464 - val_out_31_loss: 0.6433 - val_out_32_loss: 0.6395 - val_out_acc: 0.7939 - val_out_0_acc: 0.8568 - val_out_1_acc: 0.7831 - val_out_2_acc: 0.7831 - val_out_3_acc: 0.7766 - val_out_4_acc: 0.7831 - val_out_5_acc: 0.7787 - val_out_6_acc: 0.7831 - val_out_7_acc: 0.7874 - val_out_8_acc: 0.7852 - val_out_9_acc: 0.7896 - val_out_10_acc: 0.7809 - val_out_11_acc: 0.7852 - val_out_12_acc: 0.7896 - val_out_13_acc: 0.7831 - val_out_14_acc: 0.7896 - val_out_15_acc: 0.7809 - val_out_16_acc: 0.7874 - val_out_17_acc: 0.7831 - val_out_18_acc: 0.7831 - val_out_19_acc: 0.7961 - val_out_20_acc: 0.7874 - val_out_21_acc: 0.7896 - val_out_22_acc: 0.7896 - val_out_23_acc: 0.7939 - val_out_24_acc: 0.7896 - val_out_25_acc: 0.7874 - val_out_26_acc: 0.7874 - val_out_27_acc: 0.7831 - val_out_28_acc: 0.7874 - val_out_29_acc: 0.7874 - val_out_30_acc: 0.7831 - val_out_31_acc: 0.7809 - val_out_32_acc: 0.7939\n",
      "Epoch 132/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.8056 - out_loss: 0.6433 - out_0_loss: 0.4346 - out_1_loss: 0.7399 - out_2_loss: 0.7477 - out_3_loss: 0.7461 - out_4_loss: 0.7229 - out_5_loss: 0.7338 - out_6_loss: 0.7433 - out_7_loss: 0.7309 - out_8_loss: 0.7423 - out_9_loss: 0.7505 - out_10_loss: 0.7330 - out_11_loss: 0.7548 - out_12_loss: 0.7320 - out_13_loss: 0.7397 - out_14_loss: 0.7283 - out_15_loss: 0.7530 - out_16_loss: 0.7427 - out_17_loss: 0.7370 - out_18_loss: 0.7480 - out_19_loss: 0.7487 - out_20_loss: 0.7378 - out_21_loss: 0.7584 - out_22_loss: 0.7488 - out_23_loss: 0.7352 - out_24_loss: 0.7378 - out_25_loss: 0.7503 - out_26_loss: 0.7374 - out_27_loss: 0.7377 - out_28_loss: 0.7408 - out_29_loss: 0.7434 - out_30_loss: 0.7455 - out_31_loss: 0.7438 - out_32_loss: 0.7365 - out_acc: 0.7728 - out_0_acc: 0.8521 - out_1_acc: 0.7306 - out_2_acc: 0.7322 - out_3_acc: 0.7278 - out_4_acc: 0.7300 - out_5_acc: 0.7381 - out_6_acc: 0.7381 - out_7_acc: 0.7374 - out_8_acc: 0.7340 - out_9_acc: 0.7316 - out_10_acc: 0.7365 - out_11_acc: 0.7288 - out_12_acc: 0.7433 - out_13_acc: 0.7356 - out_14_acc: 0.7340 - out_15_acc: 0.7325 - out_16_acc: 0.7378 - out_17_acc: 0.7402 - out_18_acc: 0.7405 - out_19_acc: 0.7371 - out_20_acc: 0.7275 - out_21_acc: 0.7281 - out_22_acc: 0.7300 - out_23_acc: 0.7340 - out_24_acc: 0.7390 - out_25_acc: 0.7312 - out_26_acc: 0.7362 - out_27_acc: 0.7409 - out_28_acc: 0.7405 - out_29_acc: 0.7347 - out_30_acc: 0.7384 - out_31_acc: 0.7362 - out_32_acc: 0.7297 - val_loss: 26.8867 - val_out_loss: 0.7346 - val_out_0_loss: 0.5658 - val_out_1_loss: 0.7661 - val_out_2_loss: 0.7632 - val_out_3_loss: 0.7657 - val_out_4_loss: 0.7666 - val_out_5_loss: 0.7651 - val_out_6_loss: 0.7662 - val_out_7_loss: 0.7636 - val_out_8_loss: 0.7680 - val_out_9_loss: 0.7640 - val_out_10_loss: 0.7696 - val_out_11_loss: 0.7670 - val_out_12_loss: 0.7626 - val_out_13_loss: 0.7634 - val_out_14_loss: 0.7667 - val_out_15_loss: 0.7725 - val_out_16_loss: 0.7689 - val_out_17_loss: 0.7692 - val_out_18_loss: 0.7612 - val_out_19_loss: 0.7726 - val_out_20_loss: 0.7712 - val_out_21_loss: 0.7658 - val_out_22_loss: 0.7675 - val_out_23_loss: 0.7641 - val_out_24_loss: 0.7705 - val_out_25_loss: 0.7654 - val_out_26_loss: 0.7640 - val_out_27_loss: 0.7709 - val_out_28_loss: 0.7612 - val_out_29_loss: 0.7618 - val_out_30_loss: 0.7688 - val_out_31_loss: 0.7680 - val_out_32_loss: 0.7606 - val_out_acc: 0.7310 - val_out_0_acc: 0.7852 - val_out_1_acc: 0.7267 - val_out_2_acc: 0.7267 - val_out_3_acc: 0.7202 - val_out_4_acc: 0.7245 - val_out_5_acc: 0.7223 - val_out_6_acc: 0.7245 - val_out_7_acc: 0.7310 - val_out_8_acc: 0.7245 - val_out_9_acc: 0.7158 - val_out_10_acc: 0.7202 - val_out_11_acc: 0.7223 - val_out_12_acc: 0.7158 - val_out_13_acc: 0.7223 - val_out_14_acc: 0.7202 - val_out_15_acc: 0.7245 - val_out_16_acc: 0.7180 - val_out_17_acc: 0.7180 - val_out_18_acc: 0.7202 - val_out_19_acc: 0.7202 - val_out_20_acc: 0.7267 - val_out_21_acc: 0.7158 - val_out_22_acc: 0.7245 - val_out_23_acc: 0.7180 - val_out_24_acc: 0.7245 - val_out_25_acc: 0.7223 - val_out_26_acc: 0.7202 - val_out_27_acc: 0.7245 - val_out_28_acc: 0.7289 - val_out_29_acc: 0.7245 - val_out_30_acc: 0.7202 - val_out_31_acc: 0.7267 - val_out_32_acc: 0.7332\n",
      "Epoch 133/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.7138 - out_loss: 0.6426 - out_0_loss: 0.4507 - out_1_loss: 0.7214 - out_2_loss: 0.7304 - out_3_loss: 0.7430 - out_4_loss: 0.7516 - out_5_loss: 0.7336 - out_6_loss: 0.7453 - out_7_loss: 0.7410 - out_8_loss: 0.7342 - out_9_loss: 0.7411 - out_10_loss: 0.7363 - out_11_loss: 0.7306 - out_12_loss: 0.7262 - out_13_loss: 0.7319 - out_14_loss: 0.7351 - out_15_loss: 0.7418 - out_16_loss: 0.7499 - out_17_loss: 0.7343 - out_18_loss: 0.7371 - out_19_loss: 0.7548 - out_20_loss: 0.7440 - out_21_loss: 0.7386 - out_22_loss: 0.7314 - out_23_loss: 0.7383 - out_24_loss: 0.7483 - out_25_loss: 0.7540 - out_26_loss: 0.7248 - out_27_loss: 0.7509 - out_28_loss: 0.7360 - out_29_loss: 0.7315 - out_30_loss: 0.7308 - out_31_loss: 0.7337 - out_32_loss: 0.7386 - out_acc: 0.7768 - out_0_acc: 0.8397 - out_1_acc: 0.7368 - out_2_acc: 0.7365 - out_3_acc: 0.7229 - out_4_acc: 0.7353 - out_5_acc: 0.7368 - out_6_acc: 0.7381 - out_7_acc: 0.7368 - out_8_acc: 0.7300 - out_9_acc: 0.7368 - out_10_acc: 0.7381 - out_11_acc: 0.7316 - out_12_acc: 0.7405 - out_13_acc: 0.7374 - out_14_acc: 0.7347 - out_15_acc: 0.7359 - out_16_acc: 0.7269 - out_17_acc: 0.7381 - out_18_acc: 0.7362 - out_19_acc: 0.7343 - out_20_acc: 0.7405 - out_21_acc: 0.7272 - out_22_acc: 0.7371 - out_23_acc: 0.7412 - out_24_acc: 0.7412 - out_25_acc: 0.7350 - out_26_acc: 0.7436 - out_27_acc: 0.7343 - out_28_acc: 0.7421 - out_29_acc: 0.7387 - out_30_acc: 0.7405 - out_31_acc: 0.7309 - out_32_acc: 0.7371 - val_loss: 21.5100 - val_out_loss: 0.5922 - val_out_0_loss: 0.4197 - val_out_1_loss: 0.6119 - val_out_2_loss: 0.6185 - val_out_3_loss: 0.6131 - val_out_4_loss: 0.6171 - val_out_5_loss: 0.6137 - val_out_6_loss: 0.6129 - val_out_7_loss: 0.6120 - val_out_8_loss: 0.6140 - val_out_9_loss: 0.6137 - val_out_10_loss: 0.6150 - val_out_11_loss: 0.6129 - val_out_12_loss: 0.6117 - val_out_13_loss: 0.6138 - val_out_14_loss: 0.6162 - val_out_15_loss: 0.6162 - val_out_16_loss: 0.6083 - val_out_17_loss: 0.6134 - val_out_18_loss: 0.6127 - val_out_19_loss: 0.6165 - val_out_20_loss: 0.6144 - val_out_21_loss: 0.6114 - val_out_22_loss: 0.6146 - val_out_23_loss: 0.6122 - val_out_24_loss: 0.6163 - val_out_25_loss: 0.6197 - val_out_26_loss: 0.6133 - val_out_27_loss: 0.6131 - val_out_28_loss: 0.6130 - val_out_29_loss: 0.6101 - val_out_30_loss: 0.6181 - val_out_31_loss: 0.6141 - val_out_32_loss: 0.6127 - val_out_acc: 0.7679 - val_out_0_acc: 0.8395 - val_out_1_acc: 0.7679 - val_out_2_acc: 0.7614 - val_out_3_acc: 0.7701 - val_out_4_acc: 0.7636 - val_out_5_acc: 0.7614 - val_out_6_acc: 0.7657 - val_out_7_acc: 0.7636 - val_out_8_acc: 0.7636 - val_out_9_acc: 0.7701 - val_out_10_acc: 0.7657 - val_out_11_acc: 0.7636 - val_out_12_acc: 0.7657 - val_out_13_acc: 0.7679 - val_out_14_acc: 0.7701 - val_out_15_acc: 0.7679 - val_out_16_acc: 0.7679 - val_out_17_acc: 0.7614 - val_out_18_acc: 0.7657 - val_out_19_acc: 0.7679 - val_out_20_acc: 0.7679 - val_out_21_acc: 0.7679 - val_out_22_acc: 0.7744 - val_out_23_acc: 0.7657 - val_out_24_acc: 0.7744 - val_out_25_acc: 0.7657 - val_out_26_acc: 0.7701 - val_out_27_acc: 0.7614 - val_out_28_acc: 0.7614 - val_out_29_acc: 0.7657 - val_out_30_acc: 0.7657 - val_out_31_acc: 0.7679 - val_out_32_acc: 0.7657\n",
      "Epoch 134/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.0511 - out_loss: 0.6237 - out_0_loss: 0.4315 - out_1_loss: 0.7145 - out_2_loss: 0.7291 - out_3_loss: 0.7177 - out_4_loss: 0.7224 - out_5_loss: 0.7124 - out_6_loss: 0.7117 - out_7_loss: 0.7193 - out_8_loss: 0.7120 - out_9_loss: 0.7160 - out_10_loss: 0.7313 - out_11_loss: 0.7226 - out_12_loss: 0.7220 - out_13_loss: 0.7110 - out_14_loss: 0.7004 - out_15_loss: 0.7253 - out_16_loss: 0.7202 - out_17_loss: 0.7130 - out_18_loss: 0.7386 - out_19_loss: 0.7332 - out_20_loss: 0.7173 - out_21_loss: 0.7331 - out_22_loss: 0.7186 - out_23_loss: 0.7134 - out_24_loss: 0.7128 - out_25_loss: 0.7152 - out_26_loss: 0.7105 - out_27_loss: 0.7117 - out_28_loss: 0.7064 - out_29_loss: 0.7178 - out_30_loss: 0.7357 - out_31_loss: 0.7144 - out_32_loss: 0.7164 - out_acc: 0.7750 - out_0_acc: 0.8447 - out_1_acc: 0.7458 - out_2_acc: 0.7443 - out_3_acc: 0.7328 - out_4_acc: 0.7337 - out_5_acc: 0.7440 - out_6_acc: 0.7418 - out_7_acc: 0.7440 - out_8_acc: 0.7433 - out_9_acc: 0.7412 - out_10_acc: 0.7337 - out_11_acc: 0.7378 - out_12_acc: 0.7486 - out_13_acc: 0.7458 - out_14_acc: 0.7393 - out_15_acc: 0.7381 - out_16_acc: 0.7424 - out_17_acc: 0.7421 - out_18_acc: 0.7297 - out_19_acc: 0.7331 - out_20_acc: 0.7387 - out_21_acc: 0.7399 - out_22_acc: 0.7471 - out_23_acc: 0.7440 - out_24_acc: 0.7436 - out_25_acc: 0.7446 - out_26_acc: 0.7486 - out_27_acc: 0.7412 - out_28_acc: 0.7533 - out_29_acc: 0.7464 - out_30_acc: 0.7316 - out_31_acc: 0.7396 - out_32_acc: 0.7427 - val_loss: 26.1020 - val_out_loss: 0.7088 - val_out_0_loss: 0.4489 - val_out_1_loss: 0.7483 - val_out_2_loss: 0.7525 - val_out_3_loss: 0.7480 - val_out_4_loss: 0.7497 - val_out_5_loss: 0.7451 - val_out_6_loss: 0.7463 - val_out_7_loss: 0.7472 - val_out_8_loss: 0.7459 - val_out_9_loss: 0.7464 - val_out_10_loss: 0.7503 - val_out_11_loss: 0.7457 - val_out_12_loss: 0.7443 - val_out_13_loss: 0.7478 - val_out_14_loss: 0.7483 - val_out_15_loss: 0.7521 - val_out_16_loss: 0.7430 - val_out_17_loss: 0.7452 - val_out_18_loss: 0.7451 - val_out_19_loss: 0.7508 - val_out_20_loss: 0.7463 - val_out_21_loss: 0.7428 - val_out_22_loss: 0.7496 - val_out_23_loss: 0.7472 - val_out_24_loss: 0.7471 - val_out_25_loss: 0.7508 - val_out_26_loss: 0.7542 - val_out_27_loss: 0.7428 - val_out_28_loss: 0.7452 - val_out_29_loss: 0.7432 - val_out_30_loss: 0.7510 - val_out_31_loss: 0.7468 - val_out_32_loss: 0.7423 - val_out_acc: 0.7115 - val_out_0_acc: 0.8373 - val_out_1_acc: 0.7050 - val_out_2_acc: 0.7093 - val_out_3_acc: 0.7115 - val_out_4_acc: 0.7115 - val_out_5_acc: 0.7072 - val_out_6_acc: 0.7072 - val_out_7_acc: 0.7115 - val_out_8_acc: 0.7093 - val_out_9_acc: 0.7115 - val_out_10_acc: 0.7007 - val_out_11_acc: 0.7072 - val_out_12_acc: 0.7093 - val_out_13_acc: 0.7028 - val_out_14_acc: 0.7028 - val_out_15_acc: 0.7072 - val_out_16_acc: 0.7115 - val_out_17_acc: 0.7137 - val_out_18_acc: 0.7050 - val_out_19_acc: 0.7028 - val_out_20_acc: 0.7093 - val_out_21_acc: 0.7115 - val_out_22_acc: 0.7072 - val_out_23_acc: 0.7007 - val_out_24_acc: 0.7072 - val_out_25_acc: 0.7115 - val_out_26_acc: 0.7093 - val_out_27_acc: 0.7093 - val_out_28_acc: 0.7137 - val_out_29_acc: 0.7137 - val_out_30_acc: 0.7137 - val_out_31_acc: 0.7050 - val_out_32_acc: 0.7050\n",
      "Epoch 135/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.5513 - out_loss: 0.6364 - out_0_loss: 0.4425 - out_1_loss: 0.7364 - out_2_loss: 0.7337 - out_3_loss: 0.7347 - out_4_loss: 0.7262 - out_5_loss: 0.7248 - out_6_loss: 0.7220 - out_7_loss: 0.7421 - out_8_loss: 0.7335 - out_9_loss: 0.7403 - out_10_loss: 0.7283 - out_11_loss: 0.7400 - out_12_loss: 0.7266 - out_13_loss: 0.7394 - out_14_loss: 0.7243 - out_15_loss: 0.7291 - out_16_loss: 0.7344 - out_17_loss: 0.7362 - out_18_loss: 0.7384 - out_19_loss: 0.7208 - out_20_loss: 0.7294 - out_21_loss: 0.7407 - out_22_loss: 0.7317 - out_23_loss: 0.7205 - out_24_loss: 0.7439 - out_25_loss: 0.7419 - out_26_loss: 0.7395 - out_27_loss: 0.7306 - out_28_loss: 0.7429 - out_29_loss: 0.7377 - out_30_loss: 0.7377 - out_31_loss: 0.7290 - out_32_loss: 0.7354 - out_acc: 0.7821 - out_0_acc: 0.8419 - out_1_acc: 0.7393 - out_2_acc: 0.7498 - out_3_acc: 0.7365 - out_4_acc: 0.7390 - out_5_acc: 0.7436 - out_6_acc: 0.7421 - out_7_acc: 0.7374 - out_8_acc: 0.7399 - out_9_acc: 0.7356 - out_10_acc: 0.7362 - out_11_acc: 0.7412 - out_12_acc: 0.7384 - out_13_acc: 0.7393 - out_14_acc: 0.7433 - out_15_acc: 0.7316 - out_16_acc: 0.7427 - out_17_acc: 0.7359 - out_18_acc: 0.7458 - out_19_acc: 0.7483 - out_20_acc: 0.7502 - out_21_acc: 0.7384 - out_22_acc: 0.7362 - out_23_acc: 0.7443 - out_24_acc: 0.7347 - out_25_acc: 0.7356 - out_26_acc: 0.7393 - out_27_acc: 0.7415 - out_28_acc: 0.7331 - out_29_acc: 0.7359 - out_30_acc: 0.7409 - out_31_acc: 0.7436 - out_32_acc: 0.7436 - val_loss: 23.7334 - val_out_loss: 0.6534 - val_out_0_loss: 0.4696 - val_out_1_loss: 0.6753 - val_out_2_loss: 0.6753 - val_out_3_loss: 0.6748 - val_out_4_loss: 0.6778 - val_out_5_loss: 0.6771 - val_out_6_loss: 0.6756 - val_out_7_loss: 0.6790 - val_out_8_loss: 0.6772 - val_out_9_loss: 0.6768 - val_out_10_loss: 0.6815 - val_out_11_loss: 0.6724 - val_out_12_loss: 0.6782 - val_out_13_loss: 0.6769 - val_out_14_loss: 0.6805 - val_out_15_loss: 0.6792 - val_out_16_loss: 0.6790 - val_out_17_loss: 0.6780 - val_out_18_loss: 0.6732 - val_out_19_loss: 0.6793 - val_out_20_loss: 0.6785 - val_out_21_loss: 0.6756 - val_out_22_loss: 0.6771 - val_out_23_loss: 0.6777 - val_out_24_loss: 0.6781 - val_out_25_loss: 0.6808 - val_out_26_loss: 0.6774 - val_out_27_loss: 0.6758 - val_out_28_loss: 0.6774 - val_out_29_loss: 0.6771 - val_out_30_loss: 0.6775 - val_out_31_loss: 0.6756 - val_out_32_loss: 0.6749 - val_out_acc: 0.7636 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7570 - val_out_2_acc: 0.7636 - val_out_3_acc: 0.7636 - val_out_4_acc: 0.7636 - val_out_5_acc: 0.7592 - val_out_6_acc: 0.7657 - val_out_7_acc: 0.7570 - val_out_8_acc: 0.7614 - val_out_9_acc: 0.7614 - val_out_10_acc: 0.7657 - val_out_11_acc: 0.7614 - val_out_12_acc: 0.7657 - val_out_13_acc: 0.7636 - val_out_14_acc: 0.7570 - val_out_15_acc: 0.7636 - val_out_16_acc: 0.7657 - val_out_17_acc: 0.7701 - val_out_18_acc: 0.7636 - val_out_19_acc: 0.7636 - val_out_20_acc: 0.7614 - val_out_21_acc: 0.7636 - val_out_22_acc: 0.7614 - val_out_23_acc: 0.7636 - val_out_24_acc: 0.7592 - val_out_25_acc: 0.7592 - val_out_26_acc: 0.7592 - val_out_27_acc: 0.7657 - val_out_28_acc: 0.7592 - val_out_29_acc: 0.7657 - val_out_30_acc: 0.7592 - val_out_31_acc: 0.7549 - val_out_32_acc: 0.7592\n",
      "Epoch 136/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.9123 - out_loss: 0.6471 - out_0_loss: 0.4247 - out_1_loss: 0.7411 - out_2_loss: 0.7497 - out_3_loss: 0.7492 - out_4_loss: 0.7580 - out_5_loss: 0.7464 - out_6_loss: 0.7566 - out_7_loss: 0.7478 - out_8_loss: 0.7391 - out_9_loss: 0.7492 - out_10_loss: 0.7400 - out_11_loss: 0.7475 - out_12_loss: 0.7283 - out_13_loss: 0.7347 - out_14_loss: 0.7385 - out_15_loss: 0.7380 - out_16_loss: 0.7440 - out_17_loss: 0.7533 - out_18_loss: 0.7421 - out_19_loss: 0.7543 - out_20_loss: 0.7407 - out_21_loss: 0.7420 - out_22_loss: 0.7468 - out_23_loss: 0.7386 - out_24_loss: 0.7394 - out_25_loss: 0.7368 - out_26_loss: 0.7522 - out_27_loss: 0.7269 - out_28_loss: 0.7510 - out_29_loss: 0.7503 - out_30_loss: 0.7548 - out_31_loss: 0.7533 - out_32_loss: 0.7501 - out_acc: 0.7681 - out_0_acc: 0.8503 - out_1_acc: 0.7396 - out_2_acc: 0.7353 - out_3_acc: 0.7322 - out_4_acc: 0.7275 - out_5_acc: 0.7319 - out_6_acc: 0.7334 - out_7_acc: 0.7340 - out_8_acc: 0.7418 - out_9_acc: 0.7365 - out_10_acc: 0.7461 - out_11_acc: 0.7319 - out_12_acc: 0.7381 - out_13_acc: 0.7418 - out_14_acc: 0.7359 - out_15_acc: 0.7356 - out_16_acc: 0.7306 - out_17_acc: 0.7272 - out_18_acc: 0.7421 - out_19_acc: 0.7325 - out_20_acc: 0.7362 - out_21_acc: 0.7365 - out_22_acc: 0.7297 - out_23_acc: 0.7396 - out_24_acc: 0.7322 - out_25_acc: 0.7347 - out_26_acc: 0.7343 - out_27_acc: 0.7402 - out_28_acc: 0.7235 - out_29_acc: 0.7232 - out_30_acc: 0.7257 - out_31_acc: 0.7281 - out_32_acc: 0.7337 - val_loss: 22.6836 - val_out_loss: 0.6200 - val_out_0_loss: 0.4873 - val_out_1_loss: 0.6420 - val_out_2_loss: 0.6487 - val_out_3_loss: 0.6457 - val_out_4_loss: 0.6482 - val_out_5_loss: 0.6468 - val_out_6_loss: 0.6489 - val_out_7_loss: 0.6471 - val_out_8_loss: 0.6483 - val_out_9_loss: 0.6450 - val_out_10_loss: 0.6491 - val_out_11_loss: 0.6478 - val_out_12_loss: 0.6418 - val_out_13_loss: 0.6443 - val_out_14_loss: 0.6478 - val_out_15_loss: 0.6532 - val_out_16_loss: 0.6477 - val_out_17_loss: 0.6501 - val_out_18_loss: 0.6406 - val_out_19_loss: 0.6486 - val_out_20_loss: 0.6454 - val_out_21_loss: 0.6455 - val_out_22_loss: 0.6445 - val_out_23_loss: 0.6416 - val_out_24_loss: 0.6458 - val_out_25_loss: 0.6469 - val_out_26_loss: 0.6474 - val_out_27_loss: 0.6437 - val_out_28_loss: 0.6456 - val_out_29_loss: 0.6418 - val_out_30_loss: 0.6492 - val_out_31_loss: 0.6426 - val_out_32_loss: 0.6467 - val_out_acc: 0.7375 - val_out_0_acc: 0.8308 - val_out_1_acc: 0.7354 - val_out_2_acc: 0.7332 - val_out_3_acc: 0.7332 - val_out_4_acc: 0.7310 - val_out_5_acc: 0.7375 - val_out_6_acc: 0.7289 - val_out_7_acc: 0.7310 - val_out_8_acc: 0.7245 - val_out_9_acc: 0.7332 - val_out_10_acc: 0.7310 - val_out_11_acc: 0.7310 - val_out_12_acc: 0.7375 - val_out_13_acc: 0.7375 - val_out_14_acc: 0.7354 - val_out_15_acc: 0.7310 - val_out_16_acc: 0.7332 - val_out_17_acc: 0.7310 - val_out_18_acc: 0.7289 - val_out_19_acc: 0.7332 - val_out_20_acc: 0.7310 - val_out_21_acc: 0.7354 - val_out_22_acc: 0.7354 - val_out_23_acc: 0.7332 - val_out_24_acc: 0.7332 - val_out_25_acc: 0.7289 - val_out_26_acc: 0.7310 - val_out_27_acc: 0.7375 - val_out_28_acc: 0.7289 - val_out_29_acc: 0.7354 - val_out_30_acc: 0.7245 - val_out_31_acc: 0.7354 - val_out_32_acc: 0.7354\n",
      "Epoch 137/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 25.0754 - out_loss: 0.6526 - out_0_loss: 0.4429 - out_1_loss: 0.7533 - out_2_loss: 0.7441 - out_3_loss: 0.7490 - out_4_loss: 0.7416 - out_5_loss: 0.7429 - out_6_loss: 0.7443 - out_7_loss: 0.7656 - out_8_loss: 0.7429 - out_9_loss: 0.7428 - out_10_loss: 0.7568 - out_11_loss: 0.7488 - out_12_loss: 0.7619 - out_13_loss: 0.7383 - out_14_loss: 0.7492 - out_15_loss: 0.7443 - out_16_loss: 0.7446 - out_17_loss: 0.7405 - out_18_loss: 0.7594 - out_19_loss: 0.7588 - out_20_loss: 0.7496 - out_21_loss: 0.7599 - out_22_loss: 0.7557 - out_23_loss: 0.7501 - out_24_loss: 0.7588 - out_25_loss: 0.7386 - out_26_loss: 0.7517 - out_27_loss: 0.7372 - out_28_loss: 0.7531 - out_29_loss: 0.7464 - out_30_loss: 0.7570 - out_31_loss: 0.7488 - out_32_loss: 0.7437 - out_acc: 0.7734 - out_0_acc: 0.8453 - out_1_acc: 0.7409 - out_2_acc: 0.7390 - out_3_acc: 0.7427 - out_4_acc: 0.7319 - out_5_acc: 0.7334 - out_6_acc: 0.7390 - out_7_acc: 0.7285 - out_8_acc: 0.7365 - out_9_acc: 0.7306 - out_10_acc: 0.7412 - out_11_acc: 0.7291 - out_12_acc: 0.7257 - out_13_acc: 0.7396 - out_14_acc: 0.7337 - out_15_acc: 0.7384 - out_16_acc: 0.7337 - out_17_acc: 0.7393 - out_18_acc: 0.7347 - out_19_acc: 0.7359 - out_20_acc: 0.7374 - out_21_acc: 0.7288 - out_22_acc: 0.7303 - out_23_acc: 0.7328 - out_24_acc: 0.7260 - out_25_acc: 0.7374 - out_26_acc: 0.7306 - out_27_acc: 0.7337 - out_28_acc: 0.7368 - out_29_acc: 0.7381 - out_30_acc: 0.7343 - out_31_acc: 0.7365 - out_32_acc: 0.7350 - val_loss: 22.3901 - val_out_loss: 0.6176 - val_out_0_loss: 0.4188 - val_out_1_loss: 0.6418 - val_out_2_loss: 0.6406 - val_out_3_loss: 0.6427 - val_out_4_loss: 0.6431 - val_out_5_loss: 0.6357 - val_out_6_loss: 0.6396 - val_out_7_loss: 0.6370 - val_out_8_loss: 0.6406 - val_out_9_loss: 0.6423 - val_out_10_loss: 0.6377 - val_out_11_loss: 0.6377 - val_out_12_loss: 0.6371 - val_out_13_loss: 0.6398 - val_out_14_loss: 0.6401 - val_out_15_loss: 0.6386 - val_out_16_loss: 0.6368 - val_out_17_loss: 0.6385 - val_out_18_loss: 0.6398 - val_out_19_loss: 0.6413 - val_out_20_loss: 0.6425 - val_out_21_loss: 0.6370 - val_out_22_loss: 0.6391 - val_out_23_loss: 0.6390 - val_out_24_loss: 0.6388 - val_out_25_loss: 0.6418 - val_out_26_loss: 0.6388 - val_out_27_loss: 0.6429 - val_out_28_loss: 0.6379 - val_out_29_loss: 0.6364 - val_out_30_loss: 0.6415 - val_out_31_loss: 0.6402 - val_out_32_loss: 0.6409 - val_out_acc: 0.7505 - val_out_0_acc: 0.8568 - val_out_1_acc: 0.7484 - val_out_2_acc: 0.7549 - val_out_3_acc: 0.7549 - val_out_4_acc: 0.7419 - val_out_5_acc: 0.7484 - val_out_6_acc: 0.7505 - val_out_7_acc: 0.7484 - val_out_8_acc: 0.7484 - val_out_9_acc: 0.7484 - val_out_10_acc: 0.7549 - val_out_11_acc: 0.7527 - val_out_12_acc: 0.7505 - val_out_13_acc: 0.7484 - val_out_14_acc: 0.7570 - val_out_15_acc: 0.7570 - val_out_16_acc: 0.7527 - val_out_17_acc: 0.7505 - val_out_18_acc: 0.7549 - val_out_19_acc: 0.7505 - val_out_20_acc: 0.7505 - val_out_21_acc: 0.7570 - val_out_22_acc: 0.7614 - val_out_23_acc: 0.7570 - val_out_24_acc: 0.7484 - val_out_25_acc: 0.7527 - val_out_26_acc: 0.7505 - val_out_27_acc: 0.7549 - val_out_28_acc: 0.7484 - val_out_29_acc: 0.7527 - val_out_30_acc: 0.7549 - val_out_31_acc: 0.7440 - val_out_32_acc: 0.7462\n",
      "Epoch 138/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.3481 - out_loss: 0.6285 - out_0_loss: 0.4234 - out_1_loss: 0.7327 - out_2_loss: 0.7106 - out_3_loss: 0.7188 - out_4_loss: 0.7373 - out_5_loss: 0.7242 - out_6_loss: 0.7297 - out_7_loss: 0.7340 - out_8_loss: 0.7373 - out_9_loss: 0.7256 - out_10_loss: 0.7240 - out_11_loss: 0.7273 - out_12_loss: 0.7316 - out_13_loss: 0.7268 - out_14_loss: 0.7123 - out_15_loss: 0.7188 - out_16_loss: 0.7378 - out_17_loss: 0.7134 - out_18_loss: 0.7331 - out_19_loss: 0.7299 - out_20_loss: 0.7260 - out_21_loss: 0.7271 - out_22_loss: 0.7267 - out_23_loss: 0.7392 - out_24_loss: 0.7329 - out_25_loss: 0.7263 - out_26_loss: 0.7235 - out_27_loss: 0.7239 - out_28_loss: 0.7282 - out_29_loss: 0.7372 - out_30_loss: 0.7308 - out_31_loss: 0.7437 - out_32_loss: 0.7256 - out_acc: 0.7796 - out_0_acc: 0.8506 - out_1_acc: 0.7396 - out_2_acc: 0.7449 - out_3_acc: 0.7396 - out_4_acc: 0.7402 - out_5_acc: 0.7421 - out_6_acc: 0.7412 - out_7_acc: 0.7415 - out_8_acc: 0.7427 - out_9_acc: 0.7440 - out_10_acc: 0.7443 - out_11_acc: 0.7418 - out_12_acc: 0.7415 - out_13_acc: 0.7415 - out_14_acc: 0.7455 - out_15_acc: 0.7458 - out_16_acc: 0.7350 - out_17_acc: 0.7483 - out_18_acc: 0.7393 - out_19_acc: 0.7436 - out_20_acc: 0.7405 - out_21_acc: 0.7433 - out_22_acc: 0.7387 - out_23_acc: 0.7409 - out_24_acc: 0.7343 - out_25_acc: 0.7421 - out_26_acc: 0.7415 - out_27_acc: 0.7412 - out_28_acc: 0.7436 - out_29_acc: 0.7340 - out_30_acc: 0.7446 - out_31_acc: 0.7371 - out_32_acc: 0.7374 - val_loss: 24.7172 - val_out_loss: 0.6756 - val_out_0_loss: 0.4814 - val_out_1_loss: 0.7056 - val_out_2_loss: 0.7041 - val_out_3_loss: 0.7022 - val_out_4_loss: 0.7088 - val_out_5_loss: 0.7031 - val_out_6_loss: 0.7075 - val_out_7_loss: 0.7053 - val_out_8_loss: 0.7025 - val_out_9_loss: 0.7034 - val_out_10_loss: 0.7072 - val_out_11_loss: 0.7036 - val_out_12_loss: 0.7061 - val_out_13_loss: 0.7061 - val_out_14_loss: 0.7048 - val_out_15_loss: 0.7064 - val_out_16_loss: 0.7073 - val_out_17_loss: 0.7060 - val_out_18_loss: 0.7078 - val_out_19_loss: 0.7110 - val_out_20_loss: 0.7085 - val_out_21_loss: 0.7006 - val_out_22_loss: 0.7073 - val_out_23_loss: 0.7011 - val_out_24_loss: 0.7080 - val_out_25_loss: 0.7081 - val_out_26_loss: 0.7050 - val_out_27_loss: 0.7065 - val_out_28_loss: 0.7082 - val_out_29_loss: 0.7031 - val_out_30_loss: 0.7088 - val_out_31_loss: 0.7048 - val_out_32_loss: 0.7029 - val_out_acc: 0.7440 - val_out_0_acc: 0.8221 - val_out_1_acc: 0.7419 - val_out_2_acc: 0.7397 - val_out_3_acc: 0.7375 - val_out_4_acc: 0.7354 - val_out_5_acc: 0.7354 - val_out_6_acc: 0.7375 - val_out_7_acc: 0.7419 - val_out_8_acc: 0.7440 - val_out_9_acc: 0.7397 - val_out_10_acc: 0.7419 - val_out_11_acc: 0.7397 - val_out_12_acc: 0.7419 - val_out_13_acc: 0.7440 - val_out_14_acc: 0.7375 - val_out_15_acc: 0.7354 - val_out_16_acc: 0.7440 - val_out_17_acc: 0.7440 - val_out_18_acc: 0.7397 - val_out_19_acc: 0.7397 - val_out_20_acc: 0.7375 - val_out_21_acc: 0.7375 - val_out_22_acc: 0.7419 - val_out_23_acc: 0.7354 - val_out_24_acc: 0.7440 - val_out_25_acc: 0.7462 - val_out_26_acc: 0.7440 - val_out_27_acc: 0.7397 - val_out_28_acc: 0.7419 - val_out_29_acc: 0.7397 - val_out_30_acc: 0.7354 - val_out_31_acc: 0.7419 - val_out_32_acc: 0.7419\n",
      "Epoch 139/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.3474 - out_loss: 0.6301 - out_0_loss: 0.4208 - out_1_loss: 0.7251 - out_2_loss: 0.7325 - out_3_loss: 0.7285 - out_4_loss: 0.7316 - out_5_loss: 0.7289 - out_6_loss: 0.7309 - out_7_loss: 0.7179 - out_8_loss: 0.7332 - out_9_loss: 0.7383 - out_10_loss: 0.7248 - out_11_loss: 0.7350 - out_12_loss: 0.7285 - out_13_loss: 0.7175 - out_14_loss: 0.7186 - out_15_loss: 0.7199 - out_16_loss: 0.7263 - out_17_loss: 0.7288 - out_18_loss: 0.7434 - out_19_loss: 0.7313 - out_20_loss: 0.7341 - out_21_loss: 0.7171 - out_22_loss: 0.7253 - out_23_loss: 0.7224 - out_24_loss: 0.7293 - out_25_loss: 0.7220 - out_26_loss: 0.7283 - out_27_loss: 0.7368 - out_28_loss: 0.7309 - out_29_loss: 0.7217 - out_30_loss: 0.7325 - out_31_loss: 0.7299 - out_32_loss: 0.7252 - out_acc: 0.7762 - out_0_acc: 0.8518 - out_1_acc: 0.7402 - out_2_acc: 0.7334 - out_3_acc: 0.7440 - out_4_acc: 0.7433 - out_5_acc: 0.7356 - out_6_acc: 0.7412 - out_7_acc: 0.7498 - out_8_acc: 0.7387 - out_9_acc: 0.7306 - out_10_acc: 0.7399 - out_11_acc: 0.7365 - out_12_acc: 0.7368 - out_13_acc: 0.7409 - out_14_acc: 0.7486 - out_15_acc: 0.7427 - out_16_acc: 0.7415 - out_17_acc: 0.7421 - out_18_acc: 0.7384 - out_19_acc: 0.7384 - out_20_acc: 0.7325 - out_21_acc: 0.7359 - out_22_acc: 0.7374 - out_23_acc: 0.7489 - out_24_acc: 0.7303 - out_25_acc: 0.7384 - out_26_acc: 0.7418 - out_27_acc: 0.7288 - out_28_acc: 0.7393 - out_29_acc: 0.7384 - out_30_acc: 0.7402 - out_31_acc: 0.7402 - out_32_acc: 0.7374 - val_loss: 22.4033 - val_out_loss: 0.6114 - val_out_0_loss: 0.4421 - val_out_1_loss: 0.6407 - val_out_2_loss: 0.6395 - val_out_3_loss: 0.6423 - val_out_4_loss: 0.6437 - val_out_5_loss: 0.6384 - val_out_6_loss: 0.6385 - val_out_7_loss: 0.6393 - val_out_8_loss: 0.6394 - val_out_9_loss: 0.6394 - val_out_10_loss: 0.6401 - val_out_11_loss: 0.6366 - val_out_12_loss: 0.6363 - val_out_13_loss: 0.6395 - val_out_14_loss: 0.6415 - val_out_15_loss: 0.6434 - val_out_16_loss: 0.6400 - val_out_17_loss: 0.6371 - val_out_18_loss: 0.6400 - val_out_19_loss: 0.6410 - val_out_20_loss: 0.6371 - val_out_21_loss: 0.6409 - val_out_22_loss: 0.6391 - val_out_23_loss: 0.6380 - val_out_24_loss: 0.6423 - val_out_25_loss: 0.6402 - val_out_26_loss: 0.6396 - val_out_27_loss: 0.6400 - val_out_28_loss: 0.6417 - val_out_29_loss: 0.6336 - val_out_30_loss: 0.6406 - val_out_31_loss: 0.6352 - val_out_32_loss: 0.6382 - val_out_acc: 0.7874 - val_out_0_acc: 0.8308 - val_out_1_acc: 0.7809 - val_out_2_acc: 0.7701 - val_out_3_acc: 0.7744 - val_out_4_acc: 0.7744 - val_out_5_acc: 0.7852 - val_out_6_acc: 0.7722 - val_out_7_acc: 0.7809 - val_out_8_acc: 0.7809 - val_out_9_acc: 0.7722 - val_out_10_acc: 0.7831 - val_out_11_acc: 0.7831 - val_out_12_acc: 0.7831 - val_out_13_acc: 0.7831 - val_out_14_acc: 0.7831 - val_out_15_acc: 0.7809 - val_out_16_acc: 0.7766 - val_out_17_acc: 0.7831 - val_out_18_acc: 0.7852 - val_out_19_acc: 0.7679 - val_out_20_acc: 0.7809 - val_out_21_acc: 0.7831 - val_out_22_acc: 0.7744 - val_out_23_acc: 0.7787 - val_out_24_acc: 0.7744 - val_out_25_acc: 0.7874 - val_out_26_acc: 0.7852 - val_out_27_acc: 0.7744 - val_out_28_acc: 0.7831 - val_out_29_acc: 0.7766 - val_out_30_acc: 0.7787 - val_out_31_acc: 0.7787 - val_out_32_acc: 0.7831\n",
      "Epoch 140/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.4348 - out_loss: 0.6380 - out_0_loss: 0.4343 - out_1_loss: 0.7255 - out_2_loss: 0.7279 - out_3_loss: 0.7215 - out_4_loss: 0.7253 - out_5_loss: 0.7391 - out_6_loss: 0.7289 - out_7_loss: 0.7371 - out_8_loss: 0.7280 - out_9_loss: 0.7328 - out_10_loss: 0.7445 - out_11_loss: 0.7369 - out_12_loss: 0.7342 - out_13_loss: 0.7302 - out_14_loss: 0.7339 - out_15_loss: 0.7211 - out_16_loss: 0.7301 - out_17_loss: 0.7303 - out_18_loss: 0.7397 - out_19_loss: 0.7191 - out_20_loss: 0.7257 - out_21_loss: 0.7310 - out_22_loss: 0.7292 - out_23_loss: 0.7358 - out_24_loss: 0.7249 - out_25_loss: 0.7244 - out_26_loss: 0.7358 - out_27_loss: 0.7369 - out_28_loss: 0.7205 - out_29_loss: 0.7118 - out_30_loss: 0.7390 - out_31_loss: 0.7286 - out_32_loss: 0.7328 - out_acc: 0.7768 - out_0_acc: 0.8493 - out_1_acc: 0.7396 - out_2_acc: 0.7433 - out_3_acc: 0.7455 - out_4_acc: 0.7433 - out_5_acc: 0.7381 - out_6_acc: 0.7446 - out_7_acc: 0.7415 - out_8_acc: 0.7424 - out_9_acc: 0.7405 - out_10_acc: 0.7427 - out_11_acc: 0.7378 - out_12_acc: 0.7384 - out_13_acc: 0.7424 - out_14_acc: 0.7415 - out_15_acc: 0.7427 - out_16_acc: 0.7421 - out_17_acc: 0.7446 - out_18_acc: 0.7374 - out_19_acc: 0.7409 - out_20_acc: 0.7381 - out_21_acc: 0.7458 - out_22_acc: 0.7393 - out_23_acc: 0.7458 - out_24_acc: 0.7471 - out_25_acc: 0.7480 - out_26_acc: 0.7421 - out_27_acc: 0.7359 - out_28_acc: 0.7427 - out_29_acc: 0.7464 - out_30_acc: 0.7436 - out_31_acc: 0.7381 - out_32_acc: 0.7365 - val_loss: 24.8837 - val_out_loss: 0.6775 - val_out_0_loss: 0.3915 - val_out_1_loss: 0.7156 - val_out_2_loss: 0.7124 - val_out_3_loss: 0.7144 - val_out_4_loss: 0.7168 - val_out_5_loss: 0.7120 - val_out_6_loss: 0.7156 - val_out_7_loss: 0.7100 - val_out_8_loss: 0.7144 - val_out_9_loss: 0.7131 - val_out_10_loss: 0.7155 - val_out_11_loss: 0.7127 - val_out_12_loss: 0.7146 - val_out_13_loss: 0.7145 - val_out_14_loss: 0.7128 - val_out_15_loss: 0.7117 - val_out_16_loss: 0.7131 - val_out_17_loss: 0.7149 - val_out_18_loss: 0.7168 - val_out_19_loss: 0.7154 - val_out_20_loss: 0.7117 - val_out_21_loss: 0.7134 - val_out_22_loss: 0.7129 - val_out_23_loss: 0.7158 - val_out_24_loss: 0.7115 - val_out_25_loss: 0.7136 - val_out_26_loss: 0.7114 - val_out_27_loss: 0.7145 - val_out_28_loss: 0.7116 - val_out_29_loss: 0.7134 - val_out_30_loss: 0.7144 - val_out_31_loss: 0.7123 - val_out_32_loss: 0.7070 - val_out_acc: 0.7245 - val_out_0_acc: 0.8590 - val_out_1_acc: 0.7115 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7158 - val_out_4_acc: 0.7245 - val_out_5_acc: 0.7093 - val_out_6_acc: 0.7093 - val_out_7_acc: 0.7180 - val_out_8_acc: 0.7180 - val_out_9_acc: 0.7310 - val_out_10_acc: 0.7137 - val_out_11_acc: 0.7158 - val_out_12_acc: 0.7202 - val_out_13_acc: 0.7180 - val_out_14_acc: 0.7158 - val_out_15_acc: 0.7115 - val_out_16_acc: 0.7180 - val_out_17_acc: 0.7223 - val_out_18_acc: 0.7137 - val_out_19_acc: 0.7115 - val_out_20_acc: 0.7158 - val_out_21_acc: 0.7115 - val_out_22_acc: 0.7158 - val_out_23_acc: 0.7137 - val_out_24_acc: 0.7115 - val_out_25_acc: 0.7180 - val_out_26_acc: 0.7158 - val_out_27_acc: 0.7158 - val_out_28_acc: 0.7180 - val_out_29_acc: 0.7158 - val_out_30_acc: 0.7158 - val_out_31_acc: 0.7093 - val_out_32_acc: 0.7180\n",
      "Epoch 141/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.3217 - out_loss: 0.6293 - out_0_loss: 0.4004 - out_1_loss: 0.7246 - out_2_loss: 0.7365 - out_3_loss: 0.7125 - out_4_loss: 0.7190 - out_5_loss: 0.7314 - out_6_loss: 0.7265 - out_7_loss: 0.7291 - out_8_loss: 0.7377 - out_9_loss: 0.7409 - out_10_loss: 0.7254 - out_11_loss: 0.7320 - out_12_loss: 0.7233 - out_13_loss: 0.7319 - out_14_loss: 0.7372 - out_15_loss: 0.7370 - out_16_loss: 0.7337 - out_17_loss: 0.7323 - out_18_loss: 0.7244 - out_19_loss: 0.7394 - out_20_loss: 0.7197 - out_21_loss: 0.7328 - out_22_loss: 0.7162 - out_23_loss: 0.7153 - out_24_loss: 0.7317 - out_25_loss: 0.7163 - out_26_loss: 0.7254 - out_27_loss: 0.7215 - out_28_loss: 0.7225 - out_29_loss: 0.7275 - out_30_loss: 0.7227 - out_31_loss: 0.7310 - out_32_loss: 0.7348 - out_acc: 0.7746 - out_0_acc: 0.8577 - out_1_acc: 0.7399 - out_2_acc: 0.7316 - out_3_acc: 0.7399 - out_4_acc: 0.7399 - out_5_acc: 0.7421 - out_6_acc: 0.7464 - out_7_acc: 0.7452 - out_8_acc: 0.7328 - out_9_acc: 0.7362 - out_10_acc: 0.7424 - out_11_acc: 0.7390 - out_12_acc: 0.7405 - out_13_acc: 0.7387 - out_14_acc: 0.7381 - out_15_acc: 0.7266 - out_16_acc: 0.7390 - out_17_acc: 0.7347 - out_18_acc: 0.7421 - out_19_acc: 0.7340 - out_20_acc: 0.7443 - out_21_acc: 0.7384 - out_22_acc: 0.7378 - out_23_acc: 0.7421 - out_24_acc: 0.7405 - out_25_acc: 0.7474 - out_26_acc: 0.7409 - out_27_acc: 0.7471 - out_28_acc: 0.7371 - out_29_acc: 0.7371 - out_30_acc: 0.7393 - out_31_acc: 0.7378 - out_32_acc: 0.7399 - val_loss: 23.8349 - val_out_loss: 0.6512 - val_out_0_loss: 0.5216 - val_out_1_loss: 0.6773 - val_out_2_loss: 0.6798 - val_out_3_loss: 0.6784 - val_out_4_loss: 0.6796 - val_out_5_loss: 0.6734 - val_out_6_loss: 0.6773 - val_out_7_loss: 0.6757 - val_out_8_loss: 0.6784 - val_out_9_loss: 0.6763 - val_out_10_loss: 0.6835 - val_out_11_loss: 0.6776 - val_out_12_loss: 0.6774 - val_out_13_loss: 0.6788 - val_out_14_loss: 0.6792 - val_out_15_loss: 0.6802 - val_out_16_loss: 0.6780 - val_out_17_loss: 0.6786 - val_out_18_loss: 0.6795 - val_out_19_loss: 0.6826 - val_out_20_loss: 0.6801 - val_out_21_loss: 0.6814 - val_out_22_loss: 0.6821 - val_out_23_loss: 0.6757 - val_out_24_loss: 0.6831 - val_out_25_loss: 0.6812 - val_out_26_loss: 0.6768 - val_out_27_loss: 0.6776 - val_out_28_loss: 0.6785 - val_out_29_loss: 0.6795 - val_out_30_loss: 0.6786 - val_out_31_loss: 0.6780 - val_out_32_loss: 0.6746 - val_out_acc: 0.7549 - val_out_0_acc: 0.8438 - val_out_1_acc: 0.7549 - val_out_2_acc: 0.7462 - val_out_3_acc: 0.7505 - val_out_4_acc: 0.7527 - val_out_5_acc: 0.7570 - val_out_6_acc: 0.7549 - val_out_7_acc: 0.7527 - val_out_8_acc: 0.7527 - val_out_9_acc: 0.7527 - val_out_10_acc: 0.7527 - val_out_11_acc: 0.7527 - val_out_12_acc: 0.7505 - val_out_13_acc: 0.7549 - val_out_14_acc: 0.7570 - val_out_15_acc: 0.7527 - val_out_16_acc: 0.7462 - val_out_17_acc: 0.7484 - val_out_18_acc: 0.7527 - val_out_19_acc: 0.7484 - val_out_20_acc: 0.7527 - val_out_21_acc: 0.7505 - val_out_22_acc: 0.7484 - val_out_23_acc: 0.7527 - val_out_24_acc: 0.7549 - val_out_25_acc: 0.7484 - val_out_26_acc: 0.7527 - val_out_27_acc: 0.7570 - val_out_28_acc: 0.7505 - val_out_29_acc: 0.7505 - val_out_30_acc: 0.7484 - val_out_31_acc: 0.7527 - val_out_32_acc: 0.7505\n",
      "Epoch 142/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.3052 - out_loss: 0.6317 - out_0_loss: 0.4321 - out_1_loss: 0.7150 - out_2_loss: 0.7306 - out_3_loss: 0.7303 - out_4_loss: 0.7157 - out_5_loss: 0.7270 - out_6_loss: 0.7290 - out_7_loss: 0.7198 - out_8_loss: 0.7173 - out_9_loss: 0.7181 - out_10_loss: 0.7343 - out_11_loss: 0.7276 - out_12_loss: 0.7187 - out_13_loss: 0.7186 - out_14_loss: 0.7316 - out_15_loss: 0.7318 - out_16_loss: 0.7205 - out_17_loss: 0.7273 - out_18_loss: 0.7211 - out_19_loss: 0.7363 - out_20_loss: 0.7286 - out_21_loss: 0.7405 - out_22_loss: 0.7392 - out_23_loss: 0.7176 - out_24_loss: 0.7387 - out_25_loss: 0.7337 - out_26_loss: 0.7321 - out_27_loss: 0.7299 - out_28_loss: 0.7279 - out_29_loss: 0.7303 - out_30_loss: 0.7192 - out_31_loss: 0.7162 - out_32_loss: 0.7172 - out_acc: 0.7802 - out_0_acc: 0.8531 - out_1_acc: 0.7452 - out_2_acc: 0.7421 - out_3_acc: 0.7443 - out_4_acc: 0.7458 - out_5_acc: 0.7458 - out_6_acc: 0.7418 - out_7_acc: 0.7489 - out_8_acc: 0.7436 - out_9_acc: 0.7455 - out_10_acc: 0.7415 - out_11_acc: 0.7405 - out_12_acc: 0.7402 - out_13_acc: 0.7412 - out_14_acc: 0.7412 - out_15_acc: 0.7319 - out_16_acc: 0.7405 - out_17_acc: 0.7430 - out_18_acc: 0.7464 - out_19_acc: 0.7387 - out_20_acc: 0.7405 - out_21_acc: 0.7322 - out_22_acc: 0.7458 - out_23_acc: 0.7446 - out_24_acc: 0.7440 - out_25_acc: 0.7378 - out_26_acc: 0.7415 - out_27_acc: 0.7424 - out_28_acc: 0.7452 - out_29_acc: 0.7399 - out_30_acc: 0.7390 - out_31_acc: 0.7483 - out_32_acc: 0.7409 - val_loss: 22.9947 - val_out_loss: 0.6391 - val_out_0_loss: 0.4638 - val_out_1_loss: 0.6562 - val_out_2_loss: 0.6543 - val_out_3_loss: 0.6533 - val_out_4_loss: 0.6577 - val_out_5_loss: 0.6539 - val_out_6_loss: 0.6559 - val_out_7_loss: 0.6516 - val_out_8_loss: 0.6540 - val_out_9_loss: 0.6543 - val_out_10_loss: 0.6552 - val_out_11_loss: 0.6557 - val_out_12_loss: 0.6547 - val_out_13_loss: 0.6535 - val_out_14_loss: 0.6553 - val_out_15_loss: 0.6609 - val_out_16_loss: 0.6551 - val_out_17_loss: 0.6576 - val_out_18_loss: 0.6558 - val_out_19_loss: 0.6613 - val_out_20_loss: 0.6555 - val_out_21_loss: 0.6567 - val_out_22_loss: 0.6613 - val_out_23_loss: 0.6556 - val_out_24_loss: 0.6561 - val_out_25_loss: 0.6579 - val_out_26_loss: 0.6545 - val_out_27_loss: 0.6545 - val_out_28_loss: 0.6549 - val_out_29_loss: 0.6527 - val_out_30_loss: 0.6577 - val_out_31_loss: 0.6522 - val_out_32_loss: 0.6555 - val_out_acc: 0.7527 - val_out_0_acc: 0.8308 - val_out_1_acc: 0.7484 - val_out_2_acc: 0.7440 - val_out_3_acc: 0.7484 - val_out_4_acc: 0.7505 - val_out_5_acc: 0.7484 - val_out_6_acc: 0.7505 - val_out_7_acc: 0.7505 - val_out_8_acc: 0.7440 - val_out_9_acc: 0.7527 - val_out_10_acc: 0.7505 - val_out_11_acc: 0.7462 - val_out_12_acc: 0.7505 - val_out_13_acc: 0.7462 - val_out_14_acc: 0.7505 - val_out_15_acc: 0.7462 - val_out_16_acc: 0.7462 - val_out_17_acc: 0.7440 - val_out_18_acc: 0.7462 - val_out_19_acc: 0.7440 - val_out_20_acc: 0.7484 - val_out_21_acc: 0.7505 - val_out_22_acc: 0.7549 - val_out_23_acc: 0.7484 - val_out_24_acc: 0.7397 - val_out_25_acc: 0.7527 - val_out_26_acc: 0.7462 - val_out_27_acc: 0.7527 - val_out_28_acc: 0.7440 - val_out_29_acc: 0.7484 - val_out_30_acc: 0.7419 - val_out_31_acc: 0.7527 - val_out_32_acc: 0.7462\n",
      "Epoch 143/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.3252 - out_loss: 0.6340 - out_0_loss: 0.4430 - out_1_loss: 0.7252 - out_2_loss: 0.7241 - out_3_loss: 0.7358 - out_4_loss: 0.7115 - out_5_loss: 0.7270 - out_6_loss: 0.7319 - out_7_loss: 0.7267 - out_8_loss: 0.7175 - out_9_loss: 0.7272 - out_10_loss: 0.7326 - out_11_loss: 0.7274 - out_12_loss: 0.7179 - out_13_loss: 0.7334 - out_14_loss: 0.7379 - out_15_loss: 0.7294 - out_16_loss: 0.7286 - out_17_loss: 0.7294 - out_18_loss: 0.7340 - out_19_loss: 0.7286 - out_20_loss: 0.7271 - out_21_loss: 0.7245 - out_22_loss: 0.7310 - out_23_loss: 0.7170 - out_24_loss: 0.7291 - out_25_loss: 0.7289 - out_26_loss: 0.7136 - out_27_loss: 0.7259 - out_28_loss: 0.7272 - out_29_loss: 0.7233 - out_30_loss: 0.7205 - out_31_loss: 0.7329 - out_32_loss: 0.7209 - out_acc: 0.7799 - out_0_acc: 0.8487 - out_1_acc: 0.7433 - out_2_acc: 0.7471 - out_3_acc: 0.7396 - out_4_acc: 0.7455 - out_5_acc: 0.7359 - out_6_acc: 0.7421 - out_7_acc: 0.7436 - out_8_acc: 0.7489 - out_9_acc: 0.7350 - out_10_acc: 0.7402 - out_11_acc: 0.7424 - out_12_acc: 0.7436 - out_13_acc: 0.7433 - out_14_acc: 0.7430 - out_15_acc: 0.7461 - out_16_acc: 0.7446 - out_17_acc: 0.7430 - out_18_acc: 0.7424 - out_19_acc: 0.7384 - out_20_acc: 0.7433 - out_21_acc: 0.7390 - out_22_acc: 0.7381 - out_23_acc: 0.7464 - out_24_acc: 0.7359 - out_25_acc: 0.7412 - out_26_acc: 0.7508 - out_27_acc: 0.7486 - out_28_acc: 0.7424 - out_29_acc: 0.7467 - out_30_acc: 0.7436 - out_31_acc: 0.7421 - out_32_acc: 0.7430 - val_loss: 22.6821 - val_out_loss: 0.6225 - val_out_0_loss: 0.4069 - val_out_1_loss: 0.6471 - val_out_2_loss: 0.6508 - val_out_3_loss: 0.6443 - val_out_4_loss: 0.6502 - val_out_5_loss: 0.6459 - val_out_6_loss: 0.6488 - val_out_7_loss: 0.6485 - val_out_8_loss: 0.6457 - val_out_9_loss: 0.6496 - val_out_10_loss: 0.6505 - val_out_11_loss: 0.6459 - val_out_12_loss: 0.6454 - val_out_13_loss: 0.6504 - val_out_14_loss: 0.6478 - val_out_15_loss: 0.6482 - val_out_16_loss: 0.6488 - val_out_17_loss: 0.6474 - val_out_18_loss: 0.6468 - val_out_19_loss: 0.6511 - val_out_20_loss: 0.6517 - val_out_21_loss: 0.6495 - val_out_22_loss: 0.6523 - val_out_23_loss: 0.6505 - val_out_24_loss: 0.6522 - val_out_25_loss: 0.6523 - val_out_26_loss: 0.6512 - val_out_27_loss: 0.6516 - val_out_28_loss: 0.6509 - val_out_29_loss: 0.6413 - val_out_30_loss: 0.6446 - val_out_31_loss: 0.6466 - val_out_32_loss: 0.6467 - val_out_acc: 0.7766 - val_out_0_acc: 0.8503 - val_out_1_acc: 0.7744 - val_out_2_acc: 0.7787 - val_out_3_acc: 0.7744 - val_out_4_acc: 0.7831 - val_out_5_acc: 0.7787 - val_out_6_acc: 0.7766 - val_out_7_acc: 0.7809 - val_out_8_acc: 0.7831 - val_out_9_acc: 0.7744 - val_out_10_acc: 0.7787 - val_out_11_acc: 0.7722 - val_out_12_acc: 0.7809 - val_out_13_acc: 0.7787 - val_out_14_acc: 0.7701 - val_out_15_acc: 0.7831 - val_out_16_acc: 0.7722 - val_out_17_acc: 0.7722 - val_out_18_acc: 0.7722 - val_out_19_acc: 0.7744 - val_out_20_acc: 0.7766 - val_out_21_acc: 0.7809 - val_out_22_acc: 0.7722 - val_out_23_acc: 0.7766 - val_out_24_acc: 0.7722 - val_out_25_acc: 0.7636 - val_out_26_acc: 0.7809 - val_out_27_acc: 0.7766 - val_out_28_acc: 0.7831 - val_out_29_acc: 0.7722 - val_out_30_acc: 0.7766 - val_out_31_acc: 0.7744 - val_out_32_acc: 0.7766\n",
      "Epoch 144/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.2769 - out_loss: 0.6329 - out_0_loss: 0.4310 - out_1_loss: 0.7132 - out_2_loss: 0.7193 - out_3_loss: 0.7320 - out_4_loss: 0.7320 - out_5_loss: 0.7309 - out_6_loss: 0.7264 - out_7_loss: 0.7248 - out_8_loss: 0.7264 - out_9_loss: 0.7180 - out_10_loss: 0.7251 - out_11_loss: 0.7254 - out_12_loss: 0.7130 - out_13_loss: 0.7296 - out_14_loss: 0.7326 - out_15_loss: 0.7194 - out_16_loss: 0.7342 - out_17_loss: 0.7392 - out_18_loss: 0.7394 - out_19_loss: 0.7252 - out_20_loss: 0.7243 - out_21_loss: 0.7412 - out_22_loss: 0.7264 - out_23_loss: 0.7231 - out_24_loss: 0.7159 - out_25_loss: 0.7253 - out_26_loss: 0.7224 - out_27_loss: 0.7154 - out_28_loss: 0.7220 - out_29_loss: 0.7272 - out_30_loss: 0.7207 - out_31_loss: 0.7243 - out_32_loss: 0.7185 - out_acc: 0.7694 - out_0_acc: 0.8441 - out_1_acc: 0.7374 - out_2_acc: 0.7374 - out_3_acc: 0.7362 - out_4_acc: 0.7291 - out_5_acc: 0.7378 - out_6_acc: 0.7325 - out_7_acc: 0.7371 - out_8_acc: 0.7365 - out_9_acc: 0.7378 - out_10_acc: 0.7331 - out_11_acc: 0.7343 - out_12_acc: 0.7374 - out_13_acc: 0.7353 - out_14_acc: 0.7384 - out_15_acc: 0.7362 - out_16_acc: 0.7238 - out_17_acc: 0.7288 - out_18_acc: 0.7263 - out_19_acc: 0.7396 - out_20_acc: 0.7393 - out_21_acc: 0.7229 - out_22_acc: 0.7347 - out_23_acc: 0.7322 - out_24_acc: 0.7446 - out_25_acc: 0.7378 - out_26_acc: 0.7390 - out_27_acc: 0.7356 - out_28_acc: 0.7365 - out_29_acc: 0.7322 - out_30_acc: 0.7312 - out_31_acc: 0.7334 - out_32_acc: 0.7312 - val_loss: 23.3411 - val_out_loss: 0.6416 - val_out_0_loss: 0.4153 - val_out_1_loss: 0.6646 - val_out_2_loss: 0.6715 - val_out_3_loss: 0.6676 - val_out_4_loss: 0.6642 - val_out_5_loss: 0.6666 - val_out_6_loss: 0.6684 - val_out_7_loss: 0.6677 - val_out_8_loss: 0.6642 - val_out_9_loss: 0.6696 - val_out_10_loss: 0.6683 - val_out_11_loss: 0.6672 - val_out_12_loss: 0.6627 - val_out_13_loss: 0.6702 - val_out_14_loss: 0.6735 - val_out_15_loss: 0.6703 - val_out_16_loss: 0.6622 - val_out_17_loss: 0.6662 - val_out_18_loss: 0.6657 - val_out_19_loss: 0.6721 - val_out_20_loss: 0.6682 - val_out_21_loss: 0.6657 - val_out_22_loss: 0.6698 - val_out_23_loss: 0.6667 - val_out_24_loss: 0.6714 - val_out_25_loss: 0.6738 - val_out_26_loss: 0.6709 - val_out_27_loss: 0.6672 - val_out_28_loss: 0.6673 - val_out_29_loss: 0.6677 - val_out_30_loss: 0.6623 - val_out_31_loss: 0.6612 - val_out_32_loss: 0.6653 - val_out_acc: 0.7505 - val_out_0_acc: 0.8286 - val_out_1_acc: 0.7505 - val_out_2_acc: 0.7484 - val_out_3_acc: 0.7484 - val_out_4_acc: 0.7462 - val_out_5_acc: 0.7484 - val_out_6_acc: 0.7505 - val_out_7_acc: 0.7549 - val_out_8_acc: 0.7484 - val_out_9_acc: 0.7549 - val_out_10_acc: 0.7570 - val_out_11_acc: 0.7484 - val_out_12_acc: 0.7570 - val_out_13_acc: 0.7484 - val_out_14_acc: 0.7484 - val_out_15_acc: 0.7484 - val_out_16_acc: 0.7462 - val_out_17_acc: 0.7505 - val_out_18_acc: 0.7505 - val_out_19_acc: 0.7462 - val_out_20_acc: 0.7484 - val_out_21_acc: 0.7570 - val_out_22_acc: 0.7505 - val_out_23_acc: 0.7570 - val_out_24_acc: 0.7505 - val_out_25_acc: 0.7440 - val_out_26_acc: 0.7484 - val_out_27_acc: 0.7462 - val_out_28_acc: 0.7462 - val_out_29_acc: 0.7505 - val_out_30_acc: 0.7462 - val_out_31_acc: 0.7505 - val_out_32_acc: 0.7549\n",
      "Epoch 145/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 23.8119 - out_loss: 0.6205 - out_0_loss: 0.4095 - out_1_loss: 0.6944 - out_2_loss: 0.7090 - out_3_loss: 0.7175 - out_4_loss: 0.7263 - out_5_loss: 0.7073 - out_6_loss: 0.7151 - out_7_loss: 0.7052 - out_8_loss: 0.7068 - out_9_loss: 0.6980 - out_10_loss: 0.7134 - out_11_loss: 0.7097 - out_12_loss: 0.7119 - out_13_loss: 0.7076 - out_14_loss: 0.7136 - out_15_loss: 0.7231 - out_16_loss: 0.7123 - out_17_loss: 0.7189 - out_18_loss: 0.7181 - out_19_loss: 0.7132 - out_20_loss: 0.7173 - out_21_loss: 0.7077 - out_22_loss: 0.7164 - out_23_loss: 0.7139 - out_24_loss: 0.7073 - out_25_loss: 0.7143 - out_26_loss: 0.7067 - out_27_loss: 0.7240 - out_28_loss: 0.7036 - out_29_loss: 0.7148 - out_30_loss: 0.7211 - out_31_loss: 0.7146 - out_32_loss: 0.6987 - out_acc: 0.7833 - out_0_acc: 0.8568 - out_1_acc: 0.7542 - out_2_acc: 0.7548 - out_3_acc: 0.7502 - out_4_acc: 0.7421 - out_5_acc: 0.7529 - out_6_acc: 0.7458 - out_7_acc: 0.7418 - out_8_acc: 0.7523 - out_9_acc: 0.7489 - out_10_acc: 0.7467 - out_11_acc: 0.7440 - out_12_acc: 0.7452 - out_13_acc: 0.7461 - out_14_acc: 0.7412 - out_15_acc: 0.7446 - out_16_acc: 0.7514 - out_17_acc: 0.7533 - out_18_acc: 0.7483 - out_19_acc: 0.7443 - out_20_acc: 0.7492 - out_21_acc: 0.7474 - out_22_acc: 0.7486 - out_23_acc: 0.7486 - out_24_acc: 0.7508 - out_25_acc: 0.7396 - out_26_acc: 0.7433 - out_27_acc: 0.7474 - out_28_acc: 0.7539 - out_29_acc: 0.7436 - out_30_acc: 0.7421 - out_31_acc: 0.7458 - out_32_acc: 0.7514 - val_loss: 24.9220 - val_out_loss: 0.6809 - val_out_0_loss: 0.4608 - val_out_1_loss: 0.7167 - val_out_2_loss: 0.7127 - val_out_3_loss: 0.7137 - val_out_4_loss: 0.7171 - val_out_5_loss: 0.7153 - val_out_6_loss: 0.7085 - val_out_7_loss: 0.7100 - val_out_8_loss: 0.7121 - val_out_9_loss: 0.7139 - val_out_10_loss: 0.7141 - val_out_11_loss: 0.7177 - val_out_12_loss: 0.7117 - val_out_13_loss: 0.7153 - val_out_14_loss: 0.7120 - val_out_15_loss: 0.7119 - val_out_16_loss: 0.7113 - val_out_17_loss: 0.7107 - val_out_18_loss: 0.7138 - val_out_19_loss: 0.7169 - val_out_20_loss: 0.7114 - val_out_21_loss: 0.7096 - val_out_22_loss: 0.7146 - val_out_23_loss: 0.7098 - val_out_24_loss: 0.7092 - val_out_25_loss: 0.7081 - val_out_26_loss: 0.7156 - val_out_27_loss: 0.7112 - val_out_28_loss: 0.7112 - val_out_29_loss: 0.7101 - val_out_30_loss: 0.7099 - val_out_31_loss: 0.7058 - val_out_32_loss: 0.7120 - val_out_acc: 0.7245 - val_out_0_acc: 0.8048 - val_out_1_acc: 0.7289 - val_out_2_acc: 0.7202 - val_out_3_acc: 0.7202 - val_out_4_acc: 0.7180 - val_out_5_acc: 0.7202 - val_out_6_acc: 0.7267 - val_out_7_acc: 0.7267 - val_out_8_acc: 0.7245 - val_out_9_acc: 0.7223 - val_out_10_acc: 0.7223 - val_out_11_acc: 0.7158 - val_out_12_acc: 0.7267 - val_out_13_acc: 0.7267 - val_out_14_acc: 0.7180 - val_out_15_acc: 0.7158 - val_out_16_acc: 0.7202 - val_out_17_acc: 0.7202 - val_out_18_acc: 0.7223 - val_out_19_acc: 0.7202 - val_out_20_acc: 0.7180 - val_out_21_acc: 0.7202 - val_out_22_acc: 0.7202 - val_out_23_acc: 0.7245 - val_out_24_acc: 0.7223 - val_out_25_acc: 0.7267 - val_out_26_acc: 0.7267 - val_out_27_acc: 0.7267 - val_out_28_acc: 0.7223 - val_out_29_acc: 0.7267 - val_out_30_acc: 0.7223 - val_out_31_acc: 0.7223 - val_out_32_acc: 0.7267\n",
      "Epoch 146/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.1971 - out_loss: 0.6274 - out_0_loss: 0.4172 - out_1_loss: 0.7242 - out_2_loss: 0.7238 - out_3_loss: 0.7320 - out_4_loss: 0.7263 - out_5_loss: 0.7318 - out_6_loss: 0.7381 - out_7_loss: 0.7175 - out_8_loss: 0.7186 - out_9_loss: 0.7176 - out_10_loss: 0.7261 - out_11_loss: 0.7171 - out_12_loss: 0.7386 - out_13_loss: 0.7172 - out_14_loss: 0.7223 - out_15_loss: 0.7223 - out_16_loss: 0.7285 - out_17_loss: 0.7369 - out_18_loss: 0.7335 - out_19_loss: 0.7118 - out_20_loss: 0.7217 - out_21_loss: 0.7145 - out_22_loss: 0.7138 - out_23_loss: 0.7245 - out_24_loss: 0.7241 - out_25_loss: 0.7337 - out_26_loss: 0.7297 - out_27_loss: 0.7213 - out_28_loss: 0.7110 - out_29_loss: 0.7191 - out_30_loss: 0.7164 - out_31_loss: 0.7288 - out_32_loss: 0.7096 - out_acc: 0.7750 - out_0_acc: 0.8462 - out_1_acc: 0.7409 - out_2_acc: 0.7409 - out_3_acc: 0.7399 - out_4_acc: 0.7415 - out_5_acc: 0.7331 - out_6_acc: 0.7343 - out_7_acc: 0.7446 - out_8_acc: 0.7467 - out_9_acc: 0.7396 - out_10_acc: 0.7384 - out_11_acc: 0.7446 - out_12_acc: 0.7328 - out_13_acc: 0.7390 - out_14_acc: 0.7443 - out_15_acc: 0.7489 - out_16_acc: 0.7399 - out_17_acc: 0.7356 - out_18_acc: 0.7362 - out_19_acc: 0.7455 - out_20_acc: 0.7427 - out_21_acc: 0.7415 - out_22_acc: 0.7471 - out_23_acc: 0.7433 - out_24_acc: 0.7368 - out_25_acc: 0.7436 - out_26_acc: 0.7443 - out_27_acc: 0.7418 - out_28_acc: 0.7464 - out_29_acc: 0.7362 - out_30_acc: 0.7424 - out_31_acc: 0.7396 - out_32_acc: 0.7440 - val_loss: 22.0174 - val_out_loss: 0.6061 - val_out_0_loss: 0.4134 - val_out_1_loss: 0.6276 - val_out_2_loss: 0.6281 - val_out_3_loss: 0.6299 - val_out_4_loss: 0.6317 - val_out_5_loss: 0.6244 - val_out_6_loss: 0.6308 - val_out_7_loss: 0.6298 - val_out_8_loss: 0.6257 - val_out_9_loss: 0.6269 - val_out_10_loss: 0.6302 - val_out_11_loss: 0.6292 - val_out_12_loss: 0.6259 - val_out_13_loss: 0.6272 - val_out_14_loss: 0.6324 - val_out_15_loss: 0.6278 - val_out_16_loss: 0.6299 - val_out_17_loss: 0.6289 - val_out_18_loss: 0.6297 - val_out_19_loss: 0.6273 - val_out_20_loss: 0.6286 - val_out_21_loss: 0.6284 - val_out_22_loss: 0.6318 - val_out_23_loss: 0.6305 - val_out_24_loss: 0.6291 - val_out_25_loss: 0.6327 - val_out_26_loss: 0.6311 - val_out_27_loss: 0.6262 - val_out_28_loss: 0.6322 - val_out_29_loss: 0.6282 - val_out_30_loss: 0.6303 - val_out_31_loss: 0.6271 - val_out_32_loss: 0.6267 - val_out_acc: 0.7766 - val_out_0_acc: 0.8395 - val_out_1_acc: 0.7657 - val_out_2_acc: 0.7679 - val_out_3_acc: 0.7679 - val_out_4_acc: 0.7657 - val_out_5_acc: 0.7701 - val_out_6_acc: 0.7679 - val_out_7_acc: 0.7592 - val_out_8_acc: 0.7679 - val_out_9_acc: 0.7592 - val_out_10_acc: 0.7657 - val_out_11_acc: 0.7636 - val_out_12_acc: 0.7722 - val_out_13_acc: 0.7614 - val_out_14_acc: 0.7679 - val_out_15_acc: 0.7614 - val_out_16_acc: 0.7657 - val_out_17_acc: 0.7592 - val_out_18_acc: 0.7679 - val_out_19_acc: 0.7679 - val_out_20_acc: 0.7636 - val_out_21_acc: 0.7701 - val_out_22_acc: 0.7679 - val_out_23_acc: 0.7636 - val_out_24_acc: 0.7614 - val_out_25_acc: 0.7679 - val_out_26_acc: 0.7679 - val_out_27_acc: 0.7636 - val_out_28_acc: 0.7592 - val_out_29_acc: 0.7679 - val_out_30_acc: 0.7701 - val_out_31_acc: 0.7701 - val_out_32_acc: 0.7636\n",
      "Epoch 147/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 23.8893 - out_loss: 0.6209 - out_0_loss: 0.4137 - out_1_loss: 0.7115 - out_2_loss: 0.7212 - out_3_loss: 0.7124 - out_4_loss: 0.7197 - out_5_loss: 0.7227 - out_6_loss: 0.7133 - out_7_loss: 0.7118 - out_8_loss: 0.7249 - out_9_loss: 0.7131 - out_10_loss: 0.7102 - out_11_loss: 0.7080 - out_12_loss: 0.7172 - out_13_loss: 0.7153 - out_14_loss: 0.7235 - out_15_loss: 0.7208 - out_16_loss: 0.7140 - out_17_loss: 0.7057 - out_18_loss: 0.7042 - out_19_loss: 0.7059 - out_20_loss: 0.6944 - out_21_loss: 0.7068 - out_22_loss: 0.7248 - out_23_loss: 0.7167 - out_24_loss: 0.7197 - out_25_loss: 0.7193 - out_26_loss: 0.7135 - out_27_loss: 0.7119 - out_28_loss: 0.7024 - out_29_loss: 0.7270 - out_30_loss: 0.7215 - out_31_loss: 0.7130 - out_32_loss: 0.7085 - out_acc: 0.7836 - out_0_acc: 0.8562 - out_1_acc: 0.7440 - out_2_acc: 0.7393 - out_3_acc: 0.7433 - out_4_acc: 0.7412 - out_5_acc: 0.7396 - out_6_acc: 0.7412 - out_7_acc: 0.7480 - out_8_acc: 0.7402 - out_9_acc: 0.7427 - out_10_acc: 0.7502 - out_11_acc: 0.7483 - out_12_acc: 0.7393 - out_13_acc: 0.7449 - out_14_acc: 0.7381 - out_15_acc: 0.7526 - out_16_acc: 0.7452 - out_17_acc: 0.7480 - out_18_acc: 0.7467 - out_19_acc: 0.7486 - out_20_acc: 0.7542 - out_21_acc: 0.7511 - out_22_acc: 0.7412 - out_23_acc: 0.7477 - out_24_acc: 0.7430 - out_25_acc: 0.7412 - out_26_acc: 0.7523 - out_27_acc: 0.7474 - out_28_acc: 0.7502 - out_29_acc: 0.7436 - out_30_acc: 0.7409 - out_31_acc: 0.7467 - out_32_acc: 0.7495 - val_loss: 25.9900 - val_out_loss: 0.7141 - val_out_0_loss: 0.4856 - val_out_1_loss: 0.7435 - val_out_2_loss: 0.7421 - val_out_3_loss: 0.7414 - val_out_4_loss: 0.7449 - val_out_5_loss: 0.7405 - val_out_6_loss: 0.7434 - val_out_7_loss: 0.7485 - val_out_8_loss: 0.7429 - val_out_9_loss: 0.7368 - val_out_10_loss: 0.7489 - val_out_11_loss: 0.7380 - val_out_12_loss: 0.7407 - val_out_13_loss: 0.7467 - val_out_14_loss: 0.7453 - val_out_15_loss: 0.7389 - val_out_16_loss: 0.7410 - val_out_17_loss: 0.7453 - val_out_18_loss: 0.7430 - val_out_19_loss: 0.7402 - val_out_20_loss: 0.7454 - val_out_21_loss: 0.7391 - val_out_22_loss: 0.7398 - val_out_23_loss: 0.7418 - val_out_24_loss: 0.7463 - val_out_25_loss: 0.7409 - val_out_26_loss: 0.7464 - val_out_27_loss: 0.7372 - val_out_28_loss: 0.7426 - val_out_29_loss: 0.7391 - val_out_30_loss: 0.7490 - val_out_31_loss: 0.7399 - val_out_32_loss: 0.7416 - val_out_acc: 0.7050 - val_out_0_acc: 0.8351 - val_out_1_acc: 0.6963 - val_out_2_acc: 0.6920 - val_out_3_acc: 0.7007 - val_out_4_acc: 0.6985 - val_out_5_acc: 0.7028 - val_out_6_acc: 0.7007 - val_out_7_acc: 0.6920 - val_out_8_acc: 0.7007 - val_out_9_acc: 0.7050 - val_out_10_acc: 0.6855 - val_out_11_acc: 0.6941 - val_out_12_acc: 0.7072 - val_out_13_acc: 0.6876 - val_out_14_acc: 0.7007 - val_out_15_acc: 0.6985 - val_out_16_acc: 0.7007 - val_out_17_acc: 0.6941 - val_out_18_acc: 0.6855 - val_out_19_acc: 0.6963 - val_out_20_acc: 0.6941 - val_out_21_acc: 0.6941 - val_out_22_acc: 0.7007 - val_out_23_acc: 0.6941 - val_out_24_acc: 0.6855 - val_out_25_acc: 0.6963 - val_out_26_acc: 0.6985 - val_out_27_acc: 0.6985 - val_out_28_acc: 0.6920 - val_out_29_acc: 0.6985 - val_out_30_acc: 0.6985 - val_out_31_acc: 0.7028 - val_out_32_acc: 0.7007\n",
      "Epoch 148/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 23.9044 - out_loss: 0.6198 - out_0_loss: 0.4122 - out_1_loss: 0.7096 - out_2_loss: 0.7140 - out_3_loss: 0.7113 - out_4_loss: 0.7179 - out_5_loss: 0.7157 - out_6_loss: 0.7235 - out_7_loss: 0.7142 - out_8_loss: 0.7265 - out_9_loss: 0.7225 - out_10_loss: 0.7274 - out_11_loss: 0.7042 - out_12_loss: 0.7133 - out_13_loss: 0.7132 - out_14_loss: 0.7061 - out_15_loss: 0.7167 - out_16_loss: 0.7149 - out_17_loss: 0.7150 - out_18_loss: 0.7142 - out_19_loss: 0.7230 - out_20_loss: 0.7095 - out_21_loss: 0.7198 - out_22_loss: 0.7016 - out_23_loss: 0.7198 - out_24_loss: 0.7022 - out_25_loss: 0.7223 - out_26_loss: 0.7110 - out_27_loss: 0.7199 - out_28_loss: 0.7153 - out_29_loss: 0.7173 - out_30_loss: 0.6966 - out_31_loss: 0.7284 - out_32_loss: 0.7057 - out_acc: 0.7805 - out_0_acc: 0.8559 - out_1_acc: 0.7483 - out_2_acc: 0.7412 - out_3_acc: 0.7467 - out_4_acc: 0.7486 - out_5_acc: 0.7467 - out_6_acc: 0.7508 - out_7_acc: 0.7455 - out_8_acc: 0.7464 - out_9_acc: 0.7492 - out_10_acc: 0.7539 - out_11_acc: 0.7446 - out_12_acc: 0.7458 - out_13_acc: 0.7452 - out_14_acc: 0.7418 - out_15_acc: 0.7402 - out_16_acc: 0.7467 - out_17_acc: 0.7489 - out_18_acc: 0.7446 - out_19_acc: 0.7471 - out_20_acc: 0.7480 - out_21_acc: 0.7443 - out_22_acc: 0.7517 - out_23_acc: 0.7446 - out_24_acc: 0.7486 - out_25_acc: 0.7409 - out_26_acc: 0.7464 - out_27_acc: 0.7474 - out_28_acc: 0.7474 - out_29_acc: 0.7502 - out_30_acc: 0.7508 - out_31_acc: 0.7350 - out_32_acc: 0.7436 - val_loss: 23.7864 - val_out_loss: 0.6471 - val_out_0_loss: 0.4439 - val_out_1_loss: 0.6851 - val_out_2_loss: 0.6796 - val_out_3_loss: 0.6814 - val_out_4_loss: 0.6830 - val_out_5_loss: 0.6790 - val_out_6_loss: 0.6791 - val_out_7_loss: 0.6763 - val_out_8_loss: 0.6835 - val_out_9_loss: 0.6750 - val_out_10_loss: 0.6805 - val_out_11_loss: 0.6785 - val_out_12_loss: 0.6778 - val_out_13_loss: 0.6875 - val_out_14_loss: 0.6780 - val_out_15_loss: 0.6787 - val_out_16_loss: 0.6786 - val_out_17_loss: 0.6812 - val_out_18_loss: 0.6795 - val_out_19_loss: 0.6799 - val_out_20_loss: 0.6825 - val_out_21_loss: 0.6796 - val_out_22_loss: 0.6792 - val_out_23_loss: 0.6793 - val_out_24_loss: 0.6791 - val_out_25_loss: 0.6847 - val_out_26_loss: 0.6812 - val_out_27_loss: 0.6782 - val_out_28_loss: 0.6775 - val_out_29_loss: 0.6767 - val_out_30_loss: 0.6795 - val_out_31_loss: 0.6763 - val_out_32_loss: 0.6781 - val_out_acc: 0.7614 - val_out_0_acc: 0.8373 - val_out_1_acc: 0.7527 - val_out_2_acc: 0.7549 - val_out_3_acc: 0.7614 - val_out_4_acc: 0.7505 - val_out_5_acc: 0.7570 - val_out_6_acc: 0.7636 - val_out_7_acc: 0.7549 - val_out_8_acc: 0.7527 - val_out_9_acc: 0.7657 - val_out_10_acc: 0.7549 - val_out_11_acc: 0.7592 - val_out_12_acc: 0.7636 - val_out_13_acc: 0.7592 - val_out_14_acc: 0.7527 - val_out_15_acc: 0.7527 - val_out_16_acc: 0.7505 - val_out_17_acc: 0.7592 - val_out_18_acc: 0.7527 - val_out_19_acc: 0.7527 - val_out_20_acc: 0.7527 - val_out_21_acc: 0.7592 - val_out_22_acc: 0.7570 - val_out_23_acc: 0.7505 - val_out_24_acc: 0.7549 - val_out_25_acc: 0.7527 - val_out_26_acc: 0.7462 - val_out_27_acc: 0.7527 - val_out_28_acc: 0.7657 - val_out_29_acc: 0.7505 - val_out_30_acc: 0.7592 - val_out_31_acc: 0.7527 - val_out_32_acc: 0.7570\n",
      "Epoch 149/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 23.6786 - out_loss: 0.6141 - out_0_loss: 0.4091 - out_1_loss: 0.7015 - out_2_loss: 0.7045 - out_3_loss: 0.7143 - out_4_loss: 0.7202 - out_5_loss: 0.7025 - out_6_loss: 0.7076 - out_7_loss: 0.7162 - out_8_loss: 0.7046 - out_9_loss: 0.7020 - out_10_loss: 0.7047 - out_11_loss: 0.7072 - out_12_loss: 0.7113 - out_13_loss: 0.6904 - out_14_loss: 0.7006 - out_15_loss: 0.7141 - out_16_loss: 0.7149 - out_17_loss: 0.6948 - out_18_loss: 0.7127 - out_19_loss: 0.7303 - out_20_loss: 0.7061 - out_21_loss: 0.7131 - out_22_loss: 0.7267 - out_23_loss: 0.7253 - out_24_loss: 0.6948 - out_25_loss: 0.7054 - out_26_loss: 0.7021 - out_27_loss: 0.6983 - out_28_loss: 0.7093 - out_29_loss: 0.7013 - out_30_loss: 0.7035 - out_31_loss: 0.7084 - out_32_loss: 0.7067 - out_acc: 0.7802 - out_0_acc: 0.8630 - out_1_acc: 0.7492 - out_2_acc: 0.7446 - out_3_acc: 0.7396 - out_4_acc: 0.7409 - out_5_acc: 0.7474 - out_6_acc: 0.7455 - out_7_acc: 0.7402 - out_8_acc: 0.7443 - out_9_acc: 0.7440 - out_10_acc: 0.7430 - out_11_acc: 0.7508 - out_12_acc: 0.7458 - out_13_acc: 0.7545 - out_14_acc: 0.7520 - out_15_acc: 0.7433 - out_16_acc: 0.7430 - out_17_acc: 0.7520 - out_18_acc: 0.7539 - out_19_acc: 0.7424 - out_20_acc: 0.7464 - out_21_acc: 0.7430 - out_22_acc: 0.7399 - out_23_acc: 0.7412 - out_24_acc: 0.7498 - out_25_acc: 0.7489 - out_26_acc: 0.7458 - out_27_acc: 0.7458 - out_28_acc: 0.7461 - out_29_acc: 0.7464 - out_30_acc: 0.7430 - out_31_acc: 0.7421 - out_32_acc: 0.7474 - val_loss: 22.8232 - val_out_loss: 0.6280 - val_out_0_loss: 0.4949 - val_out_1_loss: 0.6493 - val_out_2_loss: 0.6489 - val_out_3_loss: 0.6486 - val_out_4_loss: 0.6515 - val_out_5_loss: 0.6469 - val_out_6_loss: 0.6524 - val_out_7_loss: 0.6494 - val_out_8_loss: 0.6446 - val_out_9_loss: 0.6466 - val_out_10_loss: 0.6487 - val_out_11_loss: 0.6496 - val_out_12_loss: 0.6479 - val_out_13_loss: 0.6475 - val_out_14_loss: 0.6525 - val_out_15_loss: 0.6529 - val_out_16_loss: 0.6515 - val_out_17_loss: 0.6547 - val_out_18_loss: 0.6484 - val_out_19_loss: 0.6517 - val_out_20_loss: 0.6526 - val_out_21_loss: 0.6512 - val_out_22_loss: 0.6525 - val_out_23_loss: 0.6489 - val_out_24_loss: 0.6496 - val_out_25_loss: 0.6502 - val_out_26_loss: 0.6501 - val_out_27_loss: 0.6501 - val_out_28_loss: 0.6516 - val_out_29_loss: 0.6477 - val_out_30_loss: 0.6528 - val_out_31_loss: 0.6504 - val_out_32_loss: 0.6456 - val_out_acc: 0.7679 - val_out_0_acc: 0.8373 - val_out_1_acc: 0.7701 - val_out_2_acc: 0.7679 - val_out_3_acc: 0.7614 - val_out_4_acc: 0.7679 - val_out_5_acc: 0.7636 - val_out_6_acc: 0.7636 - val_out_7_acc: 0.7657 - val_out_8_acc: 0.7679 - val_out_9_acc: 0.7722 - val_out_10_acc: 0.7679 - val_out_11_acc: 0.7657 - val_out_12_acc: 0.7636 - val_out_13_acc: 0.7679 - val_out_14_acc: 0.7679 - val_out_15_acc: 0.7657 - val_out_16_acc: 0.7657 - val_out_17_acc: 0.7657 - val_out_18_acc: 0.7614 - val_out_19_acc: 0.7570 - val_out_20_acc: 0.7657 - val_out_21_acc: 0.7657 - val_out_22_acc: 0.7636 - val_out_23_acc: 0.7701 - val_out_24_acc: 0.7636 - val_out_25_acc: 0.7657 - val_out_26_acc: 0.7657 - val_out_27_acc: 0.7636 - val_out_28_acc: 0.7657 - val_out_29_acc: 0.7701 - val_out_30_acc: 0.7679 - val_out_31_acc: 0.7679 - val_out_32_acc: 0.7636\n",
      "Epoch 150/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 23.7410 - out_loss: 0.6138 - out_0_loss: 0.3831 - out_1_loss: 0.7222 - out_2_loss: 0.7126 - out_3_loss: 0.7203 - out_4_loss: 0.6982 - out_5_loss: 0.7161 - out_6_loss: 0.7076 - out_7_loss: 0.7126 - out_8_loss: 0.7107 - out_9_loss: 0.7209 - out_10_loss: 0.7043 - out_11_loss: 0.7108 - out_12_loss: 0.6989 - out_13_loss: 0.7094 - out_14_loss: 0.7151 - out_15_loss: 0.7103 - out_16_loss: 0.7003 - out_17_loss: 0.7194 - out_18_loss: 0.7144 - out_19_loss: 0.7179 - out_20_loss: 0.7050 - out_21_loss: 0.7017 - out_22_loss: 0.7072 - out_23_loss: 0.7158 - out_24_loss: 0.7223 - out_25_loss: 0.7078 - out_26_loss: 0.6982 - out_27_loss: 0.7124 - out_28_loss: 0.6960 - out_29_loss: 0.7226 - out_30_loss: 0.7159 - out_31_loss: 0.7118 - out_32_loss: 0.7053 - out_acc: 0.7846 - out_0_acc: 0.8633 - out_1_acc: 0.7492 - out_2_acc: 0.7495 - out_3_acc: 0.7539 - out_4_acc: 0.7567 - out_5_acc: 0.7477 - out_6_acc: 0.7461 - out_7_acc: 0.7433 - out_8_acc: 0.7464 - out_9_acc: 0.7458 - out_10_acc: 0.7529 - out_11_acc: 0.7477 - out_12_acc: 0.7486 - out_13_acc: 0.7533 - out_14_acc: 0.7467 - out_15_acc: 0.7520 - out_16_acc: 0.7582 - out_17_acc: 0.7458 - out_18_acc: 0.7511 - out_19_acc: 0.7461 - out_20_acc: 0.7480 - out_21_acc: 0.7464 - out_22_acc: 0.7483 - out_23_acc: 0.7520 - out_24_acc: 0.7517 - out_25_acc: 0.7523 - out_26_acc: 0.7492 - out_27_acc: 0.7461 - out_28_acc: 0.7595 - out_29_acc: 0.7498 - out_30_acc: 0.7449 - out_31_acc: 0.7461 - out_32_acc: 0.7505 - val_loss: 22.7901 - val_out_loss: 0.6214 - val_out_0_loss: 0.3870 - val_out_1_loss: 0.6522 - val_out_2_loss: 0.6515 - val_out_3_loss: 0.6517 - val_out_4_loss: 0.6538 - val_out_5_loss: 0.6505 - val_out_6_loss: 0.6519 - val_out_7_loss: 0.6501 - val_out_8_loss: 0.6512 - val_out_9_loss: 0.6542 - val_out_10_loss: 0.6521 - val_out_11_loss: 0.6494 - val_out_12_loss: 0.6487 - val_out_13_loss: 0.6525 - val_out_14_loss: 0.6550 - val_out_15_loss: 0.6549 - val_out_16_loss: 0.6516 - val_out_17_loss: 0.6554 - val_out_18_loss: 0.6553 - val_out_19_loss: 0.6586 - val_out_20_loss: 0.6545 - val_out_21_loss: 0.6500 - val_out_22_loss: 0.6507 - val_out_23_loss: 0.6509 - val_out_24_loss: 0.6542 - val_out_25_loss: 0.6534 - val_out_26_loss: 0.6524 - val_out_27_loss: 0.6517 - val_out_28_loss: 0.6568 - val_out_29_loss: 0.6489 - val_out_30_loss: 0.6546 - val_out_31_loss: 0.6492 - val_out_32_loss: 0.6517 - val_out_acc: 0.7679 - val_out_0_acc: 0.8373 - val_out_1_acc: 0.7657 - val_out_2_acc: 0.7657 - val_out_3_acc: 0.7701 - val_out_4_acc: 0.7722 - val_out_5_acc: 0.7701 - val_out_6_acc: 0.7657 - val_out_7_acc: 0.7657 - val_out_8_acc: 0.7701 - val_out_9_acc: 0.7679 - val_out_10_acc: 0.7701 - val_out_11_acc: 0.7722 - val_out_12_acc: 0.7744 - val_out_13_acc: 0.7679 - val_out_14_acc: 0.7701 - val_out_15_acc: 0.7701 - val_out_16_acc: 0.7636 - val_out_17_acc: 0.7636 - val_out_18_acc: 0.7657 - val_out_19_acc: 0.7614 - val_out_20_acc: 0.7636 - val_out_21_acc: 0.7636 - val_out_22_acc: 0.7722 - val_out_23_acc: 0.7701 - val_out_24_acc: 0.7679 - val_out_25_acc: 0.7722 - val_out_26_acc: 0.7701 - val_out_27_acc: 0.7636 - val_out_28_acc: 0.7744 - val_out_29_acc: 0.7679 - val_out_30_acc: 0.7614 - val_out_31_acc: 0.7657 - val_out_32_acc: 0.7636\n",
      "Epoch 151/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 23.5660 - out_loss: 0.6101 - out_0_loss: 0.3798 - out_1_loss: 0.6934 - out_2_loss: 0.7002 - out_3_loss: 0.7028 - out_4_loss: 0.6951 - out_5_loss: 0.7120 - out_6_loss: 0.7109 - out_7_loss: 0.7013 - out_8_loss: 0.7003 - out_9_loss: 0.6881 - out_10_loss: 0.7016 - out_11_loss: 0.7097 - out_12_loss: 0.7140 - out_13_loss: 0.7139 - out_14_loss: 0.7045 - out_15_loss: 0.7116 - out_16_loss: 0.7160 - out_17_loss: 0.7147 - out_18_loss: 0.7162 - out_19_loss: 0.6987 - out_20_loss: 0.6948 - out_21_loss: 0.7165 - out_22_loss: 0.7026 - out_23_loss: 0.7043 - out_24_loss: 0.6906 - out_25_loss: 0.7036 - out_26_loss: 0.7221 - out_27_loss: 0.7119 - out_28_loss: 0.7132 - out_29_loss: 0.7011 - out_30_loss: 0.7061 - out_31_loss: 0.7006 - out_32_loss: 0.7036 - out_acc: 0.7901 - out_0_acc: 0.8577 - out_1_acc: 0.7514 - out_2_acc: 0.7585 - out_3_acc: 0.7536 - out_4_acc: 0.7514 - out_5_acc: 0.7464 - out_6_acc: 0.7474 - out_7_acc: 0.7514 - out_8_acc: 0.7502 - out_9_acc: 0.7601 - out_10_acc: 0.7502 - out_11_acc: 0.7477 - out_12_acc: 0.7409 - out_13_acc: 0.7489 - out_14_acc: 0.7567 - out_15_acc: 0.7433 - out_16_acc: 0.7412 - out_17_acc: 0.7421 - out_18_acc: 0.7455 - out_19_acc: 0.7551 - out_20_acc: 0.7529 - out_21_acc: 0.7477 - out_22_acc: 0.7526 - out_23_acc: 0.7551 - out_24_acc: 0.7548 - out_25_acc: 0.7489 - out_26_acc: 0.7461 - out_27_acc: 0.7477 - out_28_acc: 0.7489 - out_29_acc: 0.7526 - out_30_acc: 0.7467 - out_31_acc: 0.7551 - out_32_acc: 0.7486 - val_loss: 22.7457 - val_out_loss: 0.6238 - val_out_0_loss: 0.3530 - val_out_1_loss: 0.6563 - val_out_2_loss: 0.6567 - val_out_3_loss: 0.6525 - val_out_4_loss: 0.6574 - val_out_5_loss: 0.6526 - val_out_6_loss: 0.6498 - val_out_7_loss: 0.6516 - val_out_8_loss: 0.6525 - val_out_9_loss: 0.6551 - val_out_10_loss: 0.6510 - val_out_11_loss: 0.6509 - val_out_12_loss: 0.6465 - val_out_13_loss: 0.6501 - val_out_14_loss: 0.6561 - val_out_15_loss: 0.6529 - val_out_16_loss: 0.6488 - val_out_17_loss: 0.6536 - val_out_18_loss: 0.6480 - val_out_19_loss: 0.6552 - val_out_20_loss: 0.6509 - val_out_21_loss: 0.6518 - val_out_22_loss: 0.6503 - val_out_23_loss: 0.6502 - val_out_24_loss: 0.6547 - val_out_25_loss: 0.6572 - val_out_26_loss: 0.6515 - val_out_27_loss: 0.6508 - val_out_28_loss: 0.6522 - val_out_29_loss: 0.6488 - val_out_30_loss: 0.6541 - val_out_31_loss: 0.6528 - val_out_32_loss: 0.6457 - val_out_acc: 0.7636 - val_out_0_acc: 0.8655 - val_out_1_acc: 0.7636 - val_out_2_acc: 0.7614 - val_out_3_acc: 0.7592 - val_out_4_acc: 0.7570 - val_out_5_acc: 0.7570 - val_out_6_acc: 0.7592 - val_out_7_acc: 0.7679 - val_out_8_acc: 0.7549 - val_out_9_acc: 0.7570 - val_out_10_acc: 0.7614 - val_out_11_acc: 0.7614 - val_out_12_acc: 0.7657 - val_out_13_acc: 0.7636 - val_out_14_acc: 0.7636 - val_out_15_acc: 0.7614 - val_out_16_acc: 0.7592 - val_out_17_acc: 0.7614 - val_out_18_acc: 0.7614 - val_out_19_acc: 0.7614 - val_out_20_acc: 0.7592 - val_out_21_acc: 0.7614 - val_out_22_acc: 0.7570 - val_out_23_acc: 0.7592 - val_out_24_acc: 0.7570 - val_out_25_acc: 0.7636 - val_out_26_acc: 0.7592 - val_out_27_acc: 0.7614 - val_out_28_acc: 0.7592 - val_out_29_acc: 0.7657 - val_out_30_acc: 0.7614 - val_out_31_acc: 0.7592 - val_out_32_acc: 0.7592\n",
      "Epoch 152/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 24.0085 - out_loss: 0.6233 - out_0_loss: 0.3991 - out_1_loss: 0.7153 - out_2_loss: 0.7285 - out_3_loss: 0.7410 - out_4_loss: 0.7221 - out_5_loss: 0.7137 - out_6_loss: 0.7424 - out_7_loss: 0.7298 - out_8_loss: 0.7276 - out_9_loss: 0.7085 - out_10_loss: 0.7216 - out_11_loss: 0.7226 - out_12_loss: 0.7146 - out_13_loss: 0.7180 - out_14_loss: 0.7148 - out_15_loss: 0.7019 - out_16_loss: 0.7147 - out_17_loss: 0.7175 - out_18_loss: 0.7118 - out_19_loss: 0.7204 - out_20_loss: 0.7129 - out_21_loss: 0.7245 - out_22_loss: 0.7235 - out_23_loss: 0.7180 - out_24_loss: 0.6958 - out_25_loss: 0.7199 - out_26_loss: 0.7121 - out_27_loss: 0.7236 - out_28_loss: 0.7138 - out_29_loss: 0.7181 - out_30_loss: 0.7240 - out_31_loss: 0.7076 - out_32_loss: 0.7059 - out_acc: 0.7830 - out_0_acc: 0.8608 - out_1_acc: 0.7424 - out_2_acc: 0.7322 - out_3_acc: 0.7452 - out_4_acc: 0.7467 - out_5_acc: 0.7424 - out_6_acc: 0.7371 - out_7_acc: 0.7390 - out_8_acc: 0.7353 - out_9_acc: 0.7446 - out_10_acc: 0.7371 - out_11_acc: 0.7427 - out_12_acc: 0.7505 - out_13_acc: 0.7424 - out_14_acc: 0.7477 - out_15_acc: 0.7511 - out_16_acc: 0.7402 - out_17_acc: 0.7368 - out_18_acc: 0.7424 - out_19_acc: 0.7440 - out_20_acc: 0.7430 - out_21_acc: 0.7421 - out_22_acc: 0.7502 - out_23_acc: 0.7433 - out_24_acc: 0.7492 - out_25_acc: 0.7446 - out_26_acc: 0.7529 - out_27_acc: 0.7384 - out_28_acc: 0.7449 - out_29_acc: 0.7443 - out_30_acc: 0.7436 - out_31_acc: 0.7495 - out_32_acc: 0.7557 - val_loss: 22.0699 - val_out_loss: 0.6049 - val_out_0_loss: 0.4567 - val_out_1_loss: 0.6274 - val_out_2_loss: 0.6303 - val_out_3_loss: 0.6273 - val_out_4_loss: 0.6298 - val_out_5_loss: 0.6280 - val_out_6_loss: 0.6291 - val_out_7_loss: 0.6295 - val_out_8_loss: 0.6271 - val_out_9_loss: 0.6292 - val_out_10_loss: 0.6328 - val_out_11_loss: 0.6284 - val_out_12_loss: 0.6304 - val_out_13_loss: 0.6278 - val_out_14_loss: 0.6314 - val_out_15_loss: 0.6314 - val_out_16_loss: 0.6291 - val_out_17_loss: 0.6324 - val_out_18_loss: 0.6253 - val_out_19_loss: 0.6279 - val_out_20_loss: 0.6303 - val_out_21_loss: 0.6298 - val_out_22_loss: 0.6315 - val_out_23_loss: 0.6272 - val_out_24_loss: 0.6285 - val_out_25_loss: 0.6318 - val_out_26_loss: 0.6306 - val_out_27_loss: 0.6304 - val_out_28_loss: 0.6293 - val_out_29_loss: 0.6276 - val_out_30_loss: 0.6269 - val_out_31_loss: 0.6280 - val_out_32_loss: 0.6280 - val_out_acc: 0.7939 - val_out_0_acc: 0.8525 - val_out_1_acc: 0.7852 - val_out_2_acc: 0.7852 - val_out_3_acc: 0.7896 - val_out_4_acc: 0.7874 - val_out_5_acc: 0.7852 - val_out_6_acc: 0.7874 - val_out_7_acc: 0.7831 - val_out_8_acc: 0.7896 - val_out_9_acc: 0.7874 - val_out_10_acc: 0.7787 - val_out_11_acc: 0.7874 - val_out_12_acc: 0.7831 - val_out_13_acc: 0.7874 - val_out_14_acc: 0.7831 - val_out_15_acc: 0.7874 - val_out_16_acc: 0.7852 - val_out_17_acc: 0.7852 - val_out_18_acc: 0.7896 - val_out_19_acc: 0.7831 - val_out_20_acc: 0.7874 - val_out_21_acc: 0.7852 - val_out_22_acc: 0.7831 - val_out_23_acc: 0.7874 - val_out_24_acc: 0.7831 - val_out_25_acc: 0.7852 - val_out_26_acc: 0.7852 - val_out_27_acc: 0.7874 - val_out_28_acc: 0.7874 - val_out_29_acc: 0.7809 - val_out_30_acc: 0.7896 - val_out_31_acc: 0.7874 - val_out_32_acc: 0.7896\n",
      "Epoch 153/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 23.1513 - out_loss: 0.5990 - out_0_loss: 0.3960 - out_1_loss: 0.6909 - out_2_loss: 0.6876 - out_3_loss: 0.6926 - out_4_loss: 0.6913 - out_5_loss: 0.6968 - out_6_loss: 0.6901 - out_7_loss: 0.7084 - out_8_loss: 0.6938 - out_9_loss: 0.6891 - out_10_loss: 0.6903 - out_11_loss: 0.6983 - out_12_loss: 0.6961 - out_13_loss: 0.6944 - out_14_loss: 0.6912 - out_15_loss: 0.6916 - out_16_loss: 0.6840 - out_17_loss: 0.6889 - out_18_loss: 0.6979 - out_19_loss: 0.6871 - out_20_loss: 0.6998 - out_21_loss: 0.6972 - out_22_loss: 0.6940 - out_23_loss: 0.6994 - out_24_loss: 0.6955 - out_25_loss: 0.6832 - out_26_loss: 0.6857 - out_27_loss: 0.6989 - out_28_loss: 0.6854 - out_29_loss: 0.6940 - out_30_loss: 0.6970 - out_31_loss: 0.6833 - out_32_loss: 0.6824 - out_acc: 0.7926 - out_0_acc: 0.8555 - out_1_acc: 0.7626 - out_2_acc: 0.7523 - out_3_acc: 0.7567 - out_4_acc: 0.7591 - out_5_acc: 0.7610 - out_6_acc: 0.7474 - out_7_acc: 0.7523 - out_8_acc: 0.7557 - out_9_acc: 0.7576 - out_10_acc: 0.7604 - out_11_acc: 0.7529 - out_12_acc: 0.7548 - out_13_acc: 0.7520 - out_14_acc: 0.7545 - out_15_acc: 0.7526 - out_16_acc: 0.7564 - out_17_acc: 0.7554 - out_18_acc: 0.7498 - out_19_acc: 0.7567 - out_20_acc: 0.7474 - out_21_acc: 0.7505 - out_22_acc: 0.7520 - out_23_acc: 0.7533 - out_24_acc: 0.7498 - out_25_acc: 0.7548 - out_26_acc: 0.7588 - out_27_acc: 0.7511 - out_28_acc: 0.7598 - out_29_acc: 0.7557 - out_30_acc: 0.7436 - out_31_acc: 0.7514 - out_32_acc: 0.7604 - val_loss: 22.4985 - val_out_loss: 0.6101 - val_out_0_loss: 0.4225 - val_out_1_loss: 0.6450 - val_out_2_loss: 0.6417 - val_out_3_loss: 0.6444 - val_out_4_loss: 0.6470 - val_out_5_loss: 0.6432 - val_out_6_loss: 0.6431 - val_out_7_loss: 0.6404 - val_out_8_loss: 0.6434 - val_out_9_loss: 0.6449 - val_out_10_loss: 0.6462 - val_out_11_loss: 0.6399 - val_out_12_loss: 0.6393 - val_out_13_loss: 0.6446 - val_out_14_loss: 0.6441 - val_out_15_loss: 0.6420 - val_out_16_loss: 0.6393 - val_out_17_loss: 0.6418 - val_out_18_loss: 0.6441 - val_out_19_loss: 0.6408 - val_out_20_loss: 0.6455 - val_out_21_loss: 0.6406 - val_out_22_loss: 0.6447 - val_out_23_loss: 0.6400 - val_out_24_loss: 0.6426 - val_out_25_loss: 0.6458 - val_out_26_loss: 0.6427 - val_out_27_loss: 0.6449 - val_out_28_loss: 0.6438 - val_out_29_loss: 0.6419 - val_out_30_loss: 0.6436 - val_out_31_loss: 0.6431 - val_out_32_loss: 0.6407 - val_out_acc: 0.7831 - val_out_0_acc: 0.8438 - val_out_1_acc: 0.7809 - val_out_2_acc: 0.7787 - val_out_3_acc: 0.7787 - val_out_4_acc: 0.7722 - val_out_5_acc: 0.7722 - val_out_6_acc: 0.7744 - val_out_7_acc: 0.7831 - val_out_8_acc: 0.7766 - val_out_9_acc: 0.7766 - val_out_10_acc: 0.7766 - val_out_11_acc: 0.7679 - val_out_12_acc: 0.7722 - val_out_13_acc: 0.7766 - val_out_14_acc: 0.7809 - val_out_15_acc: 0.7787 - val_out_16_acc: 0.7766 - val_out_17_acc: 0.7701 - val_out_18_acc: 0.7831 - val_out_19_acc: 0.7744 - val_out_20_acc: 0.7744 - val_out_21_acc: 0.7766 - val_out_22_acc: 0.7787 - val_out_23_acc: 0.7787 - val_out_24_acc: 0.7766 - val_out_25_acc: 0.7701 - val_out_26_acc: 0.7722 - val_out_27_acc: 0.7809 - val_out_28_acc: 0.7722 - val_out_29_acc: 0.7744 - val_out_30_acc: 0.7766 - val_out_31_acc: 0.7787 - val_out_32_acc: 0.7744\n",
      "Epoch 154/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 23.9693 - out_loss: 0.6229 - out_0_loss: 0.4236 - out_1_loss: 0.7257 - out_2_loss: 0.7218 - out_3_loss: 0.7162 - out_4_loss: 0.7119 - out_5_loss: 0.7166 - out_6_loss: 0.7119 - out_7_loss: 0.7130 - out_8_loss: 0.7229 - out_9_loss: 0.7251 - out_10_loss: 0.7106 - out_11_loss: 0.7133 - out_12_loss: 0.7216 - out_13_loss: 0.7139 - out_14_loss: 0.7154 - out_15_loss: 0.7194 - out_16_loss: 0.7196 - out_17_loss: 0.7397 - out_18_loss: 0.7218 - out_19_loss: 0.7088 - out_20_loss: 0.7208 - out_21_loss: 0.7190 - out_22_loss: 0.7130 - out_23_loss: 0.7140 - out_24_loss: 0.7081 - out_25_loss: 0.7276 - out_26_loss: 0.7166 - out_27_loss: 0.6938 - out_28_loss: 0.7015 - out_29_loss: 0.7097 - out_30_loss: 0.7132 - out_31_loss: 0.7178 - out_32_loss: 0.7183 - out_acc: 0.7796 - out_0_acc: 0.8490 - out_1_acc: 0.7436 - out_2_acc: 0.7421 - out_3_acc: 0.7489 - out_4_acc: 0.7433 - out_5_acc: 0.7312 - out_6_acc: 0.7359 - out_7_acc: 0.7440 - out_8_acc: 0.7291 - out_9_acc: 0.7464 - out_10_acc: 0.7458 - out_11_acc: 0.7461 - out_12_acc: 0.7371 - out_13_acc: 0.7461 - out_14_acc: 0.7446 - out_15_acc: 0.7405 - out_16_acc: 0.7405 - out_17_acc: 0.7343 - out_18_acc: 0.7424 - out_19_acc: 0.7387 - out_20_acc: 0.7458 - out_21_acc: 0.7433 - out_22_acc: 0.7418 - out_23_acc: 0.7489 - out_24_acc: 0.7489 - out_25_acc: 0.7455 - out_26_acc: 0.7477 - out_27_acc: 0.7502 - out_28_acc: 0.7486 - out_29_acc: 0.7430 - out_30_acc: 0.7523 - out_31_acc: 0.7402 - out_32_acc: 0.7412 - val_loss: 25.1456 - val_out_loss: 0.6904 - val_out_0_loss: 0.5160 - val_out_1_loss: 0.7184 - val_out_2_loss: 0.7142 - val_out_3_loss: 0.7162 - val_out_4_loss: 0.7206 - val_out_5_loss: 0.7148 - val_out_6_loss: 0.7185 - val_out_7_loss: 0.7135 - val_out_8_loss: 0.7208 - val_out_9_loss: 0.7137 - val_out_10_loss: 0.7200 - val_out_11_loss: 0.7149 - val_out_12_loss: 0.7122 - val_out_13_loss: 0.7208 - val_out_14_loss: 0.7197 - val_out_15_loss: 0.7145 - val_out_16_loss: 0.7164 - val_out_17_loss: 0.7155 - val_out_18_loss: 0.7168 - val_out_19_loss: 0.7207 - val_out_20_loss: 0.7161 - val_out_21_loss: 0.7136 - val_out_22_loss: 0.7143 - val_out_23_loss: 0.7131 - val_out_24_loss: 0.7177 - val_out_25_loss: 0.7212 - val_out_26_loss: 0.7194 - val_out_27_loss: 0.7185 - val_out_28_loss: 0.7202 - val_out_29_loss: 0.7170 - val_out_30_loss: 0.7213 - val_out_31_loss: 0.7187 - val_out_32_loss: 0.7105 - val_out_acc: 0.7419 - val_out_0_acc: 0.8243 - val_out_1_acc: 0.7289 - val_out_2_acc: 0.7375 - val_out_3_acc: 0.7419 - val_out_4_acc: 0.7397 - val_out_5_acc: 0.7375 - val_out_6_acc: 0.7354 - val_out_7_acc: 0.7332 - val_out_8_acc: 0.7332 - val_out_9_acc: 0.7354 - val_out_10_acc: 0.7354 - val_out_11_acc: 0.7332 - val_out_12_acc: 0.7397 - val_out_13_acc: 0.7397 - val_out_14_acc: 0.7440 - val_out_15_acc: 0.7419 - val_out_16_acc: 0.7375 - val_out_17_acc: 0.7419 - val_out_18_acc: 0.7397 - val_out_19_acc: 0.7375 - val_out_20_acc: 0.7419 - val_out_21_acc: 0.7397 - val_out_22_acc: 0.7375 - val_out_23_acc: 0.7419 - val_out_24_acc: 0.7397 - val_out_25_acc: 0.7354 - val_out_26_acc: 0.7375 - val_out_27_acc: 0.7332 - val_out_28_acc: 0.7354 - val_out_29_acc: 0.7397 - val_out_30_acc: 0.7332 - val_out_31_acc: 0.7397 - val_out_32_acc: 0.7354\n",
      "Epoch 155/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 22.9828 - out_loss: 0.5947 - out_0_loss: 0.3925 - out_1_loss: 0.6899 - out_2_loss: 0.6848 - out_3_loss: 0.6972 - out_4_loss: 0.6862 - out_5_loss: 0.6797 - out_6_loss: 0.6953 - out_7_loss: 0.6726 - out_8_loss: 0.6811 - out_9_loss: 0.6842 - out_10_loss: 0.6871 - out_11_loss: 0.6842 - out_12_loss: 0.6712 - out_13_loss: 0.6845 - out_14_loss: 0.6895 - out_15_loss: 0.6978 - out_16_loss: 0.6819 - out_17_loss: 0.6829 - out_18_loss: 0.6876 - out_19_loss: 0.6822 - out_20_loss: 0.6927 - out_21_loss: 0.6954 - out_22_loss: 0.7049 - out_23_loss: 0.6944 - out_24_loss: 0.6816 - out_25_loss: 0.6950 - out_26_loss: 0.6830 - out_27_loss: 0.6884 - out_28_loss: 0.6747 - out_29_loss: 0.6923 - out_30_loss: 0.6929 - out_31_loss: 0.6768 - out_32_loss: 0.7041 - out_acc: 0.8004 - out_0_acc: 0.8617 - out_1_acc: 0.7533 - out_2_acc: 0.7585 - out_3_acc: 0.7591 - out_4_acc: 0.7650 - out_5_acc: 0.7632 - out_6_acc: 0.7579 - out_7_acc: 0.7641 - out_8_acc: 0.7598 - out_9_acc: 0.7632 - out_10_acc: 0.7579 - out_11_acc: 0.7576 - out_12_acc: 0.7650 - out_13_acc: 0.7585 - out_14_acc: 0.7598 - out_15_acc: 0.7619 - out_16_acc: 0.7595 - out_17_acc: 0.7570 - out_18_acc: 0.7607 - out_19_acc: 0.7567 - out_20_acc: 0.7551 - out_21_acc: 0.7545 - out_22_acc: 0.7523 - out_23_acc: 0.7598 - out_24_acc: 0.7591 - out_25_acc: 0.7573 - out_26_acc: 0.7601 - out_27_acc: 0.7548 - out_28_acc: 0.7650 - out_29_acc: 0.7629 - out_30_acc: 0.7564 - out_31_acc: 0.7610 - out_32_acc: 0.7570 - val_loss: 23.5262 - val_out_loss: 0.6425 - val_out_0_loss: 0.4333 - val_out_1_loss: 0.6722 - val_out_2_loss: 0.6725 - val_out_3_loss: 0.6753 - val_out_4_loss: 0.6756 - val_out_5_loss: 0.6704 - val_out_6_loss: 0.6749 - val_out_7_loss: 0.6706 - val_out_8_loss: 0.6714 - val_out_9_loss: 0.6710 - val_out_10_loss: 0.6718 - val_out_11_loss: 0.6695 - val_out_12_loss: 0.6666 - val_out_13_loss: 0.6760 - val_out_14_loss: 0.6726 - val_out_15_loss: 0.6777 - val_out_16_loss: 0.6739 - val_out_17_loss: 0.6740 - val_out_18_loss: 0.6694 - val_out_19_loss: 0.6703 - val_out_20_loss: 0.6708 - val_out_21_loss: 0.6717 - val_out_22_loss: 0.6783 - val_out_23_loss: 0.6708 - val_out_24_loss: 0.6753 - val_out_25_loss: 0.6776 - val_out_26_loss: 0.6725 - val_out_27_loss: 0.6725 - val_out_28_loss: 0.6666 - val_out_29_loss: 0.6754 - val_out_30_loss: 0.6716 - val_out_31_loss: 0.6696 - val_out_32_loss: 0.6710 - val_out_acc: 0.7679 - val_out_0_acc: 0.8265 - val_out_1_acc: 0.7722 - val_out_2_acc: 0.7701 - val_out_3_acc: 0.7701 - val_out_4_acc: 0.7657 - val_out_5_acc: 0.7657 - val_out_6_acc: 0.7766 - val_out_7_acc: 0.7679 - val_out_8_acc: 0.7722 - val_out_9_acc: 0.7701 - val_out_10_acc: 0.7592 - val_out_11_acc: 0.7657 - val_out_12_acc: 0.7722 - val_out_13_acc: 0.7679 - val_out_14_acc: 0.7766 - val_out_15_acc: 0.7679 - val_out_16_acc: 0.7636 - val_out_17_acc: 0.7657 - val_out_18_acc: 0.7657 - val_out_19_acc: 0.7679 - val_out_20_acc: 0.7722 - val_out_21_acc: 0.7722 - val_out_22_acc: 0.7701 - val_out_23_acc: 0.7657 - val_out_24_acc: 0.7701 - val_out_25_acc: 0.7679 - val_out_26_acc: 0.7679 - val_out_27_acc: 0.7701 - val_out_28_acc: 0.7744 - val_out_29_acc: 0.7679 - val_out_30_acc: 0.7657 - val_out_31_acc: 0.7657 - val_out_32_acc: 0.7636\n",
      "Epoch 156/1000\n",
      "Epoch 1/1000\n",
      "101/100 - 103s - loss: 23.7146 - out_loss: 0.6157 - out_0_loss: 0.4054 - out_1_loss: 0.7027 - out_2_loss: 0.7147 - out_3_loss: 0.7089 - out_4_loss: 0.7185 - out_5_loss: 0.7173 - out_6_loss: 0.7150 - out_7_loss: 0.7185 - out_8_loss: 0.7068 - out_9_loss: 0.7232 - out_10_loss: 0.6988 - out_11_loss: 0.7012 - out_12_loss: 0.7017 - out_13_loss: 0.7004 - out_14_loss: 0.7058 - out_15_loss: 0.7094 - out_16_loss: 0.7049 - out_17_loss: 0.7038 - out_18_loss: 0.7240 - out_19_loss: 0.7025 - out_20_loss: 0.7118 - out_21_loss: 0.7054 - out_22_loss: 0.7239 - out_23_loss: 0.7030 - out_24_loss: 0.7125 - out_25_loss: 0.7068 - out_26_loss: 0.7106 - out_27_loss: 0.7085 - out_28_loss: 0.7136 - out_29_loss: 0.7111 - out_30_loss: 0.7149 - out_31_loss: 0.6937 - out_32_loss: 0.6997 - out_acc: 0.7880 - out_0_acc: 0.8521 - out_1_acc: 0.7585 - out_2_acc: 0.7483 - out_3_acc: 0.7508 - out_4_acc: 0.7505 - out_5_acc: 0.7505 - out_6_acc: 0.7505 - out_7_acc: 0.7467 - out_8_acc: 0.7446 - out_9_acc: 0.7474 - out_10_acc: 0.7585 - out_11_acc: 0.7564 - out_12_acc: 0.7520 - out_13_acc: 0.7579 - out_14_acc: 0.7492 - out_15_acc: 0.7526 - out_16_acc: 0.7523 - out_17_acc: 0.7471 - out_18_acc: 0.7458 - out_19_acc: 0.7576 - out_20_acc: 0.7498 - out_21_acc: 0.7570 - out_22_acc: 0.7477 - out_23_acc: 0.7483 - out_24_acc: 0.7492 - out_25_acc: 0.7505 - out_26_acc: 0.7492 - out_27_acc: 0.7415 - out_28_acc: 0.7492 - out_29_acc: 0.7409 - out_30_acc: 0.7467 - out_31_acc: 0.7557 - out_32_acc: 0.7492 - val_loss: 21.8624 - val_out_loss: 0.5907 - val_out_0_loss: 0.3522 - val_out_1_loss: 0.6290 - val_out_2_loss: 0.6287 - val_out_3_loss: 0.6256 - val_out_4_loss: 0.6303 - val_out_5_loss: 0.6248 - val_out_6_loss: 0.6283 - val_out_7_loss: 0.6242 - val_out_8_loss: 0.6254 - val_out_9_loss: 0.6262 - val_out_10_loss: 0.6288 - val_out_11_loss: 0.6262 - val_out_12_loss: 0.6238 - val_out_13_loss: 0.6266 - val_out_14_loss: 0.6324 - val_out_15_loss: 0.6241 - val_out_16_loss: 0.6235 - val_out_17_loss: 0.6238 - val_out_18_loss: 0.6256 - val_out_19_loss: 0.6313 - val_out_20_loss: 0.6270 - val_out_21_loss: 0.6276 - val_out_22_loss: 0.6252 - val_out_23_loss: 0.6275 - val_out_24_loss: 0.6285 - val_out_25_loss: 0.6319 - val_out_26_loss: 0.6258 - val_out_27_loss: 0.6248 - val_out_28_loss: 0.6251 - val_out_29_loss: 0.6285 - val_out_30_loss: 0.6276 - val_out_31_loss: 0.6205 - val_out_32_loss: 0.6256 - val_out_acc: 0.7852 - val_out_0_acc: 0.8612 - val_out_1_acc: 0.7787 - val_out_2_acc: 0.7722 - val_out_3_acc: 0.7809 - val_out_4_acc: 0.7831 - val_out_5_acc: 0.7766 - val_out_6_acc: 0.7722 - val_out_7_acc: 0.7809 - val_out_8_acc: 0.7787 - val_out_9_acc: 0.7787 - val_out_10_acc: 0.7722 - val_out_11_acc: 0.7809 - val_out_12_acc: 0.7852 - val_out_13_acc: 0.7787 - val_out_14_acc: 0.7787 - val_out_15_acc: 0.7766 - val_out_16_acc: 0.7809 - val_out_17_acc: 0.7787 - val_out_18_acc: 0.7744 - val_out_19_acc: 0.7787 - val_out_20_acc: 0.7787 - val_out_21_acc: 0.7831 - val_out_22_acc: 0.7831 - val_out_23_acc: 0.7787 - val_out_24_acc: 0.7766 - val_out_25_acc: 0.7766 - val_out_26_acc: 0.7744 - val_out_27_acc: 0.7831 - val_out_28_acc: 0.7787 - val_out_29_acc: 0.7852 - val_out_30_acc: 0.7831 - val_out_31_acc: 0.7809 - val_out_32_acc: 0.7831\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'rcropnetv2'\n",
    "fit_model(data['train'], data['valid'], data_generator,\n",
    "                        make_rcropnetv2(32, 128), model_name=model_name,\n",
    "                        model_dir=model_dir, n_outputs=32+2, batch_size=32)\n",
    "save_results(model_dir, model_name, data['label_encoder'],\n",
    "             data_generator, test_data=data['test'], n_outputs=18)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NubesKeras.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read predictions CNN\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer, scale\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from auxiliary_functions import *\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plots_dir = \"plots\"\n",
    "PATH = \"resultsFirstRound/\"\n",
    "\n",
    "##############################################################\n",
    "# Obtaining best CNN execution (in validation set)\n",
    "##############################################################\n",
    "\n",
    "summary_files = [each for each in os.listdir(PATH) if each.endswith('_summary_acc.csv')]\n",
    "\n",
    "df = pd.concat((pd.read_csv(os.path.join(PATH, f)).assign(file = f).assign(model = re.search('[0-9]_+(.+?)_summary_acc.csv', f).group(1).split(\"_\")[0]).\n",
    "                assign(set = re.search('[0-9]_+(.+?)_summary_acc.csv', f).group(1).split(\"_\")[1]) for f in summary_files))\n",
    "df_val = df[df[\"set\"] == \"valid\"]\n",
    "\n",
    "# Determining file of the best CNN execution\n",
    "best_exec_acc_val = df_val[df_val[\"accuracy\"] == max(df_val[\"accuracy\"])]\n",
    "file_best_exec = best_exec_acc_val[\"file\"][0]\n",
    "\n",
    "file_best_exec_id = re.search('(.+?)_[a-z]+_summary_acc.csv', file_best_exec).group(1)\n",
    "\n",
    "##############################################################\n",
    "# Reading indices of the best CNN execution\n",
    "##############################################################\n",
    "# Reading indices of the best CNN execution\n",
    "indices_file = file_best_exec.replace(\"valid_summary_acc.csv\", \"indices.pickle\")\n",
    "with open(os.path.join(PATH,indices_file), 'rb') as f:\n",
    "    indices = pickle.load(f)\n",
    "in_train, in_valid, in_test = indices\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Reading predictions of the best CNN execution\n",
    "##############################################################\n",
    "# Reading images\n",
    "img_file='images.npz'\n",
    "file_dir = \"data/\"\n",
    "images = np.load('%s/%s' % (file_dir, img_file), 'r', True)['arr_0']\n",
    "img_train, img_test, img_valid = images[in_train], images[in_test], images[in_valid]\n",
    "\n",
    "\n",
    "\n",
    "# Reading predictions of the best CNN execution\n",
    "\n",
    "preds_file = file_best_exec.replace(\"valid_summary_acc.csv\", \"preds.csv\")\n",
    "preds_cnn_train = pd.read_csv(os.path.join(PATH, \"%s_train_preds.csv\" % file_best_exec_id), index_col=0)\n",
    "preds_cnn_test = pd.read_csv(os.path.join(PATH, \"%s_test_preds.csv\" % file_best_exec_id), index_col=0)\n",
    "preds_cnn_valid = pd.read_csv(os.path.join(PATH, \"%s_valid_preds.csv\" % file_best_exec_id), index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "del preds_cnn_train[\"obs\"]\n",
    "del preds_cnn_test[\"obs\"]\n",
    "del preds_cnn_valid[\"obs\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[(df[\"model\"]==\"vgg19\") & (df[\"set\"] == \"test\")]\n",
    "#df[(df[\"set\"] == \"test\")]\n",
    "best_run_cnn = pd.read_csv(os.path.join(PATH, file_best_exec))\n",
    "\n",
    "best_run_cnn.to_csv(\"best_cnn_results.csv\")\n",
    "best_run_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Obtaining ceil features and estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining ceil features\n",
    "feat_file='cloud_features.csv'\n",
    "\n",
    "features = pd.read_csv('%s/%s' % (\"data\" , feat_file), sep=';', decimal=',')\n",
    "\n",
    "# Obtainin y vector\n",
    "cloud_type = np.array(features['cloud.type'])\n",
    "encoder = LabelBinarizer()\n",
    "cloud_encoded = encoder.fit_transform(cloud_type)\n",
    "y_train, y_test, y_valid = cloud_encoded[in_train], cloud_encoded[in_test], cloud_encoded[in_valid]\n",
    "y_test_dec = encoder.inverse_transform(y_test)\n",
    "\n",
    "ceil_info = np.array(features[[\"ceil.height0\", \"ceil.height1\", \"ceil.height2\", \"ceil.depth0\",\n",
    "                               \"ceil.depth1\", \"ceil.depth2\",\"ceil.layers\"]])\n",
    "\n",
    "ceil_info = scale(ceil_info, copy=False)\n",
    "\n",
    "# Applying indices retrieved from the best execution to obtain ceil\n",
    "ceil_train, ceil_test, ceil_valid = ceil_info[in_train], ceil_info[in_test], ceil_info[in_valid]\n",
    "\n",
    "# Filtering columns and Applying indices retrieved from the best execution to obtain estimators\n",
    "features_estimators = features.drop([\"date\", \"file\", \"camnum\", \"cloud.type\",\"ceil.height0\", \"ceil.height1\",\n",
    "                                     \"ceil.height2\", \"ceil.depth0\",\"ceil.depth1\", \"ceil.depth2\",\"ceil.layers\"], axis=1)\n",
    "\n",
    "cols = features_estimators.columns\n",
    "features_estimators = pd.DataFrame(scale(features_estimators, copy=False), columns=cols)\n",
    "\n",
    "estimators_train, estimators_test, estimators_valid = features_estimators.iloc[in_train, :], features_estimators.iloc[in_test, :], features_estimators.iloc[in_valid, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of features\n",
    "\n",
    "- **preds_cnn_train, preds_cnn_test, preds_cnn_valid**: predictions of the best CNN model\n",
    "- **estimators_train, estimators_test, estimators_valid**: estimators to train RF classifier\n",
    "- **ceil_train, ceil_test, ceil_valid**: features to combine with outputs of RF and CNN classifiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Experiment 1: RF on estimators classification\"))\n",
    "\n",
    "preds_cnn_train_hot = encoder.transform(preds_cnn_train)\n",
    "preds_cnn_valid_hot = encoder.transform(preds_cnn_valid)\n",
    "preds_cnn_test_hot = encoder.transform(preds_cnn_test)\n",
    "\n",
    "experiment_name = \"EXP_1_RF_estimators\"\n",
    "print(\"EXPERIMENT %s\" % experiment_name)\n",
    "##############################################################\n",
    "# Classic classifiers comparison\n",
    "##############################################################\n",
    "train_classifiers_on_set(X=estimators_train, Y=encoder.inverse_transform(y_train), output_file_id=file_best_exec_id, experiment_name=experiment_name, output_dir=plots_dir)\n",
    "\n",
    "##############################################################\n",
    "# Training RF with different no. estimators\n",
    "##############################################################\n",
    "parameters_rf = {'n_estimators':list(range(100,1100,100))}\n",
    "\n",
    "# Train and evaluate Random Forest\n",
    "rf_estimators = RandomForestClassifier()\n",
    "grid_rf = GridSearchCV(rf_estimators, parameters_rf, verbose=10, n_jobs=10)\n",
    "grid_rf.fit(estimators_train, y_train)\n",
    "\n",
    "pd.DataFrame(grid_rf.cv_results_).to_csv(os.path.join(plots_dir, (\"{}_rf_train_results_{}.csv\".format(file_best_exec_id, experiment_name))))\n",
    "pd.DataFrame(grid_rf.best_params_, index=[0]).to_csv(os.path.join(plots_dir, (\"{}_rf_train_best_params_{}.csv\".format(file_best_exec_id, experiment_name))))\n",
    "\n",
    "##############################################################\n",
    "# Plotting RF comparison no. estimators\n",
    "##############################################################\n",
    "fig = plt.figure(figsize=(8.27, 6), dpi=300)\n",
    "ax = sns.lineplot(x=\"param_n_estimators\", y=\"mean_test_score\", marker=\"o\", data=pd.DataFrame(grid_rf.cv_results_))\n",
    "ax.set(xlabel='No. estimators', ylabel='Mean test score')\n",
    "fig.savefig(os.path.join(plots_dir, (\"{}_rf_train_results_no_estimators_{}.pdf\".format(file_best_exec_id, experiment_name))), bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "\n",
    "preds_estimators_test_RF_hot = grid_rf.predict(estimators_test)\n",
    "preds_estimators_test_RF = encoder.inverse_transform(preds_estimators_test_RF_hot)\n",
    "\n",
    "print(classification_report(y_pred= preds_estimators_test_RF, y_true= y_test_dec, digits= 3))\n",
    "generate_confusion_matrix_and_report(y_pred=preds_estimators_test_RF, y_test_dec=y_test_dec, output_file_id=file_best_exec_id, experiment_name=experiment_name, output_dir=plots_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Experiment 2: RF over estimators + CNN predictions\"))\n",
    "\n",
    "##############################################################\n",
    "# Experiment features combined 2: RF over estimators + CNN predictions\n",
    "##############################################################\n",
    "experiment_name = \"EXP_2_RF_estimators_CNN\"\n",
    "print(\"EXPERIMENT %s\" % experiment_name)\n",
    "\n",
    "# Test set of classifiers\n",
    "#preds_estimators_test_RF_hot = grid_rf.predict(estimators_test)\n",
    "preds_estimators_test_RF_CNN = encoder.inverse_transform((preds_estimators_test_RF_hot + preds_cnn_test_hot)/2)\n",
    "\n",
    "print(classification_report(y_pred= preds_estimators_test_RF_CNN, y_true= y_test_dec, digits= 3))\n",
    "generate_confusion_matrix_and_report(y_pred=preds_estimators_test_RF_CNN, y_test_dec=y_test_dec, output_file_id=file_best_exec_id, experiment_name=experiment_name, output_dir=plots_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Experiment 3: RF over CEIL\"))\n",
    "experiment_name = \"EXP_3_RF_ceil\"\n",
    "print(\"EXPERIMENT %s\" % experiment_name)\n",
    "\n",
    "##############################################################\n",
    "# Classic classifiers comparison\n",
    "##############################################################\n",
    "train_classifiers_on_set(X=ceil_train, Y=encoder.inverse_transform(y_train), output_file_id=file_best_exec_id, \n",
    "                         experiment_name=experiment_name, output_dir=plots_dir)\n",
    "\n",
    "##############################################################\n",
    "# Training RF with different no. estimators\n",
    "##############################################################\n",
    "\n",
    "# Train and evaluate Random Forest\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), parameters_rf, verbose=10, n_jobs=10)\n",
    "grid_rf.fit(ceil_train, y_train)\n",
    "\n",
    "pd.DataFrame(grid_rf.cv_results_).to_csv(os.path.join(plots_dir, (\"{}_rf_train_results_{}.csv\".format(file_best_exec_id, experiment_name))))\n",
    "pd.DataFrame(grid_rf.best_params_, index=[0]).to_csv(os.path.join(plots_dir, (\"{}_rf_train_best_params_{}.csv\".format(file_best_exec_id, experiment_name))))\n",
    "\n",
    "\n",
    "preds_ceil_test_RF_hot = grid_rf.predict(ceil_test)\n",
    "preds_ceil_test_RF = encoder.inverse_transform(preds_ceil_test_RF_hot)\n",
    "\n",
    "print(classification_report(y_pred= preds_ceil_test_RF, y_true= y_test_dec, digits= 3))\n",
    "generate_confusion_matrix_and_report(y_pred=preds_ceil_test_RF, y_test_dec=y_test_dec, output_file_id=file_best_exec_id,\n",
    "                                     experiment_name=experiment_name, output_dir=plots_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Experiment 4: RF over CEIL + CNN\"))\n",
    "experiment_name = \"EXP_4_RF_ceil_CNN\"\n",
    "print(\"EXPERIMENT %s\" % experiment_name)\n",
    "\n",
    "##############################################################\n",
    "# Experiment features combined 4: RF over ceil + CNN predictions\n",
    "##############################################################\n",
    "\n",
    "#preds_estimators_test_RF_hot = grid_rf.predict(estimators_test)\n",
    "preds_ceil_test_RF_CNN = encoder.inverse_transform((preds_ceil_test_RF_hot + preds_cnn_test_hot)/2)\n",
    "\n",
    "print(classification_report(y_pred= preds_ceil_test_RF_CNN, y_true= y_test_dec, digits= 3))\n",
    "generate_confusion_matrix_and_report(y_pred=preds_ceil_test_RF_CNN, y_test_dec=y_test_dec, output_file_id=file_best_exec_id,\n",
    "                                     experiment_name=experiment_name, output_dir=plots_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Experiment 5: RF over CEIL + CNN + RF over estimators\"))\n",
    "experiment_name = \"EXP_5_RF_ceil_CNN_RF_estimators\"\n",
    "print(\"EXPERIMENT %s\" % experiment_name)\n",
    "\n",
    "##############################################################\n",
    "# Experiment features combined 4: RF over ceil + CNN predictions\n",
    "##############################################################\n",
    "\n",
    "# Test set of classifiers\n",
    "#preds_estimators_test_RF_hot = grid_rf.predict(estimators_test)\n",
    "preds_ceil_test_RF_CNN = encoder.inverse_transform((preds_ceil_test_RF_hot + preds_estimators_test_RF_hot + preds_cnn_test_hot)/2)\n",
    "\n",
    "print(classification_report(y_pred= preds_ceil_test_RF_CNN, y_true= y_test_dec, digits= 3))\n",
    "generate_confusion_matrix_and_report(y_pred=preds_ceil_test_RF_CNN, y_test_dec=y_test_dec, output_file_id=file_best_exec_id,\n",
    "                                     experiment_name=experiment_name, output_dir=plots_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/37161563/how-to-graph-grid-scores-from-gridsearchcv\n",
    "\"\"\"\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "#clf.fit(estimators_train, y_train)\n",
    "\n",
    "#preds_estimators_test_RF_hot = clf.predict(estimators_test)\n",
    "#preds_estimators_test_RF = encoder.inverse_transform(preds_estimators_test_RF_hot)\n",
    "\n",
    "parameters = {'n_estimators':list(range(100,1100,100))}\n",
    "clf = GridSearchCV(rf, parameters, verbose=10, n_jobs=1)\n",
    "clf.fit(estimators_train, y_train)\n",
    "\n",
    "pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "#scores = [x[1] for x in clf.grid_scores_]\n",
    "\n",
    "#print(scores)\n",
    "\n",
    "\n",
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')\n",
    "\n",
    "\"\"\"\n",
    "# Calling Method \n",
    "#plot_grid_search(clf.cv_results_, 2, 'N Estimators', 'Max Features')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
